# AI ê°œë°œ ì‹¤ë¬´ ê°€ì´ë“œ

## ğŸ— í”„ë¡œì íŠ¸ êµ¬ì¡° ë° ì•„í‚¤í…ì²˜

### ê¶Œì¥ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ai_project/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py
â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”œâ”€â”€ vectorstore.py
â”‚   â”‚   â””â”€â”€ chains.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_service.py
â”‚   â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   â””â”€â”€ chat_service.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ requests.py
â”‚   â”‚   â””â”€â”€ responses.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ helpers.py
â”‚   â”‚   â””â”€â”€ validators.py
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py
â”‚       â””â”€â”€ routers/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ chat.py
â”‚           â””â”€â”€ documents.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ vectorstores/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ experiments/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.py
â”‚   â”œâ”€â”€ ingest_data.py
â”‚   â””â”€â”€ deploy.py
â””â”€â”€ docs/
    â”œâ”€â”€ api.md
    â”œâ”€â”€ deployment.md
    â””â”€â”€ usage.md
```

### í•µì‹¬ ì„¤ì • íŒŒì¼ë“¤

#### `src/config/settings.py`
```python
from pydantic import BaseSettings
from typing import Optional
import os

class Settings(BaseSettings):
    # API Keys
    openai_api_key: str
    langsmith_api_key: Optional[str] = None

    # LLM Settings
    model_name: str = "gpt-4.1-mini"
    temperature: float = 0.3
    max_tokens: int = 2000

    # Embedding Settings
    embedding_model: str = "text-embedding-3-small"
    embedding_dimension: int = 1536

    # Vector Store Settings
    vectorstore_type: str = "chroma"  # chroma, faiss, pinecone
    vectorstore_path: str = "./data/vectorstores"
    collection_name: str = "documents"

    # API Settings
    api_host: str = "0.0.0.0"
    api_port: int = 8000
    api_debug: bool = False

    # Database
    database_url: Optional[str] = None

    # Redis (for caching)
    redis_url: Optional[str] = None

    # Monitoring
    enable_tracing: bool = True
    log_level: str = "INFO"

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
settings = Settings()
```

#### `src/config/logging.py`
```python
import logging
import sys
from pathlib import Path

def setup_logging(log_level: str = "INFO", log_file: Optional[str] = None):
    """ë¡œê¹… ì„¤ì •"""

    # ë¡œê·¸ í¬ë§· ì„¤ì •
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)

    handlers = [console_handler]

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ì˜µì…˜)
    if log_file:
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        handlers.append(file_handler)

    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        handlers=handlers,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("openai").setLevel(logging.WARNING)
    logging.getLogger("chromadb").setLevel(logging.WARNING)
```

### í•µì‹¬ ì„œë¹„ìŠ¤ êµ¬í˜„

#### `src/core/llm.py`
```python
from functools import lru_cache
from langchain_openai import ChatOpenAI
from langchain_core.language_models import BaseChatModel
from src.config.settings import settings
import logging

logger = logging.getLogger(__name__)

@lru_cache()
def get_llm() -> BaseChatModel:
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìºì‹±ë¨)"""

    try:
        llm = ChatOpenAI(
            api_key=settings.openai_api_key,
            model=settings.model_name,
            temperature=settings.temperature,
            max_tokens=settings.max_tokens,
            request_timeout=30,
            max_retries=3
        )

        logger.info(f"LLM initialized: {settings.model_name}")
        return llm

    except Exception as e:
        logger.error(f"Failed to initialize LLM: {e}")
        raise

class LLMManager:
    """LLM ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.llm = get_llm()
        self._request_count = 0
        self._error_count = 0

    def invoke(self, prompt: str, **kwargs) -> str:
        """ì•ˆì „í•œ LLM í˜¸ì¶œ"""
        try:
            self._request_count += 1
            response = self.llm.invoke(prompt, **kwargs)
            return response.content

        except Exception as e:
            self._error_count += 1
            logger.error(f"LLM invocation failed: {e}")
            raise

    def get_stats(self) -> dict:
        """LLM ì‚¬ìš© í†µê³„"""
        return {
            "total_requests": self._request_count,
            "total_errors": self._error_count,
            "error_rate": self._error_count / max(self._request_count, 1)
        }
```

#### `src/core/vectorstore.py`
```python
from functools import lru_cache
from typing import List, Optional, Dict, Any
from langchain_chroma import Chroma
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.vectorstores import VectorStore
from src.core.embeddings import get_embeddings
from src.config.settings import settings
import logging

logger = logging.getLogger(__name__)

class VectorStoreFactory:
    """ë²¡í„° ìŠ¤í† ì–´ íŒ©í† ë¦¬"""

    @staticmethod
    def create_vectorstore(vectorstore_type: str = None) -> VectorStore:
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""

        store_type = vectorstore_type or settings.vectorstore_type
        embeddings = get_embeddings()

        if store_type == "chroma":
            return Chroma(
                collection_name=settings.collection_name,
                embedding_function=embeddings,
                persist_directory=settings.vectorstore_path
            )

        elif store_type == "faiss":
            # FAISSëŠ” ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ë¡œë“œ
            faiss_path = f"{settings.vectorstore_path}/faiss_index"

            try:
                return FAISS.load_local(faiss_path, embeddings)
            except:
                # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± (ë¹ˆ ë¬¸ì„œë¡œ)
                empty_docs = [Document(page_content="ì´ˆê¸°í™”", metadata={})]
                vectorstore = FAISS.from_documents(empty_docs, embeddings)
                vectorstore.save_local(faiss_path)
                return vectorstore

        else:
            raise ValueError(f"Unsupported vectorstore type: {store_type}")

class VectorStoreManager:
    """ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.vectorstore = VectorStoreFactory.create_vectorstore()
        self._document_count = 0

    def add_documents(self, documents: List[Document]) -> List[str]:
        """ë¬¸ì„œ ì¶”ê°€"""
        try:
            doc_ids = self.vectorstore.add_documents(documents)
            self._document_count += len(documents)

            logger.info(f"Added {len(documents)} documents to vectorstore")
            return doc_ids

        except Exception as e:
            logger.error(f"Failed to add documents: {e}")
            raise

    def similarity_search(
        self,
        query: str,
        k: int = 5,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            if filter:
                results = self.vectorstore.similarity_search(
                    query, k=k, filter=filter
                )
            else:
                results = self.vectorstore.similarity_search(query, k=k)

            logger.debug(f"Retrieved {len(results)} documents for query: {query[:50]}...")
            return results

        except Exception as e:
            logger.error(f"Similarity search failed: {e}")
            raise

    def get_retriever(self, **kwargs):
        """Retriever ë°˜í™˜"""
        search_kwargs = {
            "k": kwargs.get("k", 5),
            "search_type": kwargs.get("search_type", "similarity"),
        }

        if "filter" in kwargs:
            search_kwargs["filter"] = kwargs["filter"]

        return self.vectorstore.as_retriever(search_kwargs=search_kwargs)

    def get_stats(self) -> Dict[str, Any]:
        """ë²¡í„° ìŠ¤í† ì–´ í†µê³„"""
        return {
            "document_count": self._document_count,
            "vectorstore_type": type(self.vectorstore).__name__
        }

@lru_cache()
def get_vectorstore_manager() -> VectorStoreManager:
    """ë²¡í„° ìŠ¤í† ì–´ ë§¤ë‹ˆì € ì‹±ê¸€í†¤"""
    return VectorStoreManager()
```

#### `src/services/rag_service.py`
```python
from typing import Dict, List, Optional
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from src.core.llm import get_llm
from src.core.vectorstore import get_vectorstore_manager
from src.models.requests import RAGRequest
from src.models.responses import RAGResponse
import logging
import time

logger = logging.getLogger(__name__)

class RAGService:
    """RAG ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        self.llm = get_llm()
        self.vectorstore_manager = get_vectorstore_manager()
        self._setup_chains()

    def _setup_chains(self):
        """RAG ì²´ì¸ ì„¤ì •"""

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """
        ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ê³  ì •í™•í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

        ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µë³€í•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë©´ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.

        ì»¨í…ìŠ¤íŠ¸: {context}
        """

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}")
        ])

        # ë¬¸ì„œ ê²°í•© ì²´ì¸
        self.document_chain = create_stuff_documents_chain(
            self.llm, self.prompt
        )

        # ê²€ìƒ‰ê¸°
        self.retriever = self.vectorstore_manager.get_retriever(k=5)

        # RAG ì²´ì¸
        self.rag_chain = create_retrieval_chain(
            self.retriever, self.document_chain
        )

    def query(self, request: RAGRequest) -> RAGResponse:
        """RAG ì¿¼ë¦¬ ì²˜ë¦¬"""

        start_time = time.time()

        try:
            # ë©”íƒ€ë°ì´í„° í•„í„° ì ìš© (ìˆëŠ” ê²½ìš°)
            if request.filters:
                retriever = self.vectorstore_manager.get_retriever(
                    k=request.k or 5,
                    filter=request.filters
                )

                # í•„í„°ê°€ ì ìš©ëœ ìƒˆë¡œìš´ ì²´ì¸ ìƒì„±
                filtered_chain = create_retrieval_chain(
                    retriever, self.document_chain
                )

                result = filtered_chain.invoke({"input": request.question})
            else:
                result = self.rag_chain.invoke({"input": request.question})

            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = time.time() - start_time

            # ì‘ë‹µ êµ¬ì„±
            response = RAGResponse(
                answer=result["answer"],
                sources=[
                    {
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": getattr(doc, "score", 0.0)
                    }
                    for doc in result.get("context", [])
                ],
                execution_time=execution_time,
                retrieved_docs=len(result.get("context", []))
            )

            logger.info(f"RAG query processed in {execution_time:.2f}s")
            return response

        except Exception as e:
            logger.error(f"RAG query failed: {e}")
            raise

    def add_documents(self, documents: List[Document]) -> Dict[str, Any]:
        """ë¬¸ì„œ ì¶”ê°€"""
        try:
            doc_ids = self.vectorstore_manager.add_documents(documents)

            # ì²´ì¸ ì¬ì„¤ì • (ìƒˆë¡œìš´ ë¬¸ì„œ ë°˜ì˜)
            self._setup_chains()

            return {
                "status": "success",
                "added_documents": len(documents),
                "document_ids": doc_ids
            }

        except Exception as e:
            logger.error(f"Failed to add documents: {e}")
            raise

    def get_service_stats(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ í†µê³„"""
        llm_stats = self.llm.get_stats() if hasattr(self.llm, 'get_stats') else {}
        vectorstore_stats = self.vectorstore_manager.get_stats()

        return {
            "llm": llm_stats,
            "vectorstore": vectorstore_stats,
            "status": "healthy"
        }
```

#### `src/services/chat_service.py`
```python
from typing import Dict, List, Optional
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import BaseModel, Field
import redis
import json
from datetime import datetime, timedelta
from src.core.llm import get_llm
from src.services.rag_service import RAGService
from src.models.requests import ChatRequest
from src.models.responses import ChatResponse
import logging

logger = logging.getLogger(__name__)

class RedisChatHistory(BaseChatMessageHistory, BaseModel):
    """Redis ê¸°ë°˜ ì±„íŒ… íˆìŠ¤í† ë¦¬"""

    session_id: str
    ttl: int = 3600  # 1ì‹œê°„

    def __init__(self, session_id: str, redis_client=None, **kwargs):
        super().__init__(session_id=session_id, **kwargs)
        self.redis_client = redis_client or redis.Redis(
            host='localhost', port=6379, db=0, decode_responses=True
        )
        self._key = f"chat_history:{session_id}"

    @property
    def messages(self) -> List[BaseMessage]:
        """ë©”ì‹œì§€ ë¡œë“œ"""
        try:
            messages_data = self.redis_client.lrange(self._key, 0, -1)
            messages = []

            for msg_json in messages_data:
                msg_data = json.loads(msg_json)

                if msg_data["type"] == "human":
                    messages.append(HumanMessage(content=msg_data["content"]))
                elif msg_data["type"] == "ai":
                    messages.append(AIMessage(content=msg_data["content"]))

            return messages

        except Exception as e:
            logger.warning(f"Failed to load chat history: {e}")
            return []

    def add_messages(self, messages: List[BaseMessage]) -> None:
        """ë©”ì‹œì§€ ì¶”ê°€"""
        try:
            for message in messages:
                msg_data = {
                    "type": "human" if isinstance(message, HumanMessage) else "ai",
                    "content": message.content,
                    "timestamp": datetime.now().isoformat()
                }

                self.redis_client.rpush(self._key, json.dumps(msg_data, ensure_ascii=False))

            # TTL ì„¤ì •
            self.redis_client.expire(self._key, self.ttl)

        except Exception as e:
            logger.error(f"Failed to save chat history: {e}")

    def clear(self) -> None:
        """íˆìŠ¤í† ë¦¬ í´ë¦¬ì–´"""
        self.redis_client.delete(self._key)

class ChatService:
    """ì±„íŒ… ì„œë¹„ìŠ¤"""

    def __init__(self, enable_rag: bool = True):
        self.llm = get_llm()
        self.enable_rag = enable_rag

        if enable_rag:
            self.rag_service = RAGService()

        # Redis ì—°ê²°
        try:
            self.redis_client = redis.Redis(
                host='localhost', port=6379, db=0, decode_responses=True
            )
            self.redis_client.ping()  # ì—°ê²° í…ŒìŠ¤íŠ¸

        except Exception as e:
            logger.warning(f"Redis not available, using in-memory storage: {e}")
            self.redis_client = None

        self._setup_chains()

    def _setup_chains(self):
        """ì±„íŒ… ì²´ì¸ ì„¤ì •"""

        if self.enable_rag:
            # RAG ê¸°ë°˜ ì±„íŒ… í”„ë¡¬í”„íŠ¸
            chat_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

                ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ì„± ìˆê²Œ ëŒ€í™”í•˜ì„¸ìš”.
                ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ ë¬¸ì„œê°€ ìˆë‹¤ë©´ í•´ë‹¹ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

                ê´€ë ¨ ë¬¸ì„œ: {context}
                """),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}")
            ])

        else:
            # ì¼ë°˜ ì±„íŒ… í”„ë¡¬í”„íŠ¸
            chat_prompt = ChatPromptTemplate.from_messages([
                ("system", "ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}")
            ])

        self.chat_chain = chat_prompt | self.llm

        # íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ì²´ì¸
        self.chain_with_history = RunnableWithMessageHistory(
            self.chat_chain,
            self._get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
        )

    def _get_session_history(self, session_id: str) -> BaseChatMessageHistory:
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°"""
        if self.redis_client:
            return RedisChatHistory(session_id, self.redis_client)
        else:
            # ì¸ë©”ëª¨ë¦¬ í´ë°± (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
            if not hasattr(self, '_memory_store'):
                self._memory_store = {}

            if session_id not in self._memory_store:
                self._memory_store[session_id] = InMemoryChatHistory()

            return self._memory_store[session_id]

    def chat(self, request: ChatRequest) -> ChatResponse:
        """ì±„íŒ… ì²˜ë¦¬"""

        start_time = time.time()

        try:
            # RAG ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (RAG í™œì„±í™”ì‹œ)
            context = ""
            sources = []

            if self.enable_rag:
                # ë¬¸ì„œ ê²€ìƒ‰
                relevant_docs = self.rag_service.vectorstore_manager.similarity_search(
                    request.message, k=3
                )

                context = "\n".join([doc.page_content for doc in relevant_docs])
                sources = [
                    {
                        "content": doc.page_content[:200],
                        "metadata": doc.metadata
                    }
                    for doc in relevant_docs
                ]

            # ì±„íŒ… ì‹¤í–‰
            response = self.chain_with_history.invoke(
                {
                    "input": request.message,
                    "context": context if self.enable_rag else ""
                },
                config={"configurable": {"session_id": request.session_id}}
            )

            execution_time = time.time() - start_time

            return ChatResponse(
                response=response.content,
                session_id=request.session_id,
                sources=sources if self.enable_rag else [],
                execution_time=execution_time
            )

        except Exception as e:
            logger.error(f"Chat processing failed: {e}")
            raise

    def get_chat_history(self, session_id: str) -> List[Dict[str, str]]:
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        try:
            history = self._get_session_history(session_id)

            formatted_history = []
            for message in history.messages:
                formatted_history.append({
                    "type": "human" if isinstance(message, HumanMessage) else "ai",
                    "content": message.content,
                    "timestamp": datetime.now().isoformat()  # ì‹¤ì œë¡œëŠ” ë©”ì‹œì§€ì—ì„œ ì¶”ì¶œ
                })

            return formatted_history

        except Exception as e:
            logger.error(f"Failed to get chat history: {e}")
            return []

    def clear_chat_history(self, session_id: str) -> bool:
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
        try:
            history = self._get_session_history(session_id)
            history.clear()
            return True

        except Exception as e:
            logger.error(f"Failed to clear chat history: {e}")
            return False

class InMemoryChatHistory(BaseChatMessageHistory, BaseModel):
    """ì¸ë©”ëª¨ë¦¬ ì±„íŒ… íˆìŠ¤í† ë¦¬ (í´ë°±ìš©)"""

    messages: List[BaseMessage] = Field(default_factory=list)

    def add_messages(self, messages: List[BaseMessage]) -> None:
        self.messages.extend(messages)

    def clear(self) -> None:
        self.messages = []
```

### API ì—”ë“œí¬ì¸íŠ¸

#### `src/api/main.py`
```python
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging

from src.config.settings import settings
from src.config.logging import setup_logging
from src.api.routers import chat, documents
from src.services.rag_service import RAGService
from src.services.chat_service import ChatService

# ë¡œê¹… ì„¤ì •
setup_logging(settings.log_level)
logger = logging.getLogger(__name__)

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì „ì—­)
rag_service = None
chat_service = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    global rag_service, chat_service

    logger.info("Initializing services...")

    try:
        rag_service = RAGService()
        chat_service = ChatService(enable_rag=True)

        logger.info("Services initialized successfully")

    except Exception as e:
        logger.error(f"Failed to initialize services: {e}")
        raise

    yield

    # ì¢…ë£Œ ì‹œ
    logger.info("Shutting down services...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="AI Assistant API",
    description="LangChain ê¸°ë°˜ RAG ë° ì±„íŒ… API",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ìš´ì˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë¼ìš°í„° ë“±ë¡
app.include_router(chat.router, prefix="/api/v1/chat", tags=["chat"])
app.include_router(documents.router, prefix="/api/v1/documents", tags=["documents"])

# ì˜ì¡´ì„± ì£¼ì…
def get_rag_service() -> RAGService:
    if rag_service is None:
        raise HTTPException(status_code=500, detail="RAG service not initialized")
    return rag_service

def get_chat_service() -> ChatService:
    if chat_service is None:
        raise HTTPException(status_code=500, detail="Chat service not initialized")
    return chat_service

@app.get("/")
async def root():
    return {"message": "AI Assistant API", "status": "healthy"}

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        rag_stats = rag_service.get_service_stats() if rag_service else {}

        return {
            "status": "healthy",
            "services": {
                "rag": rag_stats,
                "chat": {"status": "ready"} if chat_service else {"status": "not_initialized"}
            }
        }

    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=500, detail="Service unhealthy")

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "src.api.main:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=settings.api_debug,
        log_level=settings.log_level.lower()
    )
```

#### `src/api/routers/chat.py`
```python
from fastapi import APIRouter, Depends, HTTPException
from typing import List, Dict
import uuid

from src.services.chat_service import ChatService
from src.models.requests import ChatRequest
from src.models.responses import ChatResponse
from src.api.main import get_chat_service
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    chat_service: ChatService = Depends(get_chat_service)
):
    """ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬"""
    try:
        # ì„¸ì…˜ IDê°€ ì—†ìœ¼ë©´ ìƒì„±
        if not request.session_id:
            request.session_id = str(uuid.uuid4())

        response = chat_service.chat(request)
        return response

    except Exception as e:
        logger.error(f"Chat failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/history/{session_id}")
async def get_chat_history(
    session_id: str,
    chat_service: ChatService = Depends(get_chat_service)
) -> List[Dict[str, str]]:
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        history = chat_service.get_chat_history(session_id)
        return history

    except Exception as e:
        logger.error(f"Failed to get chat history: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/history/{session_id}")
async def clear_chat_history(
    session_id: str,
    chat_service: ChatService = Depends(get_chat_service)
):
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
    try:
        success = chat_service.clear_chat_history(session_id)

        if success:
            return {"message": "Chat history cleared successfully"}
        else:
            raise HTTPException(status_code=500, detail="Failed to clear chat history")

    except Exception as e:
        logger.error(f"Failed to clear chat history: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

### ëª¨ë¸ ì •ì˜

#### `src/models/requests.py`
```python
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List

class RAGRequest(BaseModel):
    question: str = Field(..., description="ê²€ìƒ‰í•  ì§ˆë¬¸")
    k: Optional[int] = Field(5, description="ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜")
    filters: Optional[Dict[str, Any]] = Field(None, description="ë©”íƒ€ë°ì´í„° í•„í„°")

class ChatRequest(BaseModel):
    message: str = Field(..., description="ì±„íŒ… ë©”ì‹œì§€")
    session_id: Optional[str] = Field(None, description="ì„¸ì…˜ ID")

class DocumentUploadRequest(BaseModel):
    documents: List[Dict[str, Any]] = Field(..., description="ì—…ë¡œë“œí•  ë¬¸ì„œë“¤")

    class Config:
        schema_extra = {
            "example": {
                "documents": [
                    {
                        "content": "ë¬¸ì„œ ë‚´ìš©",
                        "metadata": {
                            "source": "example.pdf",
                            "page": 1
                        }
                    }
                ]
            }
        }
```

#### `src/models/responses.py`
```python
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional

class DocumentSource(BaseModel):
    content: str = Field(..., description="ë¬¸ì„œ ë‚´ìš©")
    metadata: Dict[str, Any] = Field(..., description="ë¬¸ì„œ ë©”íƒ€ë°ì´í„°")
    score: Optional[float] = Field(None, description="ìœ ì‚¬ë„ ì ìˆ˜")

class RAGResponse(BaseModel):
    answer: str = Field(..., description="ìƒì„±ëœ ë‹µë³€")
    sources: List[DocumentSource] = Field(..., description="ì°¸ì¡°í•œ ë¬¸ì„œë“¤")
    execution_time: float = Field(..., description="ì‹¤í–‰ ì‹œê°„ (ì´ˆ)")
    retrieved_docs: int = Field(..., description="ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜")

class ChatResponse(BaseModel):
    response: str = Field(..., description="ì±„íŒ… ì‘ë‹µ")
    session_id: str = Field(..., description="ì„¸ì…˜ ID")
    sources: List[DocumentSource] = Field(default_factory=list, description="ì°¸ì¡° ë¬¸ì„œ (RAG í™œì„±í™” ì‹œ)")
    execution_time: float = Field(..., description="ì‹¤í–‰ ì‹œê°„ (ì´ˆ)")

class DocumentUploadResponse(BaseModel):
    status: str = Field(..., description="ì—…ë¡œë“œ ìƒíƒœ")
    added_documents: int = Field(..., description="ì¶”ê°€ëœ ë¬¸ì„œ ìˆ˜")
    document_ids: List[str] = Field(..., description="ë¬¸ì„œ ID ëª©ë¡")
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ êµ¬ì¡°

### `tests/conftest.py`
```python
import pytest
import tempfile
import shutil
from pathlib import Path

from src.config.settings import Settings
from src.core.llm import get_llm
from src.core.vectorstore import VectorStoreManager
from src.services.rag_service import RAGService

@pytest.fixture(scope="session")
def test_settings():
    """í…ŒìŠ¤íŠ¸ìš© ì„¤ì •"""
    return Settings(
        openai_api_key="test_key",
        vectorstore_type="chroma",
        vectorstore_path="./test_vectorstore",
        api_debug=True
    )

@pytest.fixture
def temp_vectorstore():
    """ì„ì‹œ ë²¡í„°ìŠ¤í† ì–´"""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    shutil.rmtree(temp_dir)

@pytest.fixture
def sample_documents():
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë¬¸ì„œ"""
    from langchain_core.documents import Document

    return [
        Document(
            page_content="Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
            metadata={"source": "python.txt", "page": 1}
        ),
        Document(
            page_content="ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
            metadata={"source": "ml.txt", "page": 1}
        )
    ]

@pytest.fixture
def mock_llm(monkeypatch):
    """Mock LLM"""
    class MockLLM:
        def invoke(self, prompt, **kwargs):
            class MockResponse:
                content = "í…ŒìŠ¤íŠ¸ ì‘ë‹µ"
            return MockResponse()

    def mock_get_llm():
        return MockLLM()

    monkeypatch.setattr("src.core.llm.get_llm", mock_get_llm)
    return mock_get_llm()

@pytest.fixture
def rag_service(mock_llm, temp_vectorstore, sample_documents, monkeypatch):
    """RAG ì„œë¹„ìŠ¤ í”½ìŠ¤ì²˜"""
    # ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
    monkeypatch.setenv("VECTORSTORE_PATH", temp_vectorstore)

    service = RAGService()
    service.add_documents(sample_documents)

    return service
```

### `tests/unit/test_rag_service.py`
```python
import pytest
from src.models.requests import RAGRequest
from src.services.rag_service import RAGService

class TestRAGService:
    """RAG ì„œë¹„ìŠ¤ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""

    def test_query_basic(self, rag_service):
        """ê¸°ë³¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
        request = RAGRequest(question="Pythonì´ ë¬´ì—‡ì¸ê°€ìš”?")
        response = rag_service.query(request)

        assert response.answer is not None
        assert len(response.sources) > 0
        assert response.execution_time > 0
        assert response.retrieved_docs > 0

    def test_query_with_filter(self, rag_service):
        """í•„í„° ì ìš© ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
        request = RAGRequest(
            question="í”„ë¡œê·¸ë˜ë°ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
            filters={"source": "python.txt"}
        )
        response = rag_service.query(request)

        assert response.answer is not None
        # í•„í„°ëœ ë¬¸ì„œë§Œ ë°˜í™˜ë˜ëŠ”ì§€ í™•ì¸
        for source in response.sources:
            assert source.metadata.get("source") == "python.txt"

    def test_add_documents(self, rag_service, sample_documents):
        """ë¬¸ì„œ ì¶”ê°€ í…ŒìŠ¤íŠ¸"""
        new_docs = sample_documents[:1]  # ì²« ë²ˆì§¸ ë¬¸ì„œë§Œ

        result = rag_service.add_documents(new_docs)

        assert result["status"] == "success"
        assert result["added_documents"] == 1
        assert len(result["document_ids"]) == 1

    def test_service_stats(self, rag_service):
        """ì„œë¹„ìŠ¤ í†µê³„ í…ŒìŠ¤íŠ¸"""
        stats = rag_service.get_service_stats()

        assert "vectorstore" in stats
        assert "status" in stats
        assert stats["status"] == "healthy"
```

### `tests/integration/test_api.py`
```python
import pytest
from fastapi.testclient import TestClient
from src.api.main import app

client = TestClient(app)

class TestAPIIntegration:
    """API í†µí•© í…ŒìŠ¤íŠ¸"""

    def test_health_check(self):
        """í—¬ìŠ¤ ì²´í¬ í…ŒìŠ¤íŠ¸"""
        response = client.get("/health")

        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "healthy"

    def test_chat_endpoint(self):
        """ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
        chat_request = {
            "message": "ì•ˆë…•í•˜ì„¸ìš”",
            "session_id": "test_session"
        }

        response = client.post("/api/v1/chat/", json=chat_request)

        assert response.status_code == 200
        data = response.json()
        assert "response" in data
        assert data["session_id"] == "test_session"

    def test_chat_history(self):
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ í…ŒìŠ¤íŠ¸"""
        session_id = "test_session_history"

        # ë¨¼ì € ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡
        chat_request = {
            "message": "í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€",
            "session_id": session_id
        }
        client.post("/api/v1/chat/", json=chat_request)

        # íˆìŠ¤í† ë¦¬ ì¡°íšŒ
        response = client.get(f"/api/v1/chat/history/{session_id}")

        assert response.status_code == 200
        history = response.json()
        assert len(history) >= 2  # ì‚¬ìš©ì ë©”ì‹œì§€ + AI ì‘ë‹µ

    def test_clear_chat_history(self):
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì‚­ì œ í…ŒìŠ¤íŠ¸"""
        session_id = "test_session_clear"

        # ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡
        chat_request = {
            "message": "ì‚­ì œë  ë©”ì‹œì§€",
            "session_id": session_id
        }
        client.post("/api/v1/chat/", json=chat_request)

        # íˆìŠ¤í† ë¦¬ ì‚­ì œ
        response = client.delete(f"/api/v1/chat/history/{session_id}")

        assert response.status_code == 200
        assert response.json()["message"] == "Chat history cleared successfully"

        # íˆìŠ¤í† ë¦¬ê°€ ë¹„ì›Œì¡ŒëŠ”ì§€ í™•ì¸
        history_response = client.get(f"/api/v1/chat/history/{session_id}")
        history = history_response.json()
        assert len(history) == 0
```

## ğŸ³ Docker ë° ë°°í¬

### `Dockerfile`
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### `docker-compose.yml`
```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/aiassistant
    depends_on:
      - redis
      - postgres
    volumes:
      - ./data:/app/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=aiassistant
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  redis_data:
  postgres_data:
```

### `scripts/deploy.py`
```python
#!/usr/bin/env python3
"""
ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
"""

import subprocess
import sys
import os
from pathlib import Path

def run_command(cmd, check=True):
    """ëª…ë ¹ ì‹¤í–‰"""
    print(f"Running: {cmd}")
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

    if check and result.returncode != 0:
        print(f"Error: {result.stderr}")
        sys.exit(1)

    print(result.stdout)
    return result

def deploy_local():
    """ë¡œì»¬ ë°°í¬"""
    print("=== Local Deployment ===")

    # Docker ì´ë¯¸ì§€ ë¹Œë“œ
    run_command("docker-compose build")

    # ì„œë¹„ìŠ¤ ì‹œì‘
    run_command("docker-compose up -d")

    # ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
    run_command("docker-compose ps")

    print("Local deployment completed!")
    print("API available at: http://localhost:8000")

def deploy_production():
    """ìš´ì˜ ë°°í¬"""
    print("=== Production Deployment ===")

    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    required_vars = ["OPENAI_API_KEY", "DATABASE_URL", "REDIS_URL"]
    for var in required_vars:
        if not os.getenv(var):
            print(f"Error: {var} environment variable not set")
            sys.exit(1)

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    run_command("python -m pytest tests/")

    # Docker ì´ë¯¸ì§€ ë¹Œë“œ ë° íƒœê¹…
    image_tag = os.getenv("IMAGE_TAG", "latest")
    run_command(f"docker build -t ai-assistant:{image_tag} .")

    # ì»¨í…Œì´ë„ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— í‘¸ì‹œ (ì˜ˆ: AWS ECR)
    if os.getenv("ECR_REGISTRY"):
        registry = os.getenv("ECR_REGISTRY")
        run_command(f"docker tag ai-assistant:{image_tag} {registry}/ai-assistant:{image_tag}")
        run_command(f"docker push {registry}/ai-assistant:{image_tag}")

    print("Production deployment completed!")

def rollback():
    """ë¡¤ë°±"""
    print("=== Rollback ===")

    previous_tag = os.getenv("PREVIOUS_TAG")
    if not previous_tag:
        print("Error: PREVIOUS_TAG environment variable not set")
        sys.exit(1)

    # ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
    run_command(f"docker-compose pull ai-assistant:{previous_tag}")
    run_command("docker-compose up -d")

    print(f"Rolled back to version: {previous_tag}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python deploy.py [local|production|rollback]")
        sys.exit(1)

    action = sys.argv[1]

    if action == "local":
        deploy_local()
    elif action == "production":
        deploy_production()
    elif action == "rollback":
        rollback()
    else:
        print("Invalid action. Use: local, production, or rollback")
        sys.exit(1)
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### `src/utils/monitoring.py`
```python
import time
import logging
from functools import wraps
from typing import Dict, Any, Optional
import psutil
import threading
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""

    def __init__(self):
        self.metrics = {
            "requests_total": 0,
            "requests_success": 0,
            "requests_error": 0,
            "response_times": [],
            "memory_usage": [],
            "cpu_usage": [],
        }
        self._start_monitoring()

    def _start_monitoring(self):
        """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        def monitor_system():
            while True:
                try:
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                    memory_percent = psutil.virtual_memory().percent
                    self.metrics["memory_usage"].append({
                        "timestamp": datetime.now(),
                        "value": memory_percent
                    })

                    # CPU ì‚¬ìš©ëŸ‰
                    cpu_percent = psutil.cpu_percent(interval=1)
                    self.metrics["cpu_usage"].append({
                        "timestamp": datetime.now(),
                        "value": cpu_percent
                    })

                    # ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ (1ì‹œê°„ ì´ìƒ)
                    cutoff_time = datetime.now() - timedelta(hours=1)

                    self.metrics["memory_usage"] = [
                        m for m in self.metrics["memory_usage"]
                        if m["timestamp"] > cutoff_time
                    ]

                    self.metrics["cpu_usage"] = [
                        m for m in self.metrics["cpu_usage"]
                        if m["timestamp"] > cutoff_time
                    ]

                    time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìˆ˜ì§‘

                except Exception as e:
                    logger.error(f"System monitoring error: {e}")
                    time.sleep(60)

        monitor_thread = threading.Thread(target=monitor_system, daemon=True)
        monitor_thread.start()

    def record_request(self, execution_time: float, success: bool = True):
        """ìš”ì²­ ê¸°ë¡"""
        self.metrics["requests_total"] += 1

        if success:
            self.metrics["requests_success"] += 1
        else:
            self.metrics["requests_error"] += 1

        # ìµœê·¼ 1000ê°œ ì‘ë‹µì‹œê°„ë§Œ ë³´ê´€
        self.metrics["response_times"].append({
            "timestamp": datetime.now(),
            "value": execution_time
        })

        if len(self.metrics["response_times"]) > 1000:
            self.metrics["response_times"] = self.metrics["response_times"][-1000:]

    def get_metrics(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        # ì‘ë‹µì‹œê°„ í†µê³„
        response_times = [m["value"] for m in self.metrics["response_times"]]

        if response_times:
            avg_response_time = sum(response_times) / len(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
        else:
            avg_response_time = min_response_time = max_response_time = 0

        # í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ
        current_memory = psutil.virtual_memory().percent
        current_cpu = psutil.cpu_percent()

        return {
            "requests": {
                "total": self.metrics["requests_total"],
                "success": self.metrics["requests_success"],
                "error": self.metrics["requests_error"],
                "success_rate": (
                    self.metrics["requests_success"] / max(self.metrics["requests_total"], 1) * 100
                )
            },
            "response_times": {
                "average": avg_response_time,
                "min": min_response_time,
                "max": max_response_time,
                "count": len(response_times)
            },
            "system": {
                "memory_percent": current_memory,
                "cpu_percent": current_cpu,
                "disk_usage": psutil.disk_usage('/').percent
            }
        }

# ì „ì—­ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
monitor = PerformanceMonitor()

def track_performance(func):
    """ì„±ëŠ¥ ì¶”ì  ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        success = True

        try:
            result = func(*args, **kwargs)
            return result

        except Exception as e:
            success = False
            logger.error(f"Function {func.__name__} failed: {e}")
            raise

        finally:
            execution_time = time.time() - start_time
            monitor.record_request(execution_time, success)

    return wrapper

def get_system_metrics() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    return monitor.get_metrics()
```

ì´ ê°œë°œ ê°€ì´ë“œë¥¼ í†µí•´ ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ì»´í¬ë„ŒíŠ¸ëŠ” ëª¨ë“ˆí™”ë˜ì–´ ìˆì–´ í•„ìš”ì— ë”°ë¼ ë…ë¦½ì ìœ¼ë¡œ ê°œë°œí•˜ê³  í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.