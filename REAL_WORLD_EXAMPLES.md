# ì‹¤ë¬´ í™œìš© ì˜ˆì‹œ ëª¨ìŒì§‘

## ğŸ¢ ì—”í„°í”„ë¼ì´ì¦ˆ ê¸‰ ì‹œìŠ¤í…œ ì˜ˆì‹œ

### 1. ëŒ€ê¸°ì—… ë‚´ë¶€ ì§€ì‹ê´€ë¦¬ ì‹œìŠ¤í…œ

```python
import asyncio
import json
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path

class EnterpriseKnowledgeSystem:
    """ëŒ€ê¸°ì—…ìš© ì§€ì‹ê´€ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(self, config_path: str = "config/enterprise.json"):
        self.config = self._load_config(config_path)
        self.llm = ChatOpenAI(
            model=self.config["model"],
            temperature=self.config["temperature"],
            api_key=self.config["openai_api_key"]
        )
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

        # ë¶€ì„œë³„ ë²¡í„° ìŠ¤í† ì–´
        self.department_stores = {}
        self.access_control = AccessController()
        self.audit_logger = AuditLogger()

    def _load_config(self, config_path: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        with open(config_path, 'r') as f:
            return json.load(f)

    def setup_department_stores(self, departments: List[str]):
        """ë¶€ì„œë³„ ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •"""
        for dept in departments:
            collection_name = f"knowledge_{dept.lower()}"
            persist_dir = f"./vectorstores/{dept.lower()}"

            self.department_stores[dept] = Chroma(
                collection_name=collection_name,
                embedding_function=self.embeddings,
                persist_directory=persist_dir
            )

    async def ingest_documents(self, dept: str, file_paths: List[str], user_id: str):
        """ë¬¸ì„œ ì¼ê´„ ë“±ë¡"""
        if not self.access_control.can_upload(user_id, dept):
            raise PermissionError(f"User {user_id} cannot upload to {dept}")

        # ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        documents = []

        for file_path in file_paths:
            # íŒŒì¼ íƒ€ì…ë³„ ë¡œë” ì„ íƒ
            loader = self._get_loader(file_path)
            docs = loader.load()

            # ë©”íƒ€ë°ì´í„° ê°•í™”
            for doc in docs:
                doc.metadata.update({
                    "department": dept,
                    "uploaded_by": user_id,
                    "upload_date": datetime.now().isoformat(),
                    "file_path": file_path,
                    "security_level": self._classify_security_level(doc.page_content)
                })

            documents.extend(docs)

        # ì²­í‚¹ ë° ë²¡í„°í™”
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100
        )

        split_docs = text_splitter.split_documents(documents)

        # ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
        await self.department_stores[dept].aadd_documents(split_docs)

        # ê°ì‚¬ ë¡œê·¸
        self.audit_logger.log_upload(user_id, dept, len(split_docs))

        return len(split_docs)

    def search_knowledge(self, query: str, user_id: str, departments: Optional[List[str]] = None) -> Dict:
        """ì§€ì‹ ê²€ìƒ‰ (ê¶Œí•œ ê¸°ë°˜)"""

        # ì‚¬ìš©ì ê¶Œí•œ í™•ì¸
        accessible_depts = self.access_control.get_accessible_departments(user_id)

        if departments:
            departments = [d for d in departments if d in accessible_depts]
        else:
            departments = accessible_depts

        if not departments:
            return {"error": "No accessible departments"}

        # ë¶€ì„œë³„ ê²€ìƒ‰ ìˆ˜í–‰
        all_results = {}

        for dept in departments:
            if dept in self.department_stores:
                retriever = self.department_stores[dept].as_retriever(
                    search_type="mmr",
                    search_kwargs={"k": 5, "lambda_mult": 0.7}
                )

                docs = retriever.invoke(query)

                # ë³´ì•ˆ ë ˆë²¨ í•„í„°ë§
                filtered_docs = self._filter_by_security_level(docs, user_id)

                all_results[dept] = filtered_docs

        # í†µí•© ì‘ë‹µ ìƒì„±
        response = self._generate_unified_response(query, all_results, user_id)

        # ê²€ìƒ‰ ë¡œê·¸
        self.audit_logger.log_search(user_id, query, departments)

        return response

    def _get_loader(self, file_path: str):
        """íŒŒì¼ í™•ì¥ìë³„ ë¡œë” ì„ íƒ"""
        suffix = Path(file_path).suffix.lower()

        if suffix == '.pdf':
            return PyPDFLoader(file_path)
        elif suffix in ['.docx', '.doc']:
            return Docx2txtLoader(file_path)
        elif suffix == '.csv':
            return CSVLoader(file_path)
        elif suffix in ['.txt', '.md']:
            return TextLoader(file_path)
        else:
            return UnstructuredFileLoader(file_path)

    def _classify_security_level(self, content: str) -> str:
        """ì½˜í…ì¸  ë³´ì•ˆ ë“±ê¸‰ ë¶„ë¥˜"""
        sensitive_keywords = ["ê¸°ë°€", "confidential", "ë‚´ë¶€ì „ìš©", "ì„ì›ì§„", "ì¬ë¬´ì œí‘œ"]

        content_lower = content.lower()
        if any(keyword in content_lower for keyword in sensitive_keywords):
            return "confidential"
        else:
            return "general"

    def _filter_by_security_level(self, docs: List[Document], user_id: str) -> List[Document]:
        """ì‚¬ìš©ì ê¶Œí•œì— ë”°ë¥¸ ë¬¸ì„œ í•„í„°ë§"""
        user_clearance = self.access_control.get_security_clearance(user_id)

        filtered = []
        for doc in docs:
            doc_level = doc.metadata.get("security_level", "general")

            if self._can_access_level(user_clearance, doc_level):
                filtered.append(doc)

        return filtered

    def _can_access_level(self, user_clearance: str, doc_level: str) -> bool:
        """ë³´ì•ˆ ë“±ê¸‰ ì ‘ê·¼ ê¶Œí•œ í™•ì¸"""
        clearance_hierarchy = {
            "general": 1,
            "confidential": 2,
            "secret": 3
        }

        user_level = clearance_hierarchy.get(user_clearance, 0)
        required_level = clearance_hierarchy.get(doc_level, 1)

        return user_level >= required_level

class AccessController:
    """ì ‘ê·¼ ê¶Œí•œ ì œì–´"""

    def __init__(self):
        self.user_permissions = self._load_permissions()

    def _load_permissions(self) -> Dict:
        """ì‚¬ìš©ì ê¶Œí•œ ì •ë³´ ë¡œë“œ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ)"""
        return {
            "john.doe": {
                "departments": ["IT", "HR", "General"],
                "security_clearance": "confidential"
            },
            "jane.smith": {
                "departments": ["Finance", "General"],
                "security_clearance": "secret"
            }
        }

    def can_upload(self, user_id: str, department: str) -> bool:
        """ì—…ë¡œë“œ ê¶Œí•œ í™•ì¸"""
        user_perms = self.user_permissions.get(user_id, {})
        return department in user_perms.get("departments", [])

    def get_accessible_departments(self, user_id: str) -> List[str]:
        """ì ‘ê·¼ ê°€ëŠ¥í•œ ë¶€ì„œ ëª©ë¡ ë°˜í™˜"""
        user_perms = self.user_permissions.get(user_id, {})
        return user_perms.get("departments", [])

    def get_security_clearance(self, user_id: str) -> str:
        """ì‚¬ìš©ì ë³´ì•ˆ ë“±ê¸‰ ë°˜í™˜"""
        user_perms = self.user_permissions.get(user_id, {})
        return user_perms.get("security_clearance", "general")

class AuditLogger:
    """ê°ì‚¬ ë¡œê·¸ ì‹œìŠ¤í…œ"""

    def __init__(self, log_file: str = "logs/audit.log"):
        self.log_file = log_file
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)

    def log_upload(self, user_id: str, department: str, doc_count: int):
        """ì—…ë¡œë“œ ë¡œê·¸"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "action": "upload",
            "user_id": user_id,
            "department": department,
            "doc_count": doc_count
        }
        self._write_log(log_entry)

    def log_search(self, user_id: str, query: str, departments: List[str]):
        """ê²€ìƒ‰ ë¡œê·¸"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "action": "search",
            "user_id": user_id,
            "query": query[:100],  # ì¿¼ë¦¬ ì¼ë¶€ë§Œ ë¡œê·¸
            "departments": departments
        }
        self._write_log(log_entry)

    def _write_log(self, entry: Dict):
        """ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡"""
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    knowledge_system = EnterpriseKnowledgeSystem()

    # ë¶€ì„œë³„ ìŠ¤í† ì–´ ì„¤ì •
    departments = ["IT", "HR", "Finance", "General"]
    knowledge_system.setup_department_stores(departments)

    # ë¬¸ì„œ ì—…ë¡œë“œ
    hr_docs = ["docs/employee_handbook.pdf", "docs/benefits_guide.docx"]
    await knowledge_system.ingest_documents("HR", hr_docs, "john.doe")

    # ì§€ì‹ ê²€ìƒ‰
    result = knowledge_system.search_knowledge(
        "íœ´ê°€ ì •ì±…ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
        user_id="john.doe",
        departments=["HR"]
    )

    print(result["answer"])

# ì‹¤í–‰
if __name__ == "__main__":
    asyncio.run(main())
```

### 2. ì‹¤ì‹œê°„ ê³ ê° ì§€ì› ì‹œìŠ¤í…œ

```python
import streamlit as st
import redis
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import uuid

class RealTimeCustomerSupport:
    """ì‹¤ì‹œê°„ ê³ ê° ì§€ì› ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.3)
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

        # ì‹¤ì‹œê°„ ë°ì´í„° ì €ì¥ì†Œ
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)

        # FAQ ë²¡í„° ìŠ¤í† ì–´
        self.faq_store = Chroma(
            collection_name="customer_faq",
            embedding_function=self.embeddings,
            persist_directory="./customer_faq_store"
        )

        # ëŒ€í™” ë¶„ë¥˜ê¸°
        self.intent_classifier = self._setup_intent_classifier()

        # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ê·œì¹™
        self.escalation_rules = self._load_escalation_rules()

    def _setup_intent_classifier(self):
        """ëŒ€í™” ì˜ë„ ë¶„ë¥˜ê¸° ì„¤ì •"""
        intent_prompt = ChatPromptTemplate.from_template("""
        ê³ ê° ë©”ì‹œì§€ì˜ ì˜ë„ë¥¼ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

        ì˜ë„ ëª©ë¡:
        - question: ì¼ë°˜ì ì¸ ì§ˆë¬¸
        - complaint: ë¶ˆë§Œì‚¬í•­
        - technical_issue: ê¸°ìˆ ì  ë¬¸ì œ
        - billing: ê²°ì œ/ìš”ê¸ˆ ê´€ë ¨
        - account: ê³„ì • ê´€ë ¨
        - urgent: ê¸´ê¸‰ ìƒí™©

        ê³ ê° ë©”ì‹œì§€: {message}

        ë¶„ë¥˜ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
        {{
            "intent": "ë¶„ë¥˜ëœ ì˜ë„",
            "confidence": ì‹ ë¢°ë„(0.0-1.0),
            "urgency": ê¸´ê¸‰ë„(1-5)
        }}
        """)

        return intent_prompt | self.llm

    def _load_escalation_rules(self) -> Dict:
        """ì—ìŠ¤ì»¬ë ˆì´ì…˜ ê·œì¹™ ë¡œë“œ"""
        return {
            "urgent": {"threshold": 0.8, "target": "senior_agent"},
            "complaint": {"threshold": 0.7, "target": "supervisor"},
            "technical_issue": {"threshold": 0.6, "target": "tech_support"},
            "billing": {"threshold": 0.5, "target": "billing_team"}
        }

    def handle_customer_message(self, session_id: str, message: str, customer_info: Dict) -> Dict:
        """ê³ ê° ë©”ì‹œì§€ ì²˜ë¦¬"""

        # 1. ì˜ë„ ë¶„ë¥˜
        intent_result = self._classify_intent(message)

        # 2. ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
        session_context = self._get_session_context(session_id)

        # 3. FAQ ê²€ìƒ‰
        relevant_faqs = self._search_faq(message)

        # 4. ê°œì¸í™”ëœ ì‘ë‹µ ìƒì„±
        response = self._generate_personalized_response(
            message, intent_result, relevant_faqs, customer_info, session_context
        )

        # 5. ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”ì„± íŒë‹¨
        escalation_needed = self._check_escalation(intent_result, session_context)

        # 6. ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        self._update_session_context(session_id, message, response, intent_result)

        # 7. ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self._update_metrics(intent_result, escalation_needed)

        return {
            "response": response,
            "intent": intent_result,
            "escalation_needed": escalation_needed,
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        }

    def _classify_intent(self, message: str) -> Dict:
        """ë©”ì‹œì§€ ì˜ë„ ë¶„ë¥˜"""
        result = self.intent_classifier.invoke({"message": message})

        try:
            return json.loads(result.content)
        except:
            return {"intent": "question", "confidence": 0.5, "urgency": 1}

    def _get_session_context(self, session_id: str) -> Dict:
        """ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ"""
        context_key = f"session:{session_id}"
        context_data = self.redis_client.get(context_key)

        if context_data:
            return json.loads(context_data.decode())
        else:
            return {
                "messages": [],
                "start_time": datetime.now().isoformat(),
                "interaction_count": 0,
                "sentiment_history": [],
                "issues": []
            }

    def _search_faq(self, message: str, k: int = 3) -> List[Document]:
        """FAQ ê²€ìƒ‰"""
        return self.faq_store.similarity_search(message, k=k)

    def _generate_personalized_response(self, message: str, intent: Dict,
                                      faqs: List[Document], customer_info: Dict,
                                      session_context: Dict) -> str:
        """ê°œì¸í™”ëœ ì‘ë‹µ ìƒì„±"""

        # ê³ ê° ì •ë³´ ê¸°ë°˜ ê°œì¸í™”
        customer_tier = customer_info.get("tier", "standard")
        interaction_history = session_context.get("interaction_count", 0)

        # FAQ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        faq_context = "\n".join([doc.page_content for doc in faqs])

        personalized_prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ {customer_tier} ë“±ê¸‰ ê³ ê°ì„ ë‹´ë‹¹í•˜ëŠ” ì „ë¬¸ ê³ ê°ì§€ì› ë‹´ë‹¹ìì…ë‹ˆë‹¤.

        ê³ ê° ì •ë³´:
        - ì´ë¦„: {customer_name}
        - ë“±ê¸‰: {customer_tier}
        - ì´ë²ˆ ì„¸ì…˜ ìƒí˜¸ì‘ìš© íšŸìˆ˜: {interaction_count}
        - ê°ì§€ëœ ì˜ë„: {intent}
        - ê¸´ê¸‰ë„: {urgency}/5

        ê´€ë ¨ FAQ ì •ë³´:
        {faq_context}

        ì´ì „ ëŒ€í™” ë§¥ë½:
        {previous_messages}

        ê³ ê° ë©”ì‹œì§€: {message}

        ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        1. ê³ ê° ë“±ê¸‰ì— ë§ëŠ” ì ì ˆí•œ í†¤ ì‚¬ìš©
        2. FAQ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ ì œê³µ
        3. í•„ìš”ì‹œ ì¶”ê°€ ì§ˆë¬¸ ìœ ë„
        4. ê¸ì •ì ì´ê³  í•´ê²° ì§€í–¥ì ì¸ ìì„¸ ìœ ì§€
        """)

        # ì´ì „ ëŒ€í™” ìš”ì•½
        previous_messages = self._summarize_previous_messages(session_context.get("messages", []))

        response_chain = personalized_prompt | self.llm

        response = response_chain.invoke({
            "customer_name": customer_info.get("name", "ê³ ê°ë‹˜"),
            "customer_tier": customer_tier,
            "interaction_count": interaction_history,
            "intent": intent.get("intent"),
            "urgency": intent.get("urgency", 1),
            "faq_context": faq_context,
            "previous_messages": previous_messages,
            "message": message
        })

        return response.content

    def _check_escalation(self, intent_result: Dict, session_context: Dict) -> Dict:
        """ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”ì„± íŒë‹¨"""
        intent = intent_result.get("intent")
        confidence = intent_result.get("confidence", 0)
        urgency = intent_result.get("urgency", 1)
        interaction_count = session_context.get("interaction_count", 0)

        escalation = {"needed": False, "reason": "", "target": ""}

        # ê·œì¹™ ê¸°ë°˜ ì—ìŠ¤ì»¬ë ˆì´ì…˜
        if intent in self.escalation_rules:
            rule = self.escalation_rules[intent]
            if confidence >= rule["threshold"]:
                escalation = {
                    "needed": True,
                    "reason": f"High confidence {intent}",
                    "target": rule["target"]
                }

        # ê¸´ê¸‰ë„ ê¸°ë°˜ ì—ìŠ¤ì»¬ë ˆì´ì…˜
        if urgency >= 4:
            escalation = {
                "needed": True,
                "reason": "High urgency",
                "target": "senior_agent"
            }

        # ë°˜ë³µ ìƒí˜¸ì‘ìš© ê¸°ë°˜ ì—ìŠ¤ì»¬ë ˆì´ì…˜
        if interaction_count >= 5:
            escalation = {
                "needed": True,
                "reason": "Multiple interactions",
                "target": "supervisor"
            }

        return escalation

    def _update_session_context(self, session_id: str, message: str,
                              response: str, intent: Dict):
        """ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸"""
        context_key = f"session:{session_id}"
        context = self._get_session_context(session_id)

        # ë©”ì‹œì§€ ì¶”ê°€
        context["messages"].append({
            "timestamp": datetime.now().isoformat(),
            "customer_message": message,
            "bot_response": response,
            "intent": intent
        })

        # ìƒí˜¸ì‘ìš© íšŸìˆ˜ ì¦ê°€
        context["interaction_count"] += 1

        # ê°ì • íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (ì‹¤ì œë¡œëŠ” ê°ì • ë¶„ì„ ëª¨ë¸ ì‚¬ìš©)
        sentiment = self._analyze_sentiment(message)
        context["sentiment_history"].append(sentiment)

        # Redisì— ì €ì¥ (1ì‹œê°„ TTL)
        self.redis_client.setex(
            context_key,
            3600,  # 1 hour
            json.dumps(context, ensure_ascii=False)
        )

    def _analyze_sentiment(self, message: str) -> str:
        """ê°ì • ë¶„ì„ (ë‹¨ìˆœí™”ëœ ë²„ì „)"""
        positive_words = ["ì¢‹ë‹¤", "ê°ì‚¬", "ë§Œì¡±", "í›Œë¥­", "ì™„ë²½"]
        negative_words = ["ì‹«ë‹¤", "í™”ë‚˜", "ìµœì•…", "ì‹¤ë§", "ë¬¸ì œ"]

        message_lower = message.lower()

        positive_count = sum(1 for word in positive_words if word in message_lower)
        negative_count = sum(1 for word in negative_words if word in message_lower)

        if positive_count > negative_count:
            return "positive"
        elif negative_count > positive_count:
            return "negative"
        else:
            return "neutral"

    def _update_metrics(self, intent_result: Dict, escalation_needed: Dict):
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        current_hour = datetime.now().strftime("%Y%m%d%H")

        # ì˜ë„ë³„ ì¹´ìš´íŠ¸
        intent = intent_result.get("intent", "unknown")
        intent_key = f"metrics:intent:{intent}:{current_hour}"
        self.redis_client.incr(intent_key)
        self.redis_client.expire(intent_key, 86400)  # 24ì‹œê°„ ë³´ê´€

        # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì¹´ìš´íŠ¸
        if escalation_needed.get("needed"):
            escalation_key = f"metrics:escalation:{current_hour}"
            self.redis_client.incr(escalation_key)
            self.redis_client.expire(escalation_key, 86400)

    def get_real_time_dashboard_data(self) -> Dict:
        """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì œê³µ"""
        current_hour = datetime.now().strftime("%Y%m%d%H")

        # ì§€ë‚œ 24ì‹œê°„ ë°ì´í„°
        hours = []
        for i in range(24):
            hour = (datetime.now() - timedelta(hours=i)).strftime("%Y%m%d%H")
            hours.append(hour)

        dashboard_data = {
            "hourly_interactions": {},
            "intent_distribution": {},
            "escalation_rate": {},
            "active_sessions": 0
        }

        # ì‹œê°„ë³„ ìƒí˜¸ì‘ìš©
        for hour in hours:
            total_interactions = 0
            for intent in ["question", "complaint", "technical_issue", "billing", "account", "urgent"]:
                key = f"metrics:intent:{intent}:{hour}"
                count = int(self.redis_client.get(key) or 0)
                total_interactions += count

                if intent not in dashboard_data["intent_distribution"]:
                    dashboard_data["intent_distribution"][intent] = 0
                dashboard_data["intent_distribution"][intent] += count

            dashboard_data["hourly_interactions"][hour] = total_interactions

            # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ë¹„ìœ¨
            escalation_key = f"metrics:escalation:{hour}"
            escalation_count = int(self.redis_client.get(escalation_key) or 0)

            if total_interactions > 0:
                dashboard_data["escalation_rate"][hour] = escalation_count / total_interactions
            else:
                dashboard_data["escalation_rate"][hour] = 0

        # í™œì„± ì„¸ì…˜ ìˆ˜
        active_sessions = len(self.redis_client.keys("session:*"))
        dashboard_data["active_sessions"] = active_sessions

        return dashboard_data

# Streamlit ì›¹ ì•±
def create_support_app():
    st.set_page_config(page_title="ê³ ê°ì§€ì› ì‹œìŠ¤í…œ", layout="wide")

    support_system = RealTimeCustomerSupport()

    # ì‚¬ì´ë“œë°” - ëŒ€ì‹œë³´ë“œ
    with st.sidebar:
        st.title("ğŸ“Š ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ")

        dashboard_data = support_system.get_real_time_dashboard_data()

        st.metric("í™œì„± ì„¸ì…˜", dashboard_data["active_sessions"])

        # ì˜ë„ ë¶„í¬ ì°¨íŠ¸
        if dashboard_data["intent_distribution"]:
            st.subheader("ì˜ë„ ë¶„í¬")
            st.bar_chart(dashboard_data["intent_distribution"])

        # ì‹œê°„ë³„ ìƒí˜¸ì‘ìš©
        if dashboard_data["hourly_interactions"]:
            st.subheader("ì‹œê°„ë³„ ìƒí˜¸ì‘ìš©")
            st.line_chart(dashboard_data["hourly_interactions"])

    # ë©”ì¸ í™”ë©´ - ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
    st.title("ğŸ¤– AI ê³ ê°ì§€ì› ì‹œìŠ¤í…œ")

    # ì„¸ì…˜ ID ìƒì„±
    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())

    # ê³ ê° ì •ë³´ ì…ë ¥
    with st.expander("ê³ ê° ì •ë³´"):
        customer_name = st.text_input("ê³ ê°ëª…", value="ê¹€ê³ ê°")
        customer_tier = st.selectbox("ë“±ê¸‰", ["standard", "premium", "vip"])
        customer_info = {"name": customer_name, "tier": customer_tier}

    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

            if message["role"] == "assistant" and "metadata" in message:
                metadata = message["metadata"]

                with st.expander("ìƒì„¸ ì •ë³´"):
                    st.json(metadata)

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘..."):
                result = support_system.handle_customer_message(
                    st.session_state.session_id,
                    prompt,
                    customer_info
                )

            response = result["response"]
            st.markdown(response)

            # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì•Œë¦¼
            if result["escalation_needed"]["needed"]:
                st.warning(f"ğŸš¨ ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”: {result['escalation_needed']['reason']}")
                st.info(f"ë‹´ë‹¹íŒ€: {result['escalation_needed']['target']}")

            # ë©”íƒ€ë°ì´í„° í‘œì‹œ
            with st.expander("ë¶„ì„ ê²°ê³¼"):
                st.json({
                    "ì˜ë„": result["intent"]["intent"],
                    "ì‹ ë¢°ë„": result["intent"]["confidence"],
                    "ê¸´ê¸‰ë„": result["intent"]["urgency"],
                    "ì—ìŠ¤ì»¬ë ˆì´ì…˜": result["escalation_needed"]
                })

        # ì‘ë‹µ ì €ì¥
        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "metadata": result
        })

# ì•± ì‹¤í–‰
if __name__ == "__main__":
    create_support_app()
```

### 3. ì½”ë“œ ë¦¬ë·° ìë™í™” í”Œë«í¼

```python
import ast
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import git
from github import Github

class AutomatedCodeReviewPlatform:
    """ìë™í™”ëœ ì½”ë“œ ë¦¬ë·° í”Œë«í¼"""

    def __init__(self, github_token: str):
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0.1)  # ì½”ë“œ ë¶„ì„ìš© ëª¨ë¸
        self.github = Github(github_token)

        # ì½”ë“œ ë¶„ì„ ë„êµ¬ë“¤
        self.static_analyzers = {
            "python": ["flake8", "pylint", "black", "mypy"],
            "javascript": ["eslint", "prettier"],
            "java": ["checkstyle", "spotbugs"],
            "go": ["golint", "go vet"]
        }

        # ë¦¬ë·° í…œí”Œë¦¿
        self.review_templates = self._load_review_templates()

        # ë³´ì•ˆ ì·¨ì•½ì  íŒ¨í„´
        self.security_patterns = self._load_security_patterns()

    def _load_review_templates(self) -> Dict:
        """ë¦¬ë·° í…œí”Œë¦¿ ë¡œë“œ"""
        return {
            "code_quality": """
            ë‹¤ìŒ ì½”ë“œì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ê³  ê°œì„ ì‚¬í•­ì„ ì œì•ˆí•´ì£¼ì„¸ìš”:

            ì½”ë“œ:
            ```{language}
            {code}
            ```

            ë¶„ì„ í•­ëª©:
            1. ê°€ë…ì„± ë° ëª…ëª… ê·œì¹™
            2. ì½”ë“œ êµ¬ì¡° ë° ì„¤ê³„ íŒ¨í„´
            3. ì„±ëŠ¥ ìµœì í™” ê°€ëŠ¥ì„±
            4. í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ì„±
            5. ìœ ì§€ë³´ìˆ˜ì„±

            êµ¬ì²´ì ì¸ ê°œì„ ì‚¬í•­ê³¼ ìˆ˜ì •ëœ ì½”ë“œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
            """,

            "security_review": """
            ë‹¤ìŒ ì½”ë“œì˜ ë³´ì•ˆ ì·¨ì•½ì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

            ì½”ë“œ:
            ```{language}
            {code}
            ```

            í™•ì¸ í•­ëª©:
            1. ì…ë ¥ ê°’ ê²€ì¦
            2. SQL ì¸ì ì…˜ ê°€ëŠ¥ì„±
            3. XSS ì·¨ì•½ì 
            4. ì¸ì¦/ì¸ê°€ ì´ìŠˆ
            5. ë°ì´í„° ë…¸ì¶œ ìœ„í—˜
            6. ì•”í˜¸í™” ê´€ë ¨ ì´ìŠˆ

            ë°œê²¬ëœ ì·¨ì•½ì ê³¼ í•´ê²°ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.
            """,

            "performance_review": """
            ë‹¤ìŒ ì½”ë“œì˜ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ìµœì í™” ë°©ì•ˆì„ ì œì•ˆí•´ì£¼ì„¸ìš”:

            ì½”ë“œ:
            ```{language}
            {code}
            ```

            ë¶„ì„ í•­ëª©:
            1. ì‹œê°„ ë³µì¡ë„ ë¶„ì„
            2. ê³µê°„ ë³µì¡ë„ ë¶„ì„
            3. ë³‘ëª© êµ¬ê°„ ì‹ë³„
            4. ìºì‹± ìµœì í™”
            5. ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”
            6. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”

            êµ¬ì²´ì ì¸ ì„±ëŠ¥ ê°œì„  ì½”ë“œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
            """
        }

    def _load_security_patterns(self) -> Dict:
        """ë³´ì•ˆ ì·¨ì•½ì  íŒ¨í„´ ë¡œë“œ"""
        return {
            "python": [
                r"eval\s*\(",  # eval() ì‚¬ìš©
                r"exec\s*\(",  # exec() ì‚¬ìš©
                r"__import__\s*\(",  # ë™ì  import
                r"pickle\.loads?\s*\(",  # pickle ì—­ì§ë ¬í™”
                r"yaml\.load\s*\(",  # YAML ë¡œë“œ (ì•ˆì „í•˜ì§€ ì•Šì€)
                r"shell=True",  # ì…¸ ëª…ë ¹ ì‹¤í–‰
            ],
            "javascript": [
                r"eval\s*\(",  # eval() ì‚¬ìš©
                r"innerHTML\s*=",  # innerHTML ì§ì ‘ ì„¤ì •
                r"document\.write\s*\(",  # document.write ì‚¬ìš©
                r"setTimeout\s*\(\s*[\"']",  # setTimeout with string
            ],
            "sql": [
                r"SELECT\s+.*\+.*FROM",  # SQL ë¬¸ìì—´ ê²°í•©
                r"WHERE\s+.*\+",  # WHERE ì ˆ ë¬¸ìì—´ ê²°í•©
                r"'.*\+.*'",  # ë¬¸ìì—´ ê²°í•©ëœ ì¿¼ë¦¬
            ]
        }

    def review_pull_request(self, repo_name: str, pr_number: int) -> Dict:
        """Pull Request ìë™ ë¦¬ë·°"""

        # GitHubì—ì„œ PR ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        repo = self.github.get_repo(repo_name)
        pr = repo.get_pull(pr_number)

        # ë³€ê²½ëœ íŒŒì¼ë“¤ ë¶„ì„
        files = pr.get_files()
        review_results = []

        for file in files:
            if file.status in ["added", "modified"]:
                file_review = self._review_single_file(
                    file.filename,
                    file.patch,
                    file.raw_url
                )
                review_results.append(file_review)

        # ì „ì²´ì ì¸ PR ë¶„ì„
        overall_review = self._analyze_pr_overall(pr, review_results)

        # ë¦¬ë·° ê²°ê³¼ ì¢…í•©
        final_review = {
            "pr_number": pr_number,
            "title": pr.title,
            "files_reviewed": len(review_results),
            "overall_score": overall_review["score"],
            "recommendations": overall_review["recommendations"],
            "file_reviews": review_results,
            "approval_status": self._determine_approval_status(review_results)
        }

        # GitHubì— ë¦¬ë·° ì½”ë©˜íŠ¸ ì‘ì„±
        self._post_review_to_github(pr, final_review)

        return final_review

    def _review_single_file(self, filename: str, patch: str, raw_url: str) -> Dict:
        """ë‹¨ì¼ íŒŒì¼ ë¦¬ë·°"""

        # íŒŒì¼ ì–¸ì–´ ê°ì§€
        language = self._detect_language(filename)

        # ì „ì²´ íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        file_content = self._fetch_file_content(raw_url)

        # ì •ì  ë¶„ì„ ì‹¤í–‰
        static_analysis = self._run_static_analysis(filename, file_content, language)

        # AI ì½”ë“œ ë¦¬ë·° ì‹¤í–‰
        ai_reviews = {}

        # í’ˆì§ˆ ë¦¬ë·°
        ai_reviews["quality"] = self._ai_code_review(
            file_content, language, "code_quality"
        )

        # ë³´ì•ˆ ë¦¬ë·°
        ai_reviews["security"] = self._ai_code_review(
            file_content, language, "security_review"
        )

        # ì„±ëŠ¥ ë¦¬ë·°
        ai_reviews["performance"] = self._ai_code_review(
            file_content, language, "performance_review"
        )

        # ë³µì¡ë„ ë¶„ì„
        complexity_analysis = self._analyze_complexity(file_content, language)

        # í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        test_coverage = self._analyze_test_coverage(filename, file_content)

        return {
            "filename": filename,
            "language": language,
            "static_analysis": static_analysis,
            "ai_reviews": ai_reviews,
            "complexity": complexity_analysis,
            "test_coverage": test_coverage,
            "overall_score": self._calculate_file_score(static_analysis, ai_reviews, complexity_analysis),
            "critical_issues": self._extract_critical_issues(static_analysis, ai_reviews)
        }

    def _detect_language(self, filename: str) -> str:
        """íŒŒì¼ í™•ì¥ìë¡œ ì–¸ì–´ ê°ì§€"""
        suffix = Path(filename).suffix.lower()

        language_map = {
            ".py": "python",
            ".js": "javascript",
            ".ts": "typescript",
            ".java": "java",
            ".go": "go",
            ".cpp": "cpp",
            ".c": "c",
            ".cs": "csharp",
            ".rb": "ruby",
            ".php": "php"
        }

        return language_map.get(suffix, "unknown")

    def _fetch_file_content(self, raw_url: str) -> str:
        """ì›ê²© íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        import requests
        response = requests.get(raw_url)
        return response.text

    def _run_static_analysis(self, filename: str, content: str, language: str) -> Dict:
        """ì •ì  ë¶„ì„ ë„êµ¬ ì‹¤í–‰"""
        if language not in self.static_analyzers:
            return {"tools": [], "issues": []}

        issues = []
        tools_used = []

        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(mode='w', suffix=Path(filename).suffix, delete=False) as tmp_file:
            tmp_file.write(content)
            tmp_file_path = tmp_file.name

        try:
            for tool in self.static_analyzers[language]:
                try:
                    tool_issues = self._run_single_analyzer(tool, tmp_file_path, language)
                    issues.extend(tool_issues)
                    tools_used.append(tool)
                except Exception as e:
                    print(f"Error running {tool}: {e}")
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            Path(tmp_file_path).unlink()

        return {
            "tools": tools_used,
            "issues": issues
        }

    def _run_single_analyzer(self, tool: str, file_path: str, language: str) -> List[Dict]:
        """ê°œë³„ ì •ì  ë¶„ì„ ë„êµ¬ ì‹¤í–‰"""
        issues = []

        try:
            if tool == "flake8" and language == "python":
                result = subprocess.run(
                    ["flake8", file_path, "--format=json"],
                    capture_output=True, text=True
                )
                if result.stdout:
                    flake8_issues = json.loads(result.stdout)
                    for issue in flake8_issues:
                        issues.append({
                            "tool": "flake8",
                            "line": issue.get("line_number"),
                            "column": issue.get("column_number"),
                            "message": issue.get("text"),
                            "code": issue.get("code"),
                            "severity": "warning"
                        })

            elif tool == "pylint" and language == "python":
                result = subprocess.run(
                    ["pylint", file_path, "--output-format=json"],
                    capture_output=True, text=True
                )
                if result.stdout:
                    pylint_issues = json.loads(result.stdout)
                    for issue in pylint_issues:
                        issues.append({
                            "tool": "pylint",
                            "line": issue.get("line"),
                            "column": issue.get("column"),
                            "message": issue.get("message"),
                            "code": issue.get("message-id"),
                            "severity": issue.get("type")
                        })

        except Exception as e:
            print(f"Error running {tool}: {e}")

        return issues

    def _ai_code_review(self, content: str, language: str, review_type: str) -> Dict:
        """AI ê¸°ë°˜ ì½”ë“œ ë¦¬ë·°"""

        template = self.review_templates.get(review_type, self.review_templates["code_quality"])

        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.llm

        try:
            response = chain.invoke({
                "code": content[:4000],  # í† í° ì œí•œ ê³ ë ¤
                "language": language
            })

            return {
                "review_type": review_type,
                "analysis": response.content,
                "recommendations": self._extract_recommendations(response.content),
                "severity": self._assess_severity(response.content)
            }

        except Exception as e:
            return {
                "review_type": review_type,
                "analysis": f"Error during AI review: {e}",
                "recommendations": [],
                "severity": "unknown"
            }

    def _analyze_complexity(self, content: str, language: str) -> Dict:
        """ì½”ë“œ ë³µì¡ë„ ë¶„ì„"""

        if language == "python":
            return self._analyze_python_complexity(content)
        else:
            return {"complexity": "unknown", "metrics": {}}

    def _analyze_python_complexity(self, content: str) -> Dict:
        """Python ì½”ë“œ ë³µì¡ë„ ë¶„ì„"""
        try:
            tree = ast.parse(content)

            complexity_analyzer = PythonComplexityAnalyzer()
            complexity_analyzer.visit(tree)

            return {
                "cyclomatic_complexity": complexity_analyzer.cyclomatic_complexity,
                "function_count": complexity_analyzer.function_count,
                "class_count": complexity_analyzer.class_count,
                "max_nesting_depth": complexity_analyzer.max_nesting_depth,
                "lines_of_code": len(content.split('\n')),
                "complexity_score": complexity_analyzer.get_complexity_score()
            }

        except Exception as e:
            return {"error": str(e)}

    def _analyze_test_coverage(self, filename: str, content: str) -> Dict:
        """í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¶„ì„"""

        is_test_file = any(pattern in filename.lower() for pattern in ["test_", "_test", ".test."])

        if is_test_file:
            return {
                "is_test_file": True,
                "test_methods": self._count_test_methods(content),
                "assertions": self._count_assertions(content)
            }
        else:
            return {
                "is_test_file": False,
                "has_corresponding_test": self._check_test_file_exists(filename),
                "testable_functions": self._count_testable_functions(content)
            }

    def _calculate_file_score(self, static_analysis: Dict, ai_reviews: Dict, complexity: Dict) -> float:
        """íŒŒì¼ ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
        score = 100.0

        # ì •ì  ë¶„ì„ ì´ìŠˆë¡œ ì ìˆ˜ ì°¨ê°
        error_count = len([issue for issue in static_analysis.get("issues", [])
                          if issue.get("severity") == "error"])
        warning_count = len([issue for issue in static_analysis.get("issues", [])
                            if issue.get("severity") == "warning"])

        score -= (error_count * 10 + warning_count * 5)

        # ë³µì¡ë„ë¡œ ì ìˆ˜ ì°¨ê°
        cyclomatic = complexity.get("cyclomatic_complexity", 0)
        if cyclomatic > 10:
            score -= (cyclomatic - 10) * 2

        # AI ë¦¬ë·° ê²°ê³¼ë¡œ ì ìˆ˜ ì°¨ê°
        for review_type, review in ai_reviews.items():
            severity = review.get("severity", "low")
            if severity == "high":
                score -= 15
            elif severity == "medium":
                score -= 10
            elif severity == "low":
                score -= 5

        return max(0, score)

    def _post_review_to_github(self, pr, review_result: Dict):
        """GitHubì— ë¦¬ë·° ê²°ê³¼ í¬ìŠ¤íŒ…"""

        # ë¦¬ë·° ì½”ë©˜íŠ¸ ìƒì„±
        review_body = self._generate_review_comment(review_result)

        # PRì— ë¦¬ë·° ì‘ì„±
        pr.create_review(
            body=review_body,
            event="COMMENT"  # ë˜ëŠ” "APPROVE", "REQUEST_CHANGES"
        )

        # ì‹¬ê°í•œ ì´ìŠˆê°€ ìˆìœ¼ë©´ ê°œë³„ ì½”ë©˜íŠ¸ë„ ì¶”ê°€
        for file_review in review_result["file_reviews"]:
            if file_review["critical_issues"]:
                for issue in file_review["critical_issues"]:
                    pr.create_review_comment(
                        body=f"ğŸš¨ Critical Issue: {issue['message']}",
                        path=file_review["filename"],
                        line=issue.get("line", 1)
                    )

    def _generate_review_comment(self, review_result: Dict) -> str:
        """ë¦¬ë·° ì½”ë©˜íŠ¸ ìƒì„±"""

        comment = f"""
## ğŸ¤– ìë™ ì½”ë“œ ë¦¬ë·° ê²°ê³¼

### ğŸ“Š ì „ì²´ ìš”ì•½
- **ê²€í†  íŒŒì¼ ìˆ˜**: {review_result['files_reviewed']}
- **ì „ì²´ ì ìˆ˜**: {review_result['overall_score']:.1f}/100
- **ìŠ¹ì¸ ìƒíƒœ**: {review_result['approval_status']}

### ğŸ“‹ ì£¼ìš” ê¶Œì¥ì‚¬í•­
"""

        for rec in review_result['recommendations'][:5]:  # ìƒìœ„ 5ê°œë§Œ
            comment += f"- {rec}\n"

        comment += "\n### ğŸ“ íŒŒì¼ë³„ ìƒì„¸ ë¶„ì„\n"

        for file_review in review_result['file_reviews']:
            filename = file_review['filename']
            score = file_review['overall_score']
            critical_count = len(file_review['critical_issues'])

            comment += f"- **{filename}**: {score:.1f}/100"
            if critical_count > 0:
                comment += f" âš ï¸ {critical_count}ê°œ ì‹¬ê°í•œ ì´ìŠˆ"
            comment += "\n"

        comment += "\n---\n*ì´ ë¦¬ë·°ëŠ” AI ë„êµ¬ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ëŒì˜ ìµœì¢… ê²€í† ê°€ ê¶Œì¥ë©ë‹ˆë‹¤.*"

        return comment

class PythonComplexityAnalyzer(ast.NodeVisitor):
    """Python ì½”ë“œ ë³µì¡ë„ ë¶„ì„ê¸°"""

    def __init__(self):
        self.cyclomatic_complexity = 1  # ê¸°ë³¸ ë³µì¡ë„
        self.function_count = 0
        self.class_count = 0
        self.max_nesting_depth = 0
        self.current_nesting_depth = 0

    def visit_FunctionDef(self, node):
        self.function_count += 1
        self.current_nesting_depth += 1
        self.max_nesting_depth = max(self.max_nesting_depth, self.current_nesting_depth)

        # í•¨ìˆ˜ ë‚´ë¶€ì˜ ë³µì¡ë„ ì¦ê°€ ìš”ì†Œë“¤ ë°©ë¬¸
        self.generic_visit(node)

        self.current_nesting_depth -= 1

    def visit_ClassDef(self, node):
        self.class_count += 1
        self.generic_visit(node)

    def visit_If(self, node):
        self.cyclomatic_complexity += 1
        self.current_nesting_depth += 1
        self.max_nesting_depth = max(self.max_nesting_depth, self.current_nesting_depth)
        self.generic_visit(node)
        self.current_nesting_depth -= 1

    def visit_While(self, node):
        self.cyclomatic_complexity += 1
        self.current_nesting_depth += 1
        self.max_nesting_depth = max(self.max_nesting_depth, self.current_nesting_depth)
        self.generic_visit(node)
        self.current_nesting_depth -= 1

    def visit_For(self, node):
        self.cyclomatic_complexity += 1
        self.current_nesting_depth += 1
        self.max_nesting_depth = max(self.max_nesting_depth, self.current_nesting_depth)
        self.generic_visit(node)
        self.current_nesting_depth -= 1

    def visit_ExceptHandler(self, node):
        self.cyclomatic_complexity += 1
        self.generic_visit(node)

    def get_complexity_score(self) -> str:
        """ë³µì¡ë„ ì ìˆ˜ ë°˜í™˜"""
        if self.cyclomatic_complexity <= 5:
            return "low"
        elif self.cyclomatic_complexity <= 10:
            return "medium"
        else:
            return "high"

# ì‚¬ìš© ì˜ˆì‹œ
def main():
    # GitHub í† í° ì„¤ì •
    github_token = "your_github_token_here"

    # ì½”ë“œ ë¦¬ë·° í”Œë«í¼ ì´ˆê¸°í™”
    review_platform = AutomatedCodeReviewPlatform(github_token)

    # Pull Request ë¦¬ë·° ì‹¤í–‰
    repo_name = "organization/repository"
    pr_number = 123

    review_result = review_platform.review_pull_request(repo_name, pr_number)

    # ê²°ê³¼ ì¶œë ¥
    print(f"ë¦¬ë·° ì™„ë£Œ: {review_result['overall_score']:.1f}/100")
    print(f"ìŠ¹ì¸ ìƒíƒœ: {review_result['approval_status']}")

    for file_review in review_result['file_reviews']:
        print(f"\níŒŒì¼: {file_review['filename']}")
        print(f"ì ìˆ˜: {file_review['overall_score']:.1f}/100")

        if file_review['critical_issues']:
            print("ì‹¬ê°í•œ ì´ìŠˆ:")
            for issue in file_review['critical_issues']:
                print(f"  - {issue['message']} (ë¼ì¸ {issue.get('line', '?')})")

if __name__ == "__main__":
    main()
```

### 4. ê°œì¸í™”ëœ í•™ìŠµ ë„ìš°ë¯¸

```python
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import numpy as np
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import seaborn as sns

class PersonalizedLearningAssistant:
    """ê°œì¸í™”ëœ í•™ìŠµ ë„ìš°ë¯¸"""

    def __init__(self, db_path: str = "learning_assistant.db"):
        self.llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.5)
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.db_path = db_path

        # í•™ìŠµ ìŠ¤íƒ€ì¼ ë¶„ë¥˜ê¸°
        self.learning_style_classifier = self._setup_learning_style_classifier()

        # ì§€ì‹ ê·¸ë˜í”„
        self.knowledge_graph = self._build_knowledge_graph()

        # ê°œë…ë³„ ë²¡í„° ìŠ¤í† ì–´
        self.concept_vectorstores = {}

        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._init_database()

    def _init_database(self):
        """í•™ìŠµ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # í•™ìŠµì í”„ë¡œí•„ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learner_profiles (
                user_id TEXT PRIMARY KEY,
                learning_style TEXT,
                knowledge_level TEXT,
                preferred_pace TEXT,
                strengths TEXT,
                weaknesses TEXT,
                goals TEXT,
                created_at TIMESTAMP,
                updated_at TIMESTAMP
            )
        ''')

        # í•™ìŠµ ì„¸ì…˜ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learning_sessions (
                session_id TEXT PRIMARY KEY,
                user_id TEXT,
                concept TEXT,
                session_type TEXT,
                duration_minutes INTEGER,
                performance_score REAL,
                difficulty_level INTEGER,
                questions_asked INTEGER,
                correct_answers INTEGER,
                session_date TIMESTAMP,
                notes TEXT
            )
        ''')

        # ê°œë… ê´€ê³„ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS concept_relationships (
                concept_a TEXT,
                concept_b TEXT,
                relationship_type TEXT,
                strength REAL,
                PRIMARY KEY (concept_a, concept_b, relationship_type)
            )
        ''')

        # í•™ìŠµ ê²½ë¡œ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learning_paths (
                path_id TEXT PRIMARY KEY,
                user_id TEXT,
                path_name TEXT,
                concepts TEXT,
                estimated_duration INTEGER,
                difficulty_progression TEXT,
                created_at TIMESTAMP
            )
        ''')

        conn.commit()
        conn.close()

    def _setup_learning_style_classifier(self):
        """í•™ìŠµ ìŠ¤íƒ€ì¼ ë¶„ë¥˜ê¸° ì„¤ì •"""

        learning_style_prompt = ChatPromptTemplate.from_template("""
        ë‹¤ìŒ í•™ìŠµìì˜ ì‘ë‹µê³¼ í–‰ë™ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ í•™ìŠµ ìŠ¤íƒ€ì¼ì„ íŒë‹¨í•´ì£¼ì„¸ìš”:

        í•™ìŠµì ì •ë³´:
        - ì§ˆë¬¸ ì‘ë‹µ: {responses}
        - ì„ í˜¸í•˜ëŠ” í•™ìŠµ ë°©ì‹: {preferences}
        - ê³¼ê±° ì„±ê³¼ íŒ¨í„´: {performance_history}

        í•™ìŠµ ìŠ¤íƒ€ì¼ ë¶„ë¥˜:
        1. Visual (ì‹œê°ì ): ë‹¤ì´ì–´ê·¸ë¨, ì°¨íŠ¸, ì´ë¯¸ì§€ ì„ í˜¸
        2. Auditory (ì²­ê°ì ): ì„¤ëª…, í† ë¡ , ìŒì„± í•™ìŠµ ì„ í˜¸
        3. Kinesthetic (ì²´ê°ê°ì ): ì‹¤ìŠµ, ì‹¤í—˜, ì†ìœ¼ë¡œ í•˜ëŠ” í™œë™ ì„ í˜¸
        4. Reading/Writing (ì½ê¸°/ì“°ê¸°): í…ìŠ¤íŠ¸ ê¸°ë°˜ í•™ìŠµ ì„ í˜¸

        ë¶„ì„ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
        {{
            "primary_style": "ì£¼ìš” í•™ìŠµ ìŠ¤íƒ€ì¼",
            "secondary_style": "ë³´ì¡° í•™ìŠµ ìŠ¤íƒ€ì¼",
            "confidence": "ì‹ ë¢°ë„ (0.0-1.0)",
            "recommendations": ["ì¶”ì²œ í•™ìŠµ ë°©ë²• ë¦¬ìŠ¤íŠ¸"]
        }}
        """)

        return learning_style_prompt | self.llm

    def _build_knowledge_graph(self) -> Dict:
        """ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•"""

        # ê¸°ë³¸ ê°œë…ë“¤ê³¼ ê´€ê³„ ì •ì˜
        knowledge_graph = {
            "programming": {
                "prerequisites": [],
                "subtopics": ["variables", "functions", "loops", "conditionals", "data_structures"],
                "difficulty": 2,
                "estimated_hours": 40
            },
            "variables": {
                "prerequisites": [],
                "subtopics": ["data_types", "scope", "naming_conventions"],
                "difficulty": 1,
                "estimated_hours": 4
            },
            "functions": {
                "prerequisites": ["variables"],
                "subtopics": ["parameters", "return_values", "recursion", "lambdas"],
                "difficulty": 2,
                "estimated_hours": 8
            },
            "data_structures": {
                "prerequisites": ["variables", "functions"],
                "subtopics": ["lists", "dictionaries", "sets", "tuples"],
                "difficulty": 3,
                "estimated_hours": 12
            },
            "machine_learning": {
                "prerequisites": ["programming", "statistics", "linear_algebra"],
                "subtopics": ["supervised_learning", "unsupervised_learning", "deep_learning"],
                "difficulty": 4,
                "estimated_hours": 80
            }
        }

        return knowledge_graph

    def create_learner_profile(self, user_id: str, initial_assessment: Dict) -> Dict:
        """í•™ìŠµì í”„ë¡œí•„ ìƒì„±"""

        # í•™ìŠµ ìŠ¤íƒ€ì¼ ë¶„ì„
        style_result = self.learning_style_classifier.invoke({
            "responses": initial_assessment.get("responses", ""),
            "preferences": initial_assessment.get("preferences", ""),
            "performance_history": initial_assessment.get("performance_history", "")
        })

        try:
            style_analysis = json.loads(style_result.content)
        except:
            style_analysis = {
                "primary_style": "Visual",
                "secondary_style": "Reading/Writing",
                "confidence": 0.7,
                "recommendations": ["ë‹¤ì–‘í•œ í•™ìŠµ ë°©ë²• ì‹œë„"]
            }

        # ì§€ì‹ ìˆ˜ì¤€ í‰ê°€
        knowledge_level = self._assess_knowledge_level(user_id, initial_assessment)

        # í•™ìŠµ ëª©í‘œ ì„¤ì •
        learning_goals = self._set_learning_goals(initial_assessment, knowledge_level)

        # í”„ë¡œí•„ ì €ì¥
        profile = {
            "user_id": user_id,
            "learning_style": style_analysis["primary_style"],
            "knowledge_level": knowledge_level,
            "preferred_pace": initial_assessment.get("preferred_pace", "medium"),
            "strengths": json.dumps(initial_assessment.get("strengths", [])),
            "weaknesses": json.dumps(initial_assessment.get("weaknesses", [])),
            "goals": json.dumps(learning_goals),
            "created_at": datetime.now(),
            "updated_at": datetime.now()
        }

        self._save_learner_profile(profile)

        return profile

    def generate_personalized_lesson(self, user_id: str, concept: str) -> Dict:
        """ê°œì¸í™”ëœ ë ˆìŠ¨ ìƒì„±"""

        # í•™ìŠµì í”„ë¡œí•„ ë¡œë“œ
        profile = self._get_learner_profile(user_id)

        # ê°œë… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        concept_info = self.knowledge_graph.get(concept, {})

        # ì„ ìˆ˜ ì§€ì‹ í™•ì¸
        prerequisites_met = self._check_prerequisites(user_id, concept)

        if not prerequisites_met["all_met"]:
            return {
                "status": "prerequisites_needed",
                "missing_prerequisites": prerequisites_met["missing"],
                "recommended_path": self._generate_prerequisite_path(
                    user_id, prerequisites_met["missing"]
                )
            }

        # í˜„ì¬ ì§€ì‹ ìˆ˜ì¤€ í‰ê°€
        current_knowledge = self._assess_concept_knowledge(user_id, concept)

        # í•™ìŠµ ìŠ¤íƒ€ì¼ì— ë§ëŠ” ë ˆìŠ¨ ìƒì„±
        lesson_content = self._create_adaptive_lesson(
            concept, profile, current_knowledge, concept_info
        )

        # ì—°ìŠµ ë¬¸ì œ ìƒì„±
        practice_problems = self._generate_practice_problems(
            concept, current_knowledge["level"], profile["learning_style"]
        )

        # í•™ìŠµ ì„¸ì…˜ ê¸°ë¡ ì¤€ë¹„
        session_id = self._create_learning_session(user_id, concept)

        return {
            "status": "ready",
            "session_id": session_id,
            "concept": concept,
            "lesson_content": lesson_content,
            "practice_problems": practice_problems,
            "estimated_duration": concept_info.get("estimated_hours", 2),
            "difficulty_level": current_knowledge["level"],
            "learning_objectives": self._define_learning_objectives(concept, current_knowledge)
        }

    def _create_adaptive_lesson(self, concept: str, profile: Dict,
                              current_knowledge: Dict, concept_info: Dict) -> Dict:
        """ì ì‘í˜• ë ˆìŠ¨ ì½˜í…ì¸  ìƒì„±"""

        learning_style = profile["learning_style"]
        knowledge_level = current_knowledge["level"]

        # í•™ìŠµ ìŠ¤íƒ€ì¼ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        style_prompts = {
            "Visual": """
            {concept}ì— ëŒ€í•œ ì‹œê°ì  í•™ìŠµìë¥¼ ìœ„í•œ ë ˆìŠ¨ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

            í•™ìŠµì ì •ë³´:
            - í˜„ì¬ ì§€ì‹ ìˆ˜ì¤€: {knowledge_level}/5
            - ê°œë…: {concept}
            - í•™ìŠµ ëª©í‘œ: {objectives}

            ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. ê°œë…ì„ ì„¤ëª…í•˜ëŠ” ë‹¤ì´ì–´ê·¸ë¨ ë˜ëŠ” ì°¨íŠ¸ ì œì•ˆ
            2. ì‹œê°ì  ì˜ˆì‹œì™€ ë¹„ìœ 
            3. ë‹¨ê³„ë³„ ì‹œê°ì  í”„ë¡œì„¸ìŠ¤
            4. ì¸í¬ê·¸ë˜í”½ ìŠ¤íƒ€ì¼ì˜ ìš”ì•½
            5. ë§ˆì¸ë“œë§µ êµ¬ì¡° ì œì•ˆ

            ì‹¤ì œ ì‹œê° ìë£Œë¥¼ ìƒì„±í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, ìì„¸í•œ ì„¤ëª…ê³¼ ASCII ì•„íŠ¸ë¥¼ í™œìš©í•´ì£¼ì„¸ìš”.
            """,

            "Auditory": """
            {concept}ì— ëŒ€í•œ ì²­ê°ì  í•™ìŠµìë¥¼ ìœ„í•œ ë ˆìŠ¨ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

            í•™ìŠµì ì •ë³´:
            - í˜„ì¬ ì§€ì‹ ìˆ˜ì¤€: {knowledge_level}/5
            - ê°œë…: {concept}
            - í•™ìŠµ ëª©í‘œ: {objectives}

            ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. ëŒ€í™”í˜• ì„¤ëª… (ì§ˆë¬¸ê³¼ ë‹µë³€ í˜•ì‹)
            2. ìš´ìœ¨ì´ ìˆëŠ” ê¸°ì–µë²•
            3. ìŠ¤í† ë¦¬í…”ë§ì„ í†µí•œ ê°œë… ì„¤ëª…
            4. í† ë¡  ì£¼ì œ ì œì•ˆ
            5. "ì†Œë¦¬ë‚´ì–´ ìƒê°í•˜ê¸°" ì—°ìŠµ

            ì„¤ëª…ì€ ë§ˆì¹˜ ê°•ì˜ë¥¼ ë“£ëŠ” ê²ƒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            """,

            "Kinesthetic": """
            {concept}ì— ëŒ€í•œ ì²´ê°ê°ì  í•™ìŠµìë¥¼ ìœ„í•œ ë ˆìŠ¨ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

            í•™ìŠµì ì •ë³´:
            - í˜„ì¬ ì§€ì‹ ìˆ˜ì¤€: {knowledge_level}/5
            - ê°œë…: {concept}
            - í•™ìŠµ ëª©í‘œ: {objectives}

            ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. ì‹¤ìŠµ í™œë™ê³¼ ì½”ë”© ì˜ˆì œ
            2. ë‹¨ê³„ë³„ ë”°ë¼í•˜ê¸° íŠœí† ë¦¬ì–¼
            3. ì‹¤í—˜ê³¼ ì‹œí–‰ì°©ì˜¤ í•™ìŠµ
            4. í”„ë¡œì íŠ¸ ê¸°ë°˜ í•™ìŠµ ì œì•ˆ
            5. ëª¸ì§“ì´ë‚˜ ë™ì‘ì„ í™œìš©í•œ ê¸°ì–µë²•

            "ì§ì ‘ í•´ë³´ê¸°"ì— ì¤‘ì ì„ ë‘” ì‹¤ìš©ì ì¸ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
            """,

            "Reading/Writing": """
            {concept}ì— ëŒ€í•œ ì½ê¸°/ì“°ê¸° í•™ìŠµìë¥¼ ìœ„í•œ ë ˆìŠ¨ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

            í•™ìŠµì ì •ë³´:
            - í˜„ì¬ ì§€ì‹ ìˆ˜ì¤€: {knowledge_level}/5
            - ê°œë…: {concept}
            - í•™ìŠµ ëª©í‘œ: {objectives}

            ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì„¤ëª…
            2. í•µì‹¬ ìš©ì–´ ì •ì˜ì™€ ìš©ì–´ì§‘
            3. ìš”ì•½ ì •ë¦¬ì™€ ì²´í¬ë¦¬ìŠ¤íŠ¸
            4. ë…¸íŠ¸ ì‘ì„± ê°€ì´ë“œ
            5. ì¶”ê°€ ì½ê¸° ìë£Œ ì¶”ì²œ

            ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ í…ìŠ¤íŠ¸ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            """
        }

        # ì„ íƒëœ í•™ìŠµ ìŠ¤íƒ€ì¼ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        prompt_template = style_prompts.get(learning_style, style_prompts["Visual"])

        lesson_prompt = ChatPromptTemplate.from_template(prompt_template)
        lesson_chain = lesson_prompt | self.llm

        objectives = self._define_learning_objectives(concept, current_knowledge)

        lesson_response = lesson_chain.invoke({
            "concept": concept,
            "knowledge_level": knowledge_level,
            "objectives": objectives
        })

        return {
            "content": lesson_response.content,
            "style": learning_style,
            "difficulty": knowledge_level,
            "estimated_reading_time": len(lesson_response.content.split()) // 200  # ë¶„ ë‹¨ìœ„
        }

    def _generate_practice_problems(self, concept: str, difficulty_level: int,
                                  learning_style: str) -> List[Dict]:
        """ì—°ìŠµ ë¬¸ì œ ìƒì„±"""

        problem_prompt = ChatPromptTemplate.from_template("""
        {concept}ì— ëŒ€í•œ ì—°ìŠµ ë¬¸ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

        ìš”êµ¬ì‚¬í•­:
        - ë‚œì´ë„: {difficulty_level}/5
        - í•™ìŠµ ìŠ¤íƒ€ì¼: {learning_style}
        - ë¬¸ì œ ê°œìˆ˜: 5ê°œ

        ê° ë¬¸ì œëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

        ë¬¸ì œ 1:
        [ë¬¸ì œ ì„¤ëª…]

        ì„ íƒì§€ (ê°ê´€ì‹ì¸ ê²½ìš°):
        A) ì„ íƒì§€ 1
        B) ì„ íƒì§€ 2
        C) ì„ íƒì§€ 3
        D) ì„ íƒì§€ 4

        ì •ë‹µ: [ì •ë‹µ]
        í•´ì„¤: [ìƒì„¸ í•´ì„¤]

        ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë¬¸ì œ (ê°ê´€ì‹, ì£¼ê´€ì‹, ì‹¤ìŠµ)ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
        """)

        problem_chain = problem_prompt | self.llm

        problems_response = problem_chain.invoke({
            "concept": concept,
            "difficulty_level": difficulty_level,
            "learning_style": learning_style
        })

        # ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ë¬¸ì œë³„ë¡œ ë¶„ë¦¬
        problems = self._parse_problems(problems_response.content)

        return problems

    def evaluate_learning_session(self, session_id: str, answers: List[Dict],
                                time_spent: int) -> Dict:
        """í•™ìŠµ ì„¸ì…˜ í‰ê°€"""

        # ì„¸ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        session_info = self._get_session_info(session_id)

        # ë‹µì•ˆ ì±„ì 
        grading_results = self._grade_answers(answers, session_info["concept"])

        # ì„±ê³¼ ë¶„ì„
        performance_analysis = self._analyze_performance(grading_results, time_spent)

        # ê°œì¸í™”ëœ í”¼ë“œë°± ìƒì„±
        feedback = self._generate_personalized_feedback(
            session_info["user_id"], session_info["concept"],
            grading_results, performance_analysis
        )

        # ë‹¤ìŒ í•™ìŠµ ì¶”ì²œ
        next_recommendations = self._recommend_next_steps(
            session_info["user_id"], session_info["concept"], performance_analysis
        )

        # ì„¸ì…˜ ê²°ê³¼ ì €ì¥
        self._save_session_results(session_id, grading_results, performance_analysis)

        # í•™ìŠµì í”„ë¡œí•„ ì—…ë°ì´íŠ¸
        self._update_learner_progress(session_info["user_id"], session_info["concept"], performance_analysis)

        return {
            "session_id": session_id,
            "score": grading_results["score"],
            "correct_answers": grading_results["correct_count"],
            "total_questions": grading_results["total_count"],
            "time_spent_minutes": time_spent,
            "performance_level": performance_analysis["level"],
            "strengths": performance_analysis["strengths"],
            "areas_for_improvement": performance_analysis["weaknesses"],
            "feedback": feedback,
            "next_recommendations": next_recommendations,
            "achievement_unlocked": self._check_achievements(session_info["user_id"], performance_analysis)
        }

    def generate_learning_analytics(self, user_id: str, timeframe: str = "last_month") -> Dict:
        """í•™ìŠµ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„±"""

        # ì‹œê°„ ë²”ìœ„ ì„¤ì •
        end_date = datetime.now()
        if timeframe == "last_week":
            start_date = end_date - timedelta(weeks=1)
        elif timeframe == "last_month":
            start_date = end_date - timedelta(days=30)
        elif timeframe == "last_quarter":
            start_date = end_date - timedelta(days=90)
        else:
            start_date = end_date - timedelta(days=30)

        # í•™ìŠµ ì„¸ì…˜ ë°ì´í„° ìˆ˜ì§‘
        sessions_data = self._get_sessions_data(user_id, start_date, end_date)

        # ì„±ê³¼ ë¶„ì„
        performance_analytics = self._calculate_performance_metrics(sessions_data)

        # í•™ìŠµ íŒ¨í„´ ë¶„ì„
        learning_patterns = self._analyze_learning_patterns(sessions_data)

        # ê°œë…ë³„ ìˆ™ë ¨ë„ ë¶„ì„
        concept_mastery = self._analyze_concept_mastery(user_id, sessions_data)

        # í•™ìŠµ ëª©í‘œ ì§„ë„ ì¶”ì 
        goal_progress = self._track_goal_progress(user_id)

        # ì‹œê°í™” ë°ì´í„° ì¤€ë¹„
        visualizations = self._prepare_visualizations(sessions_data, performance_analytics)

        return {
            "timeframe": timeframe,
            "summary": {
                "total_sessions": len(sessions_data),
                "total_study_time": sum(s["duration_minutes"] for s in sessions_data),
                "average_score": performance_analytics["average_score"],
                "improvement_rate": performance_analytics["improvement_rate"]
            },
            "performance_trends": performance_analytics["trends"],
            "learning_patterns": learning_patterns,
            "concept_mastery": concept_mastery,
            "goal_progress": goal_progress,
            "visualizations": visualizations,
            "recommendations": self._generate_analytics_recommendations(
                performance_analytics, learning_patterns, concept_mastery
            )
        }

    def create_adaptive_learning_path(self, user_id: str, target_concept: str,
                                    deadline: Optional[datetime] = None) -> Dict:
        """ì ì‘í˜• í•™ìŠµ ê²½ë¡œ ìƒì„±"""

        # í˜„ì¬ ì§€ì‹ ìƒíƒœ í‰ê°€
        current_state = self._assess_comprehensive_knowledge(user_id)

        # ëª©í‘œ ê°œë…ê¹Œì§€ì˜ ê²½ë¡œ ê³„ì‚°
        learning_path = self._calculate_optimal_path(current_state, target_concept)

        # ê°œì¸í™”ëœ ì¼ì • ìƒì„±
        schedule = self._generate_personalized_schedule(
            user_id, learning_path, deadline
        )

        # ë§ˆì¼ìŠ¤í†¤ ì„¤ì •
        milestones = self._set_learning_milestones(learning_path, schedule)

        # í•™ìŠµ ê²½ë¡œ ì €ì¥
        path_id = self._save_learning_path(user_id, learning_path, schedule, milestones)

        return {
            "path_id": path_id,
            "target_concept": target_concept,
            "total_concepts": len(learning_path["concepts"]),
            "estimated_duration_weeks": schedule["total_weeks"],
            "daily_study_time_minutes": schedule["daily_minutes"],
            "learning_path": learning_path,
            "schedule": schedule,
            "milestones": milestones,
            "success_probability": self._predict_success_probability(user_id, learning_path)
        }

# Streamlit ì›¹ ì•±
def create_learning_assistant_app():
    st.set_page_config(
        page_title="AI í•™ìŠµ ë„ìš°ë¯¸",
        page_icon="ğŸ“",
        layout="wide"
    )

    assistant = PersonalizedLearningAssistant()

    # ì‚¬ì´ë“œë°” - í•™ìŠµì ì •ë³´
    with st.sidebar:
        st.title("ğŸ‘¤ í•™ìŠµì í”„ë¡œí•„")

        user_id = st.text_input("ì‚¬ìš©ì ID", value="learner_001")

        # í”„ë¡œí•„ì´ ì—†ìœ¼ë©´ ì´ˆê¸° í‰ê°€ ì§„í–‰
        profile = assistant._get_learner_profile(user_id)

        if not profile:
            st.warning("í”„ë¡œí•„ì„ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.")

            with st.expander("ì´ˆê¸° í‰ê°€"):
                preferred_pace = st.selectbox("ì„ í˜¸í•˜ëŠ” í•™ìŠµ ì†ë„", ["slow", "medium", "fast"])
                learning_goals = st.multiselect("í•™ìŠµ ëª©í‘œ", [
                    "programming", "machine_learning", "data_science",
                    "web_development", "mobile_development"
                ])
                strengths = st.multiselect("ê°•ì  ë¶„ì•¼", [
                    "logical_thinking", "creativity", "problem_solving",
                    "attention_to_detail", "persistence"
                ])

                if st.button("í”„ë¡œí•„ ìƒì„±"):
                    initial_assessment = {
                        "preferred_pace": preferred_pace,
                        "goals": learning_goals,
                        "strengths": strengths,
                        "responses": "ê¸°ì´ˆë¶€í„° ì°¨ê·¼ì°¨ê·¼ ë°°ìš°ê³  ì‹¶ìŠµë‹ˆë‹¤.",
                        "preferences": "ì‹¤ìŠµ ìœ„ì£¼ì˜ í•™ìŠµì„ ì„ í˜¸í•©ë‹ˆë‹¤."
                    }

                    profile = assistant.create_learner_profile(user_id, initial_assessment)
                    st.success("í”„ë¡œí•„ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.rerun()

        else:
            st.success(f"í•™ìŠµ ìŠ¤íƒ€ì¼: {profile['learning_style']}")
            st.info(f"ì§€ì‹ ìˆ˜ì¤€: {profile['knowledge_level']}")

    # ë©”ì¸ í™”ë©´
    if profile:
        # íƒ­ êµ¬ì„±
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“š í•™ìŠµí•˜ê¸°", "ğŸ“Š ë¶„ì„", "ğŸ¯ í•™ìŠµ ê²½ë¡œ", "ğŸ† ì„±ê³¼"])

        with tab1:
            st.title("ğŸ“š ê°œì¸í™”ëœ í•™ìŠµ")

            # ê°œë… ì„ íƒ
            concept = st.selectbox("í•™ìŠµí•  ê°œë… ì„ íƒ", [
                "programming", "variables", "functions", "data_structures",
                "machine_learning", "statistics"
            ])

            if st.button("ë ˆìŠ¨ ì‹œì‘"):
                with st.spinner("ê°œì¸í™”ëœ ë ˆìŠ¨ ìƒì„± ì¤‘..."):
                    lesson = assistant.generate_personalized_lesson(user_id, concept)

                if lesson["status"] == "prerequisites_needed":
                    st.warning("ì„ ìˆ˜ ì§€ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    st.write("ë¶€ì¡±í•œ ê°œë…:", lesson["missing_prerequisites"])

                    if st.button("ì„ ìˆ˜ ê³¼ì • ì‹œì‘"):
                        for prereq in lesson["missing_prerequisites"]:
                            prereq_lesson = assistant.generate_personalized_lesson(user_id, prereq)
                            st.write(f"## {prereq}")
                            st.write(prereq_lesson["lesson_content"]["content"])

                else:
                    # ë ˆìŠ¨ í‘œì‹œ
                    st.write(f"## {concept} í•™ìŠµ")
                    st.write(lesson["lesson_content"]["content"])

                    # ì—°ìŠµ ë¬¸ì œ
                    if lesson["practice_problems"]:
                        st.write("### ì—°ìŠµ ë¬¸ì œ")

                        answers = []
                        for i, problem in enumerate(lesson["practice_problems"]):
                            st.write(f"**ë¬¸ì œ {i+1}:**")
                            st.write(problem["question"])

                            if problem["type"] == "multiple_choice":
                                answer = st.radio(
                                    f"ì„ íƒí•˜ì„¸ìš” (ë¬¸ì œ {i+1})",
                                    problem["choices"],
                                    key=f"q_{i}"
                                )
                                answers.append({"question_id": i, "answer": answer})

                            elif problem["type"] == "short_answer":
                                answer = st.text_input(
                                    f"ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš” (ë¬¸ì œ {i+1})",
                                    key=f"q_{i}"
                                )
                                answers.append({"question_id": i, "answer": answer})

                        if st.button("ë‹µì•ˆ ì œì¶œ"):
                            time_spent = 30  # ì‹¤ì œë¡œëŠ” íƒ€ì´ë¨¸ êµ¬í˜„ í•„ìš”

                            with st.spinner("ë‹µì•ˆ ì±„ì  ì¤‘..."):
                                evaluation = assistant.evaluate_learning_session(
                                    lesson["session_id"], answers, time_spent
                                )

                            # ê²°ê³¼ í‘œì‹œ
                            st.write("### í•™ìŠµ ê²°ê³¼")

                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("ì ìˆ˜", f"{evaluation['score']:.1f}%")
                            with col2:
                                st.metric("ì •ë‹µ", f"{evaluation['correct_answers']}/{evaluation['total_questions']}")
                            with col3:
                                st.metric("ì†Œìš” ì‹œê°„", f"{evaluation['time_spent_minutes']}ë¶„")

                            # í”¼ë“œë°±
                            st.write("### ê°œì¸í™”ëœ í”¼ë“œë°±")
                            st.write(evaluation["feedback"])

                            # ë‹¤ìŒ í•™ìŠµ ì¶”ì²œ
                            st.write("### ë‹¤ìŒ í•™ìŠµ ì¶”ì²œ")
                            for rec in evaluation["next_recommendations"]:
                                st.write(f"- {rec}")

        with tab2:
            st.title("ğŸ“Š í•™ìŠµ ë¶„ì„")

            # ë¶„ì„ ê¸°ê°„ ì„ íƒ
            timeframe = st.selectbox("ë¶„ì„ ê¸°ê°„", [
                "last_week", "last_month", "last_quarter"
            ])

            if st.button("ë¶„ì„ ìƒì„±"):
                with st.spinner("í•™ìŠµ ë°ì´í„° ë¶„ì„ ì¤‘..."):
                    analytics = assistant.generate_learning_analytics(user_id, timeframe)

                # ìš”ì•½ ë©”íŠ¸ë¦­
                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    st.metric("ì´ í•™ìŠµ ì„¸ì…˜", analytics["summary"]["total_sessions"])
                with col2:
                    st.metric("ì´ í•™ìŠµ ì‹œê°„", f"{analytics['summary']['total_study_time']}ë¶„")
                with col3:
                    st.metric("í‰ê·  ì ìˆ˜", f"{analytics['summary']['average_score']:.1f}%")
                with col4:
                    st.metric("í–¥ìƒë¥ ", f"{analytics['summary']['improvement_rate']:.1f}%")

                # ì„±ê³¼ íŠ¸ë Œë“œ
                if analytics["performance_trends"]:
                    st.subheader("ì„±ê³¼ íŠ¸ë Œë“œ")
                    st.line_chart(analytics["performance_trends"])

                # ê°œë…ë³„ ìˆ™ë ¨ë„
                if analytics["concept_mastery"]:
                    st.subheader("ê°œë…ë³„ ìˆ™ë ¨ë„")
                    st.bar_chart(analytics["concept_mastery"])

                # í•™ìŠµ íŒ¨í„´
                st.subheader("í•™ìŠµ íŒ¨í„´")
                st.write(analytics["learning_patterns"])

                # ì¶”ì²œì‚¬í•­
                st.subheader("ê°œì„  ì¶”ì²œì‚¬í•­")
                for rec in analytics["recommendations"]:
                    st.write(f"- {rec}")

        with tab3:
            st.title("ğŸ¯ ê°œì¸í™”ëœ í•™ìŠµ ê²½ë¡œ")

            # ëª©í‘œ ì„¤ì •
            target_concept = st.selectbox("í•™ìŠµ ëª©í‘œ", [
                "machine_learning", "web_development", "data_science",
                "mobile_development", "devops"
            ])

            deadline = st.date_input("ëª©í‘œ ì™„ë£Œ ì¼ì (ì„ íƒì‚¬í•­)")

            if st.button("í•™ìŠµ ê²½ë¡œ ìƒì„±"):
                with st.spinner("ìµœì  í•™ìŠµ ê²½ë¡œ ê³„ì‚° ì¤‘..."):
                    learning_path = assistant.create_adaptive_learning_path(
                        user_id, target_concept, deadline
                    )

                # ê²½ë¡œ ì •ë³´
                col1, col2, col3 = st.columns(3)

                with col1:
                    st.metric("ì´ ê°œë… ìˆ˜", learning_path["total_concepts"])
                with col2:
                    st.metric("ì˜ˆìƒ ê¸°ê°„", f"{learning_path['estimated_duration_weeks']}ì£¼")
                with col3:
                    st.metric("ì¼ì¼ í•™ìŠµ ì‹œê°„", f"{learning_path['daily_study_time_minutes']}ë¶„")

                # ì„±ê³µ ì˜ˆì¸¡
                success_prob = learning_path["success_probability"]
                st.progress(success_prob)
                st.write(f"ì„±ê³µ í™•ë¥ : {success_prob:.1%}")

                # í•™ìŠµ ê²½ë¡œ ì‹œê°í™”
                st.subheader("í•™ìŠµ ê²½ë¡œ")

                for i, concept in enumerate(learning_path["learning_path"]["concepts"], 1):
                    st.write(f"{i}. **{concept}** ({learning_path['learning_path']['durations'][i-1]}ì‹œê°„)")

                # ë§ˆì¼ìŠ¤í†¤
                st.subheader("ë§ˆì¼ìŠ¤í†¤")

                for milestone in learning_path["milestones"]:
                    st.write(f"ğŸ“… {milestone['date']} - {milestone['title']}")
                    st.write(f"   ëª©í‘œ: {milestone['goal']}")

        with tab4:
            st.title("ğŸ† í•™ìŠµ ì„±ê³¼")

            # ë°°ì§€ ì‹œìŠ¤í…œ (ì˜ˆì‹œ)
            achievements = [
                {"name": "ì²« ê±¸ìŒ", "description": "ì²« í•™ìŠµ ì„¸ì…˜ ì™„ë£Œ", "earned": True},
                {"name": "ê¾¸ì¤€í•¨", "description": "7ì¼ ì—°ì† í•™ìŠµ", "earned": False},
                {"name": "ì™„ë²½ì£¼ì˜ì", "description": "100% ì ìˆ˜ ë‹¬ì„±", "earned": True},
                {"name": "íƒí—˜ê°€", "description": "5ê°œ ì´ìƒ ê°œë… í•™ìŠµ", "earned": False}
            ]

            st.subheader("íšë“í•œ ë°°ì§€")

            cols = st.columns(4)
            for i, achievement in enumerate(achievements):
                with cols[i % 4]:
                    if achievement["earned"]:
                        st.success(f"ğŸ† {achievement['name']}")
                    else:
                        st.info(f"ğŸ”’ {achievement['name']}")
                    st.caption(achievement["description"])

# ì•± ì‹¤í–‰
if __name__ == "__main__":
    create_learning_assistant_app()
```

## ğŸš€ í´ë¼ìš°ë“œ ë°°í¬ ì˜ˆì‹œ

### AWS Lambda + API Gateway
```python
import json
import boto3
from mangum import Mangum
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="AI Assistant API")

# Lambda í•¸ë“¤ëŸ¬
handler = Mangum(app)

class QueryRequest(BaseModel):
    question: str
    user_id: str
    session_id: Optional[str] = None

class QueryResponse(BaseModel):
    answer: str
    sources: List[str]
    session_id: str
    confidence: float

@app.post("/query", response_model=QueryResponse)
async def handle_query(request: QueryRequest):
    try:
        # S3ì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        s3_client = boto3.client('s3')
        vectorstore = load_vectorstore_from_s3(s3_client, "my-vectorstore-bucket")

        # RAG ì²´ì¸ ì‹¤í–‰
        result = rag_chain.invoke({
            "input": request.question,
            "user_id": request.user_id
        })

        return QueryResponse(
            answer=result["answer"],
            sources=result["sources"],
            session_id=request.session_id or generate_session_id(),
            confidence=result.get("confidence", 0.8)
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def load_vectorstore_from_s3(s3_client, bucket_name):
    # S3ì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë°ì´í„° ë¡œë“œí•˜ëŠ” ë¡œì§
    pass

# ë°°í¬ ì„¤ì • (serverless.yml ì˜ˆì‹œ)
"""
service: ai-assistant-api

provider:
  name: aws
  runtime: python3.9
  region: us-east-1
  timeout: 30
  memorySize: 1024
  environment:
    OPENAI_API_KEY: ${env:OPENAI_API_KEY}

functions:
  api:
    handler: main.handler
    events:
      - http:
          path: /{proxy+}
          method: ANY
          cors: true

plugins:
  - serverless-python-requirements
"""
```

ì´ ì˜ˆì‹œ ëª¨ìŒì§‘ì„ í†µí•´ ì‹¤ì œ ê¸°ì—… í™˜ê²½ì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ì˜ˆì‹œëŠ” ì‹¤ì œ ìš´ì˜ í™˜ê²½ì„ ê³ ë ¤í•œ í™•ì¥ì„±, ë³´ì•ˆì„±, ìœ ì§€ë³´ìˆ˜ì„±ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.