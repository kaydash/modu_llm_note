# W2_004 í…ìŠ¤íŠ¸ ë¶„í•  ì „ëµ

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ë¶„í• ê¸°ì˜ íŠ¹ì§•ê³¼ ì‚¬ìš© ì‚¬ë¡€ ì´í•´í•˜ê¸°
- ë¬¸ì„œì˜ íŠ¹ì„±ì— ë”°ë¥¸ ìµœì ì˜ ë¶„í•  ì „ëµ ì„ íƒí•˜ê¸°
- í† í° ê¸°ë°˜ ë¶„í• ê³¼ ì˜ë¯¸ ê¸°ë°˜ ë¶„í• ì˜ ì°¨ì´ì  í•™ìŠµí•˜ê¸°

## ğŸ“š í•µì‹¬ ê°œë…

### í…ìŠ¤íŠ¸ ë¶„í• ì´ ì¤‘ìš”í•œ ì´ìœ 
ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ë•Œ ë§¤ìš° ì¤‘ìš”í•œ ì „ì²˜ë¦¬ ë‹¨ê³„ì…ë‹ˆë‹¤.

**ì£¼ìš” ê³ ë ¤ì‚¬í•­:**
1. **ë¬¸ì„œì˜ êµ¬ì¡°ì™€ í˜•ì‹** - PDF, ì›¹í˜ì´ì§€, ì±… ë“±
2. **ì›í•˜ëŠ” ì²­í¬ í¬ê¸°** - í† í° ìˆ˜, ë¬¸ì ìˆ˜ ì œí•œ
3. **ë¬¸ë§¥ ë³´ì¡´ì˜ ì¤‘ìš”ë„** - ì˜ë¯¸ ë‹¨ìœ„ vs ê¸¸ì´ ë‹¨ìœ„
4. **ì²˜ë¦¬ ì†ë„** - ì‹¤ì‹œê°„ vs ë°°ì¹˜ ì²˜ë¦¬

### í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì¢…ë¥˜

#### 1. CharacterTextSplitter
- **íŠ¹ì§•**: ë¬¸ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ì‹
- **ì¥ì **: ë‹¨ìˆœí•˜ê³  ë¹ ë¦„
- **ë‹¨ì **: ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì§€ ì•ŠìŒ

#### 2. RecursiveCharacterTextSplitter
- **íŠ¹ì§•**: ì—¬ëŸ¬ êµ¬ë¶„ìë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•˜ëŠ” ì¬ê·€ì  ë¶„í• 
- **ì¥ì **: ë¬¸ë§¥ì„ ë” ì˜ ë³´ì¡´
- **ë‹¨ì **: ì™„ë²½í•œ ì˜ë¯¸ ë³´ì¡´ì€ ì–´ë ¤ì›€

#### 3. SemanticChunker
- **íŠ¹ì§•**: ì„ë² ë”©ì„ í™œìš©í•œ ì˜ë¯¸ ê¸°ë°˜ ë¶„í• 
- **ì¥ì **: ì˜ë¯¸ì ìœ¼ë¡œ ì¼ê´€ëœ ì²­í¬ ìƒì„±
- **ë‹¨ì **: ê³„ì‚° ë¹„ìš©ì´ ë†’ìŒ

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„í• ê¸°
pip install langchain-text-splitters

# ì˜ë¯¸ ê¸°ë°˜ ë¶„í• ê¸° (ì‹¤í—˜ ê¸°ëŠ¥)
pip install langchain-experimental

# í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install tiktoken transformers

# ì„ë² ë”© ëª¨ë¸
pip install sentence-transformers
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
from dotenv import load_dotenv
import os
from typing import List, Dict, Any, Optional
from pprint import pprint
import json
import tiktoken
import statistics

load_dotenv()

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.documents import Document
```

## ğŸ’» ì½”ë“œ ì˜ˆì œ

### 1. CharacterTextSplitter ì‚¬ìš©ë²•

#### ê¸°ë³¸ ì‚¬ìš©ë²•
```python
from langchain_text_splitters import CharacterTextSplitter

def create_character_splitter(
    separator: str = "\n\n",
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    is_separator_regex: bool = False
) -> CharacterTextSplitter:
    """CharacterTextSplitter ìƒì„±"""
    return CharacterTextSplitter(
        separator=separator,              # ì²­í¬ êµ¬ë¶„ì
        chunk_size=chunk_size,            # ì²­í¬ ê¸¸ì´
        chunk_overlap=chunk_overlap,      # ì²­í¬ ì¤‘ì²©
        length_function=len,              # ê¸¸ì´ í•¨ìˆ˜
        is_separator_regex=is_separator_regex,  # ì •ê·œì‹ ì‚¬ìš© ì—¬ë¶€
        keep_separator=False,             # êµ¬ë¶„ì ìœ ì§€ ì—¬ë¶€
        add_start_index=False,            # ì‹œì‘ ì¸ë±ìŠ¤ ì¶”ê°€ ì—¬ë¶€
        strip_whitespace=True,            # ê³µë°± ì œê±° ì—¬ë¶€
    )

# ê¸°ë³¸ ì‚¬ìš© ì˜ˆì‹œ
text_splitter = create_character_splitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=200
)

# í…ìŠ¤íŠ¸ ë¶„í• 
long_text = "ì—¬ê¸°ì— ê¸´ í…ìŠ¤íŠ¸ ì…ë ¥..."
chunks = text_splitter.split_text(long_text)

print(f"ë¶„í• ëœ í…ìŠ¤íŠ¸ ê°œìˆ˜: {len(chunks)}")
for i, chunk in enumerate(chunks):
    print(f"ì²­í¬ {i+1} ê¸¸ì´: {len(chunk)}")
```

#### Document ê°ì²´ ë¶„í• 
```python
def split_pdf_documents(pdf_path: str) -> List[Document]:
    """PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ ë¶„í• """
    # PDF ë¡œë”ë¡œ ë¬¸ì„œ ë¡œë“œ
    pdf_loader = PyPDFLoader(pdf_path)
    pdf_docs = pdf_loader.load()

    # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ìƒì„±
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200
    )

    # Document ê°ì²´ë“¤ì„ ë¶„í• 
    chunks = text_splitter.split_documents(pdf_docs)

    print(f"ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(pdf_docs)}")
    print(f"ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(chunks)}")

    # ê° ì²­í¬ì˜ ê¸¸ì´ ì¶œë ¥
    for i, chunk in enumerate(chunks):
        print(f"ì²­í¬ {i+1} ê¸¸ì´: {len(chunk.page_content)}")

    return chunks

# ì‚¬ìš© ì˜ˆì‹œ
chunks = split_pdf_documents('./data/transformer.pdf')
```

#### ì •ê·œí‘œí˜„ì‹ì„ í™œìš©í•œ ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
```python
def create_sentence_splitter() -> CharacterTextSplitter:
    """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ë¶„í• ê¸°"""
    return CharacterTextSplitter(
        separator=r'(?<=[.!?])\s+',  # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë’¤ ê³µë°±
        chunk_size=1000,
        chunk_overlap=200,
        is_separator_regex=True,      # ì •ê·œì‹ ì‚¬ìš©
        keep_separator=True,          # êµ¬ë¶„ì ìœ ì§€
    )

# ì‚¬ìš© ì˜ˆì‹œ
sentence_splitter = create_sentence_splitter()
sentence_chunks = sentence_splitter.split_documents(pdf_docs)

print(f"ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  ê²°ê³¼: {len(sentence_chunks)}ê°œ ì²­í¬")
```

### 2. RecursiveCharacterTextSplitter ì‚¬ìš©ë²•

#### ê¸°ë³¸ ì¬ê·€ ë¶„í• 
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

def create_recursive_splitter(
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    separators: Optional[List[str]] = None
) -> RecursiveCharacterTextSplitter:
    """ì¬ê·€ì  í…ìŠ¤íŠ¸ ë¶„í• ê¸° ìƒì„±"""
    if separators is None:
        # ê¸°ë³¸ êµ¬ë¶„ì ìˆœì„œ: ë¬¸ë‹¨ â†’ ì¤„ â†’ ê³µë°± â†’ ë¬¸ì
        separators = ["\n\n", "\n", " ", ""]

    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=separators,  # ì¬ê·€ì ìœ¼ë¡œ ìˆœì°¨ ì ìš©
    )

# ì‚¬ìš© ì˜ˆì‹œ
recursive_splitter = create_recursive_splitter()
recursive_chunks = recursive_splitter.split_documents(pdf_docs)

print(f"ì¬ê·€ ë¶„í•  ê²°ê³¼: {len(recursive_chunks)}ê°œ ì²­í¬")

# ì²­í¬ ê¸¸ì´ ë¶„í¬ í™•ì¸
chunk_lengths = [len(chunk.page_content) for chunk in recursive_chunks]
print(f"ì²­í¬ ê¸¸ì´ ë¶„í¬: {chunk_lengths}")
```

#### ì²­í¬ ê²¹ì¹¨ ë¶„ì„
```python
def analyze_chunk_overlap(chunks: List[Document]) -> Dict[str, Any]:
    """ì²­í¬ ê°„ ê²¹ì¹¨ ë¶„ì„"""
    overlap_analysis = {
        "total_chunks": len(chunks),
        "overlaps": []
    }

    for i in range(len(chunks) - 1):
        current_chunk = chunks[i].page_content
        next_chunk = chunks[i + 1].page_content

        # ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì°¾ê¸° (ê°„ë‹¨í•œ ì ‘ê·¼ë²•)
        overlap_length = 0
        min_length = min(len(current_chunk), len(next_chunk))

        for j in range(1, min_length + 1):
            if current_chunk[-j:] == next_chunk[:j]:
                overlap_length = j

        overlap_analysis["overlaps"].append({
            "chunk_pair": f"{i+1}-{i+2}",
            "overlap_length": overlap_length,
            "overlap_percentage": round(overlap_length / len(current_chunk) * 100, 2)
        })

    return overlap_analysis

# ê²¹ì¹¨ ë¶„ì„ ì‹¤í–‰
overlap_result = analyze_chunk_overlap(recursive_chunks)
pprint(overlap_result)
```

### 3. í† í° ê¸°ë°˜ ë¶„í• 

#### TikToken í™œìš©
```python
def create_tiktoken_splitter(
    model_name: str = "gpt-4-mini",
    chunk_size: int = 300,
    chunk_overlap: int = 50
) -> RecursiveCharacterTextSplitter:
    """TikToken ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ê¸°"""
    return RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        encoding_name="cl100k_base",  # OpenAI ëª¨ë¸ìš© ì¸ì½”ë”©
        # model_name=model_name,      # ë˜ëŠ” ëª¨ë¸ëª… ì§ì ‘ ì§€ì •
        chunk_size=chunk_size,        # í† í° ìˆ˜ ê¸°ì¤€
        chunk_overlap=chunk_overlap,
    )

def analyze_token_usage(chunks: List[Document]) -> Dict[str, Any]:
    """í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„"""
    tokenizer = tiktoken.get_encoding("cl100k_base")

    token_analysis = {
        "total_chunks": len(chunks),
        "token_counts": [],
        "total_tokens": 0
    }

    for i, chunk in enumerate(chunks):
        tokens = tokenizer.encode(chunk.page_content)
        token_count = len(tokens)

        token_analysis["token_counts"].append(token_count)
        token_analysis["total_tokens"] += token_count

        print(f"ì²­í¬ {i+1}: {token_count} í† í°")
        print(f"ìƒ˜í”Œ í† í°: {tokens[:5]}")  # ì²« 5ê°œ í† í°
        print(f"ìƒ˜í”Œ í…ìŠ¤íŠ¸: {tokenizer.decode(tokens[:5])}")
        print("-" * 50)

    # í†µê³„ ê³„ì‚°
    token_counts = token_analysis["token_counts"]
    if token_counts:
        token_analysis.update({
            "avg_tokens": round(statistics.mean(token_counts), 2),
            "min_tokens": min(token_counts),
            "max_tokens": max(token_counts),
            "median_tokens": statistics.median(token_counts)
        })

    return token_analysis

# í† í° ê¸°ë°˜ ë¶„í•  ì‹¤í–‰
tiktoken_splitter = create_tiktoken_splitter(chunk_size=300)
tiktoken_chunks = tiktoken_splitter.split_documents([pdf_docs[0]])

# í† í° ë¶„ì„
token_stats = analyze_token_usage(tiktoken_chunks)
pprint(token_stats)
```

#### Hugging Face í† í¬ë‚˜ì´ì € í™œìš©
```python
from transformers import AutoTokenizer

def create_hf_tokenizer_splitter(
    model_name: str = "BAAI/bge-m3",
    chunk_size: int = 300,
    chunk_overlap: int = 50
) -> RecursiveCharacterTextSplitter:
    """Hugging Face í† í¬ë‚˜ì´ì € ê¸°ë°˜ ë¶„í• ê¸°"""
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    return RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
        tokenizer=tokenizer,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )

def compare_tokenizers(text: str) -> Dict[str, Any]:
    """ì„œë¡œ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì € ë¹„êµ"""
    # TikToken
    tiktoken_tokenizer = tiktoken.get_encoding("cl100k_base")
    tiktoken_tokens = tiktoken_tokenizer.encode(text)

    # Hugging Face
    hf_tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-m3")
    hf_tokens = hf_tokenizer.encode(text)

    return {
        "text_length": len(text),
        "tiktoken_tokens": len(tiktoken_tokens),
        "hf_tokens": len(hf_tokens),
        "tiktoken_sample": tiktoken_tokens[:10],
        "hf_sample": hf_tokens[:10],
        "tiktoken_decoded": tiktoken_tokenizer.decode(tiktoken_tokens[:10]),
        "hf_decoded": hf_tokenizer.decode(hf_tokens[:10], skip_special_tokens=True)
    }

# í† í¬ë‚˜ì´ì € ë¹„êµ
sample_text = "ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤. Hello world!"
comparison = compare_tokenizers(sample_text)
pprint(comparison)
```

### 4. ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  (SemanticChunker)

#### ê¸°ë³¸ ì˜ë¯¸ ë¶„í• 
```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

def create_semantic_chunker(
    threshold_type: str = "percentile",
    embedding_model: str = "text-embedding-3-small"
) -> SemanticChunker:
    """ì˜ë¯¸ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ê¸° ìƒì„±"""
    embeddings = OpenAIEmbeddings(model=embedding_model)

    return SemanticChunker(
        embeddings=embeddings,
        breakpoint_threshold_type=threshold_type,  # gradient, percentile, standard_deviation, interquartile
    )

# ì‚¬ìš© ì˜ˆì‹œ
semantic_splitter = create_semantic_chunker(threshold_type="gradient")
semantic_chunks = semantic_splitter.split_documents([pdf_docs[0]])

print(f"ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ê²°ê³¼: {len(semantic_chunks)}ê°œ ì²­í¬")

# ê° ì²­í¬ì˜ ì˜ë¯¸ì  ì¼ê´€ì„± í™•ì¸
for i, chunk in enumerate(semantic_chunks):
    print(f"\n=== ì˜ë¯¸ ì²­í¬ {i+1} ===")
    print(f"ê¸¸ì´: {len(chunk.page_content)} characters")
    print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {chunk.page_content[:200]}...")
```

#### ì„ê³„ê°’ íƒ€ì…ë³„ ë¹„êµ
```python
def compare_semantic_thresholds(document: Document) -> Dict[str, List[Document]]:
    """ë‹¤ì–‘í•œ ì„ê³„ê°’ íƒ€ì…ìœ¼ë¡œ ì˜ë¯¸ ë¶„í•  ë¹„êµ"""
    threshold_types = ["gradient", "percentile", "standard_deviation", "interquartile"]
    results = {}

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    for threshold_type in threshold_types:
        try:
            splitter = SemanticChunker(
                embeddings=embeddings,
                breakpoint_threshold_type=threshold_type,
            )

            chunks = splitter.split_documents([document])
            results[threshold_type] = chunks

            print(f"{threshold_type}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        except Exception as e:
            print(f"{threshold_type} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            results[threshold_type] = []

    return results

# ì„ê³„ê°’ ë¹„êµ ì‹¤í–‰
semantic_results = compare_semantic_thresholds(pdf_docs[0])

# ê²°ê³¼ ìš”ì•½
for threshold_type, chunks in semantic_results.items():
    if chunks:
        chunk_lengths = [len(chunk.page_content) for chunk in chunks]
        print(f"\n{threshold_type}:")
        print(f"  ì²­í¬ ìˆ˜: {len(chunks)}")
        print(f"  í‰ê·  ê¸¸ì´: {round(statistics.mean(chunk_lengths), 2)}")
        print(f"  ê¸¸ì´ ë²”ìœ„: {min(chunk_lengths)} - {max(chunk_lengths)}")
```

### 5. í†µí•© ë¶„í•  ì „ëµ ë¹„êµ

#### ë‹¤ì¤‘ ë¶„í• ê¸° ë¹„êµ í•¨ìˆ˜
```python
def compare_all_splitters(document: Document) -> Dict[str, Any]:
    """ëª¨ë“  ë¶„í• ê¸° ì„±ëŠ¥ ë¹„êµ"""
    results = {}

    # 1. CharacterTextSplitter
    char_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=500,
        chunk_overlap=50
    )
    char_chunks = char_splitter.split_documents([document])

    # 2. RecursiveCharacterTextSplitter
    recursive_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )
    recursive_chunks = recursive_splitter.split_documents([document])

    # 3. TikToken ê¸°ë°˜
    tiktoken_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        encoding_name="cl100k_base",
        chunk_size=150,  # í† í° ìˆ˜
        chunk_overlap=20
    )
    tiktoken_chunks = tiktoken_splitter.split_documents([document])

    # 4. SemanticChunker (ê°€ëŠ¥í•œ ê²½ìš°)
    semantic_chunks = []
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        semantic_splitter = SemanticChunker(
            embeddings=embeddings,
            breakpoint_threshold_type="percentile"
        )
        semantic_chunks = semantic_splitter.split_documents([document])
    except:
        pass

    # ê²°ê³¼ ì •ë¦¬
    splitter_results = {
        "CharacterTextSplitter": char_chunks,
        "RecursiveCharacterTextSplitter": recursive_chunks,
        "TikToken": tiktoken_chunks,
        "SemanticChunker": semantic_chunks
    }

    # í†µê³„ ê³„ì‚°
    comparison_stats = {}
    for name, chunks in splitter_results.items():
        if chunks:
            lengths = [len(chunk.page_content) for chunk in chunks]
            comparison_stats[name] = {
                "chunk_count": len(chunks),
                "avg_length": round(statistics.mean(lengths), 2),
                "min_length": min(lengths),
                "max_length": max(lengths),
                "total_length": sum(lengths)
            }
        else:
            comparison_stats[name] = {"chunk_count": 0}

    return {
        "chunks": splitter_results,
        "stats": comparison_stats
    }

# ì „ì²´ ë¹„êµ ì‹¤í–‰
all_results = compare_all_splitters(pdf_docs[0])

# ê²°ê³¼ ì¶œë ¥
print("=== í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„±ëŠ¥ ë¹„êµ ===")
for name, stats in all_results["stats"].items():
    print(f"\n{name}:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
```

#### ë¶„í•  í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜
```python
def evaluate_chunk_quality(chunks: List[Document]) -> Dict[str, Any]:
    """ì²­í¬ í’ˆì§ˆ í‰ê°€"""
    if not chunks:
        return {"error": "ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤"}

    # ê¸¸ì´ ì¼ê´€ì„± í‰ê°€
    lengths = [len(chunk.page_content) for chunk in chunks]
    length_variance = statistics.variance(lengths) if len(lengths) > 1 else 0
    length_consistency = 1 / (1 + length_variance / 1000)  # ì •ê·œí™”ëœ ì¼ê´€ì„± ì ìˆ˜

    # ë‚´ìš© ì¤‘ë³µ í‰ê°€ (ê°„ë‹¨í•œ ì ‘ê·¼ë²•)
    overlap_scores = []
    for i in range(len(chunks) - 1):
        current = chunks[i].page_content.lower().split()
        next_chunk = chunks[i + 1].page_content.lower().split()

        # ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨ ê³„ì‚°
        common_words = set(current) & set(next_chunk)
        overlap_ratio = len(common_words) / max(len(set(current)), len(set(next_chunk)), 1)
        overlap_scores.append(overlap_ratio)

    avg_overlap = statistics.mean(overlap_scores) if overlap_scores else 0

    return {
        "chunk_count": len(chunks),
        "length_consistency": round(length_consistency, 3),
        "avg_overlap_ratio": round(avg_overlap, 3),
        "length_stats": {
            "mean": round(statistics.mean(lengths), 2),
            "variance": round(length_variance, 2),
            "min": min(lengths),
            "max": max(lengths)
        }
    }

# í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
for name, chunks in all_results["chunks"].items():
    if chunks:
        quality = evaluate_chunk_quality(chunks)
        print(f"\n=== {name} í’ˆì§ˆ í‰ê°€ ===")
        pprint(quality)
```

## ğŸš€ ì‹¤ìŠµí•´ë³´ê¸°

### ì‹¤ìŠµ 1: ë§ì¶¤í˜• ë¶„í• ê¸° êµ¬í˜„
ë‹¤ì–‘í•œ ë¬¸ì„œ íƒ€ì…ì— ìµœì í™”ëœ ë¶„í•  ì „ëµì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
def create_adaptive_splitter(document_type: str) -> Any:
    """ë¬¸ì„œ íƒ€ì…ë³„ ìµœì í™”ëœ ë¶„í• ê¸°"""
    if document_type == "academic_paper":
        # TODO: í•™ìˆ  ë…¼ë¬¸ìš© ë¶„í• ê¸° êµ¬í˜„
        # ì„¹ì…˜ í—¤ë”, ì°¸ê³ ë¬¸í—Œ ê³ ë ¤
        pass
    elif document_type == "news_article":
        # TODO: ë‰´ìŠ¤ ê¸°ì‚¬ìš© ë¶„í• ê¸° êµ¬í˜„
        # ë¬¸ë‹¨ ë‹¨ìœ„, ì§§ì€ ì²­í¬
        pass
    elif document_type == "legal_document":
        # TODO: ë²•ë¥  ë¬¸ì„œìš© ë¶„í• ê¸° êµ¬í˜„
        # ì¡°í•­ ë‹¨ìœ„, ì •í™•í•œ ê²½ê³„
        pass
    else:
        # TODO: ì¼ë°˜ ë¬¸ì„œìš© ê¸°ë³¸ ë¶„í• ê¸°
        pass

# í…ŒìŠ¤íŠ¸
adaptive_splitter = create_adaptive_splitter("academic_paper")
```

### ì‹¤ìŠµ 2: ì„±ëŠ¥ ìµœì í™”ëœ ë¶„í•  íŒŒì´í”„ë¼ì¸
ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ìœ„í•œ íš¨ìœ¨ì ì¸ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
def create_efficient_pipeline(
    documents: List[Document],
    target_chunk_size: int = 1000
) -> List[Document]:
    """íš¨ìœ¨ì ì¸ í…ìŠ¤íŠ¸ ë¶„í•  íŒŒì´í”„ë¼ì¸"""
    # TODO: ë¬¸ì„œ í¬ê¸°ë³„ ë‹¤ë¥¸ ì „ëµ ì ìš©
    # TODO: ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„
    # TODO: ë©”ëª¨ë¦¬ ìµœì í™”
    # TODO: ì§„í–‰ìƒí™© í‘œì‹œ
    pass

# í…ŒìŠ¤íŠ¸ ë°ì´í„°
large_documents = pdf_docs * 5  # ê°€ìƒì˜ ëŒ€ìš©ëŸ‰ ë°ì´í„°
efficient_chunks = create_efficient_pipeline(large_documents)
```

### ì‹¤ìŠµ 3: ë™ì  ì²­í¬ í¬ê¸° ì¡°ì •
ë¬¸ì„œ ë‚´ìš©ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì²­í¬ í¬ê¸°ë¥¼ ì¡°ì •í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
def dynamic_chunk_resizer(
    document: Document,
    complexity_threshold: float = 0.5
) -> List[Document]:
    """ë¬¸ì„œ ë³µì¡ë„ì— ë”°ë¥¸ ë™ì  ì²­í¬ í¬ê¸° ì¡°ì •"""
    # TODO: í…ìŠ¤íŠ¸ ë³µì¡ë„ ë¶„ì„
    # TODO: ë³µì¡ë„ì— ë”°ë¥¸ ì²­í¬ í¬ê¸° ê²°ì •
    # TODO: ì ì‘ì  ë¶„í•  ìˆ˜í–‰
    pass

# í…ŒìŠ¤íŠ¸
adaptive_chunks = dynamic_chunk_resizer(pdf_docs[0])
```

## ğŸ“‹ í•´ë‹µ

### ì‹¤ìŠµ 1 í•´ë‹µ: ë§ì¶¤í˜• ë¶„í• ê¸° êµ¬í˜„
```python
from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter
import re

def create_adaptive_splitter(document_type: str) -> Any:
    """ë¬¸ì„œ íƒ€ì…ë³„ ìµœì í™”ëœ ë¶„í• ê¸°"""
    if document_type == "academic_paper":
        # í•™ìˆ  ë…¼ë¬¸: ì„¹ì…˜ í—¤ë”ì™€ ë¬¸ë‹¨ì„ ê³ ë ¤í•œ ë¶„í• 
        return RecursiveCharacterTextSplitter(
            separators=[
                "\n# ",      # ì£¼ìš” ì„¹ì…˜
                "\n## ",     # ì„œë¸Œì„¹ì…˜
                "\n### ",    # ì„¸ë¶€ì„¹ì…˜
                "\n\n",      # ë¬¸ë‹¨
                "\n",        # ì¤„ë°”ê¿ˆ
                ". ",        # ë¬¸ì¥
                " ",         # ë‹¨ì–´
                ""           # ë¬¸ì
            ],
            chunk_size=1500,  # í•™ìˆ  ë…¼ë¬¸ì€ ê¸´ ì²­í¬ê°€ ìœ ë¦¬
            chunk_overlap=200,
        )

    elif document_type == "news_article":
        # ë‰´ìŠ¤ ê¸°ì‚¬: ë¬¸ë‹¨ ë‹¨ìœ„ì˜ ì§§ì€ ì²­í¬
        return CharacterTextSplitter(
            separator="\n\n",    # ë¬¸ë‹¨ êµ¬ë¶„
            chunk_size=800,      # ì§§ì€ ì²­í¬
            chunk_overlap=100,
            keep_separator=True,
        )

    elif document_type == "legal_document":
        # ë²•ë¥  ë¬¸ì„œ: ì¡°í•­ê³¼ í•­ëª© ë‹¨ìœ„ ì •í™•í•œ ë¶„í• 
        return CharacterTextSplitter(
            separator=r'\n(?=\d+\.|\([a-z]\)|\([0-9]+\))',  # ì¡°í•­ ë²ˆí˜¸ íŒ¨í„´
            chunk_size=1200,
            chunk_overlap=50,    # ìµœì†Œ ì¤‘ë³µ (ì •í™•ì„± ìš°ì„ )
            is_separator_regex=True,
            keep_separator=True,
        )

    else:
        # ì¼ë°˜ ë¬¸ì„œ: ê· í˜• ì¡íŒ ê¸°ë³¸ ì„¤ì •
        return RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )

# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±
def create_sample_documents():
    academic_text = """
# Introduction
This paper presents a comprehensive study on artificial intelligence.

## Methodology
We employed machine learning algorithms for data analysis.

### Data Collection
Data was collected from multiple sources over a period of six months.

## Results
Our findings indicate significant improvements in accuracy.
"""

    news_text = """
Breaking News: Major Breakthrough in AI Technology

Scientists at a leading university have announced a revolutionary development in artificial intelligence.

The new system demonstrates unprecedented capabilities in natural language processing.

Industry experts predict this will transform multiple sectors including healthcare and education.
"""

    legal_text = """
ARTICLE I - DEFINITIONS

1. For purposes of this agreement:
   (a) "Company" means the entity entering into this contract
   (b) "Service" means the work performed under this agreement

2. Terms and Conditions:
   (1) All work must be completed within specified timeframes
   (2) Quality standards must be maintained throughout
"""

    return {
        "academic_paper": Document(page_content=academic_text, metadata={"type": "academic"}),
        "news_article": Document(page_content=news_text, metadata={"type": "news"}),
        "legal_document": Document(page_content=legal_text, metadata={"type": "legal"})
    }

# ê° ë¬¸ì„œ íƒ€ì…ë³„ í…ŒìŠ¤íŠ¸
sample_docs = create_sample_documents()

for doc_type, document in sample_docs.items():
    print(f"\n=== {doc_type.upper()} ë¶„í•  í…ŒìŠ¤íŠ¸ ===")

    splitter = create_adaptive_splitter(doc_type)
    chunks = splitter.split_documents([document])

    print(f"ì²­í¬ ìˆ˜: {len(chunks)}")
    for i, chunk in enumerate(chunks, 1):
        print(f"ì²­í¬ {i} (ê¸¸ì´: {len(chunk.page_content)}):")
        print(f"  {chunk.page_content[:100]}...")
        print()
```

### ì‹¤ìŠµ 2 í•´ë‹µ: ì„±ëŠ¥ ìµœì í™”ëœ ë¶„í•  íŒŒì´í”„ë¼ì¸
```python
import concurrent.futures
import time
from typing import Generator
from tqdm import tqdm

def create_efficient_pipeline(
    documents: List[Document],
    target_chunk_size: int = 1000,
    max_workers: int = 4
) -> List[Document]:
    """íš¨ìœ¨ì ì¸ í…ìŠ¤íŠ¸ ë¶„í•  íŒŒì´í”„ë¼ì¸"""

    def classify_document_size(doc: Document) -> str:
        """ë¬¸ì„œ í¬ê¸°ë³„ ë¶„ë¥˜"""
        length = len(doc.page_content)
        if length < 2000:
            return "small"
        elif length < 10000:
            return "medium"
        else:
            return "large"

    def get_optimized_splitter(doc_size: str) -> Any:
        """ë¬¸ì„œ í¬ê¸°ë³„ ìµœì í™”ëœ ë¶„í• ê¸°"""
        if doc_size == "small":
            # ì‘ì€ ë¬¸ì„œ: ê°„ë‹¨í•œ ë¶„í• ê¸°
            return CharacterTextSplitter(
                separator="\n\n",
                chunk_size=target_chunk_size // 2,
                chunk_overlap=50
            )
        elif doc_size == "medium":
            # ì¤‘ê°„ ë¬¸ì„œ: ê· í˜• ì¡íŒ ë¶„í• ê¸°
            return RecursiveCharacterTextSplitter(
                chunk_size=target_chunk_size,
                chunk_overlap=100
            )
        else:
            # í° ë¬¸ì„œ: íš¨ìœ¨ì ì¸ ë¶„í• ê¸°
            return RecursiveCharacterTextSplitter(
                chunk_size=target_chunk_size * 2,
                chunk_overlap=200,
                separators=["\n\n", "\n", ". ", " ", ""]
            )

    def process_document(doc: Document) -> List[Document]:
        """ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬"""
        doc_size = classify_document_size(doc)
        splitter = get_optimized_splitter(doc_size)
        return splitter.split_documents([doc])

    def process_documents_parallel(docs: List[Document]) -> List[Document]:
        """ë¬¸ì„œ ë³‘ë ¬ ì²˜ë¦¬"""
        all_chunks = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ì§„í–‰ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ tqdm ì‚¬ìš©
            futures = {executor.submit(process_document, doc): doc for doc in docs}

            for future in tqdm(
                concurrent.futures.as_completed(futures),
                total=len(docs),
                desc="ë¬¸ì„œ ì²˜ë¦¬ ì¤‘"
            ):
                try:
                    chunks = future.result()
                    all_chunks.extend(chunks)
                except Exception as e:
                    print(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        return all_chunks

    def process_documents_batch(docs: List[Document], batch_size: int = 10) -> List[Document]:
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”"""
        all_chunks = []

        for i in tqdm(range(0, len(docs), batch_size), desc="ë°°ì¹˜ ì²˜ë¦¬ ì¤‘"):
            batch = docs[i:i + batch_size]
            batch_chunks = process_documents_parallel(batch)
            all_chunks.extend(batch_chunks)

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (ê°„ë‹¨í•œ êµ¬í˜„)
            if len(all_chunks) > 1000:  # ì„ê³„ì 
                print(f"í˜„ì¬ ì²­í¬ ìˆ˜: {len(all_chunks)}")

        return all_chunks

    # ë¬¸ì„œ ìˆ˜ì— ë”°ë¥¸ ì²˜ë¦¬ ì „ëµ ì„ íƒ
    start_time = time.time()

    if len(documents) < 50:
        # ì†Œê·œëª¨: ì§ì ‘ ì²˜ë¦¬
        result_chunks = process_documents_parallel(documents)
    else:
        # ëŒ€ê·œëª¨: ë°°ì¹˜ ì²˜ë¦¬
        result_chunks = process_documents_batch(documents)

    end_time = time.time()

    # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
    print(f"\n=== ì²˜ë¦¬ ì™„ë£Œ ===")
    print(f"ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    print(f"ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    print(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(result_chunks)}")
    print(f"ì²˜ë¦¬ ì†ë„: {len(documents) / (end_time - start_time):.2f} ë¬¸ì„œ/ì´ˆ")

    return result_chunks

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
def create_test_documents(count: int = 20) -> List[Document]:
    """í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ìƒì„±"""
    test_docs = []

    base_texts = [
        "ì§§ì€ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤. " * 50,
        "ì¤‘ê°„ ê¸¸ì´ì˜ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤. " * 200,
        "ê¸´ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤. " * 500,
    ]

    for i in range(count):
        text_type = i % len(base_texts)
        content = base_texts[text_type] + f" ë¬¸ì„œ ë²ˆí˜¸: {i+1}"

        doc = Document(
            page_content=content,
            metadata={"doc_id": i+1, "type": f"test_doc_{text_type}"}
        )
        test_docs.append(doc)

    return test_docs

# íš¨ìœ¨ì ì¸ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
test_documents = create_test_documents(count=25)
efficient_chunks = create_efficient_pipeline(test_documents, target_chunk_size=800)

# ê²°ê³¼ ë¶„ì„
chunk_lengths = [len(chunk.page_content) for chunk in efficient_chunks]
print(f"\nì²­í¬ ê¸¸ì´ í†µê³„:")
print(f"í‰ê· : {statistics.mean(chunk_lengths):.2f}")
print(f"ìµœì†Œ: {min(chunk_lengths)}")
print(f"ìµœëŒ€: {max(chunk_lengths)}")
```

### ì‹¤ìŠµ 3 í•´ë‹µ: ë™ì  ì²­í¬ í¬ê¸° ì¡°ì •
```python
import re
from collections import Counter

def dynamic_chunk_resizer(
    document: Document,
    complexity_threshold: float = 0.5
) -> List[Document]:
    """ë¬¸ì„œ ë³µì¡ë„ì— ë”°ë¥¸ ë™ì  ì²­í¬ í¬ê¸° ì¡°ì •"""

    def calculate_text_complexity(text: str) -> float:
        """í…ìŠ¤íŠ¸ ë³µì¡ë„ ê³„ì‚°"""
        # ë‹¤ì–‘í•œ ë³µì¡ë„ ì§€í‘œ ê³„ì‚°
        words = text.split()
        sentences = re.split(r'[.!?]+', text)

        # 1. í‰ê·  ë¬¸ì¥ ê¸¸ì´
        avg_sentence_length = len(words) / max(len(sentences), 1)

        # 2. ì–´íœ˜ ë‹¤ì–‘ì„± (unique words / total words)
        unique_words = len(set(word.lower() for word in words))
        lexical_diversity = unique_words / max(len(words), 1)

        # 3. êµ¬ë‘ì  ë°€ë„
        punctuation_count = len(re.findall(r'[^\w\s]', text))
        punctuation_density = punctuation_count / max(len(text), 1)

        # 4. ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨
        special_chars = len(re.findall(r'[\d\(\)\[\]{}]', text))
        special_char_ratio = special_chars / max(len(text), 1)

        # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚° (0-1 ë²”ìœ„ë¡œ ì •ê·œí™”)
        complexity = (
            min(avg_sentence_length / 20, 1) * 0.3 +  # ë¬¸ì¥ ê¸¸ì´ ê°€ì¤‘ì¹˜
            lexical_diversity * 0.3 +                  # ì–´íœ˜ ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜
            min(punctuation_density * 20, 1) * 0.2 +   # êµ¬ë‘ì  ê°€ì¤‘ì¹˜
            min(special_char_ratio * 10, 1) * 0.2      # íŠ¹ìˆ˜ë¬¸ì ê°€ì¤‘ì¹˜
        )

        return min(complexity, 1.0)

    def determine_chunk_size(complexity: float, base_size: int = 1000) -> int:
        """ë³µì¡ë„ì— ë”°ë¥¸ ì²­í¬ í¬ê¸° ê²°ì •"""
        if complexity < 0.3:
            # ë‚®ì€ ë³µì¡ë„: í° ì²­í¬
            return int(base_size * 1.5)
        elif complexity > 0.7:
            # ë†’ì€ ë³µì¡ë„: ì‘ì€ ì²­í¬
            return int(base_size * 0.7)
        else:
            # ì¤‘ê°„ ë³µì¡ë„: ê¸°ë³¸ ì²­í¬
            return base_size

    def analyze_document_sections(document: Document) -> List[Dict[str, Any]]:
        """ë¬¸ì„œë¥¼ ì„¹ì…˜ë³„ë¡œ ë¶„ì„"""
        content = document.page_content

        # ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• 
        paragraphs = content.split('\n\n')
        sections = []

        for i, paragraph in enumerate(paragraphs):
            if paragraph.strip():
                complexity = calculate_text_complexity(paragraph)
                sections.append({
                    'index': i,
                    'content': paragraph,
                    'complexity': complexity,
                    'length': len(paragraph)
                })

        return sections

    # ë¬¸ì„œ ì„¹ì…˜ë³„ ë¶„ì„
    sections = analyze_document_sections(document)

    if not sections:
        # ë¹ˆ ë¬¸ì„œì˜ ê²½ìš° ê¸°ë³¸ ë¶„í• ê¸° ì‚¬ìš©
        basic_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        return basic_splitter.split_documents([document])

    # ì „ì²´ ë¬¸ì„œ ë³µì¡ë„ ê³„ì‚°
    overall_complexity = statistics.mean([section['complexity'] for section in sections])
    print(f"ì „ì²´ ë¬¸ì„œ ë³µì¡ë„: {overall_complexity:.3f}")

    # ë™ì  ì²­í¬ ìƒì„±
    adaptive_chunks = []
    current_chunk = ""
    current_metadata = document.metadata.copy()

    for section in sections:
        section_complexity = section['complexity']
        target_chunk_size = determine_chunk_size(section_complexity)

        print(f"ì„¹ì…˜ {section['index']}: ë³µì¡ë„={section_complexity:.3f}, ëª©í‘œí¬ê¸°={target_chunk_size}")

        # í˜„ì¬ ì²­í¬ì— ì„¹ì…˜ ì¶”ê°€
        if current_chunk:
            test_chunk = current_chunk + "\n\n" + section['content']
        else:
            test_chunk = section['content']

        # ì²­í¬ í¬ê¸° í™•ì¸
        if len(test_chunk) <= target_chunk_size or not current_chunk:
            current_chunk = test_chunk
        else:
            # í˜„ì¬ ì²­í¬ ì™„ë£Œ, ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘
            if current_chunk:
                chunk_doc = Document(
                    page_content=current_chunk,
                    metadata={**current_metadata, "chunk_type": "adaptive"}
                )
                adaptive_chunks.append(chunk_doc)

            current_chunk = section['content']

    # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
    if current_chunk:
        chunk_doc = Document(
            page_content=current_chunk,
            metadata={**current_metadata, "chunk_type": "adaptive"}
        )
        adaptive_chunks.append(chunk_doc)

    # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë¶„í•  ìˆ˜í–‰
    if not adaptive_chunks:
        basic_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        adaptive_chunks = basic_splitter.split_documents([document])

    return adaptive_chunks

# ë³µì¡í•œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìƒì„±
def create_complex_document() -> Document:
    """ë‹¤ì–‘í•œ ë³µì¡ë„ë¥¼ ê°€ì§„ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìƒì„±"""
    content = """
ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ê°œìš”

ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ ë¶„ì•¼ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì„ í†µí•´ ë‹¤ì–‘í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

ìˆ˜í•™ì  ëª¨ë¸ë§ê³¼ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„

ë³µì¡í•œ ìˆ˜í•™ì  ê°œë…ì„ í¬í•¨í•©ë‹ˆë‹¤: f(x) = âˆ‘(i=1 to n) wi * xi + b
ì‹ ê²½ë§ì˜ í™œì„±í™” í•¨ìˆ˜ë¡œëŠ” ReLU(x) = max(0, x), Sigmoid(x) = 1/(1+e^(-x)) ë“±ì´ ìˆìŠµë‹ˆë‹¤.
ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì€ âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚y) * (âˆ‚y/âˆ‚w)ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.

ê°„ë‹¨í•œ ì‹¤ìƒí™œ ì‘ìš©

ìŒì„±ì¸ì‹ì€ ì¼ìƒìƒí™œì—ì„œ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸í°ì˜ ìŒì„±ë¹„ì„œê°€ ëŒ€í‘œì ì¸ ì˜ˆì…ë‹ˆë‹¤.
ì´ë¯¸ì§€ ì¸ì‹ ê¸°ìˆ ë„ ì‚¬ì§„ ì•±ì—ì„œ ì–¼êµ´ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì£¼ëŠ” ê¸°ëŠ¥ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

ê³ ê¸‰ ì—°êµ¬ ë™í–¥ê³¼ ë¯¸ë˜ ì „ë§

ìµœì‹  ì—°êµ¬ì—ì„œëŠ” Transformer ì•„í‚¤í…ì²˜ì˜ attention mechanismì´ í•µì‹¬ì  ì—­í• ì„ í•©ë‹ˆë‹¤.
Multi-head self-attentionì˜ ìˆ˜ì‹ì€ Attention(Q,K,V) = softmax(QK^T/âˆšd_k)Vì…ë‹ˆë‹¤.
GPT, BERT ë“±ì˜ ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ì€ ìˆ˜ì‹­ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ë©°,
training lossëŠ” cross-entropy: L = -âˆ‘(i=1 to n) yi * log(pi)ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.

ê²°ë¡ 

AI ê¸°ìˆ ì€ ê³„ì† ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ë” ë§ì€ í˜ì‹ ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤.
"""

    return Document(
        page_content=content,
        metadata={"source": "ai_overview.txt", "type": "educational"}
    )

# ë™ì  ì²­í¬ í¬ê¸° ì¡°ì • í…ŒìŠ¤íŠ¸
complex_doc = create_complex_document()
adaptive_chunks = dynamic_chunk_resizer(complex_doc, complexity_threshold=0.5)

print(f"\n=== ë™ì  ë¶„í•  ê²°ê³¼ ===")
print(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(adaptive_chunks)}")

for i, chunk in enumerate(adaptive_chunks, 1):
    complexity = calculate_text_complexity(chunk.page_content) if 'calculate_text_complexity' in locals() else 0
    print(f"\nì²­í¬ {i}:")
    print(f"  ê¸¸ì´: {len(chunk.page_content)} characters")
    print(f"  ë³µì¡ë„: {complexity:.3f}")
    print(f"  ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {chunk.page_content[:100]}...")

# ê¸°ë³¸ ë¶„í• ê¸°ì™€ ë¹„êµ
basic_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
basic_chunks = basic_splitter.split_documents([complex_doc])

print(f"\n=== ê¸°ë³¸ ë¶„í• ê³¼ ë¹„êµ ===")
print(f"ë™ì  ë¶„í• : {len(adaptive_chunks)}ê°œ ì²­í¬")
print(f"ê¸°ë³¸ ë¶„í• : {len(basic_chunks)}ê°œ ì²­í¬")

# ê¸¸ì´ ë¶„í¬ ë¹„êµ
adaptive_lengths = [len(chunk.page_content) for chunk in adaptive_chunks]
basic_lengths = [len(chunk.page_content) for chunk in basic_chunks]

print(f"\nê¸¸ì´ í†µê³„ ë¹„êµ:")
print(f"ë™ì  ë¶„í•  - í‰ê· : {statistics.mean(adaptive_lengths):.1f}, ë²”ìœ„: {min(adaptive_lengths)}-{max(adaptive_lengths)}")
print(f"ê¸°ë³¸ ë¶„í•  - í‰ê· : {statistics.mean(basic_lengths):.1f}, ë²”ìœ„: {min(basic_lengths)}-{max(basic_lengths)}")
```

## ğŸ” ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
- [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)
- [SemanticChunker](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)

### í† í°í™” ê´€ë ¨
- [TikToken Documentation](https://github.com/openai/tiktoken)
- [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/)
- [OpenAI Token Limits](https://platform.openai.com/docs/models/gpt-4)

### ê´€ë ¨ ì—°êµ¬
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer ì•„í‚¤í…ì²˜
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Text Segmentation](https://en.wikipedia.org/wiki/Text_segmentation) - í…ìŠ¤íŠ¸ ë¶„í•  ì´ë¡ 

### ìµœì í™” íŒ

#### ì„±ëŠ¥ ìµœì í™”
- **ëŒ€ìš©ëŸ‰ ë¬¸ì„œ**: ë³‘ë ¬ ì²˜ë¦¬ì™€ ë°°ì¹˜ ì²˜ë¦¬ í™œìš©
- **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œì–´
- **í† í° íš¨ìœ¨ì„±**: ëª¨ë¸ë³„ í† í° í•œê³„ ê³ ë ¤í•œ ì²­í¬ í¬ê¸° ì„¤ì •

#### í’ˆì§ˆ í–¥ìƒ
- **ë¬¸ë§¥ ë³´ì¡´**: ì ì ˆí•œ ì¤‘ë³µ(overlap) ì„¤ì •ìœ¼ë¡œ ì—°ì†ì„± ìœ ì§€
- **ì˜ë¯¸ ë‹¨ìœ„**: SemanticChunkerë¡œ ì˜ë¯¸ì  ì¼ê´€ì„± í™•ë³´
- **ë„ë©”ì¸ íŠ¹í™”**: ë¬¸ì„œ íƒ€ì…ë³„ ë§ì¶¤í˜• ë¶„í•  ì „ëµ ì ìš©

#### ì‹¤ì „ ê°€ì´ë“œ
```python
# ê¶Œì¥ ì„¤ì •ê°’
CHUNK_SIZES = {
    "short_form": 500,    # ë‰´ìŠ¤, ë¸”ë¡œê·¸
    "medium_form": 1000,  # ì¼ë°˜ ë¬¸ì„œ
    "long_form": 1500,    # í•™ìˆ  ë…¼ë¬¸, ì±…
    "technical": 800,     # ê¸°ìˆ  ë¬¸ì„œ, API ë¬¸ì„œ
}

OVERLAPS = {
    "conservative": 50,   # ìµœì†Œ ì¤‘ë³µ
    "balanced": 200,      # ê· í˜•ì¡íŒ ì¤‘ë³µ
    "aggressive": 300,    # ìµœëŒ€ ë¬¸ë§¥ ë³´ì¡´
}
```