# W2_006 ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ë²¡í„° ì €ì¥ì†Œì˜ ê°œë…ê³¼ í•„ìš”ì„± ì´í•´í•˜ê¸°
- Chroma, FAISS, Pinecone ë“± ë‹¤ì–‘í•œ ë²¡í„° DB í™œìš©í•˜ê¸°
- íš¨ìœ¨ì ì¸ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•í•˜ê¸°

## ğŸ“š í•µì‹¬ ê°œë…

### ë²¡í„° ì €ì¥ì†Œ(Vector Store)ë€?
ë²¡í„° ì €ì¥ì†ŒëŠ” ë²¡í„°í™”ëœ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ê¸° ìœ„í•œ íŠ¹ìˆ˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- **ê³ ì°¨ì› ë²¡í„° ì €ì¥**: ì„ë² ë”© ë²¡í„°ë¥¼ ìµœì í™”ëœ ë°©ì‹ìœ¼ë¡œ ì €ì¥
- **ìœ ì‚¬ë„ ê²€ìƒ‰**: ì˜ë¯¸ì ìœ¼ë¡œ ê°€ê¹Œìš´ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ê²€ìƒ‰
- **ë©”íƒ€ë°ì´í„° ê´€ë¦¬**: ë²¡í„°ì™€ ê´€ë ¨ëœ ë¶€ê°€ ì •ë³´ë¥¼ í•¨ê»˜ ì €ì¥
- **í™•ì¥ì„±**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ë° ì‹¤ì‹œê°„ ê²€ìƒ‰ ì§€ì›

### ë²¡í„° ì €ì¥ì†Œì˜ í•„ìš”ì„±

#### ì „í†µì  DB vs ë²¡í„° DB
- **ì „í†µì  DB**: ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­, êµ¬ì¡°í™”ëœ ë°ì´í„°
- **ë²¡í„° DB**: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰, ë¹„ì •í˜• ë°ì´í„°

#### ì£¼ìš” í™œìš© ì‚¬ë¡€
- **ì‹œë§¨í‹± ê²€ìƒ‰**: ì˜ë¯¸ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰
- **ì¶”ì²œ ì‹œìŠ¤í…œ**: ìœ ì‚¬í•œ ì•„ì´í…œ ì¶”ì²œ
- **ì¤‘ë³µ ê°ì§€**: ìœ ì‚¬í•œ ì½˜í…ì¸  ì‹ë³„
- **RAG ì‹œìŠ¤í…œ**: ì§ˆì˜ì‘ë‹µì„ ìœ„í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰

### LangChain ì§€ì› ë²¡í„° ì €ì¥ì†Œ

#### 1. Chroma
- **íŠ¹ì§•**: ê²½ëŸ‰í™”, ë¡œì»¬ ê°œë°œ ì¹œí™”ì 
- **ì¥ì **: ê°„í¸í•œ ì„¤ì¹˜, ë¹ ë¥¸ ì‹œì‘
- **ìš©ë„**: í”„ë¡œí† íƒ€ì´í•‘, ì†Œê·œëª¨ í”„ë¡œì íŠ¸

#### 2. FAISS
- **íŠ¹ì§•**: Facebook AIì˜ ê³ ì„±ëŠ¥ ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **ì¥ì **: ë¹ ë¥¸ ê²€ìƒ‰ ì†ë„, ë‹¤ì–‘í•œ ì¸ë±ìŠ¤ ì•Œê³ ë¦¬ì¦˜
- **ìš©ë„**: ì¤‘ëŒ€ê·œëª¨ ë°ì´í„°, ì„±ëŠ¥ ì¤‘ì‹œ

#### 3. Pinecone
- **íŠ¹ì§•**: ì™„ì „ ê´€ë¦¬í˜• í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤
- **ì¥ì **: í™•ì¥ì„±, ê³ ê°€ìš©ì„±, ê´€ë¦¬ ë¶€ë‹´ ì—†ìŒ
- **ìš©ë„**: í”„ë¡œë•ì…˜ í™˜ê²½, ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# Chroma ë²¡í„° ì €ì¥ì†Œ
pip install langchain-chroma

# FAISS ë²¡í„° ì €ì¥ì†Œ
pip install faiss-cpu  # CPU ë²„ì „
# pip install faiss-gpu  # GPU ë²„ì „

# Pinecone ë²¡í„° ì €ì¥ì†Œ
pip install langchain-pinecone pinecone-client

# ì„ë² ë”© ëª¨ë¸
pip install langchain-huggingface
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
from dotenv import load_dotenv
import os
import uuid
from typing import List, Dict, Any, Optional, Tuple
from pprint import pprint

load_dotenv()

# Pinecone API í‚¤ (í•„ìš”í•œ ê²½ìš°)
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.documents import Document
```

## ğŸ’» ì½”ë“œ ì˜ˆì œ

### 1. Chroma ë²¡í„° ì €ì¥ì†Œ

#### ê¸°ë³¸ ì„¤ì • ë° ì´ˆê¸°í™”
```python
from langchain_chroma import Chroma
from langchain_huggingface.embeddings import HuggingFaceEmbeddings

def create_chroma_vectorstore(
    collection_name: str = "default_collection",
    persist_directory: str = "./chroma_db"
) -> Chroma:
    """Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
    # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    embeddings_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

    # Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    vectorstore = Chroma(
        collection_name=collection_name,
        embedding_function=embeddings_model,
        persist_directory=persist_directory,
    )

    return vectorstore

# ë²¡í„° ì €ì¥ì†Œ ìƒì„±
chroma_db = create_chroma_vectorstore("ai_sample_collection")

# í˜„ì¬ ì €ì¥ëœ ë°ì´í„° í™•ì¸
stored_data = chroma_db.get()
print(f"ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {len(stored_data['documents'])}")
```

#### ë¬¸ì„œ ê´€ë¦¬ (CRUD Operations)
```python
def add_documents_to_chroma(
    vectorstore: Chroma,
    contents: List[str],
    metadatas: List[Dict[str, Any]],
    ids: Optional[List[str]] = None
) -> List[str]:
    """ë¬¸ì„œë¥¼ Chromaì— ì¶”ê°€"""
    # Document ê°ì²´ ìƒì„±
    documents = [
        Document(page_content=content, metadata=metadata)
        for content, metadata in zip(contents, metadatas)
    ]

    # ID ìë™ ìƒì„± (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
    if ids is None:
        ids = [f"DOC_{i+1}" for i in range(len(documents))]

    # ë¬¸ì„œ ì¶”ê°€
    added_ids = vectorstore.add_documents(documents=documents, ids=ids)
    print(f"{len(added_ids)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")

    return added_ids

# ìƒ˜í”Œ ë¬¸ì„œ ë°ì´í„°
sample_contents = [
    "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ê³¼í•™ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ì¢…ë¥˜ì…ë‹ˆë‹¤.",
    "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "ì»´í“¨í„° ë¹„ì „ì€ ì»´í“¨í„°ê°€ ë””ì§€í„¸ ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ë¥¼ ì´í•´í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤."
]

sample_metadatas = [
    {"source": "AI ê°œë¡ ", "topic": "ì •ì˜"},
    {"source": "AI ê°œë¡ ", "topic": "ë¶„ì•¼"},
    {"source": "ë”¥ëŸ¬ë‹ ì…ë¬¸", "topic": "ë¶„ì•¼"},
    {"source": "AI ê°œë¡ ", "topic": "ê¸°ìˆ "},
    {"source": "ë”¥ëŸ¬ë‹ ì…ë¬¸", "topic": "ê¸°ìˆ "}
]

# ë¬¸ì„œ ì¶”ê°€
doc_ids = add_documents_to_chroma(chroma_db, sample_contents, sample_metadatas)
```

#### ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥
```python
def search_chroma_documents(
    vectorstore: Chroma,
    query: str,
    k: int = 3,
    filter_metadata: Optional[Dict[str, Any]] = None,
    search_type: str = "similarity"
) -> List[Any]:
    """Chromaì—ì„œ ë¬¸ì„œ ê²€ìƒ‰"""

    if search_type == "similarity":
        # ê¸°ë³¸ ìœ ì‚¬ë„ ê²€ìƒ‰
        results = vectorstore.similarity_search(
            query=query,
            k=k,
            filter=filter_metadata
        )

    elif search_type == "similarity_with_score":
        # ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨ ê²€ìƒ‰
        results = vectorstore.similarity_search_with_score(
            query=query,
            k=k,
            filter=filter_metadata
        )

    elif search_type == "relevance_score":
        # ê´€ë ¨ì„± ì ìˆ˜ í¬í•¨ ê²€ìƒ‰
        results = vectorstore.similarity_search_with_relevance_scores(
            query=query,
            k=k,
            filter=filter_metadata
        )

    return results

# ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ë²• í…ŒìŠ¤íŠ¸
query = "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê´€ê³„ëŠ”?"

# 1. ê¸°ë³¸ ìœ ì‚¬ë„ ê²€ìƒ‰
print("=== ê¸°ë³¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ===")
basic_results = search_chroma_documents(chroma_db, query, k=2)
for doc in basic_results:
    print(f"- {doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]")

# 2. ì ìˆ˜ í¬í•¨ ê²€ìƒ‰
print("\n=== ì ìˆ˜ í¬í•¨ ê²€ìƒ‰ ===")
score_results = search_chroma_documents(chroma_db, query, k=2, search_type="similarity_with_score")
for doc, score in score_results:
    print(f"- ì ìˆ˜: {score:.4f}")
    print(f"  ë‚´ìš©: {doc.page_content}")
    print(f"  ë©”íƒ€ë°ì´í„°: {doc.metadata}")

# 3. í•„í„°ë§ ê²€ìƒ‰
print("\n=== í•„í„°ë§ ê²€ìƒ‰ (AI ê°œë¡  ì¶œì²˜ë§Œ) ===")
filtered_results = search_chroma_documents(
    chroma_db, query, k=3,
    filter_metadata={"source": "AI ê°œë¡ "}
)
for doc in filtered_results:
    print(f"- {doc.page_content}")
```

#### ë¬¸ì„œ ìˆ˜ì • ë° ì‚­ì œ
```python
def update_chroma_document(
    vectorstore: Chroma,
    document_id: str,
    new_content: str,
    new_metadata: Dict[str, Any]
) -> None:
    """Chroma ë¬¸ì„œ ì—…ë°ì´íŠ¸"""
    updated_document = Document(
        page_content=new_content,
        metadata=new_metadata
    )

    vectorstore.update_document(document_id=document_id, document=updated_document)
    print(f"ë¬¸ì„œ {document_id} ì—…ë°ì´íŠ¸ ì™„ë£Œ")

def delete_chroma_documents(vectorstore: Chroma, ids: List[str]) -> None:
    """Chroma ë¬¸ì„œ ì‚­ì œ"""
    vectorstore.delete(ids=ids)
    print(f"ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {ids}")

# ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì˜ˆì‹œ
update_chroma_document(
    chroma_db,
    "DOC_1",
    "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ê³¼í•™ì˜ í•µì‹¬ ë¶„ì•¼ ì¤‘ í•˜ë‚˜ë¡œ, ê¸°ê³„í•™ìŠµê³¼ ë”¥ëŸ¬ë‹ì„ í¬í•¨í•©ë‹ˆë‹¤.",
    {"source": "AI ê°œë¡ ", "topic": "ì •ì˜", "updated": True}
)

# ë¬¸ì„œ ì‚­ì œ ì˜ˆì‹œ
delete_chroma_documents(chroma_db, ["DOC_5"])

# ë³€ê²½ ì‚¬í•­ í™•ì¸
updated_results = search_chroma_documents(chroma_db, "ì¸ê³µì§€ëŠ¥ ì •ì˜", k=1)
print(f"ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œ: {updated_results[0].page_content}")
```

### 2. FAISS ë²¡í„° ì €ì¥ì†Œ

#### ê¸°ë³¸ ì„¤ì • ë° ì´ˆê¸°í™”
```python
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS

def create_faiss_vectorstore(
    embedding_model,
    dimension: Optional[int] = None
) -> FAISS:
    """FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""

    # ì„ë² ë”© ì°¨ì› ê³„ì‚° (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
    if dimension is None:
        sample_embedding = embedding_model.embed_query("test")
        dimension = len(sample_embedding)

    # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” (L2 ê±°ë¦¬ ì‚¬ìš©)
    faiss_index = faiss.IndexFlatL2(dimension)

    # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    vectorstore = FAISS(
        embedding_function=embedding_model,
        index=faiss_index,
        docstore=InMemoryDocstore(),
        index_to_docstore_id={},
    )

    print(f"FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ (ì°¨ì›: {dimension})")
    return vectorstore

# FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
embeddings_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
faiss_db = create_faiss_vectorstore(embeddings_model)

print(f"ì €ì¥ëœ ë²¡í„° ìˆ˜: {faiss_db.index.ntotal}")
```

#### FAISS ë¬¸ì„œ ê´€ë¦¬
```python
def add_documents_to_faiss(
    vectorstore: FAISS,
    documents: List[Document],
    ids: Optional[List[str]] = None
) -> List[str]:
    """FAISSì— ë¬¸ì„œ ì¶”ê°€"""
    if ids is None:
        ids = [str(uuid.uuid4()) for _ in range(len(documents))]

    added_ids = vectorstore.add_documents(documents=documents, ids=ids)
    print(f"FAISSì— {len(added_ids)}ê°œ ë¬¸ì„œ ì¶”ê°€")

    return added_ids

def search_faiss_documents(
    vectorstore: FAISS,
    query: str,
    k: int = 3,
    filter_metadata: Optional[Dict[str, Any]] = None
) -> List[Document]:
    """FAISSì—ì„œ ë¬¸ì„œ ê²€ìƒ‰"""
    results = vectorstore.similarity_search(
        query=query,
        k=k,
        filter=filter_metadata
    )

    return results

# Document ê°ì²´ ìƒì„±
documents = [
    Document(page_content=content, metadata=metadata)
    for content, metadata in zip(sample_contents, sample_metadatas)
]

# FAISSì— ë¬¸ì„œ ì¶”ê°€
faiss_ids = add_documents_to_faiss(faiss_db, documents)

# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
faiss_results = search_faiss_documents(faiss_db, "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹", k=2)
print("\n=== FAISS ê²€ìƒ‰ ê²°ê³¼ ===")
for doc in faiss_results:
    print(f"- {doc.page_content}")
    print(f"  ë©”íƒ€ë°ì´í„°: {doc.metadata}")

# FAISS ì €ì¥ ë° ë¡œë“œ
faiss_db.save_local("./faiss_index")
print("FAISS ì¸ë±ìŠ¤ ë¡œì»¬ ì €ì¥ ì™„ë£Œ")

# ì €ì¥ëœ FAISS ë¡œë“œ
loaded_faiss_db = FAISS.load_local(
    "./faiss_index",
    embeddings_model,
    allow_dangerous_deserialization=True
)
print(f"ë¡œë“œëœ FAISS ë²¡í„° ìˆ˜: {loaded_faiss_db.index.ntotal}")
```

### 3. Pinecone ë²¡í„° ì €ì¥ì†Œ

#### Pinecone ì„¤ì • ë° ì´ˆê¸°í™”
```python
def setup_pinecone_vectorstore(
    api_key: str,
    index_name: str,
    dimension: int = 1024,
    metric: str = "euclidean"
) -> Any:
    """Pinecone ë²¡í„° ì €ì¥ì†Œ ì„¤ì •"""

    try:
        from pinecone import Pinecone, ServerlessSpec
        from langchain_pinecone import PineconeVectorStore

        # Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        pc = Pinecone(api_key=api_key)

        # ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
        existing_indexes = [index_info["name"] for index_info in pc.list_indexes()]

        # ì¸ë±ìŠ¤ ìƒì„± (ì—†ëŠ” ê²½ìš°)
        if index_name not in existing_indexes:
            print(f"ìƒˆ Pinecone ì¸ë±ìŠ¤ ìƒì„±: {index_name}")
            pc.create_index(
                name=index_name,
                dimension=dimension,
                metric=metric,
                spec=ServerlessSpec(cloud="aws", region="us-east-1"),
            )

            # ì¸ë±ìŠ¤ ì¤€ë¹„ ëŒ€ê¸°
            import time
            while not pc.describe_index(index_name).status["ready"]:
                time.sleep(1)

        # ì¸ë±ìŠ¤ ì—°ê²°
        index = pc.Index(index_name)

        # PineconeVectorStore ìƒì„±
        vectorstore = PineconeVectorStore(
            index=index,
            embedding=embeddings_model
        )

        print(f"Pinecone ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì™„ë£Œ")
        return vectorstore, pc

    except ImportError:
        print("Pinecone ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None, None
    except Exception as e:
        print(f"Pinecone ì„¤ì • ì‹¤íŒ¨: {e}")
        return None, None

# Pinecone ë²¡í„° ì €ì¥ì†Œ ì„¤ì • (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
if PINECONE_API_KEY:
    pinecone_db, pinecone_client = setup_pinecone_vectorstore(
        api_key=PINECONE_API_KEY,
        index_name="ai-sample-index",
        dimension=1024
    )

    if pinecone_db:
        # ë¬¸ì„œ ì¶”ê°€
        pinecone_ids = pinecone_db.add_documents(documents=documents)
        print(f"Pineconeì— {len(pinecone_ids)}ê°œ ë¬¸ì„œ ì¶”ê°€")

        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        pinecone_results = pinecone_db.similarity_search("ìì—°ì–´ ì²˜ë¦¬", k=2)
        print("\n=== Pinecone ê²€ìƒ‰ ê²°ê³¼ ===")
        for doc in pinecone_results:
            print(f"- {doc.page_content}")
else:
    print("PINECONE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ Pinecone ì˜ˆì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
```

### 4. ë²¡í„° ì €ì¥ì†Œ ì„±ëŠ¥ ë¹„êµ

#### ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ
```python
import time
from typing import Callable

def benchmark_vectorstore_performance(
    vectorstores: Dict[str, Any],
    test_documents: List[Document],
    test_queries: List[str],
    k: int = 5
) -> Dict[str, Dict[str, float]]:
    """ë²¡í„° ì €ì¥ì†Œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""

    results = {}

    for name, vectorstore in vectorstores.items():
        print(f"\n=== {name} ë²¤ì¹˜ë§ˆí¬ ===")

        try:
            # 1. ë¬¸ì„œ ì¶”ê°€ ì„±ëŠ¥
            start_time = time.time()
            test_ids = [f"TEST_{i}" for i in range(len(test_documents))]
            vectorstore.add_documents(documents=test_documents, ids=test_ids)
            add_time = time.time() - start_time

            # 2. ê²€ìƒ‰ ì„±ëŠ¥
            search_times = []
            for query in test_queries:
                start_time = time.time()
                _ = vectorstore.similarity_search(query, k=k)
                search_time = time.time() - start_time
                search_times.append(search_time)

            avg_search_time = sum(search_times) / len(search_times)

            results[name] = {
                "add_time": add_time,
                "avg_search_time": avg_search_time,
                "total_search_time": sum(search_times),
                "docs_per_second": len(test_documents) / add_time,
                "searches_per_second": 1 / avg_search_time
            }

            print(f"ë¬¸ì„œ ì¶”ê°€ ì‹œê°„: {add_time:.3f}ì´ˆ")
            print(f"í‰ê·  ê²€ìƒ‰ ì‹œê°„: {avg_search_time:.3f}ì´ˆ")
            print(f"ì²˜ë¦¬ëŸ‰: {results[name]['docs_per_second']:.1f} docs/sec")

            # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì •ë¦¬
            vectorstore.delete(ids=test_ids)

        except Exception as e:
            print(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            results[name] = {"error": str(e)}

    return results

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
test_docs = [
    Document(page_content=f"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ {i}: AI ê´€ë ¨ ë‚´ìš©", metadata={"test_id": i})
    for i in range(50)  # 50ê°œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
]

test_queries = [
    "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ",
    "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜",
    "ë”¥ëŸ¬ë‹ ëª¨ë¸",
    "ìì—°ì–´ ì²˜ë¦¬"
]

# ë²¡í„° ì €ì¥ì†Œë³„ ì„±ëŠ¥ ë¹„êµ
vectorstores_to_test = {
    "Chroma": chroma_db,
    "FAISS": faiss_db,
}

if 'pinecone_db' in locals() and pinecone_db:
    vectorstores_to_test["Pinecone"] = pinecone_db

benchmark_results = benchmark_vectorstore_performance(
    vectorstores_to_test,
    test_docs,
    test_queries
)

# ê²°ê³¼ ìš”ì•½
print("\n" + "="*60)
print("ë²¡í„° ì €ì¥ì†Œ ì„±ëŠ¥ ë¹„êµ ìš”ì•½")
print("="*60)

for name, metrics in benchmark_results.items():
    if "error" not in metrics:
        print(f"\n{name}:")
        print(f"  ë¬¸ì„œ ì¶”ê°€ ì†ë„: {metrics['docs_per_second']:.1f} docs/sec")
        print(f"  ê²€ìƒ‰ ì†ë„: {metrics['searches_per_second']:.1f} searches/sec")
        print(f"  í‰ê·  ê²€ìƒ‰ ì‹œê°„: {metrics['avg_search_time']:.3f}ì´ˆ")
```

### 5. ê³ ê¸‰ ë²¡í„° ê²€ìƒ‰ ê¸°ë²•

#### í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ë²¡í„°)
```python
def hybrid_search(
    vectorstore: Any,
    query: str,
    k_vector: int = 10,
    k_final: int = 5,
    alpha: float = 0.7
) -> List[Document]:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰"""

    # 1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
    vector_results = vectorstore.similarity_search_with_score(query, k=k_vector)

    # 2. ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
    query_words = set(query.lower().split())

    hybrid_scores = []
    for doc, vector_score in vector_results:
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        doc_words = set(doc.page_content.lower().split())
        keyword_score = len(query_words & doc_words) / len(query_words)

        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ë²¡í„° ì ìˆ˜ëŠ” ê±°ë¦¬ì´ë¯€ë¡œ ì—­ìˆ˜ ì‚¬ìš©)
        vector_similarity = 1 / (1 + vector_score)  # ê±°ë¦¬ â†’ ìœ ì‚¬ë„ ë³€í™˜
        hybrid_score = alpha * vector_similarity + (1 - alpha) * keyword_score

        hybrid_scores.append((doc, hybrid_score))

    # ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ k_finalê°œ ë°˜í™˜
    hybrid_scores.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, _ in hybrid_scores[:k_final]]

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
hybrid_results = hybrid_search(chroma_db, "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜", k_final=3)
print("=== í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ===")
for doc in hybrid_results:
    print(f"- {doc.page_content}")
```

#### ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§
```python
def advanced_metadata_filtering(
    vectorstore: Any,
    query: str,
    filters: Dict[str, Any],
    k: int = 5
) -> List[Document]:
    """ê³ ê¸‰ ë©”íƒ€ë°ì´í„° í•„í„°ë§"""

    # ë³µí•© í•„í„° ì¡°ê±´ êµ¬ì„±
    filter_conditions = {}

    for key, value in filters.items():
        if isinstance(value, list):
            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° OR ì¡°ê±´ìœ¼ë¡œ ì²˜ë¦¬ (Chromaì˜ $in ì—°ì‚°ì)
            filter_conditions[key] = {"$in": value}
        elif isinstance(value, dict):
            # ë²”ìœ„ ì¡°ê±´ ë“± ë³µì¡í•œ í•„í„°
            filter_conditions[key] = value
        else:
            # ë‹¨ìˆœ ì¼ì¹˜
            filter_conditions[key] = value

    # í•„í„°ë§ëœ ê²€ìƒ‰ ìˆ˜í–‰
    results = vectorstore.similarity_search(
        query=query,
        k=k,
        filter=filter_conditions
    )

    return results

# ê³ ê¸‰ í•„í„°ë§ ì˜ˆì‹œ
# ë¨¼ì € ë‹¤ì–‘í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì§„ ë¬¸ì„œë“¤ì„ ì¶”ê°€
advanced_documents = [
    Document(
        page_content="Python ê¸°ë°˜ ì›¹ ê°œë°œ ê°€ì´ë“œ",
        metadata={"category": "programming", "language": "python", "difficulty": "beginner"}
    ),
    Document(
        page_content="JavaScript ê³ ê¸‰ ê¸°ë²•ë“¤",
        metadata={"category": "programming", "language": "javascript", "difficulty": "advanced"}
    ),
    Document(
        page_content="ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ íŒ¨í„´",
        metadata={"category": "database", "language": "sql", "difficulty": "intermediate"}
    ),
    Document(
        page_content="ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê°œë…",
        metadata={"category": "ai", "language": "python", "difficulty": "advanced"}
    )
]

# ë¬¸ì„œ ì¶”ê°€
adv_ids = chroma_db.add_documents(advanced_documents)

# ë³µí•© í•„í„°ë§ ê²€ìƒ‰
filtered_results = advanced_metadata_filtering(
    chroma_db,
    "í”„ë¡œê·¸ë˜ë° ê°œë°œ",
    {
        "category": ["programming", "ai"],  # programming ë˜ëŠ” ai
        "language": "python"  # python ì–¸ì–´
    },
    k=3
)

print("=== ê³ ê¸‰ í•„í„°ë§ ê²€ìƒ‰ ê²°ê³¼ ===")
for doc in filtered_results:
    print(f"- {doc.page_content}")
    print(f"  ë©”íƒ€ë°ì´í„°: {doc.metadata}")
```

## ğŸš€ ì‹¤ìŠµí•´ë³´ê¸°

### ì‹¤ìŠµ 1: ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ ì‹œìŠ¤í…œ
ì™„ì „í•œ CRUD ê¸°ëŠ¥ì„ ê°€ì§„ ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
class VectorStoreManager:
    """ë²¡í„° ì €ì¥ì†Œ í†µí•© ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, vectorstore_type: str = "chroma"):
        # TODO: ë²¡í„° ì €ì¥ì†Œ íƒ€ì…ì— ë”°ë¥¸ ì´ˆê¸°í™”
        # TODO: ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        # TODO: ê¸°ë³¸ ì»¬ë ‰ì…˜ ìƒì„±
        pass

    def add_document(self, content: str, metadata: Dict[str, Any]) -> str:
        # TODO: ë‹¨ì¼ ë¬¸ì„œ ì¶”ê°€
        # TODO: ìë™ ID ìƒì„±
        # TODO: ì¤‘ë³µ ì²´í¬
        pass

    def batch_add_documents(self, documents: List[Dict[str, Any]]) -> List[str]:
        # TODO: ëŒ€ëŸ‰ ë¬¸ì„œ ì¼ê´„ ì¶”ê°€
        # TODO: íŠ¸ëœì­ì…˜ ì²˜ë¦¬
        pass

    def search_documents(
        self,
        query: str,
        k: int = 5,
        filters: Optional[Dict] = None
    ) -> List[Dict[str, Any]]:
        # TODO: í†µí•© ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤
        # TODO: ë‹¤ì–‘í•œ ê²€ìƒ‰ ì˜µì…˜ ì§€ì›
        pass

    def update_document(self, doc_id: str, content: str, metadata: Dict) -> bool:
        # TODO: ë¬¸ì„œ ì—…ë°ì´íŠ¸
        pass

    def delete_documents(self, doc_ids: List[str]) -> int:
        # TODO: ë¬¸ì„œ ì‚­ì œ
        # TODO: ì‚­ì œëœ ë¬¸ì„œ ìˆ˜ ë°˜í™˜
        pass

# í…ŒìŠ¤íŠ¸
manager = VectorStoreManager("chroma")
```

### ì‹¤ìŠµ 2: ì„±ëŠ¥ ìµœì í™”ëœ ê²€ìƒ‰ ì‹œìŠ¤í…œ
ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµì„ ì¡°í•©í•œ ê³ ì„±ëŠ¥ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
class OptimizedSearchSystem:
    """ì„±ëŠ¥ ìµœì í™”ëœ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ"""

    def __init__(self):
        # TODO: ì—¬ëŸ¬ ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        # TODO: ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„
        # TODO: ê²€ìƒ‰ ì „ëµ ì„¤ì •
        pass

    def semantic_search(self, query: str, k: int = 5) -> List[Document]:
        # TODO: ìˆœìˆ˜ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
        pass

    def keyword_search(self, query: str, k: int = 5) -> List[Document]:
        # TODO: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        pass

    def hybrid_search(
        self,
        query: str,
        semantic_weight: float = 0.7,
        k: int = 5
    ) -> List[Document]:
        # TODO: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬í˜„
        # TODO: ì ìˆ˜ ì •ê·œí™” ë° ê²°í•©
        pass

    def cached_search(self, query: str) -> List[Document]:
        # TODO: ìºì‹œëœ ê²°ê³¼ í™œìš©
        pass

# í…ŒìŠ¤íŠ¸
search_system = OptimizedSearchSystem()
```

### ì‹¤ìŠµ 3: ë²¡í„° ì €ì¥ì†Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬
ì„œë¡œ ë‹¤ë¥¸ ë²¡í„° ì €ì¥ì†Œ ê°„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬ë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
class VectorStoreMigrator:
    """ë²¡í„° ì €ì¥ì†Œ ê°„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""

    def migrate_data(
        self,
        source_store: Any,
        target_store: Any,
        batch_size: int = 100
    ) -> Dict[str, int]:
        # TODO: ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        # TODO: ë°°ì¹˜ ë‹¨ìœ„ë¡œ íƒ€ê²Ÿì— ì‚½ì…
        # TODO: ì§„í–‰ìƒí™© í‘œì‹œ
        # TODO: ë§ˆì´ê·¸ë ˆì´ì…˜ í†µê³„ ë°˜í™˜
        pass

    def verify_migration(
        self,
        source_store: Any,
        target_store: Any
    ) -> Dict[str, Any]:
        # TODO: ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
        # TODO: ë²¡í„° ìœ ì‚¬ë„ ê²€ì¦
        # TODO: ë©”íƒ€ë°ì´í„° ì¼ê´€ì„± í™•ì¸
        pass

# í…ŒìŠ¤íŠ¸
migrator = VectorStoreMigrator()
```

## ğŸ“‹ í•´ë‹µ

### ì‹¤ìŠµ 1 í•´ë‹µ: ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ ì‹œìŠ¤í…œ
```python
import uuid
from typing import Dict, List, Any, Optional
import time
from dataclasses import dataclass

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    metadata: Dict[str, Any]
    score: float
    doc_id: str

class VectorStoreManager:
    """ë²¡í„° ì €ì¥ì†Œ í†µí•© ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, vectorstore_type: str = "chroma", **kwargs):
        self.vectorstore_type = vectorstore_type
        self.embeddings_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

        # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        if vectorstore_type.lower() == "chroma":
            self._init_chroma(**kwargs)
        elif vectorstore_type.lower() == "faiss":
            self._init_faiss(**kwargs)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë²¡í„° ì €ì¥ì†Œ: {vectorstore_type}")

        # í†µê³„ ì •ë³´
        self.stats = {
            "total_documents": 0,
            "total_searches": 0,
            "last_updated": None
        }

    def _init_chroma(self, **kwargs):
        """Chroma ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”"""
        from langchain_chroma import Chroma

        collection_name = kwargs.get("collection_name", "managed_collection")
        persist_directory = kwargs.get("persist_directory", "./managed_chroma")

        self.vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=self.embeddings_model,
            persist_directory=persist_directory
        )

        # ê¸°ì¡´ ë¬¸ì„œ ìˆ˜ í™•ì¸
        existing_data = self.vectorstore.get()
        self.stats["total_documents"] = len(existing_data["documents"])

    def _init_faiss(self, **kwargs):
        """FAISS ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”"""
        import faiss
        from langchain_community.docstore.in_memory import InMemoryDocstore
        from langchain_community.vectorstores import FAISS

        dimension = kwargs.get("dimension", 1024)
        faiss_index = faiss.IndexFlatL2(dimension)

        self.vectorstore = FAISS(
            embedding_function=self.embeddings_model,
            index=faiss_index,
            docstore=InMemoryDocstore(),
            index_to_docstore_id={}
        )

    def add_document(
        self,
        content: str,
        metadata: Dict[str, Any],
        doc_id: Optional[str] = None
    ) -> str:
        """ë‹¨ì¼ ë¬¸ì„œ ì¶”ê°€"""

        if doc_id is None:
            doc_id = str(uuid.uuid4())

        # ì¤‘ë³µ ì²´í¬ (ê¸°ì¡´ IDê°€ ìˆëŠ”ì§€ í™•ì¸)
        if self._document_exists(doc_id):
            raise ValueError(f"ë¬¸ì„œ ID {doc_id}ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

        # ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ ì •ë³´ í¬í•¨
        enhanced_metadata = {
            **metadata,
            "created_at": time.time(),
            "doc_id": doc_id
        }

        document = Document(page_content=content, metadata=enhanced_metadata)

        try:
            self.vectorstore.add_documents(documents=[document], ids=[doc_id])
            self.stats["total_documents"] += 1
            self.stats["last_updated"] = time.time()

            print(f"ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ: {doc_id}")
            return doc_id

        except Exception as e:
            print(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise

    def batch_add_documents(
        self,
        documents: List[Dict[str, Any]],
        batch_size: int = 50
    ) -> List[str]:
        """ëŒ€ëŸ‰ ë¬¸ì„œ ì¼ê´„ ì¶”ê°€"""

        added_ids = []
        total_batches = (len(documents) + batch_size - 1) // batch_size

        for batch_idx in range(0, len(documents), batch_size):
            batch = documents[batch_idx:batch_idx + batch_size]
            batch_docs = []
            batch_ids = []

            for doc_data in batch:
                doc_id = doc_data.get("id", str(uuid.uuid4()))
                content = doc_data["content"]
                metadata = doc_data.get("metadata", {})

                # ì¤‘ë³µ ì²´í¬ ê±´ë„ˆë›°ê¸° (ì„±ëŠ¥ìƒ ì´ìœ )
                enhanced_metadata = {
                    **metadata,
                    "created_at": time.time(),
                    "doc_id": doc_id,
                    "batch_id": batch_idx // batch_size
                }

                document = Document(page_content=content, metadata=enhanced_metadata)
                batch_docs.append(document)
                batch_ids.append(doc_id)

            try:
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¶”ê°€
                self.vectorstore.add_documents(documents=batch_docs, ids=batch_ids)
                added_ids.extend(batch_ids)

                print(f"ë°°ì¹˜ {(batch_idx//batch_size)+1}/{total_batches} ì™„ë£Œ ({len(batch_ids)}ê°œ ë¬¸ì„œ)")

            except Exception as e:
                print(f"ë°°ì¹˜ {(batch_idx//batch_size)+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        self.stats["total_documents"] += len(added_ids)
        self.stats["last_updated"] = time.time()

        print(f"ì´ {len(added_ids)}ê°œ ë¬¸ì„œ ì¼ê´„ ì¶”ê°€ ì™„ë£Œ")
        return added_ids

    def search_documents(
        self,
        query: str,
        k: int = 5,
        filters: Optional[Dict] = None,
        search_type: str = "similarity",
        min_score: Optional[float] = None
    ) -> List[SearchResult]:
        """í†µí•© ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤"""

        self.stats["total_searches"] += 1

        try:
            if search_type == "similarity":
                raw_results = self.vectorstore.similarity_search(
                    query=query, k=k, filter=filters
                )
                results = [
                    SearchResult(
                        content=doc.page_content,
                        metadata=doc.metadata,
                        score=1.0,  # ê¸°ë³¸ ì ìˆ˜
                        doc_id=doc.metadata.get("doc_id", "unknown")
                    )
                    for doc in raw_results
                ]

            elif search_type == "similarity_with_score":
                raw_results = self.vectorstore.similarity_search_with_score(
                    query=query, k=k, filter=filters
                )
                results = [
                    SearchResult(
                        content=doc.page_content,
                        metadata=doc.metadata,
                        score=float(score),
                        doc_id=doc.metadata.get("doc_id", "unknown")
                    )
                    for doc, score in raw_results
                ]

            elif search_type == "relevance_score":
                raw_results = self.vectorstore.similarity_search_with_relevance_scores(
                    query=query, k=k, filter=filters
                )
                results = [
                    SearchResult(
                        content=doc.page_content,
                        metadata=doc.metadata,
                        score=float(score),
                        doc_id=doc.metadata.get("doc_id", "unknown")
                    )
                    for doc, score in raw_results
                ]

            # ìµœì†Œ ì ìˆ˜ í•„í„°ë§
            if min_score is not None:
                if search_type == "similarity_with_score":
                    # ê±°ë¦¬ ì ìˆ˜ì¸ ê²½ìš° (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                    results = [r for r in results if r.score <= min_score]
                else:
                    # ìœ ì‚¬ë„ ì ìˆ˜ì¸ ê²½ìš° (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                    results = [r for r in results if r.score >= min_score]

            print(f"ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ (ì¿¼ë¦¬: '{query}')")
            return results

        except Exception as e:
            print(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def update_document(
        self,
        doc_id: str,
        content: str,
        metadata: Dict[str, Any]
    ) -> bool:
        """ë¬¸ì„œ ì—…ë°ì´íŠ¸"""

        try:
            # ë©”íƒ€ë°ì´í„°ì— ì—…ë°ì´íŠ¸ ì •ë³´ ì¶”ê°€
            enhanced_metadata = {
                **metadata,
                "updated_at": time.time(),
                "doc_id": doc_id
            }

            updated_document = Document(
                page_content=content,
                metadata=enhanced_metadata
            )

            if self.vectorstore_type.lower() == "chroma":
                self.vectorstore.update_document(
                    document_id=doc_id,
                    document=updated_document
                )
            else:
                # FAISSëŠ” ì§ì ‘ì ì¸ ì—…ë°ì´íŠ¸ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‚­ì œ í›„ ì¶”ê°€
                self.vectorstore.delete([doc_id])
                self.vectorstore.add_documents(documents=[updated_document], ids=[doc_id])

            self.stats["last_updated"] = time.time()
            print(f"ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {doc_id}")
            return True

        except Exception as e:
            print(f"ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False

    def delete_documents(self, doc_ids: List[str]) -> int:
        """ë¬¸ì„œ ì‚­ì œ"""

        try:
            # ì¡´ì¬í•˜ëŠ” ë¬¸ì„œë§Œ ì‚­ì œ ì‹œë„
            existing_ids = [doc_id for doc_id in doc_ids if self._document_exists(doc_id)]

            if not existing_ids:
                print("ì‚­ì œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0

            self.vectorstore.delete(ids=existing_ids)

            deleted_count = len(existing_ids)
            self.stats["total_documents"] -= deleted_count
            self.stats["last_updated"] = time.time()

            print(f"{deleted_count}ê°œ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
            return deleted_count

        except Exception as e:
            print(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return 0

    def get_statistics(self) -> Dict[str, Any]:
        """ê´€ë¦¬ í†µê³„ ì •ë³´ ë°˜í™˜"""
        stats = self.stats.copy()

        if stats["last_updated"]:
            stats["last_updated_human"] = time.ctime(stats["last_updated"])

        return stats

    def _document_exists(self, doc_id: str) -> bool:
        """ë¬¸ì„œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            if self.vectorstore_type.lower() == "chroma":
                result = self.vectorstore.get(ids=[doc_id])
                return len(result["ids"]) > 0
            elif self.vectorstore_type.lower() == "faiss":
                return doc_id in self.vectorstore.index_to_docstore_id.values()
            return False
        except:
            return False

# ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
print("=== ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")

# ê´€ë¦¬ì ì´ˆê¸°í™”
manager = VectorStoreManager("chroma", collection_name="test_managed_collection")

# ë‹¨ì¼ ë¬¸ì„œ ì¶”ê°€
doc_id = manager.add_document(
    content="ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” AI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤.",
    metadata={"category": "database", "importance": "high"}
)

# ëŒ€ëŸ‰ ë¬¸ì„œ ì¶”ê°€
batch_documents = [
    {
        "content": f"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ {i}: AIì™€ ë¨¸ì‹ ëŸ¬ë‹ ê´€ë ¨ ë‚´ìš©",
        "metadata": {"category": "ai", "test_batch": True, "number": i}
    }
    for i in range(10)
]

batch_ids = manager.batch_add_documents(batch_documents)

# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
search_results = manager.search_documents(
    "AI ë¨¸ì‹ ëŸ¬ë‹",
    k=5,
    search_type="similarity_with_score"
)

print("\nê²€ìƒ‰ ê²°ê³¼:")
for result in search_results:
    print(f"- ì ìˆ˜: {result.score:.4f}")
    print(f"  ë‚´ìš©: {result.content}")
    print(f"  ID: {result.doc_id}")

# í†µê³„ ì •ë³´ í™•ì¸
stats = manager.get_statistics()
print(f"\ní†µê³„ ì •ë³´:")
for key, value in stats.items():
    print(f"  {key}: {value}")

# ë¬¸ì„œ ì—…ë°ì´íŠ¸
manager.update_document(
    doc_id,
    "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” í˜„ëŒ€ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ í•„ìˆ˜ì ì¸ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤.",
    {"category": "database", "importance": "critical", "version": 2}
)

# ë¬¸ì„œ ì‚­ì œ
deleted_count = manager.delete_documents(batch_ids[:5])  # ì²˜ìŒ 5ê°œ ì‚­ì œ
```

### ì‹¤ìŠµ 2 í•´ë‹µ: ì„±ëŠ¥ ìµœì í™”ëœ ê²€ìƒ‰ ì‹œìŠ¤í…œ
```python
from functools import lru_cache
import re
from collections import Counter
import hashlib

class OptimizedSearchSystem:
    """ì„±ëŠ¥ ìµœì í™”ëœ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ"""

    def __init__(self, cache_size: int = 1000):
        # ì—¬ëŸ¬ ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        self.embeddings_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
        self.chroma_db = self._create_chroma_store()
        self.cache_size = cache_size

        # ê²€ìƒ‰ í†µê³„
        self.search_stats = {
            "semantic_searches": 0,
            "keyword_searches": 0,
            "hybrid_searches": 0,
            "cache_hits": 0,
            "cache_misses": 0
        }

        # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Redis ë“± ì‚¬ìš©)
        self._search_cache = {}

    def _create_chroma_store(self):
        """Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
        from langchain_chroma import Chroma
        return Chroma(
            collection_name="optimized_search",
            embedding_function=self.embeddings_model,
            persist_directory="./optimized_chroma"
        )

    def _get_cache_key(self, query: str, search_type: str, k: int) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        cache_string = f"{query}:{search_type}:{k}"
        return hashlib.md5(cache_string.encode()).hexdigest()

    def _preprocess_query(self, query: str) -> str:
        """ì¿¼ë¦¬ ì „ì²˜ë¦¬"""
        # ì†Œë¬¸ì ë³€í™˜, íŠ¹ìˆ˜ë¬¸ì ì œê±°
        processed = re.sub(r'[^\w\s]', ' ', query.lower())
        # ì¤‘ë³µ ê³µë°± ì œê±°
        processed = re.sub(r'\s+', ' ', processed).strip()
        return processed

    def semantic_search(self, query: str, k: int = 5) -> List[Document]:
        """ìˆœìˆ˜ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰"""
        self.search_stats["semantic_searches"] += 1

        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(query, "semantic", k)
        if cache_key in self._search_cache:
            self.search_stats["cache_hits"] += 1
            return self._search_cache[cache_key]

        self.search_stats["cache_misses"] += 1

        # ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰
        results = self.chroma_db.similarity_search(query, k=k)

        # ìºì‹œì— ì €ì¥ (í¬ê¸° ì œí•œ)
        if len(self._search_cache) < self.cache_size:
            self._search_cache[cache_key] = results

        return results

    def keyword_search(self, query: str, k: int = 5) -> List[Document]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        self.search_stats["keyword_searches"] += 1

        # ì „ì²´ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œë¡œëŠ” ì—­ìƒ‰ì¸ ì‚¬ìš© ê¶Œì¥)
        all_docs_data = self.chroma_db.get()
        all_documents = [
            Document(page_content=content, metadata=metadata)
            for content, metadata in zip(all_docs_data["documents"], all_docs_data["metadatas"])
        ]

        # ì¿¼ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        query_words = set(self._preprocess_query(query).split())

        # ê° ë¬¸ì„œì˜ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        scored_docs = []
        for doc in all_documents:
            doc_words = set(self._preprocess_query(doc.page_content).split())

            # êµì§‘í•© ê¸°ë°˜ ì ìˆ˜ (Jaccard ìœ ì‚¬ë„)
            intersection = len(query_words & doc_words)
            union = len(query_words | doc_words)

            if union > 0:
                score = intersection / union

                # TF ì ìˆ˜ ì¶”ê°€ ê³ ë ¤
                word_counts = Counter(self._preprocess_query(doc.page_content).split())
                tf_score = sum(word_counts[word] for word in query_words if word in word_counts)

                combined_score = score + (tf_score * 0.1)  # TF ê°€ì¤‘ì¹˜
                scored_docs.append((doc, combined_score))

        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in scored_docs[:k]]

    def hybrid_search(
        self,
        query: str,
        semantic_weight: float = 0.7,
        k: int = 5,
        intermediate_k: int = 20
    ) -> List[Document]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬í˜„"""
        self.search_stats["hybrid_searches"] += 1

        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(f"{query}:hybrid:{semantic_weight}", "hybrid", k)
        if cache_key in self._search_cache:
            self.search_stats["cache_hits"] += 1
            return self._search_cache[cache_key]

        self.search_stats["cache_misses"] += 1

        # 1. ì˜ë¯¸ì  ê²€ìƒ‰ (ì ìˆ˜ í¬í•¨)
        semantic_results = self.chroma_db.similarity_search_with_score(
            query, k=intermediate_k
        )

        # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
        keyword_results = self.keyword_search(query, k=intermediate_k)

        # 3. ë¬¸ì„œë³„ ì ìˆ˜ í†µí•©
        doc_scores = {}
        query_words = set(self._preprocess_query(query).split())

        # ì˜ë¯¸ì  ì ìˆ˜ ì •ê·œí™” ë° ì €ì¥
        for doc, distance_score in semantic_results:
            doc_content = doc.page_content
            # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ë‚®ì€ ê±°ë¦¬ = ë†’ì€ ìœ ì‚¬ë„)
            semantic_sim = 1 / (1 + distance_score)
            doc_scores[doc_content] = {
                "doc": doc,
                "semantic_score": semantic_sim,
                "keyword_score": 0.0
            }

        # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚° ë° ì¶”ê°€
        for doc in keyword_results:
            doc_content = doc.page_content
            doc_words = set(self._preprocess_query(doc_content).split())

            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            if len(query_words) > 0:
                keyword_score = len(query_words & doc_words) / len(query_words)
            else:
                keyword_score = 0.0

            if doc_content in doc_scores:
                doc_scores[doc_content]["keyword_score"] = keyword_score
            else:
                doc_scores[doc_content] = {
                    "doc": doc,
                    "semantic_score": 0.0,
                    "keyword_score": keyword_score
                }

        # 4. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        hybrid_results = []
        for doc_content, scores in doc_scores.items():
            hybrid_score = (
                semantic_weight * scores["semantic_score"] +
                (1 - semantic_weight) * scores["keyword_score"]
            )
            hybrid_results.append((scores["doc"], hybrid_score))

        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜
        hybrid_results.sort(key=lambda x: x[1], reverse=True)
        final_results = [doc for doc, _ in hybrid_results[:k]]

        # ìºì‹œì— ì €ì¥
        if len(self._search_cache) < self.cache_size:
            self._search_cache[cache_key] = final_results

        return final_results

    def cached_search(self, query: str, search_type: str = "semantic", k: int = 5) -> List[Document]:
        """ìºì‹œ ìš°ì„  ê²€ìƒ‰"""
        cache_key = self._get_cache_key(query, search_type, k)

        if cache_key in self._search_cache:
            self.search_stats["cache_hits"] += 1
            return self._search_cache[cache_key]

        # ìºì‹œ ë¯¸ìŠ¤ì¸ ê²½ìš° í•´ë‹¹ ê²€ìƒ‰ ë°©ë²• ì‹¤í–‰
        if search_type == "semantic":
            return self.semantic_search(query, k)
        elif search_type == "keyword":
            return self.keyword_search(query, k)
        elif search_type == "hybrid":
            return self.hybrid_search(query, k=k)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ íƒ€ì…: {search_type}")

    def clear_cache(self) -> None:
        """ìºì‹œ ë¹„ìš°ê¸°"""
        self._search_cache.clear()
        print("ê²€ìƒ‰ ìºì‹œë¥¼ ë¹„ì› ìŠµë‹ˆë‹¤.")

    def get_search_statistics(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ í†µê³„ ë°˜í™˜"""
        stats = self.search_stats.copy()

        # ìºì‹œ ì ì¤‘ë¥  ê³„ì‚°
        total_cache_requests = stats["cache_hits"] + stats["cache_misses"]
        if total_cache_requests > 0:
            stats["cache_hit_rate"] = stats["cache_hits"] / total_cache_requests
        else:
            stats["cache_hit_rate"] = 0.0

        stats["cache_size"] = len(self._search_cache)
        return stats

# ì„±ëŠ¥ ìµœì í™” ê²€ìƒ‰ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
print("=== ì„±ëŠ¥ ìµœì í™”ëœ ê²€ìƒ‰ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")

# ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
search_system = OptimizedSearchSystem()

# í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ì¶”ê°€
test_documents = [
    Document(
        page_content="ì¸ê³µì§€ëŠ¥ê³¼ ê¸°ê³„í•™ìŠµ ê¸°ìˆ ì˜ ë°œì „",
        metadata={"category": "ai", "tags": ["ai", "ml", "technology"]}
    ),
    Document(
        page_content="ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ ì„¤ê³„ ë° ìµœì í™”",
        metadata={"category": "database", "tags": ["database", "optimization"]}
    ),
    Document(
        page_content="ì›¹ ê°œë°œì„ ìœ„í•œ JavaScript í”„ë¡œê·¸ë˜ë°",
        metadata={"category": "programming", "tags": ["web", "javascript"]}
    ),
    Document(
        page_content="ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ê³¼ ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬",
        metadata={"category": "ai", "tags": ["ml", "deep learning", "algorithms"]}
    ),
    Document(
        page_content="í´ë¼ìš°ë“œ ì»´í“¨íŒ… ì•„í‚¤í…ì²˜ì™€ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤",
        metadata={"category": "cloud", "tags": ["cloud", "microservices", "architecture"]}
    )
]

search_system.chroma_db.add_documents(test_documents)

# ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ë²• í…ŒìŠ¤íŠ¸
query = "ì¸ê³µì§€ëŠ¥ ë¨¸ì‹ ëŸ¬ë‹"

print(f"\nì¿¼ë¦¬: '{query}'\n")

# 1. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
print("1. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰:")
semantic_results = search_system.semantic_search(query, k=3)
for i, doc in enumerate(semantic_results, 1):
    print(f"   {i}. {doc.page_content}")

# 2. í‚¤ì›Œë“œ ê²€ìƒ‰
print("\n2. í‚¤ì›Œë“œ ê²€ìƒ‰:")
keyword_results = search_system.keyword_search(query, k=3)
for i, doc in enumerate(keyword_results, 1):
    print(f"   {i}. {doc.page_content}")

# 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
print("\n3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰:")
hybrid_results = search_system.hybrid_search(query, semantic_weight=0.6, k=3)
for i, doc in enumerate(hybrid_results, 1):
    print(f"   {i}. {doc.page_content}")

# 4. ìºì‹œëœ ê²€ìƒ‰ (ë‘ ë²ˆì§¸ ì‹¤í–‰)
print("\n4. ìºì‹œëœ ê²€ìƒ‰ (ë°˜ë³µ ì¿¼ë¦¬):")
cached_results = search_system.cached_search(query, "hybrid", k=3)
for i, doc in enumerate(cached_results, 1):
    print(f"   {i}. {doc.page_content}")

# ê²€ìƒ‰ í†µê³„ í™•ì¸
stats = search_system.get_search_statistics()
print(f"\nê²€ìƒ‰ í†µê³„:")
for key, value in stats.items():
    print(f"  {key}: {value}")
```

### ì‹¤ìŠµ 3 í•´ë‹µ: ë²¡í„° ì €ì¥ì†Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬
```python
import time
from tqdm import tqdm
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class VectorStoreMigrator:
    """ë²¡í„° ì €ì¥ì†Œ ê°„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""

    def __init__(self):
        self.migration_log = []

    def migrate_data(
        self,
        source_store: Any,
        target_store: Any,
        batch_size: int = 100,
        verify_embeddings: bool = False
    ) -> Dict[str, int]:
        """ì†ŒìŠ¤ì—ì„œ íƒ€ê²Ÿìœ¼ë¡œ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""

        print("=== ë²¡í„° ì €ì¥ì†Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘ ===")
        start_time = time.time()

        # 1. ì†ŒìŠ¤ ë°ì´í„° ì¶”ì¶œ
        print("1. ì†ŒìŠ¤ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        source_data = self._extract_all_data(source_store)

        total_docs = len(source_data["documents"])
        print(f"   ì¶”ì¶œëœ ë¬¸ì„œ ìˆ˜: {total_docs}")

        if total_docs == 0:
            return {"migrated": 0, "failed": 0, "skipped": 0}

        # 2. ë°°ì¹˜ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜
        print("2. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘...")

        migrated_count = 0
        failed_count = 0
        skipped_count = 0

        # ì§„í–‰ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ tqdm ì‚¬ìš©
        with tqdm(total=total_docs, desc="ë§ˆì´ê·¸ë ˆì´ì…˜") as pbar:
            for i in range(0, total_docs, batch_size):
                batch_end = min(i + batch_size, total_docs)
                batch_docs = source_data["documents"][i:batch_end]
                batch_metadatas = source_data["metadatas"][i:batch_end]
                batch_ids = source_data["ids"][i:batch_end]

                # Document ê°ì²´ ìƒì„±
                documents = [
                    Document(page_content=content, metadata=metadata)
                    for content, metadata in zip(batch_docs, batch_metadatas)
                ]

                try:
                    # íƒ€ê²Ÿì— ë°°ì¹˜ ì‚½ì…
                    target_store.add_documents(documents=documents, ids=batch_ids)
                    migrated_count += len(documents)

                    # ì„ë² ë”© ê²€ì¦ (ì˜µì…˜)
                    if verify_embeddings:
                        verification_result = self._verify_batch_embeddings(
                            source_store, target_store, batch_ids[0:1]  # ì²« ë²ˆì§¸ë§Œ ê²€ì¦
                        )
                        if not verification_result["success"]:
                            print(f"   ê²½ê³ : ë°°ì¹˜ {i//batch_size + 1} ì„ë² ë”© ê²€ì¦ ì‹¤íŒ¨")

                except Exception as e:
                    failed_count += len(documents)
                    self.migration_log.append({
                        "batch": i // batch_size + 1,
                        "error": str(e),
                        "failed_ids": batch_ids
                    })
                    print(f"   ë°°ì¹˜ {i//batch_size + 1} ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")

                pbar.update(batch_end - i)

                # ì ê¹ ëŒ€ê¸° (ì‹œìŠ¤í…œ ë¶€í•˜ ë°©ì§€)
                time.sleep(0.1)

        end_time = time.time()
        migration_time = end_time - start_time

        # 3. ê²°ê³¼ ìš”ì•½
        result = {
            "migrated": migrated_count,
            "failed": failed_count,
            "skipped": skipped_count,
            "total_time": migration_time,
            "docs_per_second": migrated_count / migration_time if migration_time > 0 else 0
        }

        print(f"\n=== ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ ===")
        print(f"ì„±ê³µ: {migrated_count}ê°œ")
        print(f"ì‹¤íŒ¨: {failed_count}ê°œ")
        print(f"ì´ ì†Œìš” ì‹œê°„: {migration_time:.2f}ì´ˆ")
        print(f"ì²˜ë¦¬ ì†ë„: {result['docs_per_second']:.1f} docs/sec")

        return result

    def verify_migration(
        self,
        source_store: Any,
        target_store: Any,
        sample_size: int = 10,
        embedding_threshold: float = 0.95
    ) -> Dict[str, Any]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦"""

        print("=== ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì‹œì‘ ===")

        # 1. ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
        print("1. ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦...")
        integrity_result = self._verify_data_integrity(source_store, target_store)

        # 2. ì„ë² ë”© ìœ ì‚¬ë„ ê²€ì¦
        print("2. ì„ë² ë”© ìœ ì‚¬ë„ ê²€ì¦...")
        embedding_result = self._verify_embedding_similarity(
            source_store, target_store, sample_size, embedding_threshold
        )

        # 3. ê²€ìƒ‰ ê¸°ëŠ¥ ê²€ì¦
        print("3. ê²€ìƒ‰ ê¸°ëŠ¥ ê²€ì¦...")
        search_result = self._verify_search_functionality(source_store, target_store)

        # ì „ì²´ ê²€ì¦ ê²°ê³¼
        overall_success = (
            integrity_result["success"] and
            embedding_result["success"] and
            search_result["success"]
        )

        verification_result = {
            "overall_success": overall_success,
            "data_integrity": integrity_result,
            "embedding_similarity": embedding_result,
            "search_functionality": search_result
        }

        print(f"\n=== ê²€ì¦ ì™„ë£Œ ===")
        print(f"ì „ì²´ ê²€ì¦ ê²°ê³¼: {'ì„±ê³µ' if overall_success else 'ì‹¤íŒ¨'}")

        return verification_result

    def _extract_all_data(self, vectorstore: Any) -> Dict[str, List]:
        """ë²¡í„° ì €ì¥ì†Œì—ì„œ ëª¨ë“  ë°ì´í„° ì¶”ì¶œ"""
        try:
            if hasattr(vectorstore, 'get'):
                # Chroma ìŠ¤íƒ€ì¼
                data = vectorstore.get()
                return {
                    "documents": data["documents"],
                    "metadatas": data["metadatas"],
                    "ids": data["ids"]
                }
            elif hasattr(vectorstore, 'docstore'):
                # FAISS ìŠ¤íƒ€ì¼
                documents = []
                metadatas = []
                ids = []

                for idx, doc_id in vectorstore.index_to_docstore_id.items():
                    doc = vectorstore.docstore.search(doc_id)
                    documents.append(doc.page_content)
                    metadatas.append(doc.metadata)
                    ids.append(doc_id)

                return {
                    "documents": documents,
                    "metadatas": metadatas,
                    "ids": ids
                }
            else:
                raise NotImplementedError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë²¡í„° ì €ì¥ì†Œ íƒ€ì…")

        except Exception as e:
            print(f"ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {"documents": [], "metadatas": [], "ids": []}

    def _verify_data_integrity(
        self,
        source_store: Any,
        target_store: Any
    ) -> Dict[str, Any]:
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""

        try:
            source_data = self._extract_all_data(source_store)
            target_data = self._extract_all_data(target_store)

            source_count = len(source_data["documents"])
            target_count = len(target_data["documents"])

            # ë¬¸ì„œ ìˆ˜ ë¹„êµ
            count_match = source_count == target_count

            # ID ì¼ì¹˜ í™•ì¸
            source_ids = set(source_data["ids"])
            target_ids = set(target_data["ids"])
            ids_match = source_ids == target_ids

            # ë‚´ìš© ì¼ì¹˜ í™•ì¸ (ìƒ˜í”Œë§)
            content_matches = 0
            total_checked = 0

            for i, doc_id in enumerate(source_data["ids"][:10]):  # ì²˜ìŒ 10ê°œë§Œ í™•ì¸
                if doc_id in target_data["ids"]:
                    target_idx = target_data["ids"].index(doc_id)
                    if source_data["documents"][i] == target_data["documents"][target_idx]:
                        content_matches += 1
                    total_checked += 1

            content_match_rate = content_matches / total_checked if total_checked > 0 else 0

            return {
                "success": count_match and ids_match and content_match_rate > 0.9,
                "source_count": source_count,
                "target_count": target_count,
                "count_match": count_match,
                "ids_match": ids_match,
                "content_match_rate": content_match_rate
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

    def _verify_embedding_similarity(
        self,
        source_store: Any,
        target_store: Any,
        sample_size: int,
        threshold: float
    ) -> Dict[str, Any]:
        """ì„ë² ë”© ìœ ì‚¬ë„ ê²€ì¦"""

        try:
            # ìƒ˜í”Œ ë¬¸ì„œ ì„ íƒ
            source_data = self._extract_all_data(source_store)

            if len(source_data["documents"]) == 0:
                return {"success": False, "error": "ê²€ì¦í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤"}

            sample_indices = np.random.choice(
                len(source_data["documents"]),
                size=min(sample_size, len(source_data["documents"])),
                replace=False
            )

            similarities = []

            for idx in sample_indices:
                doc_content = source_data["documents"][idx]

                # ê° ë²¡í„° ì €ì¥ì†Œì—ì„œ ì„ë² ë”© ìƒì„±
                if hasattr(source_store, 'embedding_function'):
                    source_embedding = source_store.embedding_function.embed_query(doc_content)
                else:
                    continue

                if hasattr(target_store, 'embedding_function'):
                    target_embedding = target_store.embedding_function.embed_query(doc_content)
                else:
                    continue

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = cosine_similarity(
                    [source_embedding], [target_embedding]
                )[0][0]
                similarities.append(similarity)

            if similarities:
                avg_similarity = np.mean(similarities)
                min_similarity = np.min(similarities)
                success = avg_similarity >= threshold
            else:
                avg_similarity = 0
                min_similarity = 0
                success = False

            return {
                "success": success,
                "avg_similarity": float(avg_similarity),
                "min_similarity": float(min_similarity),
                "threshold": threshold,
                "samples_checked": len(similarities)
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

    def _verify_search_functionality(
        self,
        source_store: Any,
        target_store: Any
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê¸°ëŠ¥ ê²€ì¦"""

        try:
            test_queries = ["í…ŒìŠ¤íŠ¸", "ì¸ê³µì§€ëŠ¥", "ë°ì´í„°ë² ì´ìŠ¤"]

            search_results = []

            for query in test_queries:
                try:
                    source_results = source_store.similarity_search(query, k=3)
                    target_results = target_store.similarity_search(query, k=3)

                    # ê²°ê³¼ ìˆ˜ ë¹„êµ
                    count_match = len(source_results) == len(target_results)

                    # ìƒìœ„ ê²°ê³¼ ë‚´ìš© ë¹„êµ
                    content_match = False
                    if source_results and target_results:
                        content_match = (
                            source_results[0].page_content == target_results[0].page_content
                        )

                    search_results.append({
                        "query": query,
                        "count_match": count_match,
                        "content_match": content_match
                    })

                except Exception as e:
                    search_results.append({
                        "query": query,
                        "error": str(e)
                    })

            # ì „ì²´ ì„±ê³µë¥  ê³„ì‚°
            successful_searches = sum(
                1 for result in search_results
                if result.get("count_match", False) and result.get("content_match", False)
            )

            success_rate = successful_searches / len(test_queries)

            return {
                "success": success_rate >= 0.8,  # 80% ì´ìƒ ì„±ê³µ
                "success_rate": success_rate,
                "detailed_results": search_results
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

    def _verify_batch_embeddings(
        self,
        source_store: Any,
        target_store: Any,
        sample_ids: List[str]
    ) -> Dict[str, bool]:
        """ë°°ì¹˜ ì„ë² ë”© ê²€ì¦"""
        try:
            # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ë¡œ ëŒ€ì²´
            for doc_id in sample_ids:
                source_search = source_store.similarity_search("test", k=1)
                target_search = target_store.similarity_search("test", k=1)

                if not source_search or not target_search:
                    return {"success": False}

            return {"success": True}

        except Exception:
            return {"success": False}

    def get_migration_log(self) -> List[Dict[str, Any]]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œê·¸ ë°˜í™˜"""
        return self.migration_log.copy()

# ë²¡í„° ì €ì¥ì†Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬ í…ŒìŠ¤íŠ¸
print("=== ë²¡í„° ì €ì¥ì†Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬ í…ŒìŠ¤íŠ¸ ===")

# ë§ˆì´ê·¸ë ˆì´í„° ì´ˆê¸°í™”
migrator = VectorStoreMigrator()

# ì†ŒìŠ¤ ë° íƒ€ê²Ÿ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
embeddings_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# ì†ŒìŠ¤: Chroma
source_chroma = Chroma(
    collection_name="migration_source",
    embedding_function=embeddings_model,
    persist_directory="./migration_source"
)

# íƒ€ê²Ÿ: ìƒˆë¡œìš´ Chroma (ë˜ëŠ” FAISS)
target_chroma = Chroma(
    collection_name="migration_target",
    embedding_function=embeddings_model,
    persist_directory="./migration_target"
)

# ì†ŒìŠ¤ì— í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€
test_migration_docs = [
    Document(
        page_content=f"ë§ˆì´ê·¸ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ {i}: AI ê¸°ìˆ ê³¼ ë°ì´í„° ê³¼í•™",
        metadata={"doc_type": "test", "number": i, "category": "migration"}
    )
    for i in range(20)
]

source_chroma.add_documents(test_migration_docs)
print(f"ì†ŒìŠ¤ì— {len(test_migration_docs)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")

# ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
migration_result = migrator.migrate_data(
    source_store=source_chroma,
    target_store=target_chroma,
    batch_size=5,
    verify_embeddings=True
)

print(f"\në§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼:")
for key, value in migration_result.items():
    print(f"  {key}: {value}")

# ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦
verification_result = migrator.verify_migration(
    source_store=source_chroma,
    target_store=target_chroma,
    sample_size=5
)

print(f"\nê²€ì¦ ê²°ê³¼ ìš”ì•½:")
print(f"  ì „ì²´ ì„±ê³µ: {verification_result['overall_success']}")
print(f"  ë°ì´í„° ë¬´ê²°ì„±: {verification_result['data_integrity']['success']}")
print(f"  ì„ë² ë”© ìœ ì‚¬ë„: {verification_result['embedding_similarity']['success']}")
print(f"  ê²€ìƒ‰ ê¸°ëŠ¥: {verification_result['search_functionality']['success']}")

# ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œê·¸ í™•ì¸
migration_log = migrator.get_migration_log()
if migration_log:
    print(f"\në§ˆì´ê·¸ë ˆì´ì…˜ ì˜¤ë¥˜ ë¡œê·¸: {len(migration_log)}ê°œ")
    for log_entry in migration_log:
        print(f"  ë°°ì¹˜ {log_entry['batch']}: {log_entry['error']}")
else:
    print("\në§ˆì´ê·¸ë ˆì´ì…˜ ì˜¤ë¥˜ ì—†ìŒ")
```

## ğŸ” ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LangChain Vector Stores](https://python.langchain.com/docs/modules/data_connection/vectorstores/)
- [Chroma Documentation](https://docs.trychroma.com/)
- [FAISS Documentation](https://faiss.ai/)
- [Pinecone Documentation](https://docs.pinecone.io/)

### ë²¡í„° DB ë¹„êµ
| íŠ¹ì„± | Chroma | FAISS | Pinecone |
|------|--------|-------|----------|
| **íƒ€ì…** | ì˜¤í”ˆì†ŒìŠ¤ | ì˜¤í”ˆì†ŒìŠ¤ | ìƒìš© SaaS |
| **ë°°í¬** | ë¡œì»¬/í´ë¼ìš°ë“œ | ë¡œì»¬ | í´ë¼ìš°ë“œë§Œ |
| **í™•ì¥ì„±** | ì¤‘ê°„ | ë†’ìŒ | ë§¤ìš° ë†’ìŒ |
| **ë¹„ìš©** | ë¬´ë£Œ | ë¬´ë£Œ | ì¢…ëŸ‰ì œ |
| **ì„¤ì • ë‚œì´ë„** | ì‰¬ì›€ | ì¤‘ê°„ | ì‰¬ì›€ |
| **ì„±ëŠ¥** | ì¢‹ìŒ | ë§¤ìš° ì¢‹ìŒ | ì¢‹ìŒ |

### ìµœì í™” íŒ

#### ì¸ë±ì‹± ìµœì í™”
```python
# FAISS ì¸ë±ìŠ¤ íƒ€ì…ë³„ ì„ íƒ
index_types = {
    "small": faiss.IndexFlatL2,      # < 1M ë²¡í„°
    "medium": faiss.IndexIVFFlat,    # 1M-10M ë²¡í„°
    "large": faiss.IndexIVFPQ        # > 10M ë²¡í„°
}

# Chroma ë°°ì¹˜ í¬ê¸° ìµœì í™”
OPTIMAL_BATCH_SIZES = {
    "small_docs": 100,    # < 1KB ë¬¸ì„œ
    "medium_docs": 50,    # 1-10KB ë¬¸ì„œ
    "large_docs": 20      # > 10KB ë¬¸ì„œ
}
```

#### ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì˜ˆì‹œ
def process_large_dataset(documents: List[Document], batch_size: int = 1000):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬"""
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]

        # ë°°ì¹˜ ì²˜ë¦¬
        vectorstore.add_documents(batch)

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del batch
        import gc
        gc.collect()
```

#### ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ
```python
# ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” ì„¤ì •
search_params = {
    "chroma": {
        "include": ["metadatas", "documents", "distances"],
        "n_results": 10
    },
    "faiss": {
        "nprobe": 10,  # ê²€ìƒ‰í•  í´ëŸ¬ìŠ¤í„° ìˆ˜
        "k": 10
    }
}
```