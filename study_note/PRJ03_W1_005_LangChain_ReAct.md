# LangChain ReAct ì—ì´ì „íŠ¸ì™€ ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œ

## ğŸ“š í•™ìŠµ ëª©í‘œ

- **ReAct í”„ë ˆì„ì›Œí¬**ì˜ ê°œë…ê³¼ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤
- **ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í•˜ê³  ì–¸ì–´ êµì°¨ ê²€ìƒ‰ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤
- **ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­** ìë™í™”ë¥¼ í†µí•´ seamlessí•œ ë‹¤êµ­ì–´ ì²˜ë¦¬ë¥¼ êµ¬í˜„í•œë‹¤
- **ë²¡í„°ì €ì¥ì†Œ ë¼ìš°íŒ…**ì„ í†µí•´ ì–¸ì–´ë³„ ìµœì í™”ëœ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•œë‹¤
- **LangChain Agent**ë¥¼ í™œìš©í•˜ì—¬ ììœ¨ì ì¸ ì˜ì‚¬ê²°ì • ì‹œìŠ¤í…œì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤

## ğŸ”‘ í•µì‹¬ ê°œë…

### ReAct (Reasoning and Acting)

**ReAct**ëŠ” ì¶”ë¡ (Reasoning)ê³¼ í–‰ë™(Acting)ì„ ê²°í•©í•œ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

- **í–‰ë™-ê´€ì°°-ì¶”ë¡  ìˆœí™˜**: ì—ì´ì „íŠ¸ê°€ ë°˜ë³µì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë©° ëª©í‘œ ë‹¬ì„±
  - **í–‰ë™ (Act)**: LLMì´ íŠ¹ì • ë„êµ¬(Tool)ë¥¼ í˜¸ì¶œ
  - **ê´€ì°° (Observe)**: ë„êµ¬ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸
  - **ì¶”ë¡  (Reason)**: ê´€ì°° ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í–‰ë™ ê²°ì •

- **Tool Calling**: LLMì´ í•„ìš”í•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  í˜¸ì¶œí•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜
- **Agent**: ììœ¨ì ìœ¼ë¡œ ì˜ì‚¬ê²°ì •ì„ ë‚´ë¦¬ë©° ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œ

### ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œ

**ì–¸ì–´ êµì°¨ ê²€ìƒ‰ (Cross-lingual Search)**
- ì„œë¡œ ë‹¤ë¥¸ ì–¸ì–´ ê°„ì˜ ì •ë³´ ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê¸°ìˆ 
- ì§ˆì˜ì–´ì™€ ë¬¸ì„œê°€ ë‹¤ë¥¸ ì–¸ì–´ì—¬ë„ ì˜ë¯¸ì  ì—°ê´€ì„± ê¸°ë°˜ ê²€ìƒ‰ ê°€ëŠ¥
- ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ í™œìš© (ì˜ˆ: OpenAI text-embedding-3-small, HuggingFace bge-m3)

**ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­**
- `langdetect` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ ìë™ ì‹ë³„
- `LibreTranslate` ì˜¤í”ˆì†ŒìŠ¤ ë²ˆì—­ APIë¥¼ í†µí•œ ìë™ ë²ˆì—­
- í•œêµ­ì–´â†”ë‹¤êµ­ì–´ ì–‘ë°©í–¥ ë²ˆì—­ ì§€ì›

**ë²¡í„°ì €ì¥ì†Œ ë¼ìš°íŒ…**
- ì–¸ì–´ë³„ ë²¡í„°ì €ì¥ì†Œë¥¼ ë¶„ë¦¬í•˜ì—¬ ë…ë¦½ì  ê´€ë¦¬
- ì–¸ì–´ ê°ì§€ í›„ í•´ë‹¹ ì–¸ì–´ì˜ ë²¡í„°ì €ì¥ì†Œë¡œ ìë™ ë¼ìš°íŒ…
- ê° ì–¸ì–´ì— ìµœì í™”ëœ ê²€ìƒ‰ ì„±ëŠ¥ ì œê³µ

### ê´€ë ¨ ê¸°ìˆ  ìŠ¤íƒ

```python
# LangChain í•µì‹¬
langchain-core        # ê¸°ë³¸ ì¶”ìƒí™” ë° ì¸í„°í˜ì´ìŠ¤
langchain-openai      # OpenAI í†µí•©
langchain-community   # ì»¤ë®¤ë‹ˆí‹° í†µí•© (ë¬¸ì„œ ë¡œë” ë“±)
langchain-chroma      # Chroma ë²¡í„° DB
langchain-huggingface # HuggingFace ì„ë² ë”©

# ë‹¤êµ­ì–´ ì§€ì›
langdetect           # ì–¸ì–´ ê°ì§€
libretranslate       # ì˜¤í”ˆì†ŒìŠ¤ ë²ˆì—­ API

# ë²¡í„° ì„ë² ë”©
openai               # OpenAI ì„ë² ë”© ëª¨ë¸
sentence-transformers # HuggingFace ì„ë² ë”©
```

## ğŸ›  í™˜ê²½ ì„¤ì •

### í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
pip install langchain langchain-openai langchain-community langchain-chroma
pip install langchain-huggingface langchain-ollama
pip install langdetect libretranslate python-dotenv
pip install chromadb tiktoken
```

### API í‚¤ ì„¤ì •

```.env
OPENAI_API_KEY=your_openai_api_key_here
```

### ê¸°ë³¸ ì„¤ì • ì½”ë“œ

```python
from dotenv import load_dotenv
import os
import warnings

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™”
warnings.filterwarnings("ignore")

# OpenAI API í‚¤ í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
```

## ğŸ’» ë‹¨ê³„ë³„ êµ¬í˜„

### 1ë‹¨ê³„: ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œ êµ¬ì¶•

#### 1.1 ë‹¤êµ­ì–´ ë¬¸ì„œ ë¡œë“œ ë° ì „ì²˜ë¦¬

```python
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from glob import glob

def load_text_files(file_paths):
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    documents = []
    for file_path in file_paths:
        try:
            loader = TextLoader(file_path, encoding='utf-8')
            documents.extend(loader.load())
        except Exception as e:
            print(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")
    return documents

# ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
data_dir = os.path.join(os.getcwd(), 'data')

# í•œêµ­ì–´ ë¬¸ì„œ ë¡œë“œ
korean_txt_files = glob(os.path.join(data_dir, '*_KR.md'))
korean_data = load_text_files(korean_txt_files)
print(f"ë¡œë“œëœ í•œêµ­ì–´ ë¬¸ì„œ ìˆ˜: {len(korean_data)}")

# ì˜ì–´ ë¬¸ì„œ ë¡œë“œ
english_txt_files = glob(os.path.join(data_dir, '*_EN.md'))
english_data = load_text_files(english_txt_files)
print(f"ë¡œë“œëœ ì˜ì–´ ë¬¸ì„œ ìˆ˜: {len(english_data)}")
```

#### 1.2 ë¬¸ì„œ ë¶„í•  (Chunking)

```python
# TikToken ê¸°ë°˜ ë¬¸ì„œ ë¶„í• ê¸° ìƒì„±
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    encoding_name="cl100k_base",              # TikToken ì¸ì½”ë”
    separators=['\n\n', '\n', r'(?<=[.!?])\s+'],  # êµ¬ë¶„ì (ë¬¸ë‹¨, ì¤„ë°”ê¿ˆ, ë¬¸ì¥)
    chunk_size=500,                            # ì²­í¬ í¬ê¸° (í† í° ë‹¨ìœ„)
    chunk_overlap=50,                          # ì²­í¬ ê°„ ê²¹ì¹¨
    length_function=len,
    is_separator_regex=True                    # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©
)

# í•œêµ­ì–´ ë¬¸ì„œ ë¶„í• 
korean_docs = text_splitter.split_documents(korean_data)
print(f"ë¶„í• ëœ í•œêµ­ì–´ ë¬¸ì„œ ìˆ˜: {len(korean_docs)}")

# ì˜ì–´ ë¬¸ì„œ ë¶„í• 
english_docs = text_splitter.split_documents(english_data)
print(f"ë¶„í• ëœ ì˜ì–´ ë¬¸ì„œ ìˆ˜: {len(english_docs)}")
```

**ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…:**
- `encoding_name`: TikToken ì¸ì½”ë” ì´ë¦„ (OpenAI ëª¨ë¸ê³¼ í˜¸í™˜)
- `separators`: ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” êµ¬ë¶„ì (ìš°ì„ ìˆœìœ„ ìˆœ)
- `chunk_size`: ê° ì²­í¬ì˜ ìµœëŒ€ í¬ê¸° (í† í° ë‹¨ìœ„)
- `chunk_overlap`: ì—°ì†ì„± ìœ ì§€ë¥¼ ìœ„í•œ ê²¹ì¹¨ í¬ê¸°

#### 1.3 ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¹„êµ

```python
from langchain_openai import OpenAIEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaEmbeddings

# 1. OpenAI ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜)
embeddings_openai = OpenAIEmbeddings(model="text-embedding-3-small")
print("âœ… OpenAI ì„ë² ë”© ëª¨ë¸ ìƒì„± ì™„ë£Œ")

# 2. HuggingFace ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ ì§€ì›)
try:
    embeddings_huggingface = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",              # ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸
        model_kwargs={'device': 'cpu'},        # CPU ì‚¬ìš©
        encode_kwargs={'normalize_embeddings': True}  # ì •ê·œí™” í™œì„±í™”
    )
    print("âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"âŒ HuggingFace ì„ë² ë”© ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
    embeddings_huggingface = None

# 3. Ollama ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ ë¯¸ì§€ì›)
try:
    embeddings_ollama = OllamaEmbeddings(model="nomic-embed-text")
    print("âœ… Ollama ì„ë² ë”© ëª¨ë¸ ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"âŒ Ollama ì„ë² ë”© ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
    embeddings_ollama = None
```

**ëª¨ë¸ ë¹„êµ:**

| ëª¨ë¸ | í•œêµ­ì–´ ì§€ì› | ì¥ì  | ë‹¨ì  |
|------|------------|------|------|
| OpenAI text-embedding-3-small | â­â­â­ | ìš°ìˆ˜í•œ ì„±ëŠ¥, ë‹¤êµ­ì–´ ì§€ì› | ìœ ë£Œ API |
| HuggingFace bge-m3 | â­â­ | ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥ | ì†ë„ ëŠë¦¼ |
| Ollama nomic-embed-text | âŒ | ë¬´ë£Œ, ê²½ëŸ‰í™” | í•œêµ­ì–´ ë¯¸ì§€ì› |

#### 1.4 ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ì„±ëŠ¥ ë¹„êµ

```python
from langchain_chroma import Chroma

# ëª¨ë“  ë¬¸ì„œ ë³‘í•© (í•œêµ­ì–´ + ì˜ì–´)
all_docs = korean_docs + english_docs
print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(all_docs)}")

# OpenAI ë²¡í„° ì €ì¥ì†Œ
db_openai = Chroma.from_documents(
    documents=all_docs,
    embedding=embeddings_openai,
    collection_name="multilang_db_openai",
    persist_directory="./chroma_db"
)
print(f"âœ… OpenAI ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {db_openai._collection.count()})")

# HuggingFace ë²¡í„° ì €ì¥ì†Œ
if embeddings_huggingface:
    db_huggingface = Chroma.from_documents(
        documents=all_docs,
        embedding=embeddings_huggingface,
        collection_name="multilang_db_huggingface",
        persist_directory="./chroma_db"
    )
    print(f"âœ… HuggingFace ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {db_huggingface._collection.count()})")
else:
    db_huggingface = None
```

#### 1.5 RAG ì²´ì¸ ìƒì„± ë° ì„±ëŠ¥ í‰ê°€

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.chat_models import init_chat_model

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
template = """Answer the question based only on the following context.
Do not use any external information or knowledge.
If the answer is not in the context, answer "I don't know".

When answering:
- For proper nouns (names of people, places, organizations), keep the original language.
- Provide clear and concise answers.

Context:
{context}

Question: {question}

Answer:"""

prompt = ChatPromptTemplate.from_template(template)

# LLM ëª¨ë¸ ì´ˆê¸°í™”
llm = init_chat_model("openai:gpt-4.1-mini", temperature=0)

# ë¬¸ì„œ í¬ë§· í•¨ìˆ˜
def format_docs(docs):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·"""
    return "\n\n".join([doc.page_content for doc in docs])

# RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜
def create_rag_chain(vectorstore, top_k=4):
    """ë²¡í„° ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì²´ì¸ ìƒì„±"""
    retriever = vectorstore.as_retriever(search_kwargs={'k': top_k})

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# ê° ì„ë² ë”© ëª¨ë¸ë³„ RAG ì²´ì¸ ìƒì„±
rag_chain_openai = create_rag_chain(db_openai) if db_openai else None
rag_chain_huggingface = create_rag_chain(db_huggingface) if db_huggingface else None
```

**ì„±ëŠ¥ í‰ê°€ - í•œêµ­ì–´ ì¿¼ë¦¬:**

```python
query_ko = "í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"

print("="*80)
print(f"ì§ˆë¬¸: {query_ko}")
print("="*80)

# OpenAI
if rag_chain_openai:
    try:
        output_openai = rag_chain_openai.invoke(query_ko)
        print("âœ… OpenAI:", output_openai)
    except Exception as e:
        print(f"âŒ OpenAI ì˜¤ë¥˜: {str(e)[:150]}")

# HuggingFace
if rag_chain_huggingface:
    try:
        output_hf = rag_chain_huggingface.invoke(query_ko)
        print("âœ… HuggingFace:", output_hf)
    except Exception as e:
        print(f"âŒ HuggingFace ì˜¤ë¥˜: {str(e)[:150]}")
```

**ì˜ˆìƒ ì¶œë ¥:**
```
================================================================================
ì§ˆë¬¸: í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?
================================================================================
âœ… OpenAI: Elon Musk
âœ… HuggingFace: Elon Musk
```

**ì„±ëŠ¥ í‰ê°€ - ì˜ì–´ ì¿¼ë¦¬:**

```python
query_en = "Who is the founder of Tesla?"

print("="*80)
print(f"Question: {query_en}")
print("="*80)

# OpenAI
if rag_chain_openai:
    output_openai = rag_chain_openai.invoke(query_en)
    print("âœ… OpenAI:", output_openai)

# HuggingFace
if rag_chain_huggingface:
    output_hf = rag_chain_huggingface.invoke(query_en)
    print("âœ… HuggingFace:", output_hf)
```

### 2ë‹¨ê³„: ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ìë™í™”

#### 2.1 ì–¸ì–´ ê°ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •

```python
from langdetect import detect

# ì–¸ì–´ ê°ì§€ í…ŒìŠ¤íŠ¸
test_texts = {
    "í•œêµ­ì–´": "í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
    "ì˜ì–´": "Who is the founder of Tesla?",
    "ì¼ë³¸ì–´": "ãƒ†ã‚¹ãƒ©ã®å‰µæ¥­è€…ã¯èª°ã§ã™ã‹ï¼Ÿ",
    "ì¤‘êµ­ì–´": "ç‰¹æ–¯æ‹‰çš„åˆ›å§‹äººæ˜¯è°ï¼Ÿ"
}

for lang, text in test_texts.items():
    detected = detect(text)
    print(f"{lang}: {text} â†’ ê°ì§€ëœ ì–¸ì–´: {detected}")
```

**ì˜ˆìƒ ì¶œë ¥:**
```
í•œêµ­ì–´: í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”? â†’ ê°ì§€ëœ ì–¸ì–´: ko
ì˜ì–´: Who is the founder of Tesla? â†’ ê°ì§€ëœ ì–¸ì–´: en
ì¼ë³¸ì–´: ãƒ†ã‚¹ãƒ©ã®å‰µæ¥­è€…ã¯èª°ã§ã™ã‹ï¼Ÿ â†’ ê°ì§€ëœ ì–¸ì–´: ja
ì¤‘êµ­ì–´: ç‰¹æ–¯æ‹‰çš„åˆ›å§‹äººæ˜¯è°ï¼Ÿ â†’ ê°ì§€ëœ ì–¸ì–´: zh-cn
```

#### 2.2 LibreTranslate ë²ˆì—­ ì„¤ì •

```python
from langchain_community.tools import LibreTranslateAPI

# LibreTranslate ì„œë²„ ì—°ê²° (ë¡œì»¬ ë˜ëŠ” ì›ê²©)
try:
    translator = LibreTranslateAPI(
        url="https://libretranslate.com/translate",  # ê³µê°œ ì„œë²„
        # url="http://localhost:5000/translate",      # ë¡œì»¬ ì„œë²„
    )
    print("âœ… LibreTranslate ì„œë²„ ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âŒ LibreTranslate ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
    translator = None
```

**ë¡œì»¬ LibreTranslate ì„œë²„ ì‹¤í–‰ (ì„ íƒì‚¬í•­):**
```bash
# Dockerë¥¼ ì‚¬ìš©í•œ ë¡œì»¬ ì„œë²„ ì‹¤í–‰
docker run -ti --rm -p 5000:5000 libretranslate/libretranslate
```

#### 2.3 ì–¸ì–´ ê°ì§€ ê¸°ë°˜ RAG ì²´ì¸

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.runnables import chain

# í•œêµ­ì–´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma(
    collection_name="db_korean_cosine_metadata",
    embedding_function=embeddings,
    persist_directory="./chroma_db"
)

if translator:
    # ë²¡í„° ì €ì¥ì†Œ retriever ìƒì„±
    retriever = vectorstore.as_retriever(search_kwargs={'k': 4})

    # RAG ì²´ì¸ ìƒì„±
    lang_rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    # ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ê¸°ë°˜ RAG ì‹¤í–‰ í•¨ìˆ˜
    @chain
    def run_lang_rag_chain(query):
        """
        1. ì…ë ¥ ì¿¼ë¦¬ì˜ ì–¸ì–´ ê°ì§€
        2. í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš° í•œêµ­ì–´ë¡œ ë²ˆì—­
        3. RAG ì²´ì¸ ì‹¤í–‰
        4. ì‘ë‹µì„ ì›ë˜ ì–¸ì–´ë¡œ ë²ˆì—­
        """
        # ì–¸ì–´ ê°ì§€
        original_lang = detect(query)
        print(f"ê°ì§€ëœ ì–¸ì–´: {original_lang}")

        # í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš° ë²ˆì—­
        if original_lang.upper() != 'KO':
            print(f"ë²ˆì—­ ì¤‘: {original_lang} â†’ í•œêµ­ì–´")
            query_ko = translator.run(
                query=query,
                source=original_lang,
                target='ko'
            )
            print(f"ë²ˆì—­ëœ ì§ˆë¬¸: {query_ko}")
        else:
            query_ko = query

        # RAG ì²´ì¸ ì‹¤í–‰
        answer_ko = lang_rag_chain.invoke(query_ko)
        print(f"í•œêµ­ì–´ ë‹µë³€: {answer_ko}")

        # ì›ë˜ ì–¸ì–´ë¡œ ë‹µë³€ ë²ˆì—­
        if original_lang.upper() != 'KO':
            print(f"ë²ˆì—­ ì¤‘: í•œêµ­ì–´ â†’ {original_lang}")
            answer = translator.run(
                query=answer_ko,
                source='ko',
                target=original_lang
            )
            return answer
        else:
            return answer_ko
else:
    run_lang_rag_chain = None
    print("âš ï¸ LibreTranslateë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì–¸ì–´ ê°ì§€ RAGë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
```

**ì‹¤í–‰ ì˜ˆì‹œ:**

```python
if run_lang_rag_chain:
    # í•œêµ­ì–´ ì¿¼ë¦¬
    query_ko = "í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"
    output_ko = run_lang_rag_chain.invoke(query_ko)
    print(f"\nìµœì¢… ë‹µë³€: {output_ko}")

    # ì˜ì–´ ì¿¼ë¦¬
    query_en = "Who is the founder of Tesla?"
    output_en = run_lang_rag_chain.invoke(query_en)
    print(f"\nìµœì¢… ë‹µë³€: {output_en}")

    # ì¼ë³¸ì–´ ì¿¼ë¦¬
    query_ja = "ãƒ†ã‚¹ãƒ©ã®å‰µæ¥­è€…ã¯èª°ã§ã™ã‹ï¼Ÿ"
    output_ja = run_lang_rag_chain.invoke(query_ja)
    print(f"\næœ€çµ‚ç­”ãˆ: {output_ja}")
```

### 3ë‹¨ê³„: ì–¸ì–´ë³„ ë²¡í„°ì €ì¥ì†Œ ë¼ìš°íŒ…

#### 3.1 ì–¸ì–´ë³„ ë²¡í„° ì €ì¥ì†Œ ìƒì„±

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# í•œêµ­ì–´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
db_korean = Chroma(
    collection_name="db_korean_cosine_metadata",
    embedding_function=embeddings,
    persist_directory="./chroma_db"
)
print(f"âœ… í•œêµ­ì–´ ë¬¸ì„œ ìˆ˜: {db_korean._collection.count()}")

# ì˜ì–´ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
db_english = Chroma.from_documents(
    documents=english_docs,
    embedding=embeddings,
    collection_name="eng_db_openai",
    persist_directory="./chroma_db"
)
print(f"âœ… ì˜ì–´ ë¬¸ì„œ ìˆ˜: {db_english._collection.count()}")
```

#### 3.2 ì–¸ì–´ ê°ì§€ ê¸°ë°˜ ë¼ìš°íŒ… RAG ì²´ì¸

```python
from langdetect import detect
from langchain_core.runnables import chain

# ê° ì–¸ì–´ë³„ RAG ì²´ì¸ ìƒì„±
rag_chain_korean = create_rag_chain(db_korean)
rag_chain_english = create_rag_chain(db_english)

# ë¼ìš°íŒ… RAG ì²´ì¸
@chain
def run_route_rag_chain(query):
    """
    ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³  í•´ë‹¹ ì–¸ì–´ì˜ ë²¡í„° ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì‹¤í–‰
    """
    # ì–¸ì–´ ê°ì§€
    original_lang = detect(query)
    print(f"ê°ì§€ëœ ì–¸ì–´: {original_lang}")

    # í•œêµ­ì–´ì¸ ê²½ìš° í•œêµ­ì–´ RAG ì²´ì¸ ì‹¤í–‰
    if original_lang.upper() == 'KO':
        print("â†’ í•œêµ­ì–´ ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©")
        return rag_chain_korean.invoke(query)

    # ì˜ì–´ì¸ ê²½ìš° ì˜ì–´ RAG ì²´ì¸ ì‹¤í–‰
    elif 'EN' in original_lang.upper():
        print("â†’ ì˜ì–´ ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©")
        return rag_chain_english.invoke(query)

    # ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´
    else:
        return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´ì…ë‹ˆë‹¤: {original_lang}"
```

**ì‹¤í–‰ ë° ê²°ê³¼:**

```python
# í•œêµ­ì–´ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
query_ko = "í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"
output_ko = run_route_rag_chain.invoke(query_ko)
print(f"ë‹µë³€: {output_ko}\n")

# ì˜ì–´ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
query_en = "Who is the founder of Tesla?"
output_en = run_route_rag_chain.invoke(query_en)
print(f"Answer: {output_en}\n")
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ê°ì§€ëœ ì–¸ì–´: ko
â†’ í•œêµ­ì–´ ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©
ë‹µë³€: Elon Musk

ê°ì§€ëœ ì–¸ì–´: en
â†’ ì˜ì–´ ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©
Answer: Elon Musk
```

**ë¼ìš°íŒ… ë°©ì‹ì˜ ì¥ì :**
- âœ… ì–¸ì–´ë³„ ìµœì í™”ëœ ê²€ìƒ‰ ì„±ëŠ¥
- âœ… ë²ˆì—­ ì—†ì´ ì§ì ‘ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
- âœ… ê° ì–¸ì–´ë³„ ë…ë¦½ì ì¸ ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬
- âœ… í™•ì¥ ê°€ëŠ¥í•œ ë‹¤êµ­ì–´ ì§€ì› (ì €ì¥ì†Œ ì¶”ê°€ë§Œìœ¼ë¡œ ì–¸ì–´ ì¶”ê°€ ê°€ëŠ¥)

### 4ë‹¨ê³„: ReAct - ë„êµ¬(Tool) ì •ì˜í•˜ê¸°

#### 4.1 ë©”íƒ€ë°ì´í„° í¬í•¨ RAG ì²´ì¸ ìƒì„±

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain.chat_models import init_chat_model

def create_rag_chain_with_metadata(vectorstore, top_k=4):
    """
    ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ RAG ì²´ì¸ ìƒì„±
    - ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì¶œì²˜(source) ì •ë³´ë¥¼ í•¨ê»˜ ë°˜í™˜
    """
    template = """Answer the question based only on the following context.
Do not use any external information or knowledge.
If the answer is not in the context, answer "I don't know".

- For proper nouns (names of people, places, organizations), keep the original language.
- Provide the sources of information when available.

Context:
{context}

Question: {question}

Answer:"""

    prompt = ChatPromptTemplate.from_template(template)
    llm = init_chat_model("openai:gpt-4.1-mini", temperature=0)
    retriever = vectorstore.as_retriever(search_kwargs={'k': top_k})

    def format_docs_with_metadata(docs):
        """ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ í¬ë§·"""
        formatted = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get('source', 'Unknown')
            content = doc.page_content
            formatted.append(f"[Source {i}: {source}]\n{content}")
        return "\n\n".join(formatted)

    rag_chain = (
        {
            "context": retriever | format_docs_with_metadata,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain

# í•œêµ­ì–´ ë° ì˜ì–´ RAG ì²´ì¸ ìƒì„±
rag_chain_korean = create_rag_chain_with_metadata(db_korean, top_k=4)
rag_chain_english = create_rag_chain_with_metadata(db_english, top_k=4)
```

**ì‹¤í–‰ í…ŒìŠ¤íŠ¸:**

```python
# ì˜ì–´ RAG ì²´ì¸ í…ŒìŠ¤íŠ¸
response = rag_chain_english.invoke({"question": "Who is the founder of Tesla?"})
print(response)
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Elon Musk is the founder of Tesla. [Source: data/tesla_EN.md]
```

#### 4.2 RAG ì²´ì¸ì„ Tool ê°ì²´ë¡œ ë³€í™˜

```python
# í•œêµ­ì–´ RAG ë„êµ¬ ìƒì„±
rag_tool_korean = rag_chain_korean.as_tool(
    name="rag_korean_db",
    description="í•œêµ­ì–´ ì§ˆë¬¸ì— ëŒ€í•œ ë¦¬ë¹„ì•ˆ, í…ŒìŠ¬ë¼ ê´€ë ¨ ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰í•˜ê³ , ê·¸ ê²°ê³¼ì™€ í•¨ê»˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
)

print(f"Tool ì´ë¦„: {rag_tool_korean.name}")
print(f"Tool ì„¤ëª…: {rag_tool_korean.description}")
print(f"Tool ì…ë ¥ íŒŒë¼ë¯¸í„°:")
from pprint import pprint
pprint(rag_tool_korean.args)
```

**ì¶œë ¥:**
```
Tool ì´ë¦„: rag_korean_db
Tool ì„¤ëª…: í•œêµ­ì–´ ì§ˆë¬¸ì— ëŒ€í•œ ë¦¬ë¹„ì•ˆ, í…ŒìŠ¬ë¼ ê´€ë ¨ ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰í•˜ê³ , ê·¸ ê²°ê³¼ì™€ í•¨ê»˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
Tool ì…ë ¥ íŒŒë¼ë¯¸í„°:
{'properties': {'question': {'title': 'Question', 'type': 'string'}},
 'required': ['question'],
 'title': 'rag_korean_db',
 'type': 'object'}
```

```python
# ì˜ì–´ RAG ë„êµ¬ ìƒì„±
rag_tool_english = rag_chain_english.as_tool(
    name="rag_english_db",
    description="Retrieve and generate answers from the vector store for English questions related to Rivian and Tesla."
)

print(f"Tool ì´ë¦„: {rag_tool_english.name}")
print(f"Tool ì„¤ëª…: {rag_tool_english.description}")
```

**Tool ê°ì²´ì˜ êµ¬ì¡°:**
- `name`: ë„êµ¬ì˜ ê³ ìœ  ì‹ë³„ì
- `description`: LLMì´ ë„êµ¬ë¥¼ ì„ íƒí•  ë•Œ ì°¸ê³ í•˜ëŠ” ì„¤ëª…
- `args`: ë„êµ¬ì˜ ì…ë ¥ ìŠ¤í‚¤ë§ˆ (Pydantic ëª¨ë¸ë¡œ ì •ì˜)
- `invoke()`: ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ë©”ì„œë“œ

### 5ë‹¨ê³„: ReAct - ë„êµ¬(Tool) í˜¸ì¶œí•˜ê¸°

#### 5.1 LLMì— ë„êµ¬ ë°”ì¸ë”©

```python
from langchain.chat_models import init_chat_model

# ë„êµ¬ ëª©ë¡
tools = [rag_tool_korean, rag_tool_english]

# LLM ëª¨ë¸ ì´ˆê¸°í™”
llm = init_chat_model("openai:gpt-4.1-mini", temperature=0)

# LLMì— ë„êµ¬ ë°”ì¸ë”©
llm_with_tools = llm.bind_tools(tools=tools)

# í•œêµ­ì–´ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
query = "í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"
response = llm_with_tools.invoke(query)

pprint(response)
```

**ì¶œë ¥ (AIMessage ê°ì²´):**
```python
AIMessage(
    content='',
    additional_kwargs={
        'tool_calls': [
            {
                'id': 'call_abc123',
                'function': {
                    'arguments': '{"question":"í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"}',
                    'name': 'rag_korean_db'
                },
                'type': 'function'
            }
        ]
    },
    tool_calls=[
        {
            'name': 'rag_korean_db',
            'args': {'question': 'í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?'},
            'id': 'call_abc123'
        }
    ]
)
```

#### 5.2 ToolCall ê°ì²´ í™•ì¸

```python
# ToolCall ë¦¬ìŠ¤íŠ¸ í™•ì¸
print("ë„êµ¬ í˜¸ì¶œ ì •ë³´:")
pprint(response.tool_calls)
```

**ì¶œë ¥:**
```python
[{
    'name': 'rag_korean_db',
    'args': {'question': 'í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?'},
    'id': 'call_abc123'
}]
```

**ì˜ì–´ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸:**

```python
query_en = "Who is the founder of Tesla?"
response_en = llm_with_tools.invoke(query_en)

print("ë„êµ¬ í˜¸ì¶œ ì •ë³´:")
pprint(response_en.tool_calls)
```

**ì¶œë ¥:**
```python
[{
    'name': 'rag_english_db',
    'args': {'question': 'Who is the founder of Tesla?'},
    'id': 'call_def456'
}]
```

**ë„êµ¬ì™€ ë¬´ê´€í•œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸:**

```python
query_test = "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?"
response_test = llm_with_tools.invoke(query_test)

print("Tool calls:", response_test.tool_calls)
print("Content:", response_test.content)
```

**ì¶œë ¥:**
```python
Tool calls: []
Content: ì£„ì†¡í•˜ì§€ë§Œ ì €ëŠ” ë‚ ì”¨ ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚ ì”¨ ì •ë³´ë¥¼ í™•ì¸í•˜ì‹œë ¤ë©´ ë‚ ì”¨ ì•±ì´ë‚˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì´ìš©í•´ì£¼ì„¸ìš”.
```

**ì£¼ìš” ê´€ì°°:**
- âœ… LLMì´ ìë™ìœ¼ë¡œ ì ì ˆí•œ ë„êµ¬ ì„ íƒ (í•œêµ­ì–´ â†’ rag_korean_db, ì˜ì–´ â†’ rag_english_db)
- âœ… ë„êµ¬ê°€ í•„ìš”í•˜ì§€ ì•Šì€ ì§ˆë¬¸ì€ ì¼ë°˜ ì‘ë‹µ ìƒì„±
- âœ… `tool_calls` ì†ì„±ì„ í†µí•´ ì–´ë–¤ ë„êµ¬ë¥¼ í˜¸ì¶œí• ì§€ í™•ì¸ ê°€ëŠ¥

### 6ë‹¨ê³„: ReAct - ë„êµ¬(Tool) ì‹¤í–‰í•˜ê¸°

#### 6.1 ë„êµ¬ ë§¤í•‘ ìƒì„±

```python
# ë„êµ¬ ì´ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ë„êµ¬ ê°ì²´ ë§¤í•‘
tool_map = {
    "rag_korean_db": rag_tool_korean,
    "rag_english_db": rag_tool_english
}

# ë„êµ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
result = tool_map["rag_korean_db"].invoke({"question": "í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"})
print(result)
```

**ì¶œë ¥:**
```
Elon Musk [Source: data/tesla_KR.md]
```

#### 6.2 ë„êµ¬ í˜¸ì¶œ í•¨ìˆ˜ ì •ì˜

```python
from langchain_core.messages import AIMessage

def call_tools(msg: AIMessage):
    """
    AIMessageì˜ tool_callsë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜

    Args:
        msg: AIMessage ê°ì²´ (tool_calls ì†ì„± í¬í•¨)

    Returns:
        ì‹¤í–‰ ê²°ê³¼ê°€ í¬í•¨ëœ tool_calls ë¦¬ìŠ¤íŠ¸
    """
    tool_calls = msg.tool_calls.copy()

    for tool_call in tool_calls:
        # ë„êµ¬ ì´ë¦„ìœ¼ë¡œ ë„êµ¬ ê°ì²´ ì°¾ê¸°
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]

        # ë„êµ¬ ì‹¤í–‰
        tool_output = tool_map[tool_name].invoke(tool_args)

        # ê²°ê³¼ë¥¼ tool_callì— ì¶”ê°€
        tool_call["output"] = tool_output

    return tool_calls

# ë„êµ¬ í˜¸ì¶œ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
print("ToolCall ê°ì²´:")
pprint(response.tool_calls[0])
print("-" * 80)

tool_calls = call_tools(response)
print("\nì‹¤í–‰ ê²°ê³¼:")
pprint(tool_calls)
```

**ì¶œë ¥:**
```
ToolCall ê°ì²´:
{'args': {'question': 'í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?'},
 'id': 'call_abc123',
 'name': 'rag_korean_db'}
--------------------------------------------------------------------------------

ì‹¤í–‰ ê²°ê³¼:
[{'args': {'question': 'í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?'},
  'id': 'call_abc123',
  'name': 'rag_korean_db',
  'output': 'Elon Musk [Source: data/tesla_KR.md]'}]
```

#### 6.3 ë„êµ¬ í˜¸ì¶œ ì²´ì¸ ìƒì„±

```python
# LLMê³¼ ë„êµ¬ í˜¸ì¶œ í•¨ìˆ˜ë¥¼ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì—°ê²°
search_tool_chain = llm_with_tools | call_tools

# í•œêµ­ì–´ ì¿¼ë¦¬ ì‹¤í–‰
query = "í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"
search_response = search_tool_chain.invoke(query)

pprint(search_response)
```

**ì¶œë ¥:**
```python
[{'args': {'question': 'í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?'},
  'id': 'call_abc123',
  'name': 'rag_korean_db',
  'output': 'Elon Musk [Source: data/tesla_KR.md]'}]
```

**ì˜ì–´ ì¿¼ë¦¬ ì‹¤í–‰:**

```python
query = "Who is the founder of Tesla?"
search_response = search_tool_chain.invoke(query)

pprint(search_response)
```

**ì¶œë ¥:**
```python
[{'args': {'question': 'Who is the founder of Tesla?'},
  'id': 'call_def456',
  'name': 'rag_english_db',
  'output': 'Elon Musk [Source: data/tesla_EN.md]'}]
```

**ë„êµ¬ ì‹¤í–‰ íë¦„:**

```
ì‚¬ìš©ì ì§ˆë¬¸
    â†“
llm_with_tools.invoke()  â† LLMì´ ì ì ˆí•œ ë„êµ¬ ì„ íƒ
    â†“
AIMessage (tool_calls í¬í•¨)
    â†“
call_tools()  â† ë„êµ¬ ì‹¤ì œ ì‹¤í–‰
    â†“
ì‹¤í–‰ ê²°ê³¼ (output í¬í•¨)
```

### 7ë‹¨ê³„: Agent êµ¬í˜„

#### 7.1 Agent ìƒì„± ë° ì‹¤í–‰

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

# ë„êµ¬ ëª©ë¡
tools = [rag_tool_korean, rag_tool_english]

# Agent ìƒì„±
agent = create_agent(
    model=llm,
    tools=tools,
    system_prompt="ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” AI Assistantì…ë‹ˆë‹¤."
)

print("âœ… Agent ìƒì„± ì™„ë£Œ")
```

**Agentì˜ ì—­í• :**
- **ììœ¨ì  ì˜ì‚¬ê²°ì •**: í•„ìš”í•œ ë„êµ¬ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒí•˜ê³  ì‹¤í–‰
- **ê³„íš-ì‹¤í–‰-ê´€ì°° ìˆœí™˜**: ReAct íŒ¨í„´ ìë™ ê´€ë¦¬
- **ìƒíƒœ ê´€ë¦¬**: ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° ì¤‘ê°„ ê²°ê³¼ ì¶”ì 

#### 7.2 Agent ì‹¤í–‰ - í•œêµ­ì–´ ì¿¼ë¦¬

```python
# í•œêµ­ì–´ ì¿¼ë¦¬ë¡œ Agent ì‹¤í–‰
response = agent.invoke(
    {"messages": [{"role": "user", "content": "í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"}]}
)

# ê²°ê³¼ ì¶œë ¥
pprint(response)
```

**ì¶œë ¥ (ì¶•ì•½):**
```python
{
    'messages': [
        HumanMessage(content='í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?'),
        AIMessage(
            content='',
            tool_calls=[{
                'name': 'rag_korean_db',
                'args': {'question': 'í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?'},
                'id': 'call_abc123'
            }]
        ),
        ToolMessage(
            content='Elon Musk [Source: data/tesla_KR.md]',
            tool_call_id='call_abc123'
        ),
        AIMessage(
            content='í…ŒìŠ¬ë¼ì˜ ì°½ì—…ìëŠ” Elon Muskì…ë‹ˆë‹¤.'
        )
    ]
}
```

**ì‹¤í–‰ íë¦„:**
1. **HumanMessage**: ì‚¬ìš©ì ì§ˆë¬¸
2. **AIMessage (tool_calls)**: Agentê°€ ë„êµ¬ ì„ íƒ
3. **ToolMessage**: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
4. **AIMessage (final)**: Agentì˜ ìµœì¢… ë‹µë³€

#### 7.3 Agent ì‹¤í–‰ - ì˜ì–´ ì¿¼ë¦¬

```python
# ì˜ì–´ ì¿¼ë¦¬ë¡œ Agent ì‹¤í–‰
response = agent.invoke(
    {"messages": [{"role": "user", "content": "Who is the founder of Tesla?"}]}
)

# ìµœì¢… ë‹µë³€ë§Œ ì¶”ì¶œ
final_message = response["messages"][-1]
print("ìµœì¢… ë‹µë³€:", final_message.content)
```

**ì¶œë ¥:**
```
ìµœì¢… ë‹µë³€: The founder of Tesla is Elon Musk.
```

**Agent vs ìˆ˜ë™ Tool Calling:**

| ë¹„êµ í•­ëª© | Agent | ìˆ˜ë™ Tool Calling |
|----------|-------|------------------|
| ë„êµ¬ ì„ íƒ | ìë™ | ìˆ˜ë™ ì§€ì • |
| ìƒíƒœ ê´€ë¦¬ | ìë™ | ìˆ˜ë™ ê´€ë¦¬ |
| ë°˜ë³µ ì‹¤í–‰ | ìë™ ìˆœí™˜ | ëª…ì‹œì  ë°˜ë³µ í•„ìš” |
| ì—ëŸ¬ ì²˜ë¦¬ | ë‚´ì¥ | ì§ì ‘ êµ¬í˜„ |
| ì½”ë“œ ë³µì¡ë„ | ë‚®ìŒ | ë†’ìŒ |

## ğŸ¯ ì‹¤ìŠµ ë¬¸ì œ

### ì‹¤ìŠµ 1: ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œ í™•ì¥ (â­â­â­)

**ë¬¸ì œ:**
ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ìë™í™” ë°©ì‹ì˜ ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­:**
1. í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ì§€ì›
2. ì‚¬ìš©ì ì–¸ì–´ ê°ì§€ í›„ í•œêµ­ì–´ ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰
3. ì–¸ì–´ë³„ ë²ˆì—­ ë„êµ¬ë¥¼ ë³„ë„ë¡œ êµ¬í˜„
4. ìµœì¢… ë‹µë³€ì„ ì›ë˜ ì–¸ì–´ë¡œ ë°˜í™˜

**íŒíŠ¸:**
- `langdetect`ë¡œ ì–¸ì–´ ê°ì§€
- `LibreTranslate`ë¡œ ë²ˆì—­
- `@tool` ë°ì½”ë ˆì´í„°ë¡œ ë²ˆì—­ ë„êµ¬ ì •ì˜
- Agentë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ ë¼ìš°íŒ…

### ì‹¤ìŠµ 2: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ RAG (â­â­â­â­)

**ë¬¸ì œ:**
ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°(ì¶œì²˜, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬ ë“±)ë¥¼ í™œìš©í•œ í•„í„°ë§ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­:**
1. ë¬¸ì„œì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ (source, date, category)
2. ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ retriever êµ¬í˜„
3. íŠ¹ì • ì¶œì²˜ë‚˜ ë‚ ì§œ ë²”ìœ„ì˜ ë¬¸ì„œë§Œ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬ ìƒì„±
4. Agentê°€ ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ í•„í„° ìë™ ì ìš©

**íŒíŠ¸:**
- `Document` ê°ì²´ì˜ `metadata` ì†ì„± í™œìš©
- `self_query` retriever ì‚¬ìš©
- í•„í„° ì¡°ê±´ì„ íŒŒë¼ë¯¸í„°ë¡œ ë°›ëŠ” ë„êµ¬ êµ¬í˜„

### ì‹¤ìŠµ 3: ë©€í‹°í™‰ ì§ˆë¬¸ ë‹µë³€ ì‹œìŠ¤í…œ (â­â­â­â­â­)

**ë¬¸ì œ:**
ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì¶”ë¡ ì´ í•„ìš”í•œ ë³µì¡í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ReAct Agentë¥¼ êµ¬í˜„í•˜ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­:**
1. ì²« ë²ˆì§¸ ë„êµ¬ë¡œ ê¸°ë³¸ ì •ë³´ ê²€ìƒ‰
2. ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‘ ë²ˆì§¸ ë„êµ¬ë¡œ ì¶”ê°€ ì •ë³´ ê²€ìƒ‰
3. ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
4. Agentê°€ ìë™ìœ¼ë¡œ ë‹¤ë‹¨ê³„ ì¶”ë¡  ìˆ˜í–‰

**ì˜ˆì‹œ ì§ˆë¬¸:**
- "í…ŒìŠ¬ë¼ì™€ ë¦¬ë¹„ì•ˆì˜ ì°½ì—…ìë¥¼ ê°ê° ì°¾ê³ , ë‘ ì‚¬ëŒì˜ ê³µí†µì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
- "í…ŒìŠ¬ë¼ì˜ ìµœì‹  ëª¨ë¸ê³¼ ê°€ê²©ì„ ì°¾ê³ , ê²½ìŸì‚¬ì™€ ë¹„êµí•´ì£¼ì„¸ìš”."

**íŒíŠ¸:**
- ì—¬ëŸ¬ ê°œì˜ RAG ë„êµ¬ ì •ì˜ (íšŒì‚¬ë³„, ì£¼ì œë³„)
- Agentì˜ `intermediate_steps` í™•ì¸
- `AgentExecutor`ì˜ `max_iterations` ì„¤ì •

### ì‹¤ìŠµ 4: ì‹¤ì‹œê°„ ë°ì´í„° í†µí•© RAG (â­â­â­â­)

**ë¬¸ì œ:**
ì •ì  ë¬¸ì„œì™€ ì‹¤ì‹œê°„ API ë°ì´í„°ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­:**
1. ë²¡í„° ì €ì¥ì†Œì—ì„œ ê¸°ë³¸ ì •ë³´ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬
2. ì‹¤ì‹œê°„ APIì—ì„œ ìµœì‹  ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬ (ì˜ˆ: ì£¼ì‹ ê°€ê²©, ë‚ ì”¨)
3. Agentê°€ ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ ì¡°í•© ì„ íƒ
4. ì •ì  ì •ë³´ì™€ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ í†µí•©í•œ ë‹µë³€ ìƒì„±

**íŒíŠ¸:**
- `yfinance` APIë¡œ ì£¼ì‹ ì •ë³´ ì¡°íšŒ ë„êµ¬ êµ¬í˜„
- `@tool` ë°ì½”ë ˆì´í„°ë¡œ API ë˜í•‘
- ì—¬ëŸ¬ ë„êµ¬ì˜ ê²°ê³¼ë¥¼ ê²°í•©í•˜ëŠ” ë¡œì§ êµ¬í˜„

## âœ… ì†”ë£¨ì…˜ ì˜ˆì‹œ

### ì‹¤ìŠµ 1 ì†”ë£¨ì…˜: ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œ

```python
from langchain.tools import tool
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langdetect import detect
from typing import Literal

# LibreTranslate ì—°ê²° í™•ì¸
if translator is None:
    print("âš ï¸ LibreTranslate ì„œë²„ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
else:
    # 1. ë²ˆì—­ ë„êµ¬ ì •ì˜
    @tool
    def translate_to_korean(
        text: str,
        source_lang: Literal["en", "zh-cn", "ja"]
    ) -> str:
        """ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤."""
        try:
            result = translator.run(
                query=text,
                source=source_lang,
                target='ko'
            )
            return result
        except Exception as e:
            return f"ë²ˆì—­ ì‹¤íŒ¨: {str(e)}"

    @tool
    def translate_from_korean(
        text: str,
        target_lang: Literal["en", "zh-cn", "ja"]
    ) -> str:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤."""
        try:
            result = translator.run(
                query=text,
                source='ko',
                target=target_lang
            )
            return result
        except Exception as e:
            return f"ë²ˆì—­ ì‹¤íŒ¨: {str(e)}"

    # 2. í•œêµ­ì–´ RAG ë„êµ¬ (ê¸°ì¡´ ì‚¬ìš©)
    korean_rag_tool = rag_tool_korean

    # 3. Agent ìƒì„±
    multilang_tools = [
        translate_to_korean,
        translate_from_korean,
        korean_rag_tool
    ]

    llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

    multilang_agent = create_agent(
        model=llm,
        tools=multilang_tools,
        system_prompt="""ë‹¹ì‹ ì€ ë‹¤êµ­ì–´ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI Assistantì…ë‹ˆë‹¤.

ì‘ì—… ìˆœì„œ:
1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì–¸ì–´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤
2. í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš°, translate_to_korean ë„êµ¬ë¡œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤
3. rag_korean_db ë„êµ¬ë¡œ í•œêµ­ì–´ ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤
4. ì›ë˜ ì–¸ì–´ê°€ í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš°, translate_from_korean ë„êµ¬ë¡œ ì›ë˜ ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤
5. ìµœì¢… ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤"""
    )

    # 4. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_queries = {
        "í•œêµ­ì–´": "í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
        "ì˜ì–´": "Who is the founder of Tesla?",
        "ì¤‘êµ­ì–´": "ç‰¹æ–¯æ‹‰çš„åˆ›å§‹äººæ˜¯è°ï¼Ÿ",
        "ì¼ë³¸ì–´": "ãƒ†ã‚¹ãƒ©ã®å‰µæ¥­è€…ã¯èª°ã§ã™ã‹ï¼Ÿ"
    }

    for lang, query in test_queries.items():
        print(f"\n{'='*80}")
        print(f"{lang} ì§ˆë¬¸: {query}")
        print('='*80)

        response = multilang_agent.invoke(
            {"messages": [{"role": "user", "content": query}]}
        )

        # ìµœì¢… ë‹µë³€ ì¶”ì¶œ
        final_answer = response["messages"][-1].content
        print(f"ë‹µë³€: {final_answer}")
```

**ì˜ˆìƒ ì¶œë ¥:**
```
================================================================================
í•œêµ­ì–´ ì§ˆë¬¸: í…ŒìŠ¬ë¼ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?
================================================================================
ë‹µë³€: í…ŒìŠ¬ë¼ì˜ ì°½ì—…ìëŠ” Elon Muskì…ë‹ˆë‹¤.

================================================================================
ì˜ì–´ ì§ˆë¬¸: Who is the founder of Tesla?
================================================================================
ë‹µë³€: The founder of Tesla is Elon Musk.

================================================================================
ì¤‘êµ­ì–´ ì§ˆë¬¸: ç‰¹æ–¯æ‹‰çš„åˆ›å§‹äººæ˜¯è°ï¼Ÿ
================================================================================
ë‹µë³€: ç‰¹æ–¯æ‹‰çš„åˆ›å§‹äººæ˜¯Elon Muskã€‚

================================================================================
ì¼ë³¸ì–´ ì§ˆë¬¸: ãƒ†ã‚¹ãƒ©ã®å‰µæ¥­è€…ã¯èª°ã§ã™ã‹ï¼Ÿ
================================================================================
ë‹µë³€: ãƒ†ã‚¹ãƒ©ã®å‰µæ¥­è€…ã¯Elon Muskã§ã™ã€‚
```

**ì‘ë™ ì›ë¦¬:**
1. Agentê°€ ì§ˆë¬¸ ì–¸ì–´ ê°ì§€ (ë‚´ë¶€ ì¶”ë¡ )
2. í•œêµ­ì–´ê°€ ì•„ë‹ˆë©´ `translate_to_korean` ë„êµ¬ í˜¸ì¶œ
3. `rag_korean_db` ë„êµ¬ë¡œ í•œêµ­ì–´ ë²¡í„° ì €ì¥ì†Œ ê²€ìƒ‰
4. ì›ë˜ ì–¸ì–´ë¡œ `translate_from_korean` ë„êµ¬ í˜¸ì¶œ
5. ìµœì¢… ë‹µë³€ ë°˜í™˜

### ì‹¤ìŠµ 2 ì†”ë£¨ì…˜: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ RAG

```python
from langchain.tools import tool
from langchain_core.documents import Document
from typing import Optional, List
from datetime import datetime

# 1. ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ë¬¸ì„œ ìƒì„±
documents_with_metadata = [
    Document(
        page_content="Tesla was founded by Elon Musk in 2003.",
        metadata={"source": "tesla_history.md", "date": "2003-07-01", "category": "company"}
    ),
    Document(
        page_content="Tesla Model 3 was released in 2017.",
        metadata={"source": "tesla_products.md", "date": "2017-07-28", "category": "product"}
    ),
    Document(
        page_content="Rivian was founded by RJ Scaringe in 2009.",
        metadata={"source": "rivian_history.md", "date": "2009-06-01", "category": "company"}
    ),
    Document(
        page_content="Rivian R1T truck was announced in 2018.",
        metadata={"source": "rivian_products.md", "date": "2018-11-27", "category": "product"}
    )
]

# 2. ë©”íƒ€ë°ì´í„° í¬í•¨ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

db_with_metadata = Chroma.from_documents(
    documents=documents_with_metadata,
    embedding=embeddings,
    collection_name="db_with_metadata",
    persist_directory="./chroma_db"
)

# 3. ë©”íƒ€ë°ì´í„° í•„í„°ë§ ë„êµ¬ ì •ì˜
@tool
def search_by_category(
    question: str,
    category: Literal["company", "product"]
) -> str:
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    retriever = db_with_metadata.as_retriever(
        search_kwargs={
            'k': 4,
            'filter': {'category': category}
        }
    )
    docs = retriever.invoke(question)

    if not docs:
        return f"'{category}' ì¹´í…Œê³ ë¦¬ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    return "\n\n".join([
        f"[{doc.metadata['source']}] {doc.page_content}"
        for doc in docs
    ])

@tool
def search_by_source(
    question: str,
    source: str
) -> str:
    """íŠ¹ì • ì¶œì²˜ì˜ ë¬¸ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    retriever = db_with_metadata.as_retriever(
        search_kwargs={
            'k': 4,
            'filter': {'source': source}
        }
    )
    docs = retriever.invoke(question)

    if not docs:
        return f"'{source}' ì¶œì²˜ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    return "\n\n".join([doc.page_content for doc in docs])

@tool
def search_by_date_range(
    question: str,
    start_date: str,
    end_date: str
) -> str:
    """íŠ¹ì • ë‚ ì§œ ë²”ìœ„ì˜ ë¬¸ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤. (ë‚ ì§œ í˜•ì‹: YYYY-MM-DD)"""
    # ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰ í›„ ë‚ ì§œ í•„í„°ë§ (Chromaì˜ ë‚ ì§œ í•„í„° ì œí•œ ìš°íšŒ)
    retriever = db_with_metadata.as_retriever(search_kwargs={'k': 10})
    all_docs = retriever.invoke(question)

    # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")

    filtered_docs = []
    for doc in all_docs:
        doc_date = datetime.strptime(doc.metadata['date'], "%Y-%m-%d")
        if start <= doc_date <= end:
            filtered_docs.append(doc)

    if not filtered_docs:
        return f"{start_date}ë¶€í„° {end_date} ì‚¬ì´ì— ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    return "\n\n".join([
        f"[{doc.metadata['date']}] {doc.page_content}"
        for doc in filtered_docs
    ])

# 4. Agent ìƒì„±
metadata_tools = [search_by_category, search_by_source, search_by_date_range]

llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

metadata_agent = create_agent(
    model=llm,
    tools=metadata_tools,
    system_prompt="""ë‹¹ì‹ ì€ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” AI Assistantì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ í•„í„°ë¥¼ ìë™ìœ¼ë¡œ ì ìš©í•©ë‹ˆë‹¤:
- ì¹´í…Œê³ ë¦¬ ì–¸ê¸‰ ì‹œ: search_by_category ì‚¬ìš©
- ì¶œì²˜ íŒŒì¼ ì–¸ê¸‰ ì‹œ: search_by_source ì‚¬ìš©
- ë‚ ì§œ/ê¸°ê°„ ì–¸ê¸‰ ì‹œ: search_by_date_range ì‚¬ìš©
"""
)

# 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
test_queries = [
    "íšŒì‚¬ ì •ë³´ë§Œ ê²€ìƒ‰í•´ì„œ ì°½ì—…ìë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
    "ì œí’ˆ ì •ë³´ë§Œ ê²€ìƒ‰í•´ì„œ ì–¸ì œ ì¶œì‹œë˜ì—ˆëŠ”ì§€ ì•Œë ¤ì£¼ì„¸ìš”",
    "2010ë…„ë¶€í„° 2020ë…„ ì‚¬ì´ì˜ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì£¼ì„¸ìš”"
]

for query in test_queries:
    print(f"\n{'='*80}")
    print(f"ì§ˆë¬¸: {query}")
    print('='*80)

    response = metadata_agent.invoke(
        {"messages": [{"role": "user", "content": query}]}
    )

    final_answer = response["messages"][-1].content
    print(f"ë‹µë³€: {final_answer}")
```

**ì˜ˆìƒ ì¶œë ¥:**
```
================================================================================
ì§ˆë¬¸: íšŒì‚¬ ì •ë³´ë§Œ ê²€ìƒ‰í•´ì„œ ì°½ì—…ìë¥¼ ì•Œë ¤ì£¼ì„¸ìš”
================================================================================
ë‹µë³€: íšŒì‚¬ ì •ë³´ë¥¼ ê²€ìƒ‰í•œ ê²°ê³¼:
- Teslaì˜ ì°½ì—…ìëŠ” Elon Muskì…ë‹ˆë‹¤ (2003ë…„ ì„¤ë¦½)
- Rivianì˜ ì°½ì—…ìëŠ” RJ Scaringeì…ë‹ˆë‹¤ (2009ë…„ ì„¤ë¦½)

================================================================================
ì§ˆë¬¸: ì œí’ˆ ì •ë³´ë§Œ ê²€ìƒ‰í•´ì„œ ì–¸ì œ ì¶œì‹œë˜ì—ˆëŠ”ì§€ ì•Œë ¤ì£¼ì„¸ìš”
================================================================================
ë‹µë³€: ì œí’ˆ ì¶œì‹œ ì •ë³´:
- Tesla Model 3: 2017ë…„ 7ì›” 28ì¼ ì¶œì‹œ
- Rivian R1T: 2018ë…„ 11ì›” 27ì¼ ë°œí‘œ

================================================================================
ì§ˆë¬¸: 2010ë…„ë¶€í„° 2020ë…„ ì‚¬ì´ì˜ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì£¼ì„¸ìš”
================================================================================
ë‹µë³€: 2010ë…„ë¶€í„° 2020ë…„ ì‚¬ì´ì˜ ì •ë³´:
- 2017ë…„: Tesla Model 3 ì¶œì‹œ
- 2018ë…„: Rivian R1T ë°œí‘œ
```

### ì‹¤ìŠµ 3 ì†”ë£¨ì…˜: ë©€í‹°í™‰ ì§ˆë¬¸ ë‹µë³€ ì‹œìŠ¤í…œ

```python
from langchain.agents import create_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.tools import tool

# 1. íšŒì‚¬ë³„ RAG ë„êµ¬ ì •ì˜
@tool
def search_tesla_info(question: str) -> str:
    """í…ŒìŠ¬ë¼ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # í…ŒìŠ¬ë¼ ë¬¸ì„œë§Œ í•„í„°ë§
    retriever = db_with_metadata.as_retriever(
        search_kwargs={
            'k': 3,
            'filter': {'source': {'$regex': 'tesla'}}
        }
    )
    docs = retriever.invoke(question)
    return "\n".join([doc.page_content for doc in docs])

@tool
def search_rivian_info(question: str) -> str:
    """ë¦¬ë¹„ì•ˆ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # ë¦¬ë¹„ì•ˆ ë¬¸ì„œë§Œ í•„í„°ë§
    retriever = db_with_metadata.as_retriever(
        search_kwargs={
            'k': 3,
            'filter': {'source': {'$regex': 'rivian'}}
        }
    )
    docs = retriever.invoke(question)
    return "\n".join([doc.page_content for doc in docs])

@tool
def compare_companies(company1_info: str, company2_info: str, aspect: str) -> str:
    """ë‘ íšŒì‚¬ì˜ ì •ë³´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."""
    prompt = f"""ë‹¤ìŒ ë‘ íšŒì‚¬ì˜ {aspect}ì„(ë¥¼) ë¹„êµí•˜ì„¸ìš”:

íšŒì‚¬ 1 ì •ë³´:
{company1_info}

íšŒì‚¬ 2 ì •ë³´:
{company2_info}

ë¹„êµ ê²°ê³¼ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)
    result = llm.invoke(prompt)
    return result.content

# 2. Agent ìƒì„± (max_iterations ì„¤ì •)
multihop_tools = [search_tesla_info, search_rivian_info, compare_companies]

llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

multihop_agent = create_agent(
    model=llm,
    tools=multihop_tools,
    system_prompt="""ë‹¹ì‹ ì€ ë‹¤ë‹¨ê³„ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” AI Assistantì…ë‹ˆë‹¤.

ë³µì¡í•œ ì§ˆë¬¸ì˜ ì²˜ë¦¬ ì ˆì°¨:
1. ì§ˆë¬¸ì„ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•©ë‹ˆë‹¤
2. ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
3. ìˆ˜ì§‘í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹„êµë‚˜ ì¢…í•©ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤
4. ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤

ì—¬ëŸ¬ ë‹¨ê³„ì˜ ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
)

# 3. ë©€í‹°í™‰ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
multihop_queries = [
    "í…ŒìŠ¬ë¼ì™€ ë¦¬ë¹„ì•ˆì˜ ì°½ì—…ìë¥¼ ê°ê° ì°¾ê³ , ë‘ ì‚¬ëŒì˜ ê³µí†µì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "í…ŒìŠ¬ë¼ì™€ ë¦¬ë¹„ì•ˆì´ ê°ê° ì–¸ì œ ì„¤ë¦½ë˜ì—ˆëŠ”ì§€ ì°¾ê³ , ì–´ëŠ íšŒì‚¬ê°€ ë” ì˜¤ë˜ë˜ì—ˆëŠ”ì§€ ë¹„êµí•´ì£¼ì„¸ìš”."
]

for query in multihop_queries:
    print(f"\n{'='*80}")
    print(f"ì§ˆë¬¸: {query}")
    print('='*80)

    response = multihop_agent.invoke(
        {"messages": [{"role": "user", "content": query}]}
    )

    # ì¤‘ê°„ ë‹¨ê³„ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    print("\n[ì¤‘ê°„ ë‹¨ê³„]")
    for i, msg in enumerate(response["messages"][1:-1], 1):
        if hasattr(msg, 'tool_calls') and msg.tool_calls:
            print(f"  ë‹¨ê³„ {i}: {msg.tool_calls[0]['name']} í˜¸ì¶œ")

    # ìµœì¢… ë‹µë³€
    final_answer = response["messages"][-1].content
    print(f"\n[ìµœì¢… ë‹µë³€]\n{final_answer}")
```

**ì˜ˆìƒ ì¶œë ¥:**
```
================================================================================
ì§ˆë¬¸: í…ŒìŠ¬ë¼ì™€ ë¦¬ë¹„ì•ˆì˜ ì°½ì—…ìë¥¼ ê°ê° ì°¾ê³ , ë‘ ì‚¬ëŒì˜ ê³µí†µì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.
================================================================================

[ì¤‘ê°„ ë‹¨ê³„]
  ë‹¨ê³„ 1: search_tesla_info í˜¸ì¶œ
  ë‹¨ê³„ 2: search_rivian_info í˜¸ì¶œ
  ë‹¨ê³„ 3: compare_companies í˜¸ì¶œ

[ìµœì¢… ë‹µë³€]
í…ŒìŠ¬ë¼ì˜ ì°½ì—…ìëŠ” Elon Muskì´ê³ , ë¦¬ë¹„ì•ˆì˜ ì°½ì—…ìëŠ” RJ Scaringeì…ë‹ˆë‹¤.

ë‘ ì°½ì—…ìì˜ ê³µí†µì :
1. ì „ê¸°ì°¨ ì‚°ì—…ì˜ ì„ êµ¬ì: ë‘ ì‚¬ëŒ ëª¨ë‘ ì „ê¸°ì°¨ ì‚°ì—…ì„ í˜ì‹ í•˜ë ¤ëŠ” ë¹„ì „ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
2. ì§€ì† ê°€ëŠ¥ì„± ì¶”êµ¬: í™˜ê²½ ë³´í˜¸ì™€ ì§€ì† ê°€ëŠ¥í•œ êµí†µìˆ˜ë‹¨ ê°œë°œì— ì§‘ì¤‘í•˜ê³  ìˆìŠµë‹ˆë‹¤.
3. í˜ì‹ ì  ì ‘ê·¼: ê¸°ì¡´ ìë™ì°¨ ì‚°ì—…ì˜ ê´€í–‰ì„ ê¹¨ê³  ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤.
```

### ì‹¤ìŠµ 4 ì†”ë£¨ì…˜: ì‹¤ì‹œê°„ ë°ì´í„° í†µí•© RAG

```python
from langchain.tools import tool
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
import yfinance as yf
from datetime import datetime

# 1. ì‹¤ì‹œê°„ ì£¼ì‹ ì •ë³´ ë„êµ¬
@tool
def get_realtime_stock_price(symbol: str) -> str:
    """ì‹¤ì‹œê°„ ì£¼ì‹ ê°€ê²© ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (ì˜ˆ: TSLA, RIVN)"""
    try:
        stock = yf.Ticker(symbol)
        info = stock.info

        current_price = info.get('currentPrice', 'N/A')
        market_cap = info.get('marketCap', 'N/A')
        pe_ratio = info.get('trailingPE', 'N/A')

        # ìˆ«ì í¬ë§·íŒ…
        if isinstance(market_cap, (int, float)):
            market_cap_b = market_cap / 1_000_000_000
            market_cap = f"${market_cap_b:.2f}B"

        result = f"""
{symbol} ì‹¤ì‹œê°„ ì£¼ì‹ ì •ë³´ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')}):
- í˜„ì¬ê°€: ${current_price}
- ì‹œê°€ì´ì•¡: {market_cap}
- P/E Ratio: {pe_ratio}
"""
        return result.strip()
    except Exception as e:
        return f"ì£¼ì‹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

@tool
def get_stock_history(symbol: str, period: str = "1mo") -> str:
    """ì£¼ì‹ì˜ ê³¼ê±° ê°€ê²© ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. period: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max"""
    try:
        stock = yf.Ticker(symbol)
        hist = stock.history(period=period)

        if hist.empty:
            return f"{symbol}ì˜ {period} ê¸°ê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        latest = hist.iloc[-1]
        earliest = hist.iloc[0]

        price_change = latest['Close'] - earliest['Close']
        price_change_pct = (price_change / earliest['Close']) * 100

        result = f"""
{symbol} ì£¼ì‹ {period} ê¸°ê°„ ë³€í™”:
- ì‹œì‘ê°€: ${earliest['Close']:.2f}
- ì¢…ê°€: ${latest['Close']:.2f}
- ë³€í™”: ${price_change:.2f} ({price_change_pct:+.2f}%)
- ìµœê³ ê°€: ${hist['High'].max():.2f}
- ìµœì €ê°€: ${hist['Low'].min():.2f}
"""
        return result.strip()
    except Exception as e:
        return f"ì£¼ì‹ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

# 2. ì •ì  ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ (ê¸°ì¡´)
static_rag_tool = rag_tool_english  # ì˜ì–´ RAG ë„êµ¬ ì¬ì‚¬ìš©

# 3. í•˜ì´ë¸Œë¦¬ë“œ Agent ìƒì„±
hybrid_tools = [
    get_realtime_stock_price,
    get_stock_history,
    static_rag_tool
]

llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

hybrid_agent = create_agent(
    model=llm,
    tools=hybrid_tools,
    system_prompt="""ë‹¹ì‹ ì€ ì •ì  ë¬¸ì„œì™€ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ë‹µë³€í•˜ëŠ” AI Assistantì…ë‹ˆë‹¤.

ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤:
- íšŒì‚¬ ì—­ì‚¬, ì°½ì—…ì ë“± ì •ì  ì •ë³´: rag_english_db ì‚¬ìš©
- í˜„ì¬ ì£¼ê°€, ì‹œê°€ì´ì•¡ ë“± ì‹¤ì‹œê°„ ì •ë³´: get_realtime_stock_price ì‚¬ìš©
- ì£¼ê°€ ë³€í™” ì¶”ì´: get_stock_history ì‚¬ìš©

ì—¬ëŸ¬ ë„êµ¬ë¥¼ ì¡°í•©í•˜ì—¬ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."""
)

# 4. í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
hybrid_queries = [
    "í…ŒìŠ¬ë¼ì˜ ì°½ì—…ìëŠ” ëˆ„êµ¬ì´ë©°, í˜„ì¬ ì£¼ê°€ëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
    "í…ŒìŠ¬ë¼ì™€ ë¦¬ë¹„ì•ˆì˜ í˜„ì¬ ì‹œê°€ì´ì•¡ì„ ë¹„êµí•´ì£¼ì„¸ìš”",
    "í…ŒìŠ¬ë¼ì˜ ì§€ë‚œ 1ê°œì›” ì£¼ê°€ ë³€í™”ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
]

for query in hybrid_queries:
    print(f"\n{'='*80}")
    print(f"ì§ˆë¬¸: {query}")
    print('='*80)

    response = hybrid_agent.invoke(
        {"messages": [{"role": "user", "content": query}]}
    )

    final_answer = response["messages"][-1].content
    print(f"ë‹µë³€:\n{final_answer}")
```

**ì˜ˆìƒ ì¶œë ¥:**
```
================================================================================
ì§ˆë¬¸: í…ŒìŠ¬ë¼ì˜ ì°½ì—…ìëŠ” ëˆ„êµ¬ì´ë©°, í˜„ì¬ ì£¼ê°€ëŠ” ì–¼ë§ˆì¸ê°€ìš”?
================================================================================
ë‹µë³€:
í…ŒìŠ¬ë¼ì˜ ì°½ì—…ìëŠ” Elon Muskì…ë‹ˆë‹¤.

í˜„ì¬ ì£¼ê°€ ì •ë³´ (2025-10-25 14:30:00):
- í˜„ì¬ê°€: $242.50
- ì‹œê°€ì´ì•¡: $771.23B
- P/E Ratio: 78.34

í…ŒìŠ¬ë¼ëŠ” 2003ë…„ì— ì„¤ë¦½ë˜ì—ˆìœ¼ë©°, ì „ê¸°ì°¨ ì‹œì¥ì„ ì„ ë„í•˜ëŠ” ê¸°ì—…ì…ë‹ˆë‹¤.

================================================================================
ì§ˆë¬¸: í…ŒìŠ¬ë¼ì™€ ë¦¬ë¹„ì•ˆì˜ í˜„ì¬ ì‹œê°€ì´ì•¡ì„ ë¹„êµí•´ì£¼ì„¸ìš”
================================================================================
ë‹µë³€:
í˜„ì¬ ì‹œê°€ì´ì•¡ ë¹„êµ:

í…ŒìŠ¬ë¼ (TSLA):
- ì‹œê°€ì´ì•¡: $771.23B
- í˜„ì¬ê°€: $242.50

ë¦¬ë¹„ì•ˆ (RIVN):
- ì‹œê°€ì´ì•¡: $12.45B
- í˜„ì¬ê°€: $11.23

í…ŒìŠ¬ë¼ì˜ ì‹œê°€ì´ì•¡ì´ ë¦¬ë¹„ì•ˆë³´ë‹¤ ì•½ 62ë°° ë” í½ë‹ˆë‹¤. ì´ëŠ” í…ŒìŠ¬ë¼ê°€ ì „ê¸°ì°¨ ì‹œì¥ì—ì„œ
í›¨ì”¬ ë” í° ê·œëª¨ì™€ ì‹œì¥ ì§€ë°°ë ¥ì„ ê°€ì§€ê³  ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
```

## ğŸš€ ì‹¤ë¬´ í™œìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ê³ ê° ì§€ì› ì±—ë´‡ ì‹œìŠ¤í…œ

```python
from langchain.tools import tool
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

# 1. ë„êµ¬ ì •ì˜
@tool
def search_faq(question: str) -> str:
    """ìì£¼ ë¬»ëŠ” ì§ˆë¬¸(FAQ) ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # FAQ ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰
    faq_retriever = faq_vectorstore.as_retriever(search_kwargs={'k': 3})
    docs = faq_retriever.invoke(question)

    if not docs:
        return "ê´€ë ¨ FAQë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    return "\n\n".join([
        f"Q: {doc.metadata['question']}\nA: {doc.page_content}"
        for doc in docs
    ])

@tool
def search_product_manual(product: str, question: str) -> str:
    """ì œí’ˆ ë§¤ë‰´ì–¼ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # ì œí’ˆë³„ ë§¤ë‰´ì–¼ ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰
    manual_retriever = product_manuals[product].as_retriever(search_kwargs={'k': 3})
    docs = manual_retriever.invoke(question)

    return "\n\n".join([doc.page_content for doc in docs])

@tool
def create_support_ticket(
    customer_name: str,
    issue_description: str,
    priority: Literal["low", "medium", "high"]
) -> str:
    """ê³ ê° ì§€ì› í‹°ì¼“ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    ticket_id = f"TICKET-{datetime.now().strftime('%Y%m%d%H%M%S')}"

    # í‹°ì¼“ ìƒì„± ë¡œì§ (ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë“±)
    ticket = {
        "ticket_id": ticket_id,
        "customer_name": customer_name,
        "issue": issue_description,
        "priority": priority,
        "status": "open",
        "created_at": datetime.now().isoformat()
    }

    return f"ì§€ì› í‹°ì¼“ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. í‹°ì¼“ ë²ˆí˜¸: {ticket_id}"

# 2. ê³ ê° ì§€ì› Agent ìƒì„±
support_tools = [search_faq, search_product_manual, create_support_ticket]

llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

support_agent = create_agent(
    model=llm,
    tools=support_tools,
    system_prompt="""ë‹¹ì‹ ì€ ê³ ê° ì§€ì› AI Assistantì…ë‹ˆë‹¤.

ê³ ê° ì§ˆë¬¸ ì²˜ë¦¬ ì ˆì°¨:
1. FAQì—ì„œ ìœ ì‚¬í•œ ì§ˆë¬¸ ê²€ìƒ‰
2. FAQì— ì—†ìœ¼ë©´ ì œí’ˆ ë§¤ë‰´ì–¼ ê²€ìƒ‰
3. í•´ê²°ë˜ì§€ ì•Šìœ¼ë©´ ì§€ì› í‹°ì¼“ ìƒì„± ì œì•ˆ
4. ì¹œì ˆí•˜ê³  ëª…í™•í•œ ë‹µë³€ ì œê³µ

ê³ ê°ì˜ ë§Œì¡±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤."""
)

# 3. ê³ ê° ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜
customer_queries = [
    "ì œí’ˆ Aì˜ ë°°í„°ë¦¬ ìˆ˜ëª…ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?",
    "ì œí’ˆ Bê°€ ì¼œì§€ì§€ ì•ŠìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
    "ì œí’ˆì´ íŒŒì†ë˜ì–´ êµí™˜í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤."
]

for query in customer_queries:
    print(f"\n{'='*80}")
    print(f"ê³ ê°: {query}")
    print('='*80)

    response = support_agent.invoke(
        {"messages": [{"role": "user", "content": query}]}
    )

    answer = response["messages"][-1].content
    print(f"ìƒë‹´ì›: {answer}")
```

### ì˜ˆì‹œ 2: ë²•ë¥  ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œ

```python
from langchain.tools import tool
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma

# 1. ë²•ë¥  ë¬¸ì„œ ë„êµ¬
@tool
def search_legal_precedents(case_description: str, jurisdiction: str) -> str:
    """ìœ ì‚¬í•œ ë²•ì  íŒë¡€ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # íŒë¡€ ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰ (ê´€í•  ì§€ì—­ í•„í„°ë§)
    precedent_retriever = legal_db.as_retriever(
        search_kwargs={
            'k': 5,
            'filter': {'jurisdiction': jurisdiction}
        }
    )
    docs = precedent_retriever.invoke(case_description)

    return "\n\n".join([
        f"íŒë¡€ {i+1}: {doc.metadata['case_name']} ({doc.metadata['year']})\n{doc.page_content[:200]}..."
        for i, doc in enumerate(docs)
    ])

@tool
def search_statutes(legal_issue: str) -> str:
    """ê´€ë ¨ ë²•ë¥  ì¡°í•­ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    statute_retriever = statute_db.as_retriever(search_kwargs={'k': 3})
    docs = statute_retriever.invoke(legal_issue)

    return "\n\n".join([
        f"{doc.metadata['statute_name']} ì œ{doc.metadata['article']}ì¡°:\n{doc.page_content}"
        for doc in docs
    ])

@tool
def analyze_contract_clause(clause_text: str) -> str:
    """ê³„ì•½ì„œ ì¡°í•­ì„ ë¶„ì„í•˜ê³  ì ì¬ì  ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
    llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

    prompt = f"""ë‹¤ìŒ ê³„ì•½ì„œ ì¡°í•­ì„ ë¶„ì„í•˜ì„¸ìš”:

ì¡°í•­:
{clause_text}

ë¶„ì„ í•­ëª©:
1. ì£¼ìš” ë‚´ìš© ìš”ì•½
2. ì ì¬ì  ë²•ì  ë¦¬ìŠ¤í¬
3. ìˆ˜ì • ì œì•ˆ
4. ì£¼ì˜ì‚¬í•­
"""

    result = llm.invoke(prompt)
    return result.content

# 2. ë²•ë¥  ë¶„ì„ Agent
legal_tools = [search_legal_precedents, search_statutes, analyze_contract_clause]

llm = ChatOpenAI(model="gpt-4o", temperature=0)  # ë” ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©

legal_agent = create_agent(
    model=llm,
    tools=legal_tools,
    system_prompt="""ë‹¹ì‹ ì€ ë²•ë¥  ë¶„ì„ AI Assistantì…ë‹ˆë‹¤.

ë¶„ì„ ì ˆì°¨:
1. ë²•ë¥  ì´ìŠˆ íŒŒì•…
2. ê´€ë ¨ ë²•ë¥  ì¡°í•­ ê²€ìƒ‰
3. ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰
4. ì¢…í•© ë¶„ì„ ì œê³µ

ì£¼ì˜: ë²•ì  ì¡°ì–¸ì´ ì•„ë‹Œ ì •ë³´ ì œê³µ ëª©ì ì„ì„ ëª…ì‹œí•©ë‹ˆë‹¤."""
)

# 3. ë²•ë¥  ë¶„ì„ ì‹¤í–‰
legal_query = """
ë¶€ë™ì‚° ë§¤ë§¤ ê³„ì•½ì„œì— "ë§¤ìˆ˜ì¸ì€ ê³„ì•½ê¸ˆ ì§€ê¸‰ í›„ 7ì¼ ì´ë‚´ì— ì”ê¸ˆì„ ì§€ê¸‰í•˜ë©°,
ì§€ê¸‰í•˜ì§€ ì•Šì„ ê²½ìš° ê³„ì•½ê¸ˆì„ í¬ê¸°í•˜ê³  ê³„ì•½ì´ ìë™ í•´ì§€ëœë‹¤"ëŠ” ì¡°í•­ì´ ìˆìŠµë‹ˆë‹¤.
ì´ ì¡°í•­ì˜ ë²•ì  ìœ íš¨ì„±ê³¼ ë¦¬ìŠ¤í¬ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""

response = legal_agent.invoke(
    {"messages": [{"role": "user", "content": legal_query}]}
)

print(response["messages"][-1].content)
```

**ì˜ˆìƒ ì¶œë ¥:**
```
[ë²•ë¥  ë¶„ì„ ê²°ê³¼]

1. ê´€ë ¨ ë²•ë¥  ì¡°í•­:
   - ë¯¼ë²• ì œ565ì¡° (í•´ì œê¶Œì˜ í–‰ì‚¬)
   - ë¯¼ë²• ì œ551ì¡° (ìœ„ì•½ê¸ˆì˜ ì•½ì •)

2. ìœ ì‚¬ íŒë¡€:
   - ëŒ€ë²•ì› 2015ë‹¤12345: ê³„ì•½ê¸ˆ í¬ê¸° ì•½ì •ì˜ ìœ íš¨ì„± ì¸ì •
   - ëŒ€ë²•ì› 2018ë‹¤67890: ê³¼ë„í•œ ìœ„ì•½ê¸ˆ ì¡°í•­ì˜ ì œí•œ

3. ì¡°í•­ ë¶„ì„:
   - 7ì¼ ì”ê¸ˆ ì§€ê¸‰ ê¸°í•œì€ ë§¤ìš° ì§§ì•„ ë§¤ìˆ˜ì¸ì—ê²Œ ë¶ˆë¦¬í•¨
   - ê³„ì•½ê¸ˆ í¬ê¸° ì¡°í•­ì€ ì¼ë°˜ì ìœ¼ë¡œ ìœ íš¨í•˜ë‚˜, ê³„ì•½ê¸ˆì´ ê³¼ë„í•œ ê²½ìš° ë²•ì›ì´ ê°ì•¡ ê°€ëŠ¥
   - "ìë™ í•´ì§€" ì¡°í•­ì€ ë³„ë„ì˜ í•´ì§€ í†µì§€ ì—†ì´ í•´ì§€ë˜ëŠ” ê²ƒìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆìŒ

4. ë¦¬ìŠ¤í¬ ë° ì œì•ˆ:
   - ë¦¬ìŠ¤í¬: ì§§ì€ ê¸°í•œìœ¼ë¡œ ì¸í•œ ë§¤ìˆ˜ì¸ì˜ ì±„ë¬´ë¶ˆì´í–‰ ê°€ëŠ¥ì„±
   - ì œì•ˆ: ì”ê¸ˆ ì§€ê¸‰ ê¸°í•œì„ ìµœì†Œ 1ê°œì›”ë¡œ ì—°ì¥
   - ì œì•ˆ: ìë™ í•´ì§€ê°€ ì•„ë‹Œ ìµœê³  ì ˆì°¨ ì¶”ê°€

**ë©´ì±…ì‚¬í•­: ì´ëŠ” ì •ë³´ ì œê³µ ëª©ì ì´ë©° ë²•ì  ì¡°ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤.**
```

## ğŸ“– ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LangChain ReAct Agent ê°€ì´ë“œ](https://python.langchain.com/docs/modules/agents/agent_types/react)
- [LangChain Tools ë¬¸ì„œ](https://python.langchain.com/docs/modules/agents/tools/)
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [Chroma Vector Database](https://docs.trychroma.com/)

### ë‹¤êµ­ì–´ ì§€ì› ê´€ë ¨
- [langdetect ë¼ì´ë¸ŒëŸ¬ë¦¬](https://pypi.org/project/langdetect/)
- [LibreTranslate API](https://libretranslate.com/)
- [OpenAI ë‹¤êµ­ì–´ ì„ë² ë”©](https://platform.openai.com/docs/guides/embeddings)

### ReAct í”„ë ˆì„ì›Œí¬
- [ReAct ë…¼ë¬¸ (Yao et al., 2022)](https://arxiv.org/abs/2210.03629)
- [Tool Use and Agent Development](https://www.anthropic.com/research/tool-use)

### ì¶”ê°€ í•™ìŠµ ìë£Œ
- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/)
- [Agent Executor ì‹¬í™”](https://python.langchain.com/docs/modules/agents/agent_executors/)
- [Custom Agent êµ¬í˜„](https://python.langchain.com/docs/modules/agents/how_to/custom_agent)
