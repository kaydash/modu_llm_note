# PRJ02_W1_003 ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ ê¸°ë²• ë§¤ë‰´ì–¼ - í‚¤ì›Œë“œ ê²€ìƒ‰ / í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

## ğŸ“‹ ê°œìš”

ì´ ë…¸íŠ¸ë¶ì€ RAG ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ í•µì‹¬ ê¸°ë²•ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì˜ë¯¸ë¡ ì  ê²€ìƒ‰(Semantic Search), í‚¤ì›Œë“œ ê²€ìƒ‰(Keyword Search), ê·¸ë¦¬ê³  ë‘ ë°©ì‹ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(Hybrid Search)ì„ ë¹„êµ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ¯ í•™ìŠµ ëª©í‘œ
- í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë°©ì‹ ì‹¤ìŠµ ë° ë¹„êµ ë¶„ì„
- BM25 ì•Œê³ ë¦¬ì¦˜ê³¼ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ì˜ ì°¨ì´ì  ì´í•´
- EnsembleRetrieverë¥¼ í™œìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬í˜„
- í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš©
- K-RAG íŒ¨í‚¤ì§€ë¥¼ í†µí•œ ì •ëŸ‰ì  ì„±ëŠ¥ í‰ê°€

## ğŸ› ï¸ í™˜ê²½ ì„¤ì •

### 1. í•„ìˆ˜ íŒ¨í‚¤ì§€
```python
# í™˜ê²½ë³€ìˆ˜ ë° ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from dotenv import load_dotenv
import os
from glob import glob
from pprint import pprint
import json

# LangChain ê´€ë ¨
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document

# í•œêµ­ì–´ ì²˜ë¦¬
from kiwipiepy import Kiwi

# K-RAG í‰ê°€ íŒ¨í‚¤ì§€
from krag.tokenizers import KiwiTokenizer
from krag.retrievers import KiWiBM25RetrieverWithScore
from krag.evaluators import RougeOfflineRetrievalEvaluators

# ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”
import pandas as pd
import matplotlib.pyplot as plt

# Langfuse íŠ¸ë ˆì´ì‹±
from langfuse.callback import CallbackHandler  # ì‹¤ì œ import ê²½ë¡œ

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
```

### 2. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬
```python
# í•œêµ­ì–´ ì „ê¸°ì°¨ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_text_files(txt_files):
    data = []
    for text_file in txt_files:
        loader = TextLoader(text_file, encoding='utf-8')
        data += loader.load()
    return data

# ë°ì´í„° ë¡œë“œ
korean_txt_files = glob(os.path.join('data', '*_KR.md'))
korean_data = load_text_files(korean_txt_files)

# í…ìŠ¤íŠ¸ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    encoding_name="cl100k_base",
    separators=['\n\n', '\n', r'(?<=[.!?])\s+'],
    chunk_size=300,
    chunk_overlap=50,
    is_separator_regex=True,
    keep_separator=True,
)

korean_chunks = text_splitter.split_documents(korean_data)

# Document ê°ì²´ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
korean_docs = []
for chunk in korean_chunks:
    doc = Document(page_content=chunk.page_content, metadata=chunk.metadata)
    # íšŒì‚¬ëª… ì‹ë³„ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    doc.metadata['company'] = 'í…ŒìŠ¬ë¼' if 'í…ŒìŠ¬ë¼' in doc.metadata['source'] else 'ë¦¬ë¹„ì•ˆ'
    doc.metadata['language'] = 'ko'
    # ì¶œì²˜ ì •ë³´ë¥¼ ë¬¸ì„œ ë‚´ìš©ì— í¬í•¨
    doc.page_content = f"[ì¶œì²˜] ì´ ë¬¸ì„œëŠ” {doc.metadata['company']}ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.\n----------------------------------\n{doc.page_content}"
    korean_docs.append(doc)

print(f"ì´ {len(korean_docs)}ê°œì˜ ë¬¸ì„œ ì²­í¬ ìƒì„±ë¨")
```

## ğŸ” RAG ê²€ìƒ‰ê¸° ìœ í˜•

### 1. Semantic Search (ì˜ë¯¸ë¡ ì  ê²€ìƒ‰)

**ê°œë…**: í…ìŠ¤íŠ¸ì˜ ë²¡í„° í‘œí˜„ì„ í™œìš©í•œ ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜ ê²€ìƒ‰

**íŠ¹ì§•**:
- ğŸ¯ **ì˜ë¯¸ì  ìœ ì‚¬ì„±**: ë‹¨ì–´ê°€ ë‹¬ë¼ë„ ë¹„ìŠ·í•œ ì˜ë¯¸ë©´ ê²€ìƒ‰ ê°€ëŠ¥
- ğŸŒ **ë‹¤êµ­ì–´ ì§€ì›**: ì„ë² ë”© ëª¨ë¸ì˜ ë‹¤êµ­ì–´ ëŠ¥ë ¥ í™œìš©
- ğŸ”„ **ì»¨í…ìŠ¤íŠ¸ ì´í•´**: ë¬¸ë§¥ì„ ê³ ë ¤í•œ ê²€ìƒ‰

**êµ¬í˜„**:
```python
# OpenAI ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±
chroma_db = Chroma.from_documents(
    documents=korean_docs,
    embedding=embeddings,
    collection_name="db_korean_cosine",
    persist_directory="./chroma_db",
    collection_metadata={'hnsw:space': 'cosine'}
)

# ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
chroma_k_retriever = chroma_db.as_retriever(search_kwargs={"k": 2})

# í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
query = "ë¦¬ë¹„ì•ˆì€ ì–¸ì œ ì‚¬ì—…ì„ ì‹œì‘í–ˆë‚˜ìš”?"
retrieved_docs = chroma_k_retriever.invoke(query)

print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
for i, doc in enumerate(retrieved_docs):
    print(f"\n[ë¬¸ì„œ {i+1}] {doc.page_content[:100]}...")
```

### 2. Keyword Search (í‚¤ì›Œë“œ ê²€ìƒ‰)

**ê°œë…**: BM25 ë“± ì „í†µì  ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ì˜ ë‹¨ì–´ ë§¤ì¹­ ë°©ì‹

**íŠ¹ì§•**:
- ğŸ¯ **ì •í™•í•œ ë§¤ì¹­**: ì •í™•í•œ í‚¤ì›Œë“œ ì¼ì¹˜ì— ê°•ì 
- âš¡ **ë¹ ë¥¸ ì†ë„**: ê³„ì‚° íš¨ìœ¨ì„±ì´ ë†’ìŒ
- ğŸ“Š **í†µê³„ ê¸°ë°˜**: TF-IDF, BM25 ë“± ê²€ì¦ëœ ì•Œê³ ë¦¬ì¦˜

**BM25 ì•Œê³ ë¦¬ì¦˜ íŠ¹ì§•**:
- Term Frequency (TF): ë¬¸ì„œ ë‚´ ë‹¨ì–´ ë¹ˆë„
- Inverse Document Frequency (IDF): ì „ì²´ ë¬¸ì„œì§‘í•©ì—ì„œì˜ í¬ê·€ì„±
- Document Length Normalization: ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™”

**ê¸°ë³¸ êµ¬í˜„**:
```python
# Chroma DBì—ì„œ ë¬¸ì„œ ì¶”ì¶œ
documents = chroma_db.get()["documents"]
metadatas = chroma_db.get()["metadatas"]

# Document ê°ì²´ë¡œ ë³€í™˜
docs = [Document(page_content=content, metadata=meta)
        for content, meta in zip(documents, metadatas)]

# ê¸°ë³¸ BM25 ê²€ìƒ‰ê¸° ìƒì„±
bm25_retriever = BM25Retriever.from_documents(docs)

# ê¸°ë³¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
query = "ë¦¬ë¹„ì•ˆì€ ì–¸ì œ ì‚¬ì—…ì„ ì‹œì‘í–ˆë‚˜ìš”?"
retrieved_docs = bm25_retriever.invoke(query)

print(f"ê¸°ë³¸ BM25 ê²€ìƒ‰ ê²°ê³¼: {len(retrieved_docs)}ê°œ ë¬¸ì„œ")
for i, doc in enumerate(retrieved_docs):
    print(f"[{i}] BM25 Score: 0.0")  # í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨ë¡œ ì¸í•œ ë‚®ì€ ì ìˆ˜
```

**í•œêµ­ì–´ íŠ¹í™” ë¬¸ì œì **:
- "ë¦¬ë¹„ì•ˆì€" â†’ ì¡°ì‚¬ "ì€"ìœ¼ë¡œ ì¸í•´ "ë¦¬ë¹„ì•ˆ"ê³¼ ë§¤ì¹­ ì‹¤íŒ¨
- í•œêµ­ì–´ ì–´ë¯¸ ë³€í™”ì™€ ì¡°ì‚¬ë¡œ ì¸í•œ ê²€ìƒ‰ ì„±ëŠ¥ ì €í•˜

### 3. í•œêµ­ì–´ íŠ¹í™” BM25 êµ¬í˜„

**ë¬¸ì œì **: ê¸°ë³¸ BM25ëŠ” í•œêµ­ì–´ íŠ¹ì„± ë°˜ì˜ ë¶€ì¡±
- ì¡°ì‚¬, ì–´ë¯¸ ë³€í™”
- ë³µí•©ì–´ êµ¬ì¡°
- ë„ì–´ì“°ê¸° ë¶ˆê·œì¹™ì„±

**í•´ê²°ì±…**: Kiwi í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš©

```python
from kiwipiepy import Kiwi

# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
kiwi = Kiwi()

# ì „ê¸°ì°¨ ê´€ë ¨ ì „ë¬¸ ìš©ì–´ ë“±ë¡
kiwi.add_user_word('ë¦¬ë¹„ì•ˆ', 'NNP')  # ê³ ìœ ëª…ì‚¬
kiwi.add_user_word('í…ŒìŠ¬ë¼', 'NNP')

# í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì§• í•¨ìˆ˜
def bm25_process_func(text, kiwi_model=Kiwi()):
    """
    BM25Retrieverì—ì„œ ì‚¬ìš©í•  ì „ì²˜ë¦¬ í•¨ìˆ˜
    í•œêµ­ì–´ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ í† í°í™”
    :param text: í† í°í™”í•  ë¬¸ì¥
    :param kiwi_model: Kiwi ê°ì²´
    """
    return [t.form for t in kiwi_model.tokenize(text)]

# í•œêµ­ì–´ íŠ¹í™” BM25 ê²€ìƒ‰ê¸° ìƒì„±
bm25_retriever = BM25Retriever.from_documents(
    documents=docs,
    preprocess_func=lambda x: bm25_process_func(x, kiwi_model=kiwi),
)

# ê°œì„ ëœ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
query = "ë¦¬ë¹„ì•ˆì´ ì„¤ë¦½ëœ ì—°ë„ëŠ”?"
retrieved_docs = bm25_retriever.invoke(query)

print("=== í•œêµ­ì–´ íŠ¹í™” BM25 ê²€ìƒ‰ ê²°ê³¼ ===")
for i, doc in enumerate(retrieved_docs[:3]):
    print(f"[{i}] BM25 ì ìˆ˜ ê°œì„ ë¨")
    print(f"ë‚´ìš©: {doc.page_content[:100]}...")
    print("-" * 50)
```

**ì‹¤ì œ ì„±ëŠ¥ í–¥ìƒ**:
- "ë¦¬ë¹„ì•ˆì€" (ì‹¤íŒ¨) â†’ "ë¦¬ë¹„ì•ˆì´" (ì„±ê³µ): BM25 ì ìˆ˜ 4.61 â†’ 7.79ë¡œ í–¥ìƒ
- í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ì–´ê·¼ ì¶”ì¶œë¡œ ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­

### 4. Hybrid Search (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)

**ê°œë…**: í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ì„ ê²°í•©í•œ í†µí•© ì ‘ê·¼ë²•

**ì¥ì **:
- ğŸ¯ **ì •í™•ì„± + ìœ ì—°ì„±**: ì •í™•í•œ ë§¤ì¹­ê³¼ ì˜ë¯¸ì  ìœ ì‚¬ì„± ëª¨ë‘ í™œìš©
- ğŸ”„ **ìƒí˜¸ ë³´ì™„**: ê° ë°©ì‹ì˜ ì•½ì ì„ ë³´ì™„
- âš–ï¸ **ê°€ì¤‘ì¹˜ ì¡°ì ˆ**: ìƒí™©ì— ë§ëŠ” ë¹„ì¤‘ ì¡°ì • ê°€ëŠ¥

**K-RAG íŒ¨í‚¤ì§€ë¥¼ í™œìš©í•œ ê³ ê¸‰ êµ¬í˜„**:
```python
from krag.tokenizers import KiwiTokenizer
from krag.retrievers import KiWiBM25RetrieverWithScore

# ì „ë¬¸ K-RAG BM25 ê²€ìƒ‰ê¸° (ì ìˆ˜ í¬í•¨)
retriever_bm25_kiwi = KiWiBM25RetrieverWithScore(
    documents=korean_docs,
    kiwi_tokenizer=KiwiTokenizer(model_type='knlm', typos='basic'),
    k=3,
)

# Chroma ê²€ìƒ‰ê¸°
retriever_chroma_db = chroma_db.as_retriever(search_kwargs={"k": 5})

# EnsembleRetrieverë¡œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
from langchain.retrievers import EnsembleRetriever

ensemble_retriever = EnsembleRetriever(
    retrievers=[retriever_bm25_kiwi, retriever_chroma_db],
    weights=[0.5, 0.5]  # BM25ì™€ Chroma ë™ì¼ ê°€ì¤‘ì¹˜
)

# í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
test_query = "ë¦¬ë¹„ì•ˆì´ ì„¤ë¦½ëœ ì—°ë„ëŠ”?"
test_results = ensemble_retriever.invoke(test_query)

print(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼: {len(test_results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
print("êµ¬ì„±: BM25 ê²€ìƒ‰ê¸°(ê°€ì¤‘ì¹˜: 0.5) + Chroma ê²€ìƒ‰ê¸°(ê°€ì¤‘ì¹˜: 0.5)")
```

## ğŸ“Š ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ

### 1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„

```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ (Excel íŒŒì¼)
import pandas as pd
df_qa_test = pd.read_excel("data/testset.xlsx")

print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì •ë³´:")
print(f"- ì´ ì§ˆë¬¸ ìˆ˜: {len(df_qa_test)}")
print(f"- ì²« ë²ˆì§¸ ì§ˆë¬¸: {df_qa_test['user_input'].iloc[0]}")
```

### 2. K-RAG ê¸°ë°˜ í‰ê°€ í•¨ìˆ˜

```python
from krag.evaluators import RougeOfflineRetrievalEvaluators

def evaluate_qa_test(df_qa_test, retriever, k=3):
    """QA í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€"""

    # ì‹¤ì œ ê´€ë ¨ ë¬¸ì„œì™€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
    actual_docs = []
    predicted_docs = []

    for _, row in df_qa_test.iterrows():
        question = row['user_input']
        reference_contexts = eval(row['reference_contexts'])

        # ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
        context_docs = [Document(page_content=ctx) for ctx in reference_contexts]
        actual_docs.append(context_docs)

        # ê²€ìƒ‰ ìˆ˜í–‰
        retrieved_docs = retriever.invoke(question)
        predicted_docs.append(retrieved_docs)

    # ROUGE ê¸°ë°˜ í‰ê°€ ìˆ˜í–‰
    evaluator = RougeOfflineRetrievalEvaluators(
        actual_docs=actual_docs,
        predicted_docs=predicted_docs,
        match_method="rouge2",
        threshold=0.3
    )

    # í‰ê°€ì§€í‘œ ê³„ì‚°
    hit_rate = evaluator.calculate_hit_rate(k=k)['hit_rate']
    mrr = evaluator.calculate_mrr(k=k)['mrr']
    recall = evaluator.calculate_recall(k=k)['macro_recall']
    precision = evaluator.calculate_precision(k=k)['macro_precision']
    f1_score = evaluator.calculate_f1_score(k=k)['macro_f1']
    map_score = evaluator.calculate_map(k=k)['map']
    ndcg = evaluator.calculate_ndcg(k=k)['ndcg']

    return {
        'hit_rate': hit_rate,
        'mrr': mrr,
        'recall': recall,
        'precision': precision,
        'f1_score': f1_score,
        'map': map_score,
        'ndcg': ndcg
    }
```

### 2. í‰ê°€ í•¨ìˆ˜ êµ¬í˜„

```python
def evaluate_qa_test(df_qa_test, retriever, k=3):
    """QA í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€"""

    # ì‹¤ì œ ê´€ë ¨ ë¬¸ì„œì™€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
    actual_docs = []
    predicted_docs = []

    for _, row in df_qa_test.iterrows():
        question = row['user_input']
        reference_contexts = eval(row['reference_contexts'])  # ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        # ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
        context_docs = [Document(page_content=ctx) for ctx in reference_contexts]
        actual_docs.append(context_docs)

        # ê²€ìƒ‰ ìˆ˜í–‰
        retrieved_docs = retriever.invoke(question)
        predicted_docs.append(retrieved_docs)

    # ROUGE ê¸°ë°˜ í‰ê°€ ìˆ˜í–‰
    evaluator = RougeOfflineRetrievalEvaluators(
        actual_docs=actual_docs,
        predicted_docs=predicted_docs,
        match_method="rouge2",
        threshold=0.3
    )

    # í‰ê°€ì§€í‘œ ê³„ì‚°
    hit_rate = evaluator.calculate_hit_rate(k=k)['hit_rate']
    mrr = evaluator.calculate_mrr(k=k)['mrr']
    recall = evaluator.calculate_recall(k=k)['macro_recall']
    precision = evaluator.calculate_precision(k=k)['macro_precision']
    f1_score = evaluator.calculate_f1_score(k=k)['macro_f1']
    map_score = evaluator.calculate_map(k=k)['map']
    ndcg = evaluator.calculate_ndcg(k=k)['ndcg']

    return {
        'hit_rate': hit_rate,
        'mrr': mrr,
        'recall': recall,
        'precision': precision,
        'f1_score': f1_score,
        'map': map_score,
        'ndcg': ndcg
    }
```

## ğŸ§ª ì‹¤í—˜ ë° ë¹„êµ ë¶„ì„

### 1. kê°’ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜

```python
# k=1~5ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰
k_values = [1, 2, 3, 4, 5]

# BM25 ê²€ìƒ‰ê¸° ì„±ëŠ¥ (ì‹¤ì œ ì¸¡ì • ê²°ê³¼)
for k in k_values:
    retriever_bm25_kiwi.k = k
    result = evaluate_qa_test(df_qa_test, retriever_bm25_kiwi, k=k)
    print(f"BM25 k={k}: Hit_Rate={result['hit_rate']:.3f}, MRR={result['mrr']:.3f}")

# ì‹¤ì œ ì¶œë ¥ ê²°ê³¼:
# BM25 k=1: Hit_Rate=0.286, MRR=0.735
# BM25 k=2: Hit_Rate=0.449, MRR=0.755
# BM25 k=3: Hit_Rate=0.653, MRR=0.769
# BM25 k=4: Hit_Rate=0.735, MRR=0.784
# BM25 k=5: Hit_Rate=0.776, MRR=0.788

# Chroma ê²€ìƒ‰ê¸° ì„±ëŠ¥ (ì‹¤ì œ ì¸¡ì • ê²°ê³¼)
for k in k_values:
    retriever_chroma_db.search_kwargs = {"k": k}
    result = evaluate_qa_test(df_qa_test, retriever_chroma_db, k=k)
    print(f"Chroma k={k}: Hit_Rate={result['hit_rate']:.3f}, MRR={result['mrr']:.3f}")

# ì‹¤ì œ ì¶œë ¥ ê²°ê³¼:
# Chroma k=1: Hit_Rate=0.286, MRR=0.673
# Chroma k=2: Hit_Rate=0.408, MRR=0.714
# Chroma k=3: Hit_Rate=0.551, MRR=0.741
# Chroma k=4: Hit_Rate=0.592, MRR=0.752
# Chroma k=5: Hit_Rate=0.633, MRR=0.756

# Ensemble (í•˜ì´ë¸Œë¦¬ë“œ) ê²€ìƒ‰ê¸° ì„±ëŠ¥ (ì‹¤ì œ ì¸¡ì • ê²°ê³¼)
for k in k_values:
    ensemble_result = evaluate_qa_test(df_qa_test, ensemble_retriever, k=k)
    print(f"Ensemble k={k}: Hit_Rate={ensemble_result['hit_rate']:.3f}, MRR={ensemble_result['mrr']:.3f}")

# ì‹¤ì œ ì¶œë ¥ ê²°ê³¼:
# Ensemble k=1: Hit_Rate=0.286, MRR=0.735
# Ensemble k=2: Hit_Rate=0.469, MRR=0.776
# Ensemble k=3: Hit_Rate=0.633, MRR=0.782
# Ensemble k=4: Hit_Rate=0.714, MRR=0.793
# Ensemble k=5: Hit_Rate=0.776, MRR=0.810
```

### 2. ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼í‘œ

```python
# ì‹¤ì œ ì‹¤í—˜ ê²°ê³¼ ì¢…í•©í‘œ
results_comparison = pd.DataFrame({
    'k': [1, 2, 3, 4, 5],
    'BM25_Hit_Rate': [0.286, 0.449, 0.653, 0.735, 0.776],
    'BM25_MRR': [0.735, 0.755, 0.769, 0.784, 0.788],
    'Chroma_Hit_Rate': [0.286, 0.408, 0.551, 0.592, 0.633],
    'Chroma_MRR': [0.673, 0.714, 0.741, 0.752, 0.756],
    'Ensemble_Hit_Rate': [0.286, 0.469, 0.633, 0.714, 0.776],
    'Ensemble_MRR': [0.735, 0.776, 0.782, 0.793, 0.810]
})

print("=== ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ===")
print(results_comparison)

# ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ ìš”ì•½
print("\n=== k=3ì—ì„œì˜ ì„±ëŠ¥ ë¹„êµ ===")
print(f"BM25 (í‚¤ì›Œë“œ):     Hit Rate = 0.653, MRR = 0.769")
print(f"Chroma (ì˜ë¯¸ë¡ ì ): Hit Rate = 0.551, MRR = 0.741")
print(f"Ensemble (í•˜ì´ë¸Œë¦¬ë“œ): Hit Rate = 0.633, MRR = 0.782")
```

### 3. ì„±ëŠ¥ ì‹œê°í™”

```python
import matplotlib.pyplot as plt

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib í•œê¸€ ê¹¨ì§ ë°©ì§€)
plt.rcParams['font.family'] = 'DejaVu Sans'

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Hit Rate ë¹„êµ
ax1.plot(results_comparison['k'], results_comparison['BM25_Hit_Rate'],
         'o-', label='BM25 (Keyword)', linewidth=2, markersize=8)
ax1.plot(results_comparison['k'], results_comparison['Chroma_Hit_Rate'],
         's-', label='Chroma (Semantic)', linewidth=2, markersize=8)
ax1.plot(results_comparison['k'], results_comparison['Ensemble_Hit_Rate'],
         '^-', label='Ensemble (Hybrid)', linewidth=2, markersize=8)
ax1.set_xlabel('k (ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜)')
ax1.set_ylabel('Hit Rate')
ax1.set_title('Hit Rate ë¹„êµ')
ax1.legend()
ax1.grid(True, alpha=0.3)

# MRR ë¹„êµ
ax2.plot(results_comparison['k'], results_comparison['BM25_MRR'],
         'o-', label='BM25 (Keyword)', linewidth=2, markersize=8)
ax2.plot(results_comparison['k'], results_comparison['Chroma_MRR'],
         's-', label='Chroma (Semantic)', linewidth=2, markersize=8)
ax2.plot(results_comparison['k'], results_comparison['Ensemble_MRR'],
         '^-', label='Ensemble (Hybrid)', linewidth=2, markersize=8)
ax2.set_xlabel('k (ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜)')
ax2.set_ylabel('MRR')
ax2.set_title('MRR ë¹„êµ')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## ğŸ›ï¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìµœì í™”

### 1. ê°€ì¤‘ì¹˜ íŠœë‹

```python
def optimize_ensemble_weights(df_qa_test, bm25_retriever, semantic_retriever, k=3):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì˜ ìµœì  ê°€ì¤‘ì¹˜ íƒìƒ‰"""

    weight_combinations = [
        (0.1, 0.9), (0.2, 0.8), (0.3, 0.7), (0.4, 0.6), (0.5, 0.5),
        (0.6, 0.4), (0.7, 0.3), (0.8, 0.2), (0.9, 0.1)
    ]

    best_score = 0
    best_weights = (0.5, 0.5)
    results = {}

    for semantic_weight, keyword_weight in weight_combinations:
        ensemble_retriever = EnsembleRetriever(
            retrievers=[semantic_retriever, bm25_retriever],
            weights=[semantic_weight, keyword_weight]
        )

        result = evaluate_qa_test(df_qa_test, ensemble_retriever, k=k)
        f1_score = result['f1_score']

        results[(semantic_weight, keyword_weight)] = result

        if f1_score > best_score:
            best_score = f1_score
            best_weights = (semantic_weight, keyword_weight)

    return best_weights, best_score, results
```

### 2. ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •

```python
class AdaptiveEnsembleRetriever:
    """ì¿¼ë¦¬ ìœ í˜•ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ê²€ìƒ‰ê¸°"""

    def __init__(self, semantic_retriever, keyword_retriever):
        self.semantic_retriever = semantic_retriever
        self.keyword_retriever = keyword_retriever

    def _analyze_query_type(self, query):
        """ì¿¼ë¦¬ ìœ í˜• ë¶„ì„"""
        # ê³ ìœ ëª…ì‚¬ë‚˜ ì •í™•í•œ ìš©ì–´ê°€ ë§ìœ¼ë©´ í‚¤ì›Œë“œ ê²€ìƒ‰ ë¹„ì¤‘ ì¦ê°€
        proper_nouns = ['í…ŒìŠ¬ë¼', 'ë¦¬ë¹„ì•ˆ', 'í¬ë“œ', 'Tesla', 'Rivian']
        exact_terms = ['ê°€ê²©', 'ìˆ˜ëŸ‰', 'ë‚ ì§œ', 'ìˆ˜ì¹˜']

        if any(noun in query for noun in proper_nouns):
            return 'factual'  # ì‚¬ì‹¤ì  ì§ˆë¬¸
        elif any(term in query for term in exact_terms):
            return 'precise'  # ì •í™•í•œ ì •ë³´ ìš”êµ¬
        else:
            return 'conceptual'  # ê°œë…ì  ì§ˆë¬¸

    def invoke(self, query):
        """ì¿¼ë¦¬ ìœ í˜•ì— ë”°ë¥¸ ì ì‘ì  ê²€ìƒ‰"""
        query_type = self._analyze_query_type(query)

        if query_type == 'factual':
            weights = [0.3, 0.7]  # í‚¤ì›Œë“œ ê²€ìƒ‰ ë¹„ì¤‘ ë†’ì„
        elif query_type == 'precise':
            weights = [0.2, 0.8]  # í‚¤ì›Œë“œ ê²€ìƒ‰ ë”ìš± ê°•í™”
        else:
            weights = [0.7, 0.3]  # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ë¹„ì¤‘ ë†’ì„

        ensemble = EnsembleRetriever(
            retrievers=[self.semantic_retriever, self.keyword_retriever],
            weights=weights
        )

        return ensemble.invoke(query)
```

## ğŸ’¡ ê²€ìƒ‰ ë°©ì‹ë³„ íŠ¹ì„± ë¶„ì„

### 1. ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

**ì£¼ìš” ë°œê²¬ì‚¬í•­**:

1. **BM25 (í‚¤ì›Œë“œ ê²€ìƒ‰)**:
   - Hit Rate: 0.286 â†’ 0.776 (k=1â†’5)
   - MRR: 0.735 â†’ 0.788 (ì•ˆì •ì  ì„±ëŠ¥)
   - **ì¥ì **: ì •í™•í•œ ìš©ì–´ ë§¤ì¹­, ë¹ ë¥¸ ì‘ë‹µ ì†ë„
   - **ë‹¨ì **: ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒŒì•… í•œê³„

2. **Chroma (ì˜ë¯¸ë¡ ì  ê²€ìƒ‰)**:
   - Hit Rate: 0.286 â†’ 0.633 (BM25ë³´ë‹¤ ë‚®ìŒ)
   - MRR: 0.673 â†’ 0.756 (BM25ë³´ë‹¤ ë‚®ìŒ)
   - **ì¥ì **: ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒŒì•…, ì»¨í…ìŠ¤íŠ¸ ì´í•´
   - **ë‹¨ì **: ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ì—ì„œ ìƒëŒ€ì  ì•½ì„¸

3. **Ensemble (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)**:
   - Hit Rate: 0.286 â†’ 0.776 (BM25ì™€ ë™ë“±)
   - MRR: 0.735 â†’ 0.810 (ëª¨ë“  ë°©ì‹ ì¤‘ ìµœê³ )
   - **ì¥ì **: ë‘ ë°©ì‹ì˜ ì¥ì  ê²°í•©, ìµœê³  ì„±ëŠ¥
   - **íŠ¹íˆ MRRì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥** (ì²« ë²ˆì§¸ ì •ë‹µ ìœ„ì¹˜ ìµœì í™”)

### 2. ì„±ëŠ¥ ë¹„êµí‘œ

| íŠ¹ì„± | BM25 (í‚¤ì›Œë“œ) | Chroma (ì˜ë¯¸ë¡ ì ) | Ensemble (í•˜ì´ë¸Œë¦¬ë“œ) |
|------|-------------|-----------------|-------------------|
| **Hit Rate @3** | 0.653 ğŸŸ¢ | 0.551 ğŸ”¶ | 0.633 ğŸŸ¡ |
| **MRR @3** | 0.769 ğŸŸ¡ | 0.741 ğŸ”¶ | 0.782 ğŸŸ¢ |
| **Hit Rate @5** | 0.776 ğŸŸ¢ | 0.633 ğŸ”¶ | 0.776 ğŸŸ¢ |
| **MRR @5** | 0.788 ğŸŸ¡ | 0.756 ğŸ”¶ | 0.810 ğŸŸ¢ |
| **ì²˜ë¦¬ ì†ë„** | ğŸŸ¢ ë¹ ë¦„ | ğŸ”¶ ë³´í†µ | ğŸ”¶ ë³´í†µ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | ğŸŸ¢ ë‚®ìŒ | ğŸ”¶ ë†’ìŒ | ğŸ”¶ ë†’ìŒ |

### 3. ì‹¤ë¬´ ê¶Œì¥ì‚¬í•­

```python
# ìƒí™©ë³„ ìµœì  ê²€ìƒ‰ ì „ëµ
def recommend_search_strategy(query_type, performance_priority):
    """
    ìƒí™©ë³„ ê²€ìƒ‰ ì „ëµ ì¶”ì²œ
    """
    if performance_priority == 'speed':
        return 'BM25'  # ë¹ ë¥¸ ì‘ë‹µì´ ì¤‘ìš”í•œ ê²½ìš°
    elif performance_priority == 'accuracy':
        return 'Ensemble'  # ìµœê³  ì •í™•ë„ê°€ ì¤‘ìš”í•œ ê²½ìš°
    elif query_type == 'exact_match':
        return 'BM25'  # ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì¤‘ìš”í•œ ê²½ìš°
    elif query_type == 'semantic':
        return 'Chroma'  # ì˜ë¯¸ì  ìœ ì‚¬ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°
    else:
        return 'Ensemble'  # ì¼ë°˜ì ì¸ ê²½ìš°
```

## ğŸ› ï¸ ê³ ê¸‰ ê¸°ëŠ¥ êµ¬í˜„

### 1. ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì‹œìŠ¤í…œ

```python
class MultiStageRetriever:
    """ë‹¤ë‹¨ê³„ ê²€ìƒ‰ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ"""

    def __init__(self, retrievers, thresholds):
        self.retrievers = retrievers
        self.thresholds = thresholds

    def invoke(self, query, target_count=5):
        """ë‹¨ê³„ë³„ ê²€ìƒ‰ ìˆ˜í–‰"""
        all_results = []

        for i, (retriever, threshold) in enumerate(zip(self.retrievers, self.thresholds)):
            results = retriever.invoke(query)

            # ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§
            filtered_results = [
                doc for doc in results
                if doc.metadata.get('score', 0) > threshold
            ]

            all_results.extend(filtered_results)

            if len(all_results) >= target_count:
                break

        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        unique_results = self._remove_duplicates(all_results)
        return unique_results[:target_count]

    def _remove_duplicates(self, docs):
        """ë¬¸ì„œ ì¤‘ë³µ ì œê±°"""
        seen = set()
        unique_docs = []

        for doc in docs:
            content_hash = hash(doc.page_content)
            if content_hash not in seen:
                seen.add(content_hash)
                unique_docs.append(doc)

        return unique_docs
```

### 2. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

```python
class RetrievalMonitor:
    """ê²€ìƒ‰ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…"""

    def __init__(self):
        self.query_logs = []
        self.performance_history = []

    def log_query(self, query, retriever_type, results, metrics):
        """ì¿¼ë¦¬ ë° ê²°ê³¼ ë¡œê¹…"""
        log_entry = {
            'timestamp': datetime.now(),
            'query': query,
            'retriever_type': retriever_type,
            'result_count': len(results),
            'metrics': metrics
        }
        self.query_logs.append(log_entry)

    def analyze_performance_trends(self):
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
        if not self.query_logs:
            return None

        df = pd.DataFrame(self.query_logs)

        # ê²€ìƒ‰ê¸°ë³„ í‰ê·  ì„±ëŠ¥
        performance_by_type = df.groupby('retriever_type').agg({
            'metrics': lambda x: np.mean([m['f1_score'] for m in x])
        })

        return performance_by_type
```

## ğŸ”§ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### 1. í•œêµ­ì–´ ì²˜ë¦¬ ë¬¸ì œ

```python
# ì¸ì½”ë”© ì˜¤ë¥˜ í•´ê²°
def safe_load_korean_text(file_path):
    """ì•ˆì „í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¡œë“œ"""
    encodings = ['utf-8', 'cp949', 'euc-kr']

    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                return f.read()
        except UnicodeDecodeError:
            continue

    raise ValueError(f"Could not decode {file_path}")

# í˜•íƒœì†Œ ë¶„ì„ ì˜¤ë¥˜ ì²˜ë¦¬
def robust_tokenize(text, kiwi_model):
    """ê²¬ê³ í•œ í† í¬ë‚˜ì´ì§•"""
    try:
        return bm25_process_func(text, kiwi_model)
    except Exception as e:
        # ê¸°ë³¸ ê³µë°± ë¶„ë¦¬ë¡œ fallback
        return text.split()
```

### 2. ì„±ëŠ¥ ìµœì í™”

```python
# ìºì‹±ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_embedding(text):
    """ì„ë² ë”© ê²°ê³¼ ìºì‹±"""
    return embedding_model.embed_query(text)

# ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ íš¨ìœ¨ì„± í–¥ìƒ
def batch_search(queries, retriever, batch_size=10):
    """ë°°ì¹˜ ë‹¨ìœ„ ê²€ìƒ‰ ì²˜ë¦¬"""
    results = []

    for i in range(0, len(queries), batch_size):
        batch = queries[i:i+batch_size]
        batch_results = [retriever.invoke(q) for q in batch]
        results.extend(batch_results)

    return results
```

## ğŸ“š ì‹¤ë¬´ í™œìš© íŒ

### 1. ê²€ìƒ‰ ë°©ì‹ ì„ íƒ ê°€ì´ë“œ

```python
def recommend_search_strategy(query_characteristics):
    """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¥¸ ê²€ìƒ‰ ì „ëµ ì¶”ì²œ"""

    if query_characteristics['has_exact_terms']:
        return 'keyword_heavy'  # í‚¤ì›Œë“œ ê²€ìƒ‰ ë¹„ì¤‘ ë†’ì„
    elif query_characteristics['is_conceptual']:
        return 'semantic_heavy'  # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ë¹„ì¤‘ ë†’ì„
    elif query_characteristics['is_multilingual']:
        return 'semantic_only'  # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ë§Œ ì‚¬ìš©
    else:
        return 'balanced_hybrid'  # ê· í˜• ì¡íŒ í•˜ì´ë¸Œë¦¬ë“œ
```

### 2. A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬

```python
class RetrievalABTest:
    """ê²€ìƒ‰ ì‹œìŠ¤í…œ A/B í…ŒìŠ¤íŠ¸"""

    def __init__(self, strategy_a, strategy_b):
        self.strategy_a = strategy_a
        self.strategy_b = strategy_b
        self.results = {'a': [], 'b': []}

    def run_test(self, test_queries, metrics=['f1_score', 'mrr']):
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        for query in test_queries:
            # ì „ëµ A í…ŒìŠ¤íŠ¸
            result_a = self.strategy_a.invoke(query)
            self.results['a'].append(self._evaluate(query, result_a))

            # ì „ëµ B í…ŒìŠ¤íŠ¸
            result_b = self.strategy_b.invoke(query)
            self.results['b'].append(self._evaluate(query, result_b))

    def analyze_results(self):
        """ê²°ê³¼ ë¶„ì„ ë° í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
        from scipy.stats import ttest_rel

        scores_a = [r['f1_score'] for r in self.results['a']]
        scores_b = [r['f1_score'] for r in self.results['b']]

        t_stat, p_value = ttest_rel(scores_a, scores_b)

        return {
            'mean_a': np.mean(scores_a),
            'mean_b': np.mean(scores_b),
            'p_value': p_value,
            'significant': p_value < 0.05
        }
```

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ì´ ë…¸íŠ¸ë¶ì„ ì™„ë£Œí•œ í›„ ë‹¤ìŒ ê³¼ì •ë“¤ì„ ì§„í–‰í•˜ì„¸ìš”:

1. **PRJ02_W1_004**: ì¿¼ë¦¬ í™•ì¥(Query Expansion) ê¸°ë²• í•™ìŠµ
2. **ê³ ê¸‰ í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ**: ë” ë³µì¡í•œ ì•™ìƒë¸” ë°©ë²•ë¡ 
3. **ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ ì¶”ì 

---

**ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸**: ë‹¨ì¼ ê²€ìƒ‰ ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì´ í•„ìˆ˜ì ì´ë©°, íŠ¹íˆ í•œêµ­ì–´ì™€ ê°™ì€ êµì°©ì–´ëŠ” í˜•íƒœì†Œ ë¶„ì„ì´ ê²€ìƒ‰ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.