# W2_001_Tokenizing_Embedding.md - í† í°í™”ì™€ ì„ë² ë”© ë§ˆìŠ¤í„°í•˜ê¸°

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- í† í°í™”(Tokenization)ì˜ ê°œë…ê³¼ ë‹¤ì–‘í•œ ê¸°ë²• ì´í•´
- ì„ë² ë”©(Embedding)ì˜ ì›ë¦¬ì™€ í™œìš©ë²• í•™ìŠµ
- í•œêµ­ì–´ NLPë¥¼ ìœ„í•œ ì‹¤ìš©ì  ë„êµ¬ ìŠµë“
- í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°ê³¼ ë¹„êµ ë¶„ì„ ëŠ¥ë ¥ ê°œë°œ

## ğŸ“š í•µì‹¬ ê°œë…

### í† í°í™”(Tokenization)
- **ê°œë…**: í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ê°€ëŠ¥í•œ ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •
- **ëª©ì **: ì»´í“¨í„°ê°€ ìì—°ì–´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ ì²« ë²ˆì§¸ ë‹¨ê³„
- **ì˜ë¯¸ ë‹¨ìœ„**: ì˜ë¯¸ ìˆëŠ” ìµœì†Œ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 

### ì„ë² ë”©(Embedding)
- **ê°œë…**: í…ìŠ¤íŠ¸ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜ ë²¡í„°ë¡œ ë³€í™˜
- **ëª©ì **: ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ê±°ë¦¬ì— ë°°ì¹˜
- **ì‘ìš©**: ë¬¸ì„œ ê²€ìƒ‰, í…ìŠ¤íŠ¸ ë¶„ë¥˜, ê°ì • ë¶„ì„, ê¸°ê³„ ë²ˆì—­

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# ê¸°ë³¸ NLP ë„êµ¬
pip install kiwipiepy transformers sentence-transformers

# ê³¼í•™ ê³„ì‚° ë° ì‹œê°í™”
pip install numpy pandas matplotlib seaborn scikit-learn

# ì „í†µì  ì„ë² ë”© ëª¨ë¸
pip install gensim

# UV íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì‚¬ìš© ì‹œ
uv add kiwipiepy transformers sentence-transformers numpy pandas matplotlib seaborn scikit-learn gensim
```

### ê¸°ë³¸ ì„í¬íŠ¸ ë° ì„¤ì •
```python
import time
from functools import wraps
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)
import matplotlib.font_manager as fm
font_path = "c:/Windows/Fonts/malgun.ttf"  # Windows ê²½ë¡œ
matplotlib.rc('font', family=fm.FontProperties(fname=font_path).get_name())

# í•œê¸€ í°íŠ¸ ì„¤ì • (Mac)
matplotlib.rc('font', family='Gulim')

# ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ì²˜ë¦¬
matplotlib.rc("axes", unicode_minus=False)

def measure_time(func):
    """ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        print(f"{func.__name__} ì‹¤í–‰ ì‹œê°„: {end-start:.4f}ì´ˆ")
        return result
    return wrapper
```

## ğŸ’» í† í°í™”(Tokenization)

### 1. ë‹¨ì–´ ë‹¨ìœ„ í† í°í™” (Word Tokenization)

#### ê°œë…ê³¼ íŠ¹ì§•
- **í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜**: ì˜ë¯¸ ìˆëŠ” ìµœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
- **ë¬¸ë²• ìš”ì†Œ ë¶„ë¦¬**: ì¡°ì‚¬, ì–´ë¯¸ ë“± ê°œë³„ í† í°ìœ¼ë¡œ ì²˜ë¦¬
- **í•œêµ­ì–´ íŠ¹í™”**: êµì°©ì–´ íŠ¹ì„±ì„ ì˜ ë°˜ì˜
- **í™œìš© ë¶„ì•¼**: ë¬¸ì¥ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, í…ìŠ¤íŠ¸ ë¶„ë¥˜

#### Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš©
```python
from kiwipiepy import Kiwi

# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
kiwi = Kiwi()

@measure_time
def tokenize_with_kiwi(text):
    """í† í°í™” í•¨ìˆ˜"""
    try:
        return kiwi.tokenize(text)
    except Exception as e:
        print(f"í† í°í™” ì˜¤ë¥˜: {e}")
        return []

# í† í°í™” ì‹¤í–‰
text = "ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•˜ëŠ” ê²ƒì€ ì •ë§ í¥ë¯¸ë¡­ê³  ìœ ìš©í•©ë‹ˆë‹¤!"
tokens = tokenize_with_kiwi(text)

print("í† í°í™” ê²°ê³¼:")
for i, token in enumerate(tokens, 1):
    print(f"{i:>3} {token.form:^8} {token.tag:^8} ({token.start}-{token.start + token.len})")
```

#### í’ˆì‚¬ íƒœê·¸ ë§¤í•‘
```python
# í’ˆì‚¬ íƒœê·¸ í•œêµ­ì–´ ë§¤í•‘
pos_dict = {
    # ì²´ì–¸
    'NNG': 'ì¼ë°˜ëª…ì‚¬', 'NNP': 'ê³ ìœ ëª…ì‚¬', 'NNB': 'ì˜ì¡´ëª…ì‚¬',
    'NP': 'ëŒ€ëª…ì‚¬', 'NR': 'ìˆ˜ì‚¬',
    # ìš©ì–¸
    'VV': 'ë™ì‚¬', 'VA': 'í˜•ìš©ì‚¬', 'VX': 'ë³´ì¡°ìš©ì–¸',
    'VCP': 'ê¸ì •ì§€ì •ì‚¬', 'VCN': 'ë¶€ì •ì§€ì •ì‚¬',
    # ê´€í˜•ì‚¬/ë¶€ì‚¬
    'MM': 'ê´€í˜•ì‚¬', 'MAG': 'ì¼ë°˜ë¶€ì‚¬', 'MAJ': 'ì ‘ì†ë¶€ì‚¬',
    # ì¡°ì‚¬
    'JKS': 'ì£¼ê²©ì¡°ì‚¬', 'JKO': 'ëª©ì ê²©ì¡°ì‚¬', 'JKG': 'ê´€í˜•ê²©ì¡°ì‚¬',
    'JKB': 'ë¶€ì‚¬ê²©ì¡°ì‚¬', 'JX': 'ë³´ì¡°ì‚¬', 'JC': 'ì ‘ì†ì¡°ì‚¬',
    # ì–´ë¯¸
    'EF': 'ì¢…ê²°ì–´ë¯¸', 'EC': 'ì—°ê²°ì–´ë¯¸', 'ETM': 'ê´€í˜•ì‚¬í˜•ì „ì„±ì–´ë¯¸',
    # ì ‘ì‚¬
    'XSV': 'ë™ì‚¬íŒŒìƒì ‘ë¯¸ì‚¬', 'XSA': 'í˜•ìš©ì‚¬íŒŒìƒì ‘ë¯¸ì‚¬', 'XR': 'ì–´ê·¼',
    # ê¸°í˜¸
    'SF': 'ë§ˆì¹¨í‘œë¥˜', 'SP': 'ì‰¼í‘œë¥˜', 'SN': 'ìˆ«ì'
}

# ê²°ê³¼ ì¶œë ¥
print(f"{'ë²ˆí˜¸':>3} {'ë‹¨ì–´':^8} {'í’ˆì‚¬íƒœê·¸':^8} {'í•œêµ­ì–´í’ˆì‚¬':^12} {'ìœ„ì¹˜':^6}")
print("-" * 45)
for i, token in enumerate(tokens, 1):
    korean_pos = pos_dict.get(token.tag, token.tag)
    pos_range = f"{token.start}-{token.start + token.len}"
    print(f"{i:>3} {token.form:^8} {token.tag:^8} {korean_pos:^12} {pos_range:^6}")
```

### 2. ì„œë¸Œì›Œë“œ í† í°í™” (Subword Tokenization)

#### ê°œë…ê³¼ íŠ¹ì§•
- **ì„œë¸Œì›Œë“œ ë‹¨ìœ„**: í˜•íƒœì†Œë³´ë‹¤ ì‘ì€ ì˜ë¯¸ ë‹¨ìœ„ ì‚¬ìš©
- **íŒ¨í„´ ê¸°ë°˜**: ìì£¼ ë“±ì¥í•˜ëŠ” ë¬¸ìì—´ íŒ¨í„´ì„ í† í°ìœ¼ë¡œ í™œìš©
- **OOV í•´ê²°**: Out-of-Vocabulary ë¬¸ì œ íš¨ê³¼ì  í•´ê²°
- **ë‹¤êµ­ì–´ ì§€ì›**: ì–¸ì–´ì— ê´€ê³„ì—†ì´ ì¼ê´€ëœ ì²˜ë¦¬

#### ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

| ì•Œê³ ë¦¬ì¦˜ | íŠ¹ì§• | ëŒ€í‘œ ëª¨ë¸ | ì¥ì  |
|---------|------|----------|------|
| BPE (Byte Pair Encoding) | ë¹ˆë„ ê¸°ë°˜ ë³‘í•© | GPT ì‹œë¦¬ì¦ˆ | ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì  |
| WordPiece | í™•ë¥  ê¸°ë°˜ ë¶„í•  | BERT, ELECTRA | ì–¸ì–´í•™ì  ì˜ë¯¸ ë³´ì¡´ |
| SentencePiece | ì–¸ì–´ ë…ë¦½ì  | T5, mT5, XLM-R | ë‹¤êµ­ì–´ ì§€ì› ìš°ìˆ˜ |

#### BERT í† í¬ë‚˜ì´ì € (WordPiece) í™œìš©
```python
from transformers import AutoTokenizer

# KcBERT í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')

# í† í°í™” ìˆ˜í–‰
text = "ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤"
tokens = tokenizer.tokenize(text)

print(f"ì›ë¬¸: {text}")
print(f"í† í°: {tokens}")
# ì¶œë ¥: ['ìì—°', '##ì–´', '##ì²˜ë¦¬', '##ë¥¼', 'ê³µë¶€', '##í•©ë‹ˆë‹¤']

# í† í°í™” ë¶„ì„ í•¨ìˆ˜
def analyze_tokenization(texts, tokenizer):
    """í† í°í™” ë¶„ì„ í•¨ìˆ˜"""
    results = []

    for text in texts:
        tokens = tokenizer.tokenize(text)
        subword_tokens = [token for token in tokens if token.startswith('##')]

        results.append({
            'text': text,
            'token_count': len(tokens),
            'subword_count': len(subword_tokens),
            'tokens': tokens
        })

    return results

# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤
test_texts = [
    "ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤",
    "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ ë¯¸ë˜ê¸°ìˆ ì´ë‹¤",
    "COVID-19ë¡œ ì¸í•œ ë¹„ëŒ€ë©´ ìˆ˜ì—…ì´ ì¦ê°€í–ˆë‹¤",
    "ìŠ¤ë§ˆíŠ¸í°ì˜ ìŒì„±ì¸ì‹ ê¸°ëŠ¥ì´ ë°œì „í–ˆë‹¤"
]

# ë¶„ì„ ê²°ê³¼ ì¶œë ¥
results = analyze_tokenization(test_texts, tokenizer)
for i, result in enumerate(results, 1):
    print(f"\n{i}. ì›ë¬¸: {result['text']}")
    print(f"   í† í° ìˆ˜: {result['token_count']}ê°œ")
    print(f"   ì„œë¸Œì›Œë“œ: {result['subword_count']}ê°œ")
    print(f"   í† í°ë“¤: {result['tokens']}")
```

#### SentencePiece í† í¬ë‚˜ì´ì € í™œìš©
```python
from transformers import AutoTokenizer

# BGE-M3 í† í¬ë‚˜ì´ì € (SentencePiece ê¸°ë°˜)
tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-m3")

text = "ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤"
tokens = tokenizer.tokenize(text)
print(f"SentencePiece í† í°: {tokens}")
# ì¶œë ¥: ['â–ìì—°', 'ì–´', 'ì²˜ë¦¬', 'ë¥¼', 'â–ê³µë¶€', 'í•©ë‹ˆë‹¤']
# â– ëŠ” ë‹¨ì–´ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜ ê¸°í˜¸
```

## ğŸ¯ ì„ë² ë”©(Embedding)

### 1. ë‹¨ì–´ ì„ë² ë”© (Word Embedding)

#### 1.1 Bag of Words (BoW)

**ê°œë…ê³¼ íŠ¹ì§•**
- **ë¹ˆë„ ê¸°ë°˜**: ë‹¨ì–´ì˜ ì¶œí˜„ ë¹ˆë„ë¥¼ ë²¡í„°ë¡œ í‘œí˜„
- **ìˆœì„œ ë¬´ì‹œ**: ë‹¨ì–´ ìˆœì„œ ì •ë³´ëŠ” ê³ ë ¤í•˜ì§€ ì•ŠìŒ
- **í¬ì†Œ ë²¡í„°**: ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0ì¸ sparse ë²¡í„° ìƒì„±

```python
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

def create_bow_vectors(texts):
    # í† í°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    tokenized_texts = []
    for text in texts:
        tokens = tokenize_with_kiwi(text)
        token_words = [token.form for token in tokens if token.tag not in ['SF', 'SP']]
        tokenized_texts.append(' '.join(token_words))

    # BoW ë²¡í„°í™”
    vectorizer = CountVectorizer()
    bow_matrix = vectorizer.fit_transform(tokenized_texts)

    return bow_matrix, vectorizer

# ì˜ˆì œ í…ìŠ¤íŠ¸
texts = [
    "ìì—°ì–´ ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤",
    "ìì—°ì–´ëŠ” ì»´í“¨í„° ì–¸ì–´ê°€ ì•„ë‹ˆë¼ ì¸ê°„ì˜ ì–¸ì–´ì…ë‹ˆë‹¤",
    "ìì—°ì–´ ìˆ˜ì—… ì‹œê°„ì— ìì—°ì–´ ì²˜ë¦¬ ë°©ë²•ì„ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤"
]

bow_matrix, vectorizer = create_bow_vectors(texts)

# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ í‘œì‹œ
feature_names = vectorizer.get_feature_names_out()
bow_df = pd.DataFrame(
    bow_matrix.toarray(),
    columns=feature_names,
    index=[f'ë¬¸ì„œ{i+1}' for i in range(len(texts))]
)

print("BoW ë§¤íŠ¸ë¦­ìŠ¤:")
print(bow_df)
```

#### 1.2 TF-IDF (Term Frequency-Inverse Document Frequency)

**ê°œë…ê³¼ íŠ¹ì§•**
- **ê°€ì¤‘ì¹˜ ë¶€ì—¬**: ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜ ì ìš©
- **í¬ê·€ì„± ê³ ë ¤**: í¬ê·€í•œ ë‹¨ì–´ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
- **ìƒëŒ€ì  ì¤‘ìš”ì„±**: ë¬¸ì„œ ì§‘í•© ë‚´ì—ì„œì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ ì¸¡ì •

```python
from sklearn.feature_extraction.text import TfidfVectorizer

def create_tfidf_vectors(texts):
    # í† í°í™”
    tokenized_texts = []
    for text in texts:
        tokens = tokenize_with_kiwi(text)
        token_words = [token.form for token in tokens if token.tag not in ['SF', 'SP']]
        tokenized_texts.append(' '.join(token_words))

    # TF-IDF ë²¡í„°í™”
    tfidf = TfidfVectorizer()
    tfidf_matrix = tfidf.fit_transform(tokenized_texts)

    return tfidf_matrix, tfidf

tfidf_matrix, tfidf = create_tfidf_vectors(texts)

# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ í‘œì‹œ
feature_names = tfidf.get_feature_names_out()
tfidf_df = pd.DataFrame(
    tfidf_matrix.toarray(),
    columns=feature_names,
    index=[f'ë¬¸ì„œ{i+1}' for i in range(len(texts))]
)

print("TF-IDF ë§¤íŠ¸ë¦­ìŠ¤:")
print(tfidf_df.round(3))
```

#### 1.3 Word2Vec

**ê°œë…ê³¼ íŠ¹ì§•**
- **ì˜ë¯¸ì  ê´€ê³„**: ë‹¨ì–´ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë²¡í„° ê³µê°„ì— í‘œí˜„
- **ë¬¸ë§¥ ê³ ë ¤**: ì£¼ë³€ ë‹¨ì–´ì˜ ë¬¸ë§¥ì„ ê³ ë ¤í•œ ë‹¨ì–´ í‘œí˜„
- **ë‘ ê°€ì§€ ëª¨ë¸**: CBOW(Continuous Bag of Words)ì™€ Skip-gram
- **ë²¡í„° ì—°ì‚°**: ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ í‘œí˜„

```python
from gensim.models import Word2Vec

def train_word2vec_model(texts, vector_size=100, window=3, min_count=1):
    """Word2Vec ëª¨ë¸ í›ˆë ¨"""
    # í† í°í™”
    tokenized_corpus = []
    for text in texts:
        tokens = tokenize_with_kiwi(text)
        token_words = [token.form for token in tokens if len(token.form) > 1]
        tokenized_corpus.append(token_words)

    # Skip-gram ëª¨ë¸ í›ˆë ¨
    model = Word2Vec(
        sentences=tokenized_corpus,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        workers=4,
        sg=1,  # 1: Skip-gram, 0: CBOW
        epochs=100
    )

    return model, tokenized_corpus

# ë” í° ì½”í¼ìŠ¤ë¡œ ëª¨ë¸ í›ˆë ¨
word2vec_texts = [
    "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤",
    "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì´ ìì—°ì–´ ì²˜ë¦¬ì— í™œìš©ë©ë‹ˆë‹¤",
    "ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤",
    "í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤",
    "ë‹¨ì–´ì˜ ì˜ë¯¸ì™€ ë¬¸ë§¥ì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤"
]

word2vec_model, tokenized_corpus = train_word2vec_model(word2vec_texts)

print(f"ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(word2vec_model.wv.key_to_index)}ê°œ")
print(f"ë²¡í„° ì°¨ì›: {word2vec_model.vector_size}ì°¨ì›")

# ë‹¨ì–´ ë²¡í„° í™•ì¸
test_words = ['ìì—°ì–´ ì²˜ë¦¬', 'ì–¸ì–´', 'ë°ì´í„°', 'ì»´í“¨í„°']
available_words = [word for word in test_words if word in word2vec_model.wv]

for word in available_words:
    vector = word2vec_model.wv[word]
    print(f"'{word}' ë²¡í„° (ì²˜ìŒ 10ì°¨ì›): {vector[:10].round(3)}")

# ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°
def find_similar_words(word, model, topn=3):
    try:
        similar_words = model.wv.most_similar(word, topn=topn)
        print(f"\n'{word}'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:")
        for similar_word, similarity in similar_words:
            print(f"  {similar_word}: {similarity:.3f}")
    except:
        print(f"'{word}'ì˜ ìœ ì‚¬ ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

for word in available_words:
    find_similar_words(word, word2vec_model)
```

### 2. ë¬¸ì¥ ì„ë² ë”© (Sentence Embedding)

#### ê°œë…ê³¼ íŠ¹ì§•
- **ì „ì²´ ë¬¸ì¥ í‘œí˜„**: ë¬¸ì¥ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í‘œí˜„
- **ì˜ë¯¸ ë³´ì¡´**: ë¬¸ì¥ì˜ ì˜ë¯¸ì  íŠ¹ì„±ì„ ë²¡í„°ì— ë³´ì¡´
- **ê³ ì • í¬ê¸°**: ë¬¸ì¥ ê¸¸ì´ì™€ ê´€ê³„ì—†ì´ ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°
- **ìœ ì‚¬ë„ ê³„ì‚°**: ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥

#### 2.1 í‰ê·  ê¸°ë°˜ ë¬¸ì¥ ì„ë² ë”©

```python
def create_sentence_embedding_avg(sentence, word2vec_model):
    """Word2Vec í‰ê· ì„ ì´ìš©í•œ ë¬¸ì¥ ì„ë² ë”©"""
    tokens = tokenize_with_kiwi(sentence)
    token_words = [token.form for token in tokens if token.form in word2vec_model.wv]

    if not token_words:
        return np.zeros(word2vec_model.vector_size)

    # ë‹¨ì–´ ë²¡í„°ë“¤ì˜ í‰ê·  ê³„ì‚°
    sentence_vector = np.zeros(word2vec_model.vector_size)
    for word in token_words:
        sentence_vector += word2vec_model.wv[word]

    return sentence_vector / len(token_words)

# ë¬¸ì¥ ì„ë² ë”© ìƒì„±
test_sentence = "ìì—°ì–´ ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤"
sentence_embed = create_sentence_embedding_avg(test_sentence, word2vec_model)

print(f"ë¬¸ì¥: '{test_sentence}'")
print(f"ì„ë² ë”© ì°¨ì›: {len(sentence_embed)}")
print(f"ì„ë² ë”© ë²¡í„° (ì²˜ìŒ 10ì°¨ì›): {sentence_embed[:10].round(3)}")
```

#### 2.2 SBERT (Sentence-BERT)

```python
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

# í•œêµ­ì–´ ìµœì í™” ëª¨ë¸ë“¤
korean_models = {
    'ko-sroberta': 'jhgan/ko-sroberta-multitask',
    'bge-m3': 'BAAI/bge-m3'
}

def load_sentence_transformer(model_key='ko-sroberta'):
    """SBERT ëª¨ë¸ ë¡œë”©"""
    try:
        model_name = korean_models[model_key]
        model = SentenceTransformer(model_name)
        print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
        return model
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {e}")
        return None

@measure_time
def create_sentence_embeddings(sentences, model):
    """ë°°ì¹˜ ë¬¸ì¥ ì„ë² ë”© ìƒì„±"""
    try:
        embeddings = model.encode(sentences, convert_to_tensor=False)
        return embeddings
    except Exception as e:
        print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
        return None

# ko-sroberta ëª¨ë¸ ë¡œë“œ
sbert_model = load_sentence_transformer('ko-sroberta')

# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤
test_sentences = [
    "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤",
    "ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•´ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”",
    "ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤"
]

# ì„ë² ë”© ìƒì„±
sbert_embeddings = create_sentence_embeddings(test_sentences, sbert_model)

print(f"ì„ë² ë”© í˜•íƒœ: {sbert_embeddings.shape}")
print(f"ì„ë² ë”© ì°¨ì›: {sbert_embeddings.shape[1]}")

# ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ ì„ë² ë”© ì¼ë¶€ ì¶œë ¥
print(f"\nì²« ë²ˆì§¸ ë¬¸ì¥ ì„ë² ë”© (ì²˜ìŒ 10ì°¨ì›):")
print(f"'{test_sentences[0]}'")
print(f"{sbert_embeddings[0][:10].round(3)}")

# BGE-M3 ëª¨ë¸ë„ ë™ì¼í•˜ê²Œ í™œìš© ê°€ëŠ¥
bge_model = load_sentence_transformer('bge-m3')
bge_embeddings = create_sentence_embeddings(test_sentences, bge_model)
print(f"\nBGE-M3 ì„ë² ë”© ì°¨ì›: {bge_embeddings.shape[1]}")
```

## ğŸ“Š í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¹„êµ

### ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ ë¹„êµ

| ë©”íŠ¸ë¦­ | ìˆ˜ì‹ | ë²”ìœ„ | í•´ì„ | íŠ¹ì§• | ì£¼ìš” ìš©ë„ |
|--------|------|------|------|------|----------|
| ìœ í´ë¦¬ë“œ ê±°ë¦¬ | $\sqrt{\sum(a_i-b_i)^2}$ | [0, âˆ) | ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬ | ì ˆëŒ€ ê±°ë¦¬ | í´ëŸ¬ìŠ¤í„°ë§ |
| ì½”ì‚¬ì¸ ìœ ì‚¬ë„ | $\frac{a \cdot b}{\\|a\\|\\|b\\|}$ | [-1, 1] | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬ | ë°©í–¥ì„± ì¤‘ì‹œ | ë¬¸ì„œ ê²€ìƒ‰ |
| ë‚´ì  | $\sum a_i \cdot b_i$ | (-âˆ, âˆ) | í´ìˆ˜ë¡ ìœ ì‚¬ | í¬ê¸°+ë°©í–¥ | ì¶”ì²œ ì‹œìŠ¤í…œ |

### ìœ ì‚¬ë„ ê³„ì‚° ì‹œìŠ¤í…œ

```python
from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity

class SimilarityCalculator:
    """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° í´ë˜ìŠ¤"""

    def __init__(self, sbert_model):
        self.model = sbert_model

    def calculate_all_similarities(self, sentence1, sentence2):
        """ëª¨ë“  ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        # ì„ë² ë”© ìƒì„±
        embeddings = self.model.encode([sentence1, sentence2])
        emb1, emb2 = embeddings[0], embeddings[1]

        # ê°ì¢… ìœ ì‚¬ë„ ê³„ì‚°
        euclidean_dist = euclidean_distances([emb1], [emb2])[0][0]
        cosine_sim = cosine_similarity([emb1], [emb2])[0][0]
        dot_product = np.dot(emb1, emb2)

        return {
            'euclidean_distance': euclidean_dist,
            'cosine_similarity': cosine_sim,
            'dot_product': dot_product
        }

    def compare_sentence_pairs(self, sentence_pairs):
        """ì—¬ëŸ¬ ë¬¸ì¥ ìŒ ë¹„êµ"""
        results = []

        for pair in sentence_pairs:
            sent1, sent2 = pair
            similarities = self.calculate_all_similarities(sent1, sent2)

            result = {
                'sentence1': sent1,
                'sentence2': sent2,
                'euclidean': similarities['euclidean_distance'],
                'cosine': similarities['cosine_similarity'],
                'dot_product': similarities['dot_product']
            }
            results.append(result)

        return results

# ìœ ì‚¬ë„ ê³„ì‚°ê¸° ìƒì„±
similarity_calc = SimilarityCalculator(sbert_model)

# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ìŒë“¤
sentence_pairs = [
    ("í•™ìƒì´ í•™êµì—ì„œ ê³µë¶€í•œë‹¤", "í•™ìƒì´ ë„ì„œê´€ì—ì„œ ê³µë¶€í•œë‹¤"),    # ìœ ì‚¬
    ("ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë°°ìš´ë‹¤", "ì»´í“¨í„°ë¡œ ì–¸ì–´ë¥¼ ë¶„ì„í•œë‹¤"),        # ìœ ì‚¬
    ("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë‹¤", "ë‚´ì¼ ë¹„ê°€ ì˜¬ ì˜ˆì •ì´ë‹¤"),             # ê´€ë ¨
    ("ì˜í™”ê°€ ì¬ë¯¸ìˆë‹¤", "ìˆ˜í•™ ë¬¸ì œë¥¼ í‘¼ë‹¤")                   # ë¬´ê´€
]

# ìœ ì‚¬ë„ ë¹„êµ ì‹¤í–‰
comparison_results = similarity_calc.compare_sentence_pairs(sentence_pairs)

# ê²°ê³¼ ì¶œë ¥
for i, result in enumerate(comparison_results, 1):
    print(f"\n{i}. ë¬¸ì¥ ë¹„êµ:")
    print(f"   A: {result['sentence1']}")
    print(f"   B: {result['sentence2']}")
    print(f"   ìœ í´ë¦¬ë“œ ê±°ë¦¬: {result['euclidean']:.4f}")
    print(f"   ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {result['cosine']:.4f}")
    print(f"   ë‚´ì : {result['dot_product']:.4f}")
```

### ìœ ì‚¬ë„ ì‹œê°í™”

```python
def plot_similarity_comparison(results, title="ìœ ì‚¬ë„ ë¹„êµ"):
    """ìœ ì‚¬ë„ ë¹„êµ ì‹œê°í™”"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # ë°ì´í„° ì¤€ë¹„
    labels = [f"ìŒ{i+1}" for i in range(len(results))]
    euclidean_scores = [r['euclidean'] for r in results]
    cosine_scores = [r['cosine'] for r in results]
    dot_scores = [r['dot_product'] for r in results]

    # ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
    axes[0].bar(labels, euclidean_scores, color='skyblue', alpha=0.7)
    axes[0].set_title('ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)')
    axes[0].set_ylabel('ê±°ë¦¬')
    axes[0].grid(True, alpha=0.3)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
    axes[1].bar(labels, cosine_scores, color='lightgreen', alpha=0.7)
    axes[1].set_title('ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)')
    axes[1].set_ylabel('ìœ ì‚¬ë„')
    axes[1].set_ylim(0, 1)
    axes[1].grid(True, alpha=0.3)

    # ë‚´ì  (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
    axes[2].bar(labels, dot_scores, color='salmon', alpha=0.7)
    axes[2].set_title('ë‚´ì  (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)')
    axes[2].set_ylabel('ë‚´ì ê°’')
    axes[2].grid(True, alpha=0.3)

    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

# ì‹œê°í™” ì‹¤í–‰
plot_similarity_comparison(comparison_results, "SBERT ëª¨ë¸ ìœ ì‚¬ë„ ë¹„êµ")
```

## ğŸš€ ì‹¤ìŠµí•´ë³´ê¸°

### ì‹¤ìŠµ: ë¬¸ì„œ ìœ ì‚¬ë„ ë¹„êµ ì‹œìŠ¤í…œ êµ¬í˜„

**ëª©í‘œ**: ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì„ í† í°í™”í•˜ê³  ì„ë² ë”©í•œ í›„ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ëŠ” ì‹œìŠ¤í…œ êµ¬í˜„

#### êµ¬í˜„í•´ì•¼ í•  ê¸°ëŠ¥
1. ë¬¸ì„œë¥¼ í† í°í™”í•˜ê³  BoW ë²¡í„°ë¡œ ë³€í™˜
2. ë¬¸ì„œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
3. ë¬¸ì„œë“¤ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
4. ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ì°¾ê¸°

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from sentence_transformers import SentenceTransformer

# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë¬¸ì„œ
documents = [
    "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ê³¼í•™ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ì¢…ë¥˜ì…ë‹ˆë‹¤.",
    "ìì—°ì–´ ì²˜ë¦¬ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."
]

# ë¬¸ì œ 1: ë¬¸ì„œë¥¼ í† í°í™”í•˜ê³  BoW ë²¡í„°ë¡œ ë³€í™˜í•˜ì‹œì˜¤
def tokenize_documents(docs):
    """ë¬¸ì„œë“¤ì„ í† í°í™”í•˜ê³  BoW ë²¡í„°ë¡œ ë³€í™˜"""
    # í•œêµ­ì–´ í† í°í™”
    tokenized_docs = []
    for doc in docs:
        tokens = tokenize_with_kiwi(doc)
        token_words = [token.form for token in tokens
                      if token.tag not in ['SF', 'SP', 'SS'] and len(token.form) > 1]
        tokenized_docs.append(' '.join(token_words))

    # BoW ë²¡í„°í™”
    vectorizer = CountVectorizer()
    bow_vectors = vectorizer.fit_transform(tokenized_docs)

    return bow_vectors, vectorizer

# ë¬¸ì œ 2: ë¬¸ì„œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì‹œì˜¤
def create_embeddings(docs, model):
    """SBERTë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
    embeddings = model.encode(docs)
    return embeddings

# ë¬¸ì œ 3: ë¬¸ì„œë“¤ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì‹œì˜¤
def calculate_similarity_matrix(vectors):
    """ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
    similarity_matrix = cosine_similarity(vectors)
    return similarity_matrix

# ë¬¸ì œ 4: ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒì„ ì°¾ìœ¼ì‹œì˜¤
def find_most_similar_pair(similarity_matrix, docs):
    """ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ì°¾ê¸°"""
    n = len(docs)
    max_similarity = 0
    most_similar_pair = (0, 1)

    # ëŒ€ê°ì„  ì œì™¸í•˜ê³  ìµœëŒ€ ìœ ì‚¬ë„ ì°¾ê¸°
    for i in range(n):
        for j in range(i+1, n):
            if similarity_matrix[i][j] > max_similarity:
                max_similarity = similarity_matrix[i][j]
                most_similar_pair = (i, j)

    return {
        'most_similar_pair': (docs[most_similar_pair[0]], docs[most_similar_pair[1]]),
        'similarity_score': max_similarity,
        'similarity_matrix': similarity_matrix.tolist()
    }

# ì‹¤í–‰ ì˜ˆì œ
def run_document_similarity_analysis():
    """ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ ì‹¤í–‰"""
    # 1. BoW ê¸°ë°˜ ë¶„ì„
    bow_vectors, vectorizer = tokenize_documents(documents)
    bow_similarity = calculate_similarity_matrix(bow_vectors)
    bow_result = find_most_similar_pair(bow_similarity, documents)

    print("=== BoW ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ ===")
    print(f"ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ:")
    print(f"  ë¬¸ì„œ 1: {bow_result['most_similar_pair'][0]}")
    print(f"  ë¬¸ì„œ 2: {bow_result['most_similar_pair'][1]}")
    print(f"  ìœ ì‚¬ë„: {bow_result['similarity_score']:.4f}")

    # 2. SBERT ê¸°ë°˜ ë¶„ì„
    sbert_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
    sbert_embeddings = create_embeddings(documents, sbert_model)
    sbert_similarity = calculate_similarity_matrix(sbert_embeddings)
    sbert_result = find_most_similar_pair(sbert_similarity, documents)

    print("\n=== SBERT ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ ===")
    print(f"ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ:")
    print(f"  ë¬¸ì„œ 1: {sbert_result['most_similar_pair'][0]}")
    print(f"  ë¬¸ì„œ 2: {sbert_result['most_similar_pair'][1]}")
    print(f"  ìœ ì‚¬ë„: {sbert_result['similarity_score']:.4f}")

    return bow_result, sbert_result

# ë¶„ì„ ì‹¤í–‰
bow_result, sbert_result = run_document_similarity_analysis()
```

## ğŸ“‹ í•´ë‹µ

### ì™„ì „í•œ ë¬¸ì„œ ìœ ì‚¬ë„ ë¹„êµ ì‹œìŠ¤í…œ

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer

class DocumentSimilarityAnalyzer:
    """ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ í´ë˜ìŠ¤"""

    def __init__(self):
        self.kiwi = Kiwi()
        self.sbert_model = None

    def load_sbert_model(self, model_name='jhgan/ko-sroberta-multitask'):
        """SBERT ëª¨ë¸ ë¡œë“œ"""
        try:
            self.sbert_model = SentenceTransformer(model_name)
            print(f"SBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
        except Exception as e:
            print(f"SBERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def preprocess_documents(self, docs):
        """ë¬¸ì„œ ì „ì²˜ë¦¬"""
        processed_docs = []
        for doc in docs:
            tokens = self.kiwi.tokenize(doc)
            # ì˜ë¯¸ìˆëŠ” í† í°ë§Œ ì¶”ì¶œ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬)
            meaningful_tokens = [
                token.form for token in tokens
                if token.tag.startswith(('NN', 'VV', 'VA', 'XR')) and len(token.form) > 1
            ]
            processed_docs.append(' '.join(meaningful_tokens))
        return processed_docs

    def create_bow_vectors(self, docs):
        """BoW ë²¡í„° ìƒì„±"""
        processed_docs = self.preprocess_documents(docs)
        vectorizer = CountVectorizer()
        vectors = vectorizer.fit_transform(processed_docs)
        return vectors.toarray(), vectorizer

    def create_tfidf_vectors(self, docs):
        """TF-IDF ë²¡í„° ìƒì„±"""
        processed_docs = self.preprocess_documents(docs)
        vectorizer = TfidfVectorizer()
        vectors = vectorizer.fit_transform(processed_docs)
        return vectors.toarray(), vectorizer

    def create_sbert_embeddings(self, docs):
        """SBERT ì„ë² ë”© ìƒì„±"""
        if self.sbert_model is None:
            raise ValueError("SBERT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return self.sbert_model.encode(docs)

    def calculate_similarity_matrix(self, vectors):
        """ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        return cosine_similarity(vectors)

    def find_most_similar_pair(self, similarity_matrix, docs):
        """ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ì°¾ê¸°"""
        n = len(docs)
        max_similarity = 0
        most_similar_indices = (0, 1)

        for i in range(n):
            for j in range(i+1, n):
                if similarity_matrix[i][j] > max_similarity:
                    max_similarity = similarity_matrix[i][j]
                    most_similar_indices = (i, j)

        return {
            'most_similar_pair': (docs[most_similar_indices[0]], docs[most_similar_indices[1]]),
            'indices': most_similar_indices,
            'similarity_score': max_similarity,
            'similarity_matrix': similarity_matrix
        }

    def comprehensive_analysis(self, docs):
        """ì¢…í•©ì ì¸ ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„"""
        results = {}

        # 1. BoW ë¶„ì„
        bow_vectors, bow_vectorizer = self.create_bow_vectors(docs)
        bow_similarity = self.calculate_similarity_matrix(bow_vectors)
        results['bow'] = self.find_most_similar_pair(bow_similarity, docs)

        # 2. TF-IDF ë¶„ì„
        tfidf_vectors, tfidf_vectorizer = self.create_tfidf_vectors(docs)
        tfidf_similarity = self.calculate_similarity_matrix(tfidf_vectors)
        results['tfidf'] = self.find_most_similar_pair(tfidf_similarity, docs)

        # 3. SBERT ë¶„ì„
        if self.sbert_model:
            sbert_embeddings = self.create_sbert_embeddings(docs)
            sbert_similarity = self.calculate_similarity_matrix(sbert_embeddings)
            results['sbert'] = self.find_most_similar_pair(sbert_similarity, docs)

        return results

    def visualize_similarity_matrix(self, similarity_matrix, docs, method_name):
        """ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì‹œê°í™”"""
        plt.figure(figsize=(10, 8))

        # íˆíŠ¸ë§µ ìƒì„±
        sns.heatmap(
            similarity_matrix,
            annot=True,
            fmt='.3f',
            cmap='YlOrRd',
            xticklabels=[f"ë¬¸ì„œ{i+1}" for i in range(len(docs))],
            yticklabels=[f"ë¬¸ì„œ{i+1}" for i in range(len(docs))],
            cbar_kws={'label': 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„'}
        )

        plt.title(f'{method_name} ë¬¸ì„œ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤')
        plt.xlabel('ë¬¸ì„œ')
        plt.ylabel('ë¬¸ì„œ')
        plt.tight_layout()
        plt.show()

# ì‚¬ìš© ì˜ˆì œ
analyzer = DocumentSimilarityAnalyzer()
analyzer.load_sbert_model()

# í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤
test_documents = [
    "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ê³¼í•™ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ì¢…ë¥˜ì…ë‹ˆë‹¤.",
    "ìì—°ì–´ ì²˜ë¦¬ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.",
    "ì»´í“¨í„° ë¹„ì „ì€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤."
]

# ì¢…í•© ë¶„ì„ ì‹¤í–‰
comprehensive_results = analyzer.comprehensive_analysis(test_documents)

# ê²°ê³¼ ì¶œë ¥
for method, result in comprehensive_results.items():
    print(f"\n=== {method.upper()} ë¶„ì„ ê²°ê³¼ ===")
    print(f"ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ (ì¸ë±ìŠ¤ {result['indices']}):")
    print(f"  ë¬¸ì„œ A: {result['most_similar_pair'][0]}")
    print(f"  ë¬¸ì„œ B: {result['most_similar_pair'][1]}")
    print(f"  ìœ ì‚¬ë„: {result['similarity_score']:.4f}")

    # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì‹œê°í™”
    analyzer.visualize_similarity_matrix(
        result['similarity_matrix'],
        test_documents,
        method.upper()
    )

# ìƒì„¸ ë¶„ì„ í…Œì´ë¸” ìƒì„±
def create_detailed_comparison_table(results, docs):
    """ìƒì„¸ ë¹„êµ í…Œì´ë¸” ìƒì„±"""
    comparison_data = []

    for method, result in results.items():
        indices = result['indices']
        comparison_data.append({
            'ë°©ë²•': method.upper(),
            'ë¬¸ì„œ1 ì¸ë±ìŠ¤': indices[0],
            'ë¬¸ì„œ2 ì¸ë±ìŠ¤': indices[1],
            'ìœ ì‚¬ë„ ì ìˆ˜': f"{result['similarity_score']:.4f}",
            'ë¬¸ì„œ1 ë‚´ìš©': result['most_similar_pair'][0][:30] + "...",
            'ë¬¸ì„œ2 ë‚´ìš©': result['most_similar_pair'][1][:30] + "..."
        })

    return pd.DataFrame(comparison_data)

comparison_table = create_detailed_comparison_table(comprehensive_results, test_documents)
print("\n=== ë°©ë²•ë³„ ë¹„êµ ê²°ê³¼ ===")
print(comparison_table.to_string(index=False))
```

## ğŸ” ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°](https://github.com/bab2min/Kiwi) - í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„
- [Transformers](https://huggingface.co/docs/transformers/) - í† í¬ë‚˜ì´ì €ì™€ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸
- [Sentence Transformers](https://www.sbert.net/) - ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸
- [Gensim](https://radimrehurek.com/gensim/) - Word2Vec êµ¬í˜„

### í•™ìŠµ ìë£Œ
- [í•œêµ­ì–´ NLP ê°€ì´ë“œ](https://wikidocs.net/book/2155) - í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬
- [SBERT ë…¼ë¬¸](https://arxiv.org/abs/1908.10084) - Sentence-BERT ì›ë¦¬
- [Word2Vec íŠœí† ë¦¬ì–¼](https://radimrehurek.com/gensim/models/word2vec.html) - ë‹¨ì–´ ì„ë² ë”©

### ê°œë°œ ë„êµ¬
- [HuggingFace Model Hub](https://huggingface.co/models) - ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸
- [Scikit-learn](https://scikit-learn.org/) - ë¨¸ì‹ ëŸ¬ë‹ ë„êµ¬
- [Matplotlib](https://matplotlib.org/) - ë°ì´í„° ì‹œê°í™”

### ì¶”ê°€ í•™ìŠµ
- FastText, GloVe ë“± ë‹¤ë¥¸ ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸
- Transformer ê¸°ë°˜ ìµœì‹  ì–¸ì–´ ëª¨ë¸ ì´í•´
- ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ê¸°ë²•
- ë‹¤êµ­ì–´ ì„ë² ë”©ê³¼ í¬ë¡œìŠ¤ë§êµ¬ì–¼ ëª¨ë¸