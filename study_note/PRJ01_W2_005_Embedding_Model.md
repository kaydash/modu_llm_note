# W2_005 ì„ë² ë”© ëª¨ë¸ í™œìš©

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- í…ìŠ¤íŠ¸ ì„ë² ë”©ì˜ ê°œë…ê³¼ ì¤‘ìš”ì„± ì´í•´í•˜ê¸°
- OpenAI, Hugging Face, Ollama ì„ë² ë”© ëª¨ë¸ ë¹„êµ í™œìš©í•˜ê¸°
- ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„í•˜ê¸°

## ğŸ“š í•µì‹¬ ê°œë…

### ë¬¸ì„œ ì„ë² ë”©(Document Embedding)ì´ë€?
ë¬¸ì„œ ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ê³ ì • ê¸¸ì´ì˜ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

**ì£¼ìš” ëª©ì :**
- **ì˜ë¯¸ í‘œí˜„**: í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ì  íŠ¹ì„±ì„ ìˆ˜ì¹˜í™”
- **ìœ ì‚¬ë„ ê³„ì‚°**: í…ìŠ¤íŠ¸ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ì„± ì¸¡ì •
- **ë²¡í„° ê²€ìƒ‰**: ê³ ì°¨ì› ë²¡í„° ê³µê°„ì—ì„œì˜ íš¨ìœ¨ì  ê²€ìƒ‰
- **RAG êµ¬í˜„**: ì˜ë¯¸ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ë° ìƒì„±

**ì„ë² ë”© ë²¡í„°ì˜ íŠ¹ì„±:**
- **ê³ ì • ì°¨ì›**: ëª¨ë“  í…ìŠ¤íŠ¸ê°€ ê°™ì€ ê¸¸ì´ì˜ ë²¡í„°ë¡œ ë³€í™˜
- **ì˜ë¯¸ ë³´ì¡´**: ìœ ì‚¬í•œ ì˜ë¯¸ì˜ í…ìŠ¤íŠ¸ëŠ” ê°€ê¹Œìš´ ë²¡í„°ê°’
- **ì—°ì‚° ê°€ëŠ¥**: ë²¡í„° ì—°ì‚°ì„ í†µí•œ ì˜ë¯¸ì  ê´€ê³„ ë¶„ì„

### LangChain ì„ë² ë”© ëª¨ë¸ ì¢…ë¥˜

#### 1. OpenAI Embeddings
- **íŠ¹ì§•**: ë†’ì€ í’ˆì§ˆ, ë‹¤êµ­ì–´ ì§€ì›, ì¼ê´€ëœ ì„±ëŠ¥
- **ëª¨ë¸**: text-embedding-3-small/large, text-embedding-ada-002
- **ì¥ì **: ìš°ìˆ˜í•œ ì„±ëŠ¥, ê°„í¸í•œ ì‚¬ìš©
- **ë‹¨ì **: API ë¹„ìš©, ì¸í„°ë„· ì—°ê²° í•„ìš”

#### 2. Hugging Face Embeddings
- **íŠ¹ì§•**: ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥, ë‹¤ì–‘í•œ ëª¨ë¸ ì„ íƒ
- **ëª¨ë¸**: BAAI/bge-m3, sentence-transformers ê³„ì—´
- **ì¥ì **: ë¬´ë£Œ ì‚¬ìš©, ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
- **ë‹¨ì **: ë¡œì»¬ ìì› í•„ìš”, ì´ˆê¸° ë‹¤ìš´ë¡œë“œ

#### 3. Ollama Embeddings
- **íŠ¹ì§•**: ê²½ëŸ‰í™”, ë¡œì»¬ ì„œë²„ ê¸°ë°˜
- **ëª¨ë¸**: nomic-embed-text, bge-m3 ë“±
- **ì¥ì **: ë¹ ë¥¸ ì¶”ë¡ , í”„ë¼ì´ë²„ì‹œ ë³´í˜¸
- **ë‹¨ì **: ë³„ë„ ì„œë²„ ì„¤ì • í•„ìš”

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# OpenAI ì„ë² ë”©
pip install langchain-openai

# Hugging Face ì„ë² ë”©
pip install langchain-huggingface transformers sentence-transformers

# Ollama ì„ë² ë”©
pip install langchain-ollama

# ìœ ì‚¬ë„ ê³„ì‚°
pip install numpy scipy scikit-learn
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
from dotenv import load_dotenv
import os
import numpy as np
from typing import List, Tuple, Dict, Any
from pprint import pprint

load_dotenv()

# OpenAI API í‚¤ ì„¤ì • í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    print("Warning: OPENAI_API_KEY not set")

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.documents import Document
from langchain_community.utils.math import cosine_similarity
```

## ğŸ’» ì½”ë“œ ì˜ˆì œ

### 1. OpenAI ì„ë² ë”© ëª¨ë¸

#### ê¸°ë³¸ ì‚¬ìš©ë²•
```python
from langchain_openai import OpenAIEmbeddings

def create_openai_embeddings(
    model: str = "text-embedding-3-small",
    dimensions: Optional[int] = None
) -> OpenAIEmbeddings:
    """OpenAI ì„ë² ë”© ëª¨ë¸ ìƒì„±"""
    return OpenAIEmbeddings(
        model=model,
        dimensions=dimensions,  # ì°¨ì› ì¶•ì†Œ ê°€ëŠ¥ (None=ê¸°ë³¸ê°’)
        chunk_size=1000,        # ë°°ì¹˜ í¬ê¸°
        max_retries=2,          # ì¬ì‹œë„ íšŸìˆ˜
        show_progress_bar=False # ì§„í–‰ìƒíƒœ í‘œì‹œ
    )

# ëª¨ë¸ ìƒì„±
embeddings_openai = create_openai_embeddings()

print(f"ëª¨ë¸: {embeddings_openai.model}")
print(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {embeddings_openai.embedding_ctx_length}")
print(f"ì°¨ì›: {embeddings_openai.dimensions}")
```

#### ë¬¸ì„œ ì„ë² ë”© ìƒì„±
```python
def embed_documents_openai(
    documents: List[str],
    model: OpenAIEmbeddings
) -> List[List[float]]:
    """ë¬¸ì„œ ì»¬ë ‰ì…˜ ì„ë² ë”© ìƒì„±"""
    document_embeddings = model.embed_documents(documents)

    print(f"ì„ë² ë”©ëœ ë¬¸ì„œ ìˆ˜: {len(document_embeddings)}")
    print(f"ì„ë² ë”© ì°¨ì›: {len(document_embeddings[0])}")

    return document_embeddings

# ì˜ˆì œ ë¬¸ì„œë“¤
documents = [
    "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ê³¼í•™ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ì¢…ë¥˜ì…ë‹ˆë‹¤.",
    "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "ì»´í“¨í„° ë¹„ì „ì€ ì»´í“¨í„°ê°€ ë””ì§€í„¸ ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ë¥¼ ì´í•´í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤."
]

# ë¬¸ì„œ ì„ë² ë”©
document_embeddings = embed_documents_openai(documents, embeddings_openai)

# ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì„ë² ë”© ì¼ë¶€ í™•ì¸
print(f"ì²« ë²ˆì§¸ ë¬¸ì„œ ì„ë² ë”© (ì²˜ìŒ 10ê°œ ê°’): {document_embeddings[0][:10]}")
```

#### ì¿¼ë¦¬ ì„ë² ë”© ë° ê²€ìƒ‰
```python
def find_most_similar_document(
    query: str,
    documents: List[str],
    document_embeddings: List[List[float]],
    embeddings_model: OpenAIEmbeddings
) -> Tuple[str, float, int]:
    """ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°"""
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = embeddings_model.embed_query(query)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = cosine_similarity([query_embedding], document_embeddings)[0]

    # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì¸ë±ìŠ¤
    most_similar_idx = np.argmax(similarities)

    return (
        documents[most_similar_idx],
        float(similarities[most_similar_idx]),
        most_similar_idx
    )

# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
queries = [
    "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê´€ê³„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
    "ì»´í“¨í„°ê°€ ì´ë¯¸ì§€ë¥¼ ì´í•´í•˜ëŠ” ë°©ë²•ì€?"
]

print("=== OpenAI ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼ ===")
for query in queries:
    doc, similarity, idx = find_most_similar_document(
        query, documents, document_embeddings, embeddings_openai
    )
    print(f"ì¿¼ë¦¬: {query}")
    print(f"ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ: {doc}")
    print(f"ìœ ì‚¬ë„: {similarity:.4f}")
    print(f"ë¬¸ì„œ ì¸ë±ìŠ¤: {idx}")
    print("-" * 50)
```

#### ì°¨ì› ë¹„êµ ë¶„ì„
```python
def compare_embedding_dimensions(
    texts: List[str],
    dimensions_list: List[int] = [512, 1024, 1536]
) -> Dict[int, Dict[str, Any]]:
    """ë‹¤ì–‘í•œ ì°¨ì›ìœ¼ë¡œ ì„ë² ë”© ì„±ëŠ¥ ë¹„êµ"""
    results = {}

    for dim in dimensions_list:
        print(f"\n=== {dim}ì°¨ì› ì„ë² ë”© í…ŒìŠ¤íŠ¸ ===")

        # ëª¨ë¸ ìƒì„±
        model = create_openai_embeddings(dimensions=dim)

        # ì„ë² ë”© ìƒì„±
        embeddings = model.embed_documents(texts)

        # í†µê³„ ê³„ì‚°
        embedding_matrix = np.array(embeddings)

        results[dim] = {
            "model": model,
            "embeddings": embeddings,
            "shape": embedding_matrix.shape,
            "mean": np.mean(embedding_matrix),
            "std": np.std(embedding_matrix),
            "norm_avg": np.mean([np.linalg.norm(emb) for emb in embeddings])
        }

        print(f"ì„ë² ë”© í˜•íƒœ: {embedding_matrix.shape}")
        print(f"í‰ê· ê°’: {results[dim]['mean']:.6f}")
        print(f"í‘œì¤€í¸ì°¨: {results[dim]['std']:.6f}")
        print(f"í‰ê·  ë…¸ë¦„: {results[dim]['norm_avg']:.6f}")

    return results

# ì°¨ì› ë¹„êµ ì‹¤í–‰
sample_texts = [
    "ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ì‚¬íšŒë¥¼ ë³€í™”ì‹œí‚¤ê³  ìˆë‹¤",
    "AI ê¸°ìˆ ì´ ìš°ë¦¬ì˜ ë¯¸ë˜ë¥¼ ë°”ê¾¸ê³  ìˆë‹¤"
]

dimension_results = compare_embedding_dimensions(sample_texts)

# ì°¨ì›ë³„ ìœ ì‚¬ë„ ë¹„êµ
print("\n=== ì°¨ì›ë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„êµ ===")
for dim, result in dimension_results.items():
    emb1, emb2 = result["embeddings"]
    similarity = cosine_similarity([emb1], [emb2])[0][0]
    print(f"{dim}ì°¨ì›: ìœ ì‚¬ë„ = {similarity:.6f}")
```

### 2. Hugging Face ì„ë² ë”© ëª¨ë¸

#### ê¸°ë³¸ ì„¤ì • ë° ì‚¬ìš©
```python
from langchain_huggingface.embeddings import HuggingFaceEmbeddings

def create_huggingface_embeddings(
    model_name: str = "BAAI/bge-m3",
    device: str = "cpu"
) -> HuggingFaceEmbeddings:
    """Hugging Face ì„ë² ë”© ëª¨ë¸ ìƒì„±"""
    return HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={"device": device},  # "cpu", "cuda", "mps"
        encode_kwargs={"normalize_embeddings": True},  # ì •ê·œí™”
        show_progress=False,
        multi_process=False
    )

# ë‹¤ì–‘í•œ ëª¨ë¸ ë¹„êµ
hf_models = {
    "bge-m3": "BAAI/bge-m3",                           # 1024ì°¨ì›, ë‹¤êµ­ì–´
    "multilingual-e5": "intfloat/multilingual-e5-large", # 1024ì°¨ì›, ë‹¤êµ­ì–´
    "bge-small": "BAAI/bge-small-en-v1.5",            # 384ì°¨ì›, ì˜ì–´
    "all-MiniLM": "sentence-transformers/all-MiniLM-L6-v2"  # 384ì°¨ì›, ë‹¤êµ­ì–´
}

# ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
def compare_hf_models(documents: List[str], query: str) -> Dict[str, Any]:
    """Hugging Face ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ"""
    results = {}

    for model_alias, model_name in hf_models.items():
        print(f"\n=== {model_alias} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")

        try:
            # ëª¨ë¸ ë¡œë“œ
            embeddings_model = create_huggingface_embeddings(model_name)

            # ë¬¸ì„œ ì„ë² ë”©
            doc_embeddings = embeddings_model.embed_documents(documents)

            # ì¿¼ë¦¬ ê²€ìƒ‰
            doc, similarity, idx = find_most_similar_document(
                query, documents, doc_embeddings, embeddings_model
            )

            results[model_alias] = {
                "model_name": model_name,
                "embedding_dim": len(doc_embeddings[0]),
                "best_match": doc,
                "similarity": similarity,
                "doc_index": idx
            }

            print(f"ëª¨ë¸ëª…: {model_name}")
            print(f"ì„ë² ë”© ì°¨ì›: {len(doc_embeddings[0])}")
            print(f"ìµœê³  ìœ ì‚¬ë„: {similarity:.4f}")
            print(f"ë§¤ì¹­ ë¬¸ì„œ: {doc[:50]}...")

        except Exception as e:
            print(f"ëª¨ë¸ {model_alias} ë¡œë“œ ì‹¤íŒ¨: {e}")
            results[model_alias] = {"error": str(e)}

    return results

# Hugging Face ëª¨ë¸ ë¹„êµ ì‹¤í–‰
hf_results = compare_hf_models(documents, "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì€?")

# ê²°ê³¼ ìš”ì•½
print("\n=== Hugging Face ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ ===")
for model_alias, result in hf_results.items():
    if "error" not in result:
        print(f"{model_alias}: {result['embedding_dim']}ì°¨ì›, ìœ ì‚¬ë„ {result['similarity']:.4f}")
```

#### í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ì‚¬ìš©
```python
def create_korean_embeddings() -> HuggingFaceEmbeddings:
    """í•œêµ­ì–´ì— íŠ¹í™”ëœ ì„ë² ë”© ëª¨ë¸"""
    return HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",  # í•œêµ­ì–´ íŠ¹í™”
        # model_name="BM-K/KoSimCSE-roberta-multitask",  # ëŒ€ì•ˆ í•œêµ­ì–´ ëª¨ë¸
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )

# í•œêµ­ì–´ ë¬¸ì„œ í…ŒìŠ¤íŠ¸
korean_documents = [
    "ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤.",
    "ë¶€ì‚°ì€ ëŒ€í•œë¯¼êµ­ì˜ ì œ2ì˜ ë„ì‹œì…ë‹ˆë‹¤.",
    "ê¹€ì¹˜ëŠ” í•œêµ­ì˜ ì „í†µ ë°œíš¨ ìŒì‹ì…ë‹ˆë‹¤.",
    "í•œê¸€ì€ ì„¸ì¢…ëŒ€ì™•ì´ ë§Œë“  ë¬¸ìì…ë‹ˆë‹¤.",
    "íƒœê·¹ê¸°ëŠ” ëŒ€í•œë¯¼êµ­ì˜ êµ­ê¸°ì…ë‹ˆë‹¤."
]

korean_queries = [
    "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
    "í•œêµ­ì˜ ì „í†µ ìŒì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "í•œêµ­ì˜ ë¬¸ìëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?"
]

def test_korean_performance():
    """í•œêµ­ì–´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("=== í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ===")

    try:
        korean_embeddings = create_korean_embeddings()
        korean_doc_embeddings = korean_embeddings.embed_documents(korean_documents)

        print(f"í•œêµ­ì–´ ì„ë² ë”© ì°¨ì›: {len(korean_doc_embeddings[0])}")

        for query in korean_queries:
            doc, similarity, idx = find_most_similar_document(
                query, korean_documents, korean_doc_embeddings, korean_embeddings
            )
            print(f"\nì¿¼ë¦¬: {query}")
            print(f"ë‹µë³€: {doc}")
            print(f"ìœ ì‚¬ë„: {similarity:.4f}")

    except Exception as e:
        print(f"í•œêµ­ì–´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

test_korean_performance()
```

### 3. Ollama ì„ë² ë”© ëª¨ë¸

#### Ollama ì„¤ì • ë° ì‚¬ìš©
```python
from langchain_ollama import OllamaEmbeddings

def create_ollama_embeddings(
    model: str = "nomic-embed-text",
    base_url: str = "http://localhost:11434"
) -> OllamaEmbeddings:
    """Ollama ì„ë² ë”© ëª¨ë¸ ìƒì„±"""
    return OllamaEmbeddings(
        model=model,
        base_url=base_url,
        # ì¶”ê°€ íŒŒë¼ë¯¸í„°
        num_ctx=2048,      # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
        temperature=0.0,   # ê²°ì •ì  ì¶œë ¥
    )

def test_ollama_connection(base_url: str = "http://localhost:11434") -> bool:
    """Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        import requests
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        if response.status_code == 200:
            available_models = response.json().get("models", [])
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸: {[m['name'] for m in available_models]}")
            return True
    except Exception as e:
        print(f"Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

    return False

# Ollama ì„œë²„ í™•ì¸ ë° ëª¨ë¸ í…ŒìŠ¤íŠ¸
if test_ollama_connection():
    print("\n=== Ollama ì„ë² ë”© í…ŒìŠ¤íŠ¸ ===")

    ollama_models = ["nomic-embed-text", "bge-m3", "all-minilm"]

    for model_name in ollama_models:
        try:
            print(f"\n--- {model_name} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---")

            ollama_embeddings = create_ollama_embeddings(model_name)

            # ìƒ˜í”Œ ì„ë² ë”© ìƒì„±
            sample_embeddings = ollama_embeddings.embed_documents(documents[:2])
            print(f"ì„ë² ë”© ì°¨ì›: {len(sample_embeddings[0])}")

            # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            doc, similarity, idx = find_most_similar_document(
                "AI ê¸°ìˆ ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
                documents,
                ollama_embeddings.embed_documents(documents),
                ollama_embeddings
            )

            print(f"ê²€ìƒ‰ ê²°ê³¼: {doc[:50]}...")
            print(f"ìœ ì‚¬ë„: {similarity:.4f}")

        except Exception as e:
            print(f"ëª¨ë¸ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
else:
    print("Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•„ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
```

### 4. ëª¨ë¸ ê°„ ì„±ëŠ¥ ë¹„êµ

#### ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ
```python
import time
from typing import Optional

def benchmark_embedding_models(
    documents: List[str],
    queries: List[str],
    models_config: Dict[str, Dict[str, Any]]
) -> Dict[str, Dict[str, Any]]:
    """ì„ë² ë”© ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""

    results = {}

    for model_name, config in models_config.items():
        print(f"\n=== {model_name} ë²¤ì¹˜ë§ˆí¬ ===")

        try:
            # ëª¨ë¸ ìƒì„±
            start_time = time.time()

            if config["type"] == "openai":
                model = OpenAIEmbeddings(
                    model=config["model_name"],
                    dimensions=config.get("dimensions")
                )
            elif config["type"] == "huggingface":
                model = HuggingFaceEmbeddings(
                    model_name=config["model_name"],
                    model_kwargs={"device": config.get("device", "cpu")}
                )
            elif config["type"] == "ollama":
                model = OllamaEmbeddings(
                    model=config["model_name"],
                    base_url=config.get("base_url", "http://localhost:11434")
                )

            model_load_time = time.time() - start_time

            # ë¬¸ì„œ ì„ë² ë”© ìƒì„±
            start_time = time.time()
            doc_embeddings = model.embed_documents(documents)
            embed_time = time.time() - start_time

            # ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ì„±ëŠ¥
            query_results = []
            total_search_time = 0

            for query in queries:
                start_time = time.time()
                doc, similarity, idx = find_most_similar_document(
                    query, documents, doc_embeddings, model
                )
                search_time = time.time() - start_time

                query_results.append({
                    "query": query,
                    "best_match": doc,
                    "similarity": similarity,
                    "search_time": search_time
                })

                total_search_time += search_time

            # ê²°ê³¼ ì €ì¥
            results[model_name] = {
                "config": config,
                "model_load_time": model_load_time,
                "embedding_time": embed_time,
                "embedding_dimension": len(doc_embeddings[0]),
                "total_search_time": total_search_time,
                "avg_search_time": total_search_time / len(queries),
                "query_results": query_results,
                "success": True
            }

            print(f"ëª¨ë¸ ë¡œë“œ: {model_load_time:.3f}ì´ˆ")
            print(f"ì„ë² ë”© ìƒì„±: {embed_time:.3f}ì´ˆ")
            print(f"í‰ê·  ê²€ìƒ‰ ì‹œê°„: {results[model_name]['avg_search_time']:.3f}ì´ˆ")
            print(f"ì„ë² ë”© ì°¨ì›: {len(doc_embeddings[0])}")

        except Exception as e:
            print(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            results[model_name] = {
                "config": config,
                "error": str(e),
                "success": False
            }

    return results

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
benchmark_config = {
    "OpenAI-Small": {
        "type": "openai",
        "model_name": "text-embedding-3-small"
    },
    "OpenAI-Small-512": {
        "type": "openai",
        "model_name": "text-embedding-3-small",
        "dimensions": 512
    },
    "BGE-M3": {
        "type": "huggingface",
        "model_name": "BAAI/bge-m3"
    },
    "MiniLM": {
        "type": "huggingface",
        "model_name": "sentence-transformers/all-MiniLM-L6-v2"
    }
}

# ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°
test_documents = documents + [
    "ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì€ í…ìŠ¤íŠ¸ ë¶„ì„ì— í™œìš©ë©ë‹ˆë‹¤.",
    "ì»´í“¨í„° ë¹„ì „ì€ ì´ë¯¸ì§€ ì¸ì‹ ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "ë¡œë´‡ê³µí•™ì€ ì¸ê³µì§€ëŠ¥ê³¼ ê¸°ê³„ê³µí•™ì„ ê²°í•©í•©ë‹ˆë‹¤."
]

test_queries = [
    "AI ê¸°ìˆ ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "ë”¥ëŸ¬ë‹ê³¼ ì‹ ê²½ë§ì˜ ê´€ê³„ëŠ”?",
    "ìì—°ì–´ ì²˜ë¦¬ì˜ ì‘ìš© ë¶„ì•¼ëŠ”?"
]

benchmark_results = benchmark_embedding_models(
    test_documents,
    test_queries,
    benchmark_config
)

# ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½
print("\n" + "="*60)
print("ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
print("="*60)

for model_name, result in benchmark_results.items():
    if result["success"]:
        print(f"\n{model_name}:")
        print(f"  ì°¨ì›: {result['embedding_dimension']}")
        print(f"  ì„ë² ë”© ì‹œê°„: {result['embedding_time']:.3f}ì´ˆ")
        print(f"  í‰ê·  ê²€ìƒ‰ ì‹œê°„: {result['avg_search_time']:.3f}ì´ˆ")

        # í‰ê·  ìœ ì‚¬ë„
        similarities = [qr["similarity"] for qr in result["query_results"]]
        avg_similarity = sum(similarities) / len(similarities)
        print(f"  í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.4f}")
    else:
        print(f"\n{model_name}: ì‹¤íŒ¨ - {result['error']}")
```

## ğŸš€ ì‹¤ìŠµí•´ë³´ê¸°

### ì‹¤ìŠµ 1: ë‹¤êµ­ì–´ ì„ë² ë”© ë¹„êµ
ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ ëœ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© í’ˆì§ˆì„ ë¹„êµí•´ë³´ì„¸ìš”.

```python
def multilingual_embedding_test():
    """ë‹¤êµ­ì–´ ì„ë² ë”© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    multilingual_texts = {
        "korean": "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ë¥¼ ë°”ê¿€ ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "english": "Artificial intelligence is technology that will change the future.",
        "japanese": "äººå·¥çŸ¥èƒ½ã¯æœªæ¥ã‚’å¤‰ãˆã‚‹æŠ€è¡“ã§ã™ã€‚",
        "chinese": "äººå·¥æ™ºèƒ½æ˜¯å°†æ”¹å˜æœªæ¥çš„æŠ€æœ¯ã€‚"
    }

    # TODO: OpenAIì™€ ë‹¤êµ­ì–´ HuggingFace ëª¨ë¸ë¡œ ê°ê° ì„ë² ë”©
    # TODO: ì–¸ì–´ë³„ ì„ë² ë”© ë²¡í„° ë¶„ì„
    # TODO: ì–¸ì–´ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
    # TODO: ê²°ê³¼ ë¹„êµ ë° ë¶„ì„
    pass
```

### ì‹¤ìŠµ 2: ë„ë©”ì¸ íŠ¹í™” ì„ë² ë”© ìµœì í™”
íŠ¹ì • ë„ë©”ì¸(ì˜ë£Œ, ë²•ë¥ , ê¸°ìˆ  ë“±)ì— ìµœì í™”ëœ ì„ë² ë”©ì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
def domain_specific_embeddings():
    """ë„ë©”ì¸ íŠ¹í™” ì„ë² ë”© ìµœì í™”"""
    domains = {
        "medical": [
            "ë‹¹ë‡¨ë³‘ì€ í˜ˆë‹¹ ì¡°ì ˆ ì¥ì• ë¡œ ì¸í•œ ì§ˆë³‘ì…ë‹ˆë‹¤.",
            "ê³ í˜ˆì••ì€ ì‹¬í˜ˆê´€ ì§ˆí™˜ì˜ ì£¼ìš” ìœ„í—˜ ìš”ì¸ì…ë‹ˆë‹¤.",
            "MRIëŠ” ìê¸°ê³µëª…ì˜ìƒì„ ì´ìš©í•œ ì§„ë‹¨ ë°©ë²•ì…ë‹ˆë‹¤."
        ],
        "legal": [
            "ê³„ì•½ì„œëŠ” ë‹¹ì‚¬ì ê°„ì˜ ì•½ì†ì„ ëª…ë¬¸í™”í•œ ë¬¸ì„œì…ë‹ˆë‹¤.",
            "ì €ì‘ê¶Œì€ ì°½ì‘ë¬¼ì— ëŒ€í•œ ë…ì ì  ê¶Œë¦¬ì…ë‹ˆë‹¤.",
            "ë¯¼ë²•ì€ ê°œì¸ ê°„ì˜ ë²•ë¥  ê´€ê³„ë¥¼ ê·œì •í•©ë‹ˆë‹¤."
        ],
        "technical": [
            "APIëŠ” ì‘ìš© í”„ë¡œê·¸ë¨ ê°„ ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.",
            "í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì€ ì¸í„°ë„·ì„ í†µí•œ ì»´í“¨íŒ… ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.",
            "ë¸”ë¡ì²´ì¸ì€ ë¶„ì‚° ì›ì¥ ê¸°ìˆ ì…ë‹ˆë‹¤."
        ]
    }

    # TODO: ë„ë©”ì¸ë³„ë¡œ ìµœì í™”ëœ ì„ë² ë”© ëª¨ë¸ ì„ íƒ
    # TODO: ë„ë©”ì¸ ë‚´ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ vs ë„ë©”ì¸ ê°„ ìœ ì‚¬ë„ ë¹„êµ
    # TODO: ë„ë©”ì¸ íŠ¹í™” ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
    pass
```

### ì‹¤ìŠµ 3: ì„ë² ë”© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
ì„ë² ë”© ë²¡í„°ë¥¼ í™œìš©í•œ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
def embedding_based_clustering():
    """ì„ë² ë”© ê¸°ë°˜ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§"""
    # TODO: ë‹¤ì–‘í•œ ì£¼ì œì˜ ë¬¸ì„œë“¤ ìˆ˜ì§‘
    # TODO: ë¬¸ì„œë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
    # TODO: K-means ë˜ëŠ” DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì ìš©
    # TODO: í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ë¬¸ì„œ ì„ ì •
    # TODO: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™” (t-SNE, UMAP)
    pass
```

## ğŸ“‹ í•´ë‹µ

### ì‹¤ìŠµ 1 í•´ë‹µ: ë‹¤êµ­ì–´ ì„ë² ë”© ë¹„êµ
```python
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
import seaborn as sns

def multilingual_embedding_test():
    """ë‹¤êµ­ì–´ ì„ë² ë”© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    multilingual_texts = {
        "korean": "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ë¥¼ ë°”ê¿€ ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "english": "Artificial intelligence is technology that will change the future.",
        "japanese": "äººå·¥çŸ¥èƒ½ã¯æœªæ¥ã‚’å¤‰ãˆã‚‹æŠ€è¡“ã§ã™ã€‚",
        "chinese": "äººå·¥æ™ºèƒ½æ˜¯å°†æ”¹å˜æœªæ¥çš„æŠ€æœ¯ã€‚"
    }

    # ëª¨ë¸ë“¤ ì„¤ì •
    models = {
        "OpenAI": OpenAIEmbeddings(model="text-embedding-3-small"),
        "BGE-M3": HuggingFaceEmbeddings(model_name="BAAI/bge-m3"),
        "Multilingual-E5": HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
    }

    results = {}

    for model_name, model in models.items():
        print(f"\n=== {model_name} ë‹¤êµ­ì–´ í…ŒìŠ¤íŠ¸ ===")

        try:
            # ê° ì–¸ì–´ë³„ ì„ë² ë”© ìƒì„±
            language_embeddings = {}
            for lang, text in multilingual_texts.items():
                embedding = model.embed_query(text)
                language_embeddings[lang] = embedding
                print(f"{lang}: ì„ë² ë”© ì°¨ì› {len(embedding)}")

            # ì–¸ì–´ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            languages = list(multilingual_texts.keys())
            embeddings_matrix = [language_embeddings[lang] for lang in languages]

            similarity_matrix = cosine_similarity(embeddings_matrix)

            # ê²°ê³¼ ì €ì¥
            results[model_name] = {
                "embeddings": language_embeddings,
                "similarity_matrix": similarity_matrix,
                "languages": languages
            }

            # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì¶œë ¥
            print("ì–¸ì–´ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„:")
            print("        ", "  ".join(f"{lang:8}" for lang in languages))
            for i, lang1 in enumerate(languages):
                row = f"{lang1:8}"
                for j, lang2 in enumerate(languages):
                    row += f"  {similarity_matrix[i][j]:6.4f}"
                print(row)

            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚° (ëŒ€ê°ì„  ì œì™¸)
            non_diagonal_similarities = []
            for i in range(len(languages)):
                for j in range(len(languages)):
                    if i != j:
                        non_diagonal_similarities.append(similarity_matrix[i][j])

            avg_similarity = np.mean(non_diagonal_similarities)
            print(f"í‰ê·  ì–¸ì–´ ê°„ ìœ ì‚¬ë„: {avg_similarity:.4f}")

        except Exception as e:
            print(f"ëª¨ë¸ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results[model_name] = {"error": str(e)}

    # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
    print("\n" + "="*50)
    print("ëª¨ë¸ë³„ ë‹¤êµ­ì–´ ì„±ëŠ¥ ë¹„êµ")
    print("="*50)

    for model_name, result in results.items():
        if "error" not in result:
            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            sim_matrix = result["similarity_matrix"]
            non_diagonal = []
            for i in range(len(sim_matrix)):
                for j in range(len(sim_matrix)):
                    if i != j:
                        non_diagonal.append(sim_matrix[i][j])

            avg_sim = np.mean(non_diagonal)
            std_sim = np.std(non_diagonal)

            print(f"{model_name}:")
            print(f"  í‰ê·  ì–¸ì–´ê°„ ìœ ì‚¬ë„: {avg_sim:.4f} (Â±{std_sim:.4f})")

            # ê°€ì¥ ìœ ì‚¬í•œ ì–¸ì–´ ìŒ
            max_sim = 0
            max_pair = None
            for i, lang1 in enumerate(result["languages"]):
                for j, lang2 in enumerate(result["languages"]):
                    if i != j and sim_matrix[i][j] > max_sim:
                        max_sim = sim_matrix[i][j]
                        max_pair = (lang1, lang2)

            if max_pair:
                print(f"  ê°€ì¥ ìœ ì‚¬í•œ ì–¸ì–´ ìŒ: {max_pair[0]}-{max_pair[1]} ({max_sim:.4f})")

    return results

# ë‹¤êµ­ì–´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
multilingual_results = multilingual_embedding_test()
```

### ì‹¤ìŠµ 2 í•´ë‹µ: ë„ë©”ì¸ íŠ¹í™” ì„ë² ë”© ìµœì í™”
```python
from sklearn.metrics import silhouette_score
from collections import defaultdict

def domain_specific_embeddings():
    """ë„ë©”ì¸ íŠ¹í™” ì„ë² ë”© ìµœì í™”"""
    domains = {
        "medical": [
            "ë‹¹ë‡¨ë³‘ì€ í˜ˆë‹¹ ì¡°ì ˆ ì¥ì• ë¡œ ì¸í•œ ì§ˆë³‘ì…ë‹ˆë‹¤.",
            "ê³ í˜ˆì••ì€ ì‹¬í˜ˆê´€ ì§ˆí™˜ì˜ ì£¼ìš” ìœ„í—˜ ìš”ì¸ì…ë‹ˆë‹¤.",
            "MRIëŠ” ìê¸°ê³µëª…ì˜ìƒì„ ì´ìš©í•œ ì§„ë‹¨ ë°©ë²•ì…ë‹ˆë‹¤.",
            "í•­ìƒì œëŠ” ì„¸ê·  ê°ì—¼ì„ ì¹˜ë£Œí•˜ëŠ” ì•½ë¬¼ì…ë‹ˆë‹¤.",
            "ë°±ì‹ ì€ ë©´ì—­ ì²´ê³„ë¥¼ ê°•í™”í•˜ì—¬ ì§ˆë³‘ì„ ì˜ˆë°©í•©ë‹ˆë‹¤."
        ],
        "legal": [
            "ê³„ì•½ì„œëŠ” ë‹¹ì‚¬ì ê°„ì˜ ì•½ì†ì„ ëª…ë¬¸í™”í•œ ë¬¸ì„œì…ë‹ˆë‹¤.",
            "ì €ì‘ê¶Œì€ ì°½ì‘ë¬¼ì— ëŒ€í•œ ë…ì ì  ê¶Œë¦¬ì…ë‹ˆë‹¤.",
            "ë¯¼ë²•ì€ ê°œì¸ ê°„ì˜ ë²•ë¥  ê´€ê³„ë¥¼ ê·œì •í•©ë‹ˆë‹¤.",
            "í˜•ë²•ì€ ë²”ì£„ì™€ í˜•ë²Œì— ê´€í•œ ë²•ë¥ ì…ë‹ˆë‹¤.",
            "í—Œë²•ì€ êµ­ê°€ì˜ ê¸°ë³¸ë²•ìœ¼ë¡œ ìµœê³  ê·œë²”ì…ë‹ˆë‹¤."
        ],
        "technical": [
            "APIëŠ” ì‘ìš© í”„ë¡œê·¸ë¨ ê°„ ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.",
            "í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì€ ì¸í„°ë„·ì„ í†µí•œ ì»´í“¨íŒ… ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.",
            "ë¸”ë¡ì²´ì¸ì€ ë¶„ì‚° ì›ì¥ ê¸°ìˆ ì…ë‹ˆë‹¤.",
            "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
            "ë°ì´í„°ë² ì´ìŠ¤ëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤."
        ]
    }

    # ëª¨ë“  ë¬¸ì„œì™€ ë„ë©”ì¸ ë¼ë²¨ ì¤€ë¹„
    all_documents = []
    domain_labels = []
    document_to_domain = {}

    for domain, docs in domains.items():
        for doc in docs:
            all_documents.append(doc)
            domain_labels.append(domain)
            document_to_domain[doc] = domain

    # ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸
    models_to_test = {
        "OpenAI-General": OpenAIEmbeddings(model="text-embedding-3-small"),
        "BGE-M3": HuggingFaceEmbeddings(model_name="BAAI/bge-m3"),
        # "BioBERT": HuggingFaceEmbeddings(model_name="dmis-lab/biobert-base-cased-v1.1"),  # ì˜ë£Œ íŠ¹í™”
        # "LegalBERT": HuggingFaceEmbeddings(model_name="nlpaueb/legal-bert-base-uncased"),  # ë²•ë¥  íŠ¹í™”
    }

    domain_analysis_results = {}

    for model_name, model in models_to_test.items():
        print(f"\n=== {model_name} ë„ë©”ì¸ ë¶„ì„ ===")

        try:
            # ëª¨ë“  ë¬¸ì„œ ì„ë² ë”©
            all_embeddings = model.embed_documents(all_documents)

            # ë„ë©”ì¸ ë‚´ ìœ ì‚¬ë„ vs ë„ë©”ì¸ ê°„ ìœ ì‚¬ë„ ë¶„ì„
            within_domain_similarities = []
            between_domain_similarities = []

            for i, doc1 in enumerate(all_documents):
                for j, doc2 in enumerate(all_documents):
                    if i != j:
                        similarity = cosine_similarity(
                            [all_embeddings[i]],
                            [all_embeddings[j]]
                        )[0][0]

                        if document_to_domain[doc1] == document_to_domain[doc2]:
                            within_domain_similarities.append(similarity)
                        else:
                            between_domain_similarities.append(similarity)

            # í†µê³„ ê³„ì‚°
            within_mean = np.mean(within_domain_similarities)
            within_std = np.std(within_domain_similarities)
            between_mean = np.mean(between_domain_similarities)
            between_std = np.std(between_domain_similarities)

            # ë„ë©”ì¸ êµ¬ë¶„ ëŠ¥ë ¥ ì¸¡ì • (í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ)
            embeddings_array = np.array(all_embeddings)

            # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚°
            try:
                from sklearn.preprocessing import LabelEncoder
                le = LabelEncoder()
                numeric_labels = le.fit_transform(domain_labels)
                silhouette = silhouette_score(embeddings_array, numeric_labels)
            except:
                silhouette = 0.0

            # ë„ë©”ì¸ë³„ ì¤‘ì‹¬ì  ê³„ì‚°
            domain_centers = {}
            for domain in domains.keys():
                domain_embeddings = [
                    all_embeddings[i] for i, label in enumerate(domain_labels)
                    if label == domain
                ]
                domain_centers[domain] = np.mean(domain_embeddings, axis=0)

            # ê²°ê³¼ ì €ì¥
            domain_analysis_results[model_name] = {
                "within_domain_mean": within_mean,
                "within_domain_std": within_std,
                "between_domain_mean": between_mean,
                "between_domain_std": between_std,
                "separation_ratio": within_mean / between_mean if between_mean > 0 else 0,
                "silhouette_score": silhouette,
                "domain_centers": domain_centers,
                "all_embeddings": all_embeddings
            }

            print(f"ë„ë©”ì¸ ë‚´ í‰ê·  ìœ ì‚¬ë„: {within_mean:.4f} (Â±{within_std:.4f})")
            print(f"ë„ë©”ì¸ ê°„ í‰ê·  ìœ ì‚¬ë„: {between_mean:.4f} (Â±{between_std:.4f})")
            print(f"ë¶„ë¦¬ ë¹„ìœ¨: {within_mean/between_mean:.4f}" if between_mean > 0 else "ë¶„ë¦¬ ë¹„ìœ¨: N/A")
            print(f"ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {silhouette:.4f}")

            # ë„ë©”ì¸ë³„ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            domain_queries = {
                "medical": "í˜ˆì••ê³¼ ì‹¬ì¥ë³‘ì˜ ê´€ê³„ëŠ”?",
                "legal": "ê³„ì•½ ìœ„ë°˜ ì‹œ ë²•ì  ì±…ì„ì€?",
                "technical": "í´ë¼ìš°ë“œì™€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ ë°©ë²•ì€?"
            }

            print("\në„ë©”ì¸ íŠ¹í™” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
            for query_domain, query in domain_queries.items():
                doc, similarity, idx = find_most_similar_document(
                    query, all_documents, all_embeddings, model
                )
                predicted_domain = domain_labels[idx]

                correct = predicted_domain == query_domain
                print(f"  {query_domain} ì¿¼ë¦¬: {'âœ“' if correct else 'âœ—'} "
                      f"(ì˜ˆì¸¡: {predicted_domain}, ìœ ì‚¬ë„: {similarity:.4f})")

        except Exception as e:
            print(f"ëª¨ë¸ {model_name} ë¶„ì„ ì‹¤íŒ¨: {e}")
            domain_analysis_results[model_name] = {"error": str(e)}

    # ìµœì¢… ê²°ê³¼ ë¹„êµ
    print("\n" + "="*60)
    print("ë„ë©”ì¸ íŠ¹í™” ì„±ëŠ¥ ë¹„êµ ìš”ì•½")
    print("="*60)

    for model_name, result in domain_analysis_results.items():
        if "error" not in result:
            print(f"\n{model_name}:")
            print(f"  ë„ë©”ì¸ ë¶„ë¦¬ ëŠ¥ë ¥: {result['separation_ratio']:.4f}")
            print(f"  í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ: {result['silhouette_score']:.4f}")
            print(f"  ë„ë©”ì¸ ë‚´ ì¼ê´€ì„±: {result['within_domain_mean']:.4f}")

    return domain_analysis_results

# ë„ë©”ì¸ íŠ¹í™” ë¶„ì„ ì‹¤í–‰
domain_results = domain_specific_embeddings()
```

### ì‹¤ìŠµ 3 í•´ë‹µ: ì„ë² ë”© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
```python
from sklearn.cluster import KMeans, DBSCAN
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def embedding_based_clustering():
    """ì„ë² ë”© ê¸°ë°˜ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§"""

    # ë‹¤ì–‘í•œ ì£¼ì œì˜ ë¬¸ì„œë“¤
    diverse_documents = [
        # AI/Tech í´ëŸ¬ìŠ¤í„°
        "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì„ ì´ìš©í•œ ê¸°ê³„í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤.",
        "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",

        # ì˜ë£Œ/ê±´ê°• í´ëŸ¬ìŠ¤í„°
        "ë‹¹ë‡¨ë³‘ì€ í˜ˆë‹¹ ì¡°ì ˆì— ë¬¸ì œê°€ ìˆëŠ” ì§ˆë³‘ì…ë‹ˆë‹¤.",
        "ê³ í˜ˆì••ì€ ì‹¬í˜ˆê´€ ì§ˆí™˜ì˜ ì£¼ìš” ìœ„í—˜ ìš”ì¸ì…ë‹ˆë‹¤.",
        "ìš´ë™ì€ ê±´ê°• ìœ ì§€ì— í•„ìˆ˜ì ì¸ í™œë™ì…ë‹ˆë‹¤.",
        "ê· í˜• ì¡íŒ ì‹ë‹¨ì€ ê±´ê°•í•œ ì‚¶ì˜ ê¸°ì´ˆì…ë‹ˆë‹¤.",

        # êµìœ¡/í•™ìŠµ í´ëŸ¬ìŠ¤í„°
        "êµìœ¡ì€ ê°œì¸ì˜ ì„±ì¥ê³¼ ë°œì „ì„ ìœ„í•œ ê³¼ì •ì…ë‹ˆë‹¤.",
        "ì˜¨ë¼ì¸ í•™ìŠµì€ ë””ì§€í„¸ ì‹œëŒ€ì˜ ìƒˆë¡œìš´ êµìœ¡ ë°©ë²•ì…ë‹ˆë‹¤.",
        "ë…ì„œëŠ” ì§€ì‹ ìŠµë“ê³¼ ì‚¬ê³ ë ¥ í–¥ìƒì— ë„ì›€ì´ ë©ë‹ˆë‹¤.",
        "ì°½ì˜ì„± êµìœ¡ì€ ë¯¸ë˜ ì¸ì¬ ì–‘ì„±ì˜ í•µì‹¬ì…ë‹ˆë‹¤.",

        # í™˜ê²½/ì§€ì†ê°€ëŠ¥ì„± í´ëŸ¬ìŠ¤í„°
        "ê¸°í›„ ë³€í™”ëŠ” ì „ ì§€êµ¬ì  í™˜ê²½ ë¬¸ì œì…ë‹ˆë‹¤.",
        "ì¬ìƒ ì—ë„ˆì§€ëŠ” ì§€ì† ê°€ëŠ¥í•œ ë°œì „ì˜ ì—´ì‡ ì…ë‹ˆë‹¤.",
        "í”Œë¼ìŠ¤í‹± ì˜¤ì—¼ì€ í•´ì–‘ ìƒíƒœê³„ë¥¼ ìœ„í˜‘í•©ë‹ˆë‹¤.",
        "ì¹œí™˜ê²½ ê¸°ìˆ ì€ í™˜ê²½ ë³´í˜¸ì˜ ì¤‘ìš”í•œ ìˆ˜ë‹¨ì…ë‹ˆë‹¤."
    ]

    # ì‹¤ì œ ì£¼ì œ ë¼ë²¨ (ê²€ì¦ìš©)
    true_labels = [
        "AI/Tech", "AI/Tech", "AI/Tech", "AI/Tech",
        "Healthcare", "Healthcare", "Healthcare", "Healthcare",
        "Education", "Education", "Education", "Education",
        "Environment", "Environment", "Environment", "Environment"
    ]

    # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

    print("=== ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ ===")

    # 1. ë¬¸ì„œ ì„ë² ë”© ìƒì„±
    print("1. ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
    document_embeddings = embedding_model.embed_documents(diverse_documents)
    embeddings_array = np.array(document_embeddings)

    print(f"   - ë¬¸ì„œ ìˆ˜: {len(diverse_documents)}")
    print(f"   - ì„ë² ë”© ì°¨ì›: {len(document_embeddings[0])}")

    # 2. K-means í´ëŸ¬ìŠ¤í„°ë§
    print("\n2. K-means í´ëŸ¬ìŠ¤í„°ë§...")
    n_clusters = 4  # ì‹¤ì œë¡œëŠ” 4ê°œ ì£¼ì œ

    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    kmeans_labels = kmeans.fit_predict(embeddings_array)

    # K-means ê²°ê³¼ ë¶„ì„
    kmeans_silhouette = silhouette_score(embeddings_array, kmeans_labels)

    print(f"   - í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}")
    print(f"   - ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {kmeans_silhouette:.4f}")

    # 3. DBSCAN í´ëŸ¬ìŠ¤í„°ë§
    print("\n3. DBSCAN í´ëŸ¬ìŠ¤í„°ë§...")

    # ì ì ˆí•œ eps ê°’ ì°¾ê¸° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    from sklearn.neighbors import NearestNeighbors

    neighbors = NearestNeighbors(n_neighbors=4)
    neighbors_fit = neighbors.fit(embeddings_array)
    distances, indices = neighbors_fit.kneighbors(embeddings_array)
    distances = np.sort(distances[:, 3])  # 4ë²ˆì§¸ ìµœê·¼ì ‘ ì´ì›ƒ ê±°ë¦¬

    # ê±°ë¦¬ì˜ ë³€í™”ìœ¨ì´ í° ì§€ì ì„ epsë¡œ ì‚¬ìš©
    eps = np.percentile(distances, 75)

    dbscan = DBSCAN(eps=eps, min_samples=2)
    dbscan_labels = dbscan.fit_predict(embeddings_array)

    n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
    n_noise = list(dbscan_labels).count(-1)

    print(f"   - ìë™ ê°ì§€ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters_dbscan}")
    print(f"   - ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ìˆ˜: {n_noise}")

    if n_clusters_dbscan > 0:
        # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ì œì™¸í•˜ê³  ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚°
        non_noise_mask = dbscan_labels != -1
        if np.sum(non_noise_mask) > 1 and len(set(dbscan_labels[non_noise_mask])) > 1:
            dbscan_silhouette = silhouette_score(
                embeddings_array[non_noise_mask],
                dbscan_labels[non_noise_mask]
            )
            print(f"   - ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {dbscan_silhouette:.4f}")
        else:
            dbscan_silhouette = 0
            print(f"   - ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: N/A (í´ëŸ¬ìŠ¤í„° ë¶€ì¡±)")
    else:
        dbscan_silhouette = 0
        print(f"   - ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: N/A (í´ëŸ¬ìŠ¤í„° ì—†ìŒ)")

    # 4. í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ë¬¸ì„œ ì„ ì •
    print("\n4. í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ë¬¸ì„œ ì„ ì •...")

    def find_cluster_representative(cluster_embeddings, cluster_docs, cluster_indices):
        """í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì„œ ì°¾ê¸°"""
        if len(cluster_embeddings) == 0:
            return None, None, -1

        cluster_center = np.mean(cluster_embeddings, axis=0)

        # ì¤‘ì‹¬ì ê³¼ì˜ ê±°ë¦¬ ê³„ì‚°
        distances = [
            np.linalg.norm(embedding - cluster_center)
            for embedding in cluster_embeddings
        ]

        closest_idx = np.argmin(distances)
        return cluster_docs[closest_idx], distances[closest_idx], cluster_indices[closest_idx]

    # K-means í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ë¬¸ì„œ
    print("\n   K-means í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ë¬¸ì„œ:")
    for cluster_id in range(n_clusters):
        cluster_mask = kmeans_labels == cluster_id
        cluster_docs = [diverse_documents[i] for i, mask in enumerate(cluster_mask) if mask]
        cluster_embeddings = embeddings_array[cluster_mask]
        cluster_indices = [i for i, mask in enumerate(cluster_mask) if mask]

        rep_doc, distance, doc_idx = find_cluster_representative(
            cluster_embeddings, cluster_docs, cluster_indices
        )

        print(f"   í´ëŸ¬ìŠ¤í„° {cluster_id} (í¬ê¸°: {len(cluster_docs)}):")
        print(f"     ëŒ€í‘œ ë¬¸ì„œ: {rep_doc}")
        print(f"     ì‹¤ì œ ì£¼ì œ: {true_labels[doc_idx]}")
        print()

    # 5. í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”
    print("5. í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”...")

    # ì°¨ì› ì¶•ì†Œ (t-SNE)
    print("   t-SNE ì°¨ì› ì¶•ì†Œ ìˆ˜í–‰ ì¤‘...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(diverse_documents)-1))
    embeddings_2d = tsne.fit_transform(embeddings_array)

    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))

    # ì‹¤ì œ ì£¼ì œë³„ ë¶„í¬
    true_label_colors = {'AI/Tech': 'red', 'Healthcare': 'blue', 'Education': 'green', 'Environment': 'orange'}
    for i, label in enumerate(true_labels):
        axes[0].scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],
                       c=true_label_colors[label], label=label if label not in [true_labels[j] for j in range(i)] else "",
                       alpha=0.7, s=100)

    axes[0].set_title('ì‹¤ì œ ì£¼ì œ ë¶„í¬')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # K-means ê²°ê³¼
    scatter = axes[1].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                             c=kmeans_labels, cmap='viridis', alpha=0.7, s=100)
    axes[1].set_title(f'K-means í´ëŸ¬ìŠ¤í„°ë§ (ì‹¤ë£¨ì—£: {kmeans_silhouette:.3f})')
    plt.colorbar(scatter, ax=axes[1])
    axes[1].grid(True, alpha=0.3)

    # DBSCAN ê²°ê³¼
    scatter = axes[2].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                             c=dbscan_labels, cmap='viridis', alpha=0.7, s=100)
    axes[2].set_title(f'DBSCAN í´ëŸ¬ìŠ¤í„°ë§ (í´ëŸ¬ìŠ¤í„°: {n_clusters_dbscan})')
    plt.colorbar(scatter, ax=axes[2])
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('clustering_results.png', dpi=150, bbox_inches='tight')
    plt.show()

    # 6. í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€
    print("\n6. í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€...")

    # ì‹¤ì œ ë¼ë²¨ê³¼ì˜ ì¼ì¹˜ë„ ê³„ì‚° (Adjusted Rand Index)
    from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score

    # ì‹¤ì œ ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    true_numeric_labels = le.fit_transform(true_labels)

    # K-means í‰ê°€
    kmeans_ari = adjusted_rand_score(true_numeric_labels, kmeans_labels)
    kmeans_nmi = normalized_mutual_info_score(true_numeric_labels, kmeans_labels)

    print(f"K-means ì„±ëŠ¥:")
    print(f"   - Adjusted Rand Index: {kmeans_ari:.4f}")
    print(f"   - Normalized Mutual Information: {kmeans_nmi:.4f}")
    print(f"   - ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {kmeans_silhouette:.4f}")

    # DBSCAN í‰ê°€ (ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ê³ ë ¤)
    if n_clusters_dbscan > 0:
        dbscan_ari = adjusted_rand_score(true_numeric_labels, dbscan_labels)
        dbscan_nmi = normalized_mutual_info_score(true_numeric_labels, dbscan_labels)

        print(f"\nDBSCAN ì„±ëŠ¥:")
        print(f"   - Adjusted Rand Index: {dbscan_ari:.4f}")
        print(f"   - Normalized Mutual Information: {dbscan_nmi:.4f}")
        print(f"   - ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {dbscan_silhouette:.4f}")

    # ê²°ê³¼ ìš”ì•½ ë°˜í™˜
    return {
        "documents": diverse_documents,
        "true_labels": true_labels,
        "embeddings": embeddings_array,
        "embeddings_2d": embeddings_2d,
        "kmeans_labels": kmeans_labels,
        "dbscan_labels": dbscan_labels,
        "kmeans_metrics": {
            "silhouette": kmeans_silhouette,
            "ari": kmeans_ari,
            "nmi": kmeans_nmi
        },
        "dbscan_metrics": {
            "silhouette": dbscan_silhouette if n_clusters_dbscan > 0 else 0,
            "ari": dbscan_ari if n_clusters_dbscan > 0 else 0,
            "nmi": dbscan_nmi if n_clusters_dbscan > 0 else 0,
            "n_clusters": n_clusters_dbscan,
            "n_noise": n_noise
        }
    }

# í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ ì‹¤í–‰
clustering_results = embedding_based_clustering()

print("\n" + "="*50)
print("í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ ìµœì¢… ìš”ì•½")
print("="*50)

kmeans_metrics = clustering_results["kmeans_metrics"]
dbscan_metrics = clustering_results["dbscan_metrics"]

print(f"ìµœì  í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜:")
if kmeans_metrics["ari"] > dbscan_metrics["ari"]:
    print(f"  â†’ K-means (ARI: {kmeans_metrics['ari']:.4f})")
else:
    print(f"  â†’ DBSCAN (ARI: {dbscan_metrics['ari']:.4f})")

print(f"\nì£¼ìš” ì„±ëŠ¥ ì§€í‘œ:")
print(f"  K-means ARI: {kmeans_metrics['ari']:.4f}")
print(f"  DBSCAN ARI: {dbscan_metrics['ari']:.4f}")
print(f"  DBSCAN ìë™ ê°ì§€ í´ëŸ¬ìŠ¤í„° ìˆ˜: {dbscan_metrics['n_clusters']}")
```

## ğŸ” ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LangChain Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/)
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [Hugging Face sentence-transformers](https://www.sbert.net/)

### ì„ë² ë”© ëª¨ë¸ ë¹„êµ
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„
- [BGE Models](https://huggingface.co/BAAI) - ê³ ì„±ëŠ¥ ë‹¤êµ­ì–´ ì„ë² ë”©
- [E5 Models](https://huggingface.co/intfloat) - Microsoft E5 ì„ë² ë”© ì‹œë¦¬ì¦ˆ

### ì„±ëŠ¥ ìµœì í™”
```python
# ì„ë² ë”© ìºì‹± ì˜ˆì œ
from functools import lru_cache

class CachedEmbeddings:
    def __init__(self, base_embeddings):
        self.base_embeddings = base_embeddings

    @lru_cache(maxsize=1000)
    def embed_query(self, text: str) -> List[float]:
        return self.base_embeddings.embed_query(text)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self.embed_query(text) for text in texts]

# ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
def batch_embed_documents(
    texts: List[str],
    model,
    batch_size: int = 100
) -> List[List[float]]:
    """ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ë°°ì¹˜ ì²˜ë¦¬"""
    all_embeddings = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_embeddings = model.embed_documents(batch)
        all_embeddings.extend(batch_embeddings)

    return all_embeddings
```

### ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ
| ëª¨ë¸ | ì°¨ì› | ì–¸ì–´ | ìš©ë„ | ë¹„ìš© |
|------|------|------|------|------|
| OpenAI text-embedding-3-small | 1536 | ë‹¤êµ­ì–´ | ë²”ìš©, ê³ í’ˆì§ˆ | ìœ ë£Œ |
| OpenAI text-embedding-3-large | 3072 | ë‹¤êµ­ì–´ | ìµœê³  ì„±ëŠ¥ | ê³ ë¹„ìš© |
| BAAI/bge-m3 | 1024 | ë‹¤êµ­ì–´ | ê· í˜•, ë¬´ë£Œ | ë¬´ë£Œ |
| sentence-transformers/all-MiniLM-L6-v2 | 384 | ë‹¤êµ­ì–´ | ê²½ëŸ‰, ë¹ ë¦„ | ë¬´ë£Œ |
| intfloat/multilingual-e5-large | 1024 | ë‹¤êµ­ì–´ | ê³ ì„±ëŠ¥ | ë¬´ë£Œ |