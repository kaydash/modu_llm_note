# RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ - ìƒì„± ë©”íŠ¸ë¦­ê³¼ ì •ëŸ‰ì  í‰ê°€ ê°€ì´ë“œ

## ğŸ“š í•™ìŠµ ëª©í‘œ
- RAG ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ê³¼ ìƒì„± ë‹¨ê³„ë³„ í‰ê°€ ë°©ë²•ë¡ ì„ ì´í•´í•œë‹¤
- ROUGE, BLEU ë“± ì •ëŸ‰ì  í‰ê°€ ì§€í‘œì˜ ì›ë¦¬ì™€ í™œìš©ë²•ì„ ìŠµë“í•œë‹¤
- íœ´ë¦¬ìŠ¤í‹± í‰ê°€, ë¬¸ìì—´ ê±°ë¦¬, ì„ë² ë”© ê¸°ë°˜ í‰ê°€ ê¸°ë²•ì„ ì‹¤ìŠµí•œë‹¤
- LangSmithë¥¼ í™œìš©í•œ ì²´ê³„ì ì¸ í‰ê°€ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤
- ì‹¤ë¬´ì—ì„œ RAG ì‹œìŠ¤í…œ í’ˆì§ˆì„ ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³  ê°œì„ í•  ìˆ˜ ìˆë‹¤

## ğŸ”‘ í•µì‹¬ ê°œë…

### RAG í‰ê°€ì˜ ë‘ ì¶•
- **ê²€ìƒ‰(Retrieval) í‰ê°€**: ê´€ë ¨ ë¬¸ì„œë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì°¾ì•„ì˜¤ëŠ”ê°€?
- **ìƒì„±(Generation) í‰ê°€**: ì°¾ì•„ì˜¨ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì¢‹ì€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ê°€?

### í‰ê°€ ë°©ë²•ë¡  ë¶„ë¥˜
1. **íœ´ë¦¬ìŠ¤í‹± í‰ê°€**: ëª…í™•í•œ ê·œì¹™ ê¸°ë°˜ (ê¸¸ì´, JSON ìœ íš¨ì„± ë“±)
2. **ì •ëŸ‰ì  ë©”íŠ¸ë¦­**: ROUGE, BLEU ë“± ìˆ˜ì¹˜ ê¸°ë°˜ ë¹„êµ
3. **ì˜ë¯¸ì  í‰ê°€**: ì„ë² ë”© ê±°ë¦¬, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë“±
4. **LLM-as-Judge**: ëŒ€í˜• ì–¸ì–´ëª¨ë¸ì„ í™œìš©í•œ í’ˆì§ˆ í‰ê°€

### í‰ê°€ ì§€í‘œì˜ í•œê³„ì™€ ë³´ì™„
- **ë‹¨ì–´ ì¤‘ì²© ê¸°ë°˜ ì§€í‘œì˜ í•œê³„**: ì˜ë¯¸ì  ìœ ì‚¬ì„± í¬ì°© ì–´ë ¤ì›€
- **ì°¸ì¡° ë‹µì•ˆì˜ í’ˆì§ˆ ì˜ì¡´ì„±**: í‰ê°€ ê¸°ì¤€ì˜ ê°ê´€ì„± í™•ë³´ í•„ìš”
- **ë¬¸ë§¥ ì´í•´ ë¶€ì¡±**: ë‹¨ìˆœ ë¹„êµë¡œëŠ” ì‹¤ì œ í’ˆì§ˆ ì¸¡ì • í•œê³„
- **ë‹¤ì°¨ì›ì  ì ‘ê·¼**: ì—¬ëŸ¬ ì§€í‘œë¥¼ ì¢…í•©í•˜ì—¬ ê· í˜•ìˆëŠ” í‰ê°€ í•„ìš”

## ğŸ›  í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# ê¸°ë³¸ í‰ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install langchain langchain-openai langchain-chroma
pip install korouge-score nltk
pip install rapidfuzz  # ë¬¸ìì—´ ê±°ë¦¬ ê³„ì‚°ìš©
pip install kiwisolver kiwipiepy  # í•œêµ­ì–´ í† í¬ë‚˜ì´ì €

# LangSmith í‰ê°€ ë„êµ¬
pip install langsmith

# ì¶”ê°€ ë¶„ì„ ë„êµ¬
pip install pandas numpy matplotlib seaborn
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
# .env íŒŒì¼
OPENAI_API_KEY=your_openai_api_key
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key
LANGCHAIN_PROJECT=RAG_Evaluation
```

### ê¸°ë³¸ ì„¤ì •
```python
import os
from dotenv import load_dotenv
import warnings
import pandas as pd
import numpy as np

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
warnings.filterwarnings("ignore")

# ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
chroma_db = Chroma(
    collection_name="db_korean_cosine_metadata",
    embedding_function=embeddings,
    persist_directory="./chroma_db",
)
```

## ğŸ’» ë‹¨ê³„ë³„ êµ¬í˜„

### 1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„

```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ êµ¬ì¡° ì˜ˆì‹œ
test_data = {
    'questions': [
        "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
        "ì „ê¸°ì°¨ì˜ ì£¼ìš” ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ììœ¨ì£¼í–‰ ê¸°ìˆ ì˜ í˜„ì¬ ìƒí™©ì€ ì–´ë–¤ê°€ìš”?"
    ],
    'reference_answers': [
        "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤.",
        "ì „ê¸°ì°¨ì˜ ì£¼ìš” ì¥ì ìœ¼ë¡œëŠ” í™˜ê²½ ì¹œí™”ì„±, ì—°ë£Œë¹„ ì ˆì•½, ì¡°ìš©í•œ ìš´í–‰ ë“±ì´ ìˆìŠµë‹ˆë‹¤.",
        "ììœ¨ì£¼í–‰ ê¸°ìˆ ì€ í˜„ì¬ ë ˆë²¨ 2-3 ë‹¨ê³„ì— ìˆìœ¼ë©°, ì™„ì „ ììœ¨ì£¼í–‰ì„ ìœ„í•´ ì§€ì†ì ìœ¼ë¡œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤."
    ],
    'contexts': [
        ["í…ŒìŠ¬ë¼ëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ê°€ CEOë¡œ ìˆëŠ” ì „ê¸°ì°¨ íšŒì‚¬ì…ë‹ˆë‹¤.", "..."],
        ["ì „ê¸°ì°¨ëŠ” ë°°í„°ë¦¬ë¡œ êµ¬ë™ë˜ëŠ” ì¹œí™˜ê²½ ìë™ì°¨ì…ë‹ˆë‹¤.", "..."],
        ["ììœ¨ì£¼í–‰ì€ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì„ í™œìš©í•œ ë¬´ì¸ ìš´ì „ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.", "..."]
    ]
}

df_test = pd.DataFrame(test_data)
```

### 2ë‹¨ê³„: ë‹¤ì–‘í•œ ê²€ìƒ‰ê¸° ì„¤ì •

```python
from krag.tokenizers import KiwiTokenizer
from krag.retrievers import KiWiBM25RetrieverWithScore
from langchain.retrievers import EnsembleRetriever

# BM25 ê²€ìƒ‰ê¸° ì„¤ì • (í‚¤ì›Œë“œ ê¸°ë°˜)
kiwi_tokenizer = KiwiTokenizer(model_type='knlm', typos='basic')
bm25_retriever = KiWiBM25RetrieverWithScore(
    documents=documents,
    kiwi_tokenizer=kiwi_tokenizer,
    k=4
)

# ë²¡í„° ê²€ìƒ‰ê¸° ì„¤ì • (ì˜ë¯¸ ê¸°ë°˜)
vector_retriever = chroma_db.as_retriever(search_kwargs={'k': 4})

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì„¤ì • (BM25 + Vector)
hybrid_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.5, 0.5]  # ë™ë“±í•œ ê°€ì¤‘ì¹˜
)
```

### 3ë‹¨ê³„: RAG ì²´ì¸ êµ¬ì„±

```python
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

def create_rag_chain(retriever, llm):
    """
    RAG ì²´ì¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    template = """ë‹¤ìŒ ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    ë§¥ë½ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ë‹¤ë©´ 'ë‹µë³€ì— í•„ìš”í•œ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.

    [ë§¥ë½]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ë‹µë³€]
    """

    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join([doc.page_content for doc in docs])

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain

# RAG ì²´ì¸ ìƒì„±
llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.1)
rag_chain = create_rag_chain(hybrid_retriever, llm)
```

## ğŸ” í‰ê°€ ë°©ë²•ë¡  ìƒì„¸ êµ¬í˜„

### 1. íœ´ë¦¬ìŠ¤í‹± í‰ê°€

```python
def evaluate_response_quality(response: str) -> dict:
    """
    ì‘ë‹µì˜ ê¸°ë³¸ì ì¸ í’ˆì§ˆì„ íœ´ë¦¬ìŠ¤í‹±í•˜ê²Œ í‰ê°€
    """
    results = {}

    # ê¸¸ì´ í‰ê°€
    char_length = len(response)
    results['length_score'] = 1.0 if 50 <= char_length <= 500 else 0.5
    results['char_length'] = char_length

    # í† í° ê¸¸ì´ í‰ê°€
    kiwi_tokenizer = KiwiTokenizer()
    tokens = kiwi_tokenizer.tokenize(response)
    token_count = len(tokens)
    results['token_score'] = 1.0 if 10 <= token_count <= 150 else 0.5
    results['token_count'] = token_count

    # ì™„ì„±ë„ í‰ê°€ (ë§ˆì¹¨í‘œ í™•ì¸)
    results['completeness_score'] = 1.0 if response.endswith(('.', '!', '?')) else 0.5

    # ë¶€ì •ì  ì‘ë‹µ í™•ì¸
    negative_phrases = ['ëª¨ë¥´ê² ìŠµë‹ˆë‹¤', 'ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤', 'ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤']
    results['positive_response'] = not any(phrase in response for phrase in negative_phrases)

    # ì¢…í•© ì ìˆ˜ ê³„ì‚°
    scores = [results['length_score'], results['token_score'],
              results['completeness_score']]
    results['overall_score'] = sum(scores) / len(scores)

    return results

# ì‚¬ìš© ì˜ˆì‹œ
test_response = "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤. ê·¸ëŠ” 2008ë…„ë¶€í„° í…ŒìŠ¬ë¼ë¥¼ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤."
quality_results = evaluate_response_quality(test_response)
print("íœ´ë¦¬ìŠ¤í‹± í‰ê°€ ê²°ê³¼:", quality_results)
```

### 2. ROUGE ì ìˆ˜ ê³„ì‚°

```python
from korouge_score import rouge_scorer
from krag.tokenizers import KiwiTokenizer

class CustomKiwiTokenizer(KiwiTokenizer):
    def tokenize(self, text):
        return [t.form for t in super().tokenize(text)]

def calculate_rouge_scores(reference: str, generated: str) -> dict:
    """
    ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    """
    kiwi_tokenizer = CustomKiwiTokenizer(model_type='knlm', typos='basic')
    scorer = rouge_scorer.RougeScorer(
        ["rouge1", "rouge2", "rougeL"],
        tokenizer=kiwi_tokenizer
    )

    scores = scorer.score(reference, generated)

    return {
        'rouge1_f1': scores['rouge1'].fmeasure,
        'rouge1_precision': scores['rouge1'].precision,
        'rouge1_recall': scores['rouge1'].recall,
        'rouge2_f1': scores['rouge2'].fmeasure,
        'rouge2_precision': scores['rouge2'].precision,
        'rouge2_recall': scores['rouge2'].recall,
        'rougeL_f1': scores['rougeL'].fmeasure,
        'rougeL_precision': scores['rougeL'].precision,
        'rougeL_recall': scores['rougeL'].recall
    }

# ì‚¬ìš© ì˜ˆì‹œ
reference = "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤."
generated = "ì¼ë¡  ë¨¸ìŠ¤í¬ê°€ í…ŒìŠ¬ë¼ì˜ ìµœê³ ê²½ì˜ìì…ë‹ˆë‹¤."
rouge_results = calculate_rouge_scores(reference, generated)
print("ROUGE ì ìˆ˜:", rouge_results)
```

### 3. BLEU ì ìˆ˜ ê³„ì‚°

```python
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from typing import List, Union

def calculate_bleu_score(
    reference: Union[str, List[str]],
    hypothesis: str,
    weights: tuple = (0.25, 0.25, 0.25, 0.25),
    tokenizer=None
) -> dict:
    """
    BLEU ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
    """
    if tokenizer is None:
        tokenizer = CustomKiwiTokenizer(model_type='knlm', typos='basic')

    try:
        # ì°¸ì¡° í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if isinstance(reference, str):
            references = [tokenizer.tokenize(reference)]
        else:
            references = [tokenizer.tokenize(ref) for ref in reference]

        # ìƒì„± í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        hypothesis_tokens = tokenizer.tokenize(hypothesis)

        # ê°œë³„ n-gram ì ìˆ˜ ê³„ì‚°
        bleu_scores = {}
        for i in range(1, 5):  # BLEU-1ë¶€í„° BLEU-4ê¹Œì§€
            weight = [0] * 4
            weight[i-1] = 1.0
            score = sentence_bleu(
                references,
                hypothesis_tokens,
                weights=weight,
                smoothing_function=SmoothingFunction().method1
            )
            bleu_scores[f'bleu_{i}'] = score

        # ì „ì²´ BLEU ì ìˆ˜ (ê· ë“± ê°€ì¤‘ì¹˜)
        overall_bleu = sentence_bleu(
            references,
            hypothesis_tokens,
            weights=weights,
            smoothing_function=SmoothingFunction().method1
        )
        bleu_scores['bleu_overall'] = overall_bleu

        return bleu_scores

    except Exception as e:
        print(f"BLEU ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
        return {'bleu_overall': 0.0, 'bleu_1': 0.0, 'bleu_2': 0.0, 'bleu_3': 0.0, 'bleu_4': 0.0}

# ì‚¬ìš© ì˜ˆì‹œ
bleu_results = calculate_bleu_score(reference, generated)
print("BLEU ì ìˆ˜:", bleu_results)
```

### 4. ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ í‰ê°€

```python
from langchain.evaluation import load_evaluator
from langchain_openai import OpenAIEmbeddings

def calculate_semantic_similarity(reference: str, generated: str) -> dict:
    """
    ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
    """
    # ì„ë² ë”© ê¸°ë°˜ í‰ê°€ê¸° ìƒì„±
    embedding_evaluator = load_evaluator(
        evaluator='embedding_distance',
        distance_metric='cosine',
        embeddings=OpenAIEmbeddings(model="text-embedding-3-small")
    )

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    result = embedding_evaluator.evaluate_strings(
        prediction=generated,
        reference=reference
    )

    # ë¬¸ìì—´ ê±°ë¦¬ë„ í•¨ê»˜ ê³„ì‚°
    string_evaluator = load_evaluator(
        evaluator="string_distance",
        distance="levenshtein"
    )

    string_result = string_evaluator.evaluate_strings(
        prediction=generated,
        reference=reference
    )

    return {
        'cosine_similarity': 1 - result['score'],  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
        'cosine_distance': result['score'],
        'levenshtein_distance': string_result['score'],
        'levenshtein_similarity': 1 - string_result['score']
    }

# ì‚¬ìš© ì˜ˆì‹œ
similarity_results = calculate_semantic_similarity(reference, generated)
print("ìœ ì‚¬ë„ í‰ê°€ ê²°ê³¼:", similarity_results)
```

## ğŸ¯ ì‹¤ìŠµ ë¬¸ì œ

### ê¸°ì´ˆ ì‹¤ìŠµ
1. **ê¸°ë³¸ í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•**
   - 5ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë§Œë“œì„¸ìš”
   - RAG ì²´ì¸ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê³  ê¸°ë³¸ í’ˆì§ˆì„ í‰ê°€í•˜ì„¸ìš”

2. **ë©”íŠ¸ë¦­ ë¹„êµ ì‹¤ìŠµ**
   - ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ 3ê°œì˜ ë‹µë³€ì„ ë§Œë“œì„¸ìš”
   - ROUGE, BLEU, ì„ë² ë”© ìœ ì‚¬ë„ë¡œ ê°ê° í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•˜ì„¸ìš”

### ì‘ìš© ì‹¤ìŠµ
3. **ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ**
   - OpenAI, Google Gemini, Ollama ëª¨ë¸ë¡œ ë™ì¼í•œ ì§ˆë¬¸ì— ë‹µë³€ ìƒì„±
   - ê° ëª¨ë¸ì˜ ë‹µë³€ì„ ë‹¤ì–‘í•œ ì§€í‘œë¡œ í‰ê°€í•˜ê³  ì„±ëŠ¥í‘œ ì‘ì„±

4. **ê²€ìƒ‰ê¸° ì„±ëŠ¥ ë¹„êµ**
   - BM25, Vector, Hybrid ê²€ìƒ‰ê¸°ë¥¼ ê°ê° ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•
   - ê²€ìƒ‰ í’ˆì§ˆì´ ìµœì¢… ë‹µë³€ í’ˆì§ˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„

### ì‹¬í™” ì‹¤ìŠµ
5. **ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ**
   - íœ´ë¦¬ìŠ¤í‹±, ì •ëŸ‰ì , ì˜ë¯¸ì  í‰ê°€ë¥¼ í†µí•©í•œ ì¢…í•© ì ìˆ˜ ì‹œìŠ¤í…œ ê°œë°œ
   - ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ë©° ìµœì ì˜ í‰ê°€ ëª¨ë¸ ì°¾ê¸°

## âœ… ì†”ë£¨ì…˜ ì˜ˆì‹œ

### ì‹¤ìŠµ 1: ì¢…í•© í‰ê°€ í•¨ìˆ˜
```python
def comprehensive_evaluation(reference: str, generated: str) -> dict:
    """
    ëª¨ë“  í‰ê°€ ì§€í‘œë¥¼ ì¢…í•©í•œ í‰ê°€ í•¨ìˆ˜
    """
    results = {}

    # íœ´ë¦¬ìŠ¤í‹± í‰ê°€
    heuristic_results = evaluate_response_quality(generated)
    results.update({f"heuristic_{k}": v for k, v in heuristic_results.items()})

    # ROUGE ì ìˆ˜
    rouge_results = calculate_rouge_scores(reference, generated)
    results.update(rouge_results)

    # BLEU ì ìˆ˜
    bleu_results = calculate_bleu_score(reference, generated)
    results.update(bleu_results)

    # ì˜ë¯¸ì  ìœ ì‚¬ë„
    similarity_results = calculate_semantic_similarity(reference, generated)
    results.update(similarity_results)

    # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
    weights = {
        'quality': 0.2,
        'rouge': 0.3,
        'bleu': 0.2,
        'semantic': 0.3
    }

    quality_score = heuristic_results['overall_score']
    rouge_score = rouge_results['rouge1_f1']
    bleu_score = bleu_results['bleu_overall']
    semantic_score = similarity_results['cosine_similarity']

    overall_score = (
        weights['quality'] * quality_score +
        weights['rouge'] * rouge_score +
        weights['bleu'] * bleu_score +
        weights['semantic'] * semantic_score
    )

    results['comprehensive_score'] = overall_score

    return results

# ì‚¬ìš© ì˜ˆì‹œ
evaluation_results = comprehensive_evaluation(reference, generated)
print("ì¢…í•© í‰ê°€ ê²°ê³¼:", evaluation_results)
```

### ì‹¤ìŠµ 2: ë°°ì¹˜ í‰ê°€ ì‹œìŠ¤í…œ
```python
def batch_evaluate_rag_system(test_questions: List[str],
                             reference_answers: List[str],
                             rag_chain) -> pd.DataFrame:
    """
    ë°°ì¹˜ë¡œ RAG ì‹œìŠ¤í…œ í‰ê°€
    """
    results = []

    for i, (question, reference) in enumerate(zip(test_questions, reference_answers)):
        print(f"í‰ê°€ ì¤‘... {i+1}/{len(test_questions)}")

        # RAGë¡œ ë‹µë³€ ìƒì„±
        try:
            generated = rag_chain.invoke(question)

            # ì¢…í•© í‰ê°€ ì‹¤í–‰
            eval_result = comprehensive_evaluation(reference, generated)
            eval_result['question'] = question
            eval_result['reference'] = reference
            eval_result['generated'] = generated
            eval_result['success'] = True

        except Exception as e:
            eval_result = {
                'question': question,
                'reference': reference,
                'generated': f"ì˜¤ë¥˜: {str(e)}",
                'success': False,
                'comprehensive_score': 0.0
            }

        results.append(eval_result)

    df_results = pd.DataFrame(results)
    return df_results

# ì‚¬ìš© ì˜ˆì‹œ
test_questions = [
    "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
    "ì „ê¸°ì°¨ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ììœ¨ì£¼í–‰ ê¸°ìˆ ì˜ í˜„ ìƒí™©ì€?"
]

reference_answers = [
    "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤.",
    "ì „ê¸°ì°¨ëŠ” í™˜ê²½ ì¹œí™”ì ì´ê³  ì—°ë£Œë¹„ê°€ ì ˆì•½ë©ë‹ˆë‹¤.",
    "ììœ¨ì£¼í–‰ ê¸°ìˆ ì€ í˜„ì¬ ë ˆë²¨ 2-3 ë‹¨ê³„ì…ë‹ˆë‹¤."
]

evaluation_df = batch_evaluate_rag_system(test_questions, reference_answers, rag_chain)
print("ë°°ì¹˜ í‰ê°€ ì™„ë£Œ!")
print(evaluation_df[['question', 'comprehensive_score', 'rouge1_f1', 'cosine_similarity']].head())
```

### ì‹¤ìŠµ 3: ì„±ëŠ¥ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ
```python
import matplotlib.pyplot as plt
import seaborn as sns

def create_evaluation_dashboard(df_results: pd.DataFrame):
    """
    í‰ê°€ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” ëŒ€ì‹œë³´ë“œ ìƒì„±
    """
    plt.rcParams['font.family'] = 'DejaVu Sans'  # í•œê¸€ í°íŠ¸ ì„¤ì •

    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # 1. ì¢…í•© ì ìˆ˜ ë¶„í¬
    axes[0,0].hist(df_results['comprehensive_score'], bins=10, alpha=0.7)
    axes[0,0].set_title('ì¢…í•© ì ìˆ˜ ë¶„í¬')
    axes[0,0].set_xlabel('ì ìˆ˜')
    axes[0,0].set_ylabel('ë¹ˆë„')

    # 2. ë©”íŠ¸ë¦­ë³„ ìƒê´€ê´€ê³„
    metrics = ['rouge1_f1', 'bleu_overall', 'cosine_similarity', 'comprehensive_score']
    correlation_matrix = df_results[metrics].corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=axes[0,1])
    axes[0,1].set_title('ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„')

    # 3. ì§ˆë¬¸ë³„ ì„±ëŠ¥
    axes[1,0].bar(range(len(df_results)), df_results['comprehensive_score'])
    axes[1,0].set_title('ì§ˆë¬¸ë³„ ì„±ëŠ¥')
    axes[1,0].set_xlabel('ì§ˆë¬¸ ë²ˆí˜¸')
    axes[1,0].set_ylabel('ì¢…í•© ì ìˆ˜')

    # 4. ë©”íŠ¸ë¦­ ë¹„êµ
    metrics_to_plot = ['rouge1_f1', 'bleu_overall', 'cosine_similarity']
    x = range(len(df_results))
    width = 0.25

    for i, metric in enumerate(metrics_to_plot):
        axes[1,1].bar([xi + width*i for xi in x], df_results[metric],
                     width, label=metric, alpha=0.8)

    axes[1,1].set_title('ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ ë¹„êµ')
    axes[1,1].set_xlabel('ì§ˆë¬¸ ë²ˆí˜¸')
    axes[1,1].set_ylabel('ì ìˆ˜')
    axes[1,1].legend()

    plt.tight_layout()
    plt.show()

    # ìš”ì•½ í†µê³„
    print("\n=== í‰ê°€ ê²°ê³¼ ìš”ì•½ ===")
    print(f"í‰ê·  ì¢…í•© ì ìˆ˜: {df_results['comprehensive_score'].mean():.3f}")
    print(f"í‰ê·  ROUGE-1 F1: {df_results['rouge1_f1'].mean():.3f}")
    print(f"í‰ê·  BLEU: {df_results['bleu_overall'].mean():.3f}")
    print(f"í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {df_results['cosine_similarity'].mean():.3f}")
    print(f"ì„±ê³µë¥ : {df_results['success'].mean():.1%}")

# ì‚¬ìš© ì˜ˆì‹œ
create_evaluation_dashboard(evaluation_df)
```

## ğŸš€ ì‹¤ë¬´ í™œìš© ì˜ˆì‹œ

### 1. A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ

```python
class RAGABTester:
    def __init__(self):
        self.models = {}
        self.test_results = []

    def register_model(self, name: str, rag_chain):
        """ëª¨ë¸ ë“±ë¡"""
        self.models[name] = rag_chain

    def run_ab_test(self, test_questions: List[str],
                    reference_answers: List[str]) -> pd.DataFrame:
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        all_results = []

        for model_name, rag_chain in self.models.items():
            print(f"\n{model_name} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")

            model_results = batch_evaluate_rag_system(
                test_questions, reference_answers, rag_chain
            )
            model_results['model'] = model_name
            all_results.append(model_results)

        combined_results = pd.concat(all_results, ignore_index=True)
        return combined_results

    def analyze_results(self, results_df: pd.DataFrame):
        """ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±"""
        model_performance = results_df.groupby('model').agg({
            'comprehensive_score': ['mean', 'std'],
            'rouge1_f1': 'mean',
            'bleu_overall': 'mean',
            'cosine_similarity': 'mean',
            'success': 'mean'
        }).round(3)

        print("\n=== ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ===")
        print(model_performance)

        # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        from scipy import stats
        models = results_df['model'].unique()

        if len(models) == 2:
            model_a = results_df[results_df['model'] == models[0]]['comprehensive_score']
            model_b = results_df[results_df['model'] == models[1]]['comprehensive_score']

            t_stat, p_value = stats.ttest_ind(model_a, model_b)
            print(f"\ní†µê³„ì  ìœ ì˜ì„± ê²€ì •:")
            print(f"t-statistic: {t_stat:.4f}")
            print(f"p-value: {p_value:.4f}")
            print(f"ìœ ì˜ë¯¸í•œ ì°¨ì´: {'ì˜ˆ' if p_value < 0.05 else 'ì•„ë‹ˆì˜¤'}")

        return model_performance

# ì‚¬ìš© ì˜ˆì‹œ
ab_tester = RAGABTester()
ab_tester.register_model("OpenAI", openai_rag_chain)
ab_tester.register_model("Gemini", gemini_rag_chain)

ab_results = ab_tester.run_ab_test(test_questions, reference_answers)
performance_summary = ab_tester.analyze_results(ab_results)
```

### 2. ì§€ì†ì  í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

```python
class RAGQualityMonitor:
    def __init__(self, quality_threshold: float = 0.7):
        self.quality_threshold = quality_threshold
        self.performance_history = []
        self.alerts = []

    def evaluate_single_response(self, question: str, reference: str,
                                generated: str) -> dict:
        """ë‹¨ì¼ ì‘ë‹µ í‰ê°€"""
        try:
            evaluation = comprehensive_evaluation(reference, generated)
            evaluation['timestamp'] = pd.Timestamp.now()
            evaluation['question'] = question
            evaluation['reference'] = reference
            evaluation['generated'] = generated

            # í’ˆì§ˆ ì„ê³„ê°’ í™•ì¸
            if evaluation['comprehensive_score'] < self.quality_threshold:
                self.alerts.append({
                    'timestamp': evaluation['timestamp'],
                    'issue': 'Low Quality Response',
                    'score': evaluation['comprehensive_score'],
                    'question': question
                })

            self.performance_history.append(evaluation)
            return evaluation

        except Exception as e:
            error_eval = {
                'timestamp': pd.Timestamp.now(),
                'question': question,
                'reference': reference,
                'generated': generated,
                'error': str(e),
                'comprehensive_score': 0.0
            }
            self.alerts.append({
                'timestamp': error_eval['timestamp'],
                'issue': 'System Error',
                'error': str(e),
                'question': question
            })
            return error_eval

    def get_performance_report(self, days: int = 7) -> dict:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.performance_history:
            return {"message": "í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        df_history = pd.DataFrame(self.performance_history)
        cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=days)
        recent_data = df_history[df_history['timestamp'] > cutoff_date]

        if recent_data.empty:
            return {"message": f"ìµœê·¼ {days}ì¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        report = {
            'period': f'ìµœê·¼ {days}ì¼',
            'total_evaluations': len(recent_data),
            'average_score': recent_data['comprehensive_score'].mean(),
            'score_trend': 'improving' if recent_data['comprehensive_score'].corr(
                range(len(recent_data))) > 0 else 'declining',
            'quality_rate': (recent_data['comprehensive_score'] >= self.quality_threshold).mean(),
            'recent_alerts': len([a for a in self.alerts
                                if a['timestamp'] > cutoff_date])
        }

        return report

    def plot_performance_trend(self, days: int = 30):
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ì‹œê°í™”"""
        if not self.performance_history:
            print("í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df_history = pd.DataFrame(self.performance_history)
        cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=days)
        recent_data = df_history[df_history['timestamp'] > cutoff_date]

        if recent_data.empty:
            print(f"ìµœê·¼ {days}ì¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        plt.figure(figsize=(12, 6))
        plt.plot(recent_data['timestamp'], recent_data['comprehensive_score'],
                marker='o', alpha=0.7)
        plt.axhline(y=self.quality_threshold, color='r', linestyle='--',
                   label=f'í’ˆì§ˆ ì„ê³„ê°’ ({self.quality_threshold})')
        plt.title(f'RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ íŠ¸ë Œë“œ (ìµœê·¼ {days}ì¼)')
        plt.xlabel('ë‚ ì§œ')
        plt.ylabel('ì¢…í•© ì ìˆ˜')
        plt.legend()
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

# ì‚¬ìš© ì˜ˆì‹œ
quality_monitor = RAGQualityMonitor(quality_threshold=0.75)

# ì‹¤ì œ ìš´ì˜ ì¤‘ í‰ê°€
for question, reference in zip(test_questions, reference_answers):
    generated = rag_chain.invoke(question)
    evaluation = quality_monitor.evaluate_single_response(question, reference, generated)

# ì„±ëŠ¥ ë¦¬í¬íŠ¸ í™•ì¸
performance_report = quality_monitor.get_performance_report()
print("ì„±ëŠ¥ ë¦¬í¬íŠ¸:", performance_report)

# íŠ¸ë Œë“œ ì‹œê°í™”
quality_monitor.plot_performance_trend(days=30)
```

### 3. ìë™ í‰ê°€ íŒŒì´í”„ë¼ì¸

```python
from typing import Callable, Dict, Any
import json
from datetime import datetime

class AutoEvaluationPipeline:
    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.evaluation_history = []
        self.benchmarks = {}

    def _load_config(self, config_path: str) -> dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        default_config = {
            'evaluation_metrics': ['rouge', 'bleu', 'semantic', 'heuristic'],
            'quality_thresholds': {
                'rouge1_f1': 0.3,
                'bleu_overall': 0.2,
                'cosine_similarity': 0.7,
                'comprehensive_score': 0.6
            },
            'alert_settings': {
                'enable_alerts': True,
                'alert_threshold': 0.5,
                'consecutive_failures': 3
            }
        }

        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                custom_config = json.load(f)
            default_config.update(custom_config)

        return default_config

    def register_benchmark(self, name: str, test_data: Dict[str, Any]):
        """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë“±ë¡"""
        self.benchmarks[name] = {
            'questions': test_data['questions'],
            'references': test_data['references'],
            'created_at': datetime.now()
        }

    def run_evaluation(self, rag_chain, benchmark_name: str) -> Dict[str, Any]:
        """ìë™ í‰ê°€ ì‹¤í–‰"""
        if benchmark_name not in self.benchmarks:
            raise ValueError(f"ë²¤ì¹˜ë§ˆí¬ '{benchmark_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        benchmark = self.benchmarks[benchmark_name]
        questions = benchmark['questions']
        references = benchmark['references']

        print(f"ë²¤ì¹˜ë§ˆí¬ '{benchmark_name}' í‰ê°€ ì‹œì‘...")

        results = []
        failed_count = 0

        for i, (question, reference) in enumerate(zip(questions, references)):
            try:
                # RAG ì²´ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„±
                generated = rag_chain.invoke(question)

                # ì¢…í•© í‰ê°€
                evaluation = comprehensive_evaluation(reference, generated)
                evaluation.update({
                    'question_id': i,
                    'question': question,
                    'reference': reference,
                    'generated': generated,
                    'success': True
                })

                # í’ˆì§ˆ ì„ê³„ê°’ í™•ì¸
                quality_checks = {}
                for metric, threshold in self.config['quality_thresholds'].items():
                    if metric in evaluation:
                        quality_checks[f'{metric}_pass'] = evaluation[metric] >= threshold

                evaluation.update(quality_checks)
                results.append(evaluation)

                print(f"ì§„í–‰ë¥ : {i+1}/{len(questions)} âœ“")

            except Exception as e:
                failed_count += 1
                error_result = {
                    'question_id': i,
                    'question': question,
                    'reference': reference,
                    'generated': f'ERROR: {str(e)}',
                    'success': False,
                    'comprehensive_score': 0.0,
                    'error': str(e)
                }
                results.append(error_result)
                print(f"ì§„í–‰ë¥ : {i+1}/{len(questions)} âœ— (ì˜¤ë¥˜: {str(e)[:50]}...)")

        # í‰ê°€ ê²°ê³¼ ìš”ì•½
        evaluation_summary = self._summarize_results(results, benchmark_name)

        # íˆìŠ¤í† ë¦¬ì— ì €ì¥
        self.evaluation_history.append({
            'benchmark_name': benchmark_name,
            'timestamp': datetime.now(),
            'results': results,
            'summary': evaluation_summary
        })

        return evaluation_summary

    def _summarize_results(self, results: List[Dict], benchmark_name: str) -> Dict:
        """í‰ê°€ ê²°ê³¼ ìš”ì•½"""
        df_results = pd.DataFrame(results)

        summary = {
            'benchmark_name': benchmark_name,
            'timestamp': datetime.now().isoformat(),
            'total_questions': len(results),
            'success_rate': df_results['success'].mean(),
            'failed_count': (~df_results['success']).sum(),
            'metrics': {}
        }

        # ì„±ê³µí•œ ì¼€ì´ìŠ¤ì— ëŒ€í•œ ë©”íŠ¸ë¦­ ê³„ì‚°
        successful_results = df_results[df_results['success'] == True]

        if len(successful_results) > 0:
            metric_columns = ['comprehensive_score', 'rouge1_f1', 'bleu_overall', 'cosine_similarity']

            for metric in metric_columns:
                if metric in successful_results.columns:
                    summary['metrics'][metric] = {
                        'mean': float(successful_results[metric].mean()),
                        'std': float(successful_results[metric].std()),
                        'min': float(successful_results[metric].min()),
                        'max': float(successful_results[metric].max())
                    }

        # í’ˆì§ˆ ì„ê³„ê°’ í†µê³¼ìœ¨
        threshold_metrics = {}
        for metric, threshold in self.config['quality_thresholds'].items():
            pass_column = f'{metric}_pass'
            if pass_column in df_results.columns:
                threshold_metrics[metric] = {
                    'threshold': threshold,
                    'pass_rate': float(df_results[pass_column].mean())
                }

        summary['threshold_analysis'] = threshold_metrics

        # ì•Œë¦¼ ì¡°ê±´ í™•ì¸
        if self.config['alert_settings']['enable_alerts']:
            overall_score = summary['metrics'].get('comprehensive_score', {}).get('mean', 0)
            if overall_score < self.config['alert_settings']['alert_threshold']:
                summary['alert'] = {
                    'type': 'LOW_PERFORMANCE',
                    'message': f"í‰ê·  ì ìˆ˜({overall_score:.3f})ê°€ ì„ê³„ê°’({self.config['alert_settings']['alert_threshold']})ë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤.",
                    'severity': 'HIGH' if overall_score < 0.3 else 'MEDIUM'
                }

        return summary

    def export_results(self, filepath: str, format: str = 'json'):
        """í‰ê°€ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        if not self.evaluation_history:
            print("ë‚´ë³´ë‚¼ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if format.lower() == 'json':
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.evaluation_history, f, ensure_ascii=False, indent=2, default=str)
        elif format.lower() == 'csv':
            # ëª¨ë“  í‰ê°€ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹˜ê¸°
            all_results = []
            for evaluation in self.evaluation_history:
                for result in evaluation['results']:
                    result['evaluation_timestamp'] = evaluation['timestamp']
                    result['benchmark_name'] = evaluation['benchmark_name']
                    all_results.append(result)

            df_all = pd.DataFrame(all_results)
            df_all.to_csv(filepath, index=False, encoding='utf-8-sig')

        print(f"í‰ê°€ ê²°ê³¼ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì‚¬ìš© ì˜ˆì‹œ
pipeline = AutoEvaluationPipeline()

# ë²¤ì¹˜ë§ˆí¬ ë“±ë¡
benchmark_data = {
    'questions': test_questions,
    'references': reference_answers
}
pipeline.register_benchmark('tesla_qa_v1', benchmark_data)

# ìë™ í‰ê°€ ì‹¤í–‰
evaluation_summary = pipeline.run_evaluation(rag_chain, 'tesla_qa_v1')
print("ìë™ í‰ê°€ ì™„ë£Œ!")
print(json.dumps(evaluation_summary, indent=2, ensure_ascii=False, default=str))

# ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
pipeline.export_results('evaluation_results.json')
pipeline.export_results('evaluation_results.csv', format='csv')
```

## ğŸ“– ì°¸ê³  ìë£Œ

### í‰ê°€ ë©”íŠ¸ë¦­ ê´€ë ¨
- [ROUGE ë…¼ë¬¸ ì›ë³¸](https://aclanthology.org/W04-1013/)
- [BLEU ë…¼ë¬¸ ì›ë³¸](https://www.aclweb.org/anthology/P02-1040/)
- [RAG í‰ê°€ ë°©ë²•ë¡  ìµœì‹  ì—°êµ¬](https://arxiv.org/abs/2405.07437)

### LangChain í‰ê°€ ë„êµ¬
- [LangChain Evaluation Guide](https://python.langchain.com/docs/guides/evaluation/)
- [LangSmith Evaluation Docs](https://docs.smith.langchain.com/evaluation)

### í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬
- [KiWi í† í¬ë‚˜ì´ì €](https://github.com/bab2min/kiwipiepy)
- [í•œêµ­ì–´ ROUGE êµ¬í˜„](https://github.com/gucci-j/korouge-score)

### ì¶”ê°€ í•™ìŠµ ìë£Œ
- [RAG ì‹œìŠ¤í…œ í‰ê°€ best practices](https://python.langchain.com/docs/use_cases/question_answering/evaluation/)
- [ë©€í‹°ëª¨ë‹¬ RAG í‰ê°€ ë°©ë²•](https://python.langchain.com/docs/integrations/retrievers/)
- [í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§](https://docs.smith.langchain.com/tracing)

ì´ ê°€ì´ë“œë¥¼ í†µí•´ RAG ì‹œìŠ¤í…œì˜ í’ˆì§ˆì„ ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³ , ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆëŠ” ì²´ê³„ì ì¸ í‰ê°€ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œëŠ” ì—¬ëŸ¬ í‰ê°€ ì§€í‘œë¥¼ ì¡°í•©í•˜ì—¬ ê· í˜•ì¡íŒ í’ˆì§ˆ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.