# PRJ02_W1_002 ì •ë³´ ê²€ìƒ‰ í‰ê°€ì§€í‘œ ë§¤ë‰´ì–¼

## ğŸ“‹ ê°œìš”

ì´ ë…¸íŠ¸ë¶ì€ ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í•µì‹¬ ì§€í‘œë“¤(Hit Rate, MRR, NDCG, mAP)ì„ ì‹¬ë„ ìˆê²Œ ë‹¤ë£¹ë‹ˆë‹¤. K-RAG íŒ¨í‚¤ì§€ë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì œ ê²€ìƒ‰ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³  ë¶„ì„í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ¯ í•™ìŠµ ëª©í‘œ
- Hit Rate, MRR, NDCG, mAP ë“± í•µì‹¬ ê²€ìƒ‰ í‰ê°€ ì§€í‘œì˜ ì´í•´
- K-RAG íŒ¨í‚¤ì§€ë¥¼ í™œìš©í•œ ì‹¤ì œ í‰ê°€ ìˆ˜í–‰
- Precision, Recall, F1 Scoreì˜ Micro/Macro Average ê³„ì‚°
- ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ì˜ ì‹¤ë¬´ì  ì ìš©

## ğŸ› ï¸ í™˜ê²½ ì„¤ì •

### 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# K-RAG íŒ¨í‚¤ì§€ ì„¤ì¹˜
uv pip install krag
```

### 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
```python
# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
from glob import glob
from pprint import pprint
import json
import numpy as np
from typing import List, Tuple

# LangChain ê´€ë ¨
from langchain_core.documents import Document
from textwrap import dedent

# K-RAG í‰ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬
from krag.evaluators import (
    OfflineRetrievalEvaluators,
    RougeOfflineRetrievalEvaluators,
)

# Langfuse íŠ¸ë ˆì´ì‹±
from langfuse.langchain import CallbackHandler
```

## ğŸ“Š í‰ê°€ ì§€í‘œ ê°œë…

### 1. ê²€ìƒ‰ í‰ê°€ì˜ ë‘ ê°€ì§€ ì ‘ê·¼ë²•

#### Non-Rank Based Metrics
- **Accuracy, Precision, Recall@k**: ê´€ë ¨ì„±ì˜ ì´ì§„ì  í‰ê°€
- ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ” ê¸°ë³¸ì ì¸ í‰ê°€ ë°©ì‹

#### Rank-Based Metrics
- **MRR, MAP, NDCG**: ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœìœ„ë¥¼ ê³ ë ¤í•œ í‰ê°€
- ì‹¤ì œ ì‚¬ìš©ì ê²½í—˜ì„ ë” ì˜ ë°˜ì˜

### 2. ìƒì„± í‰ê°€ ë°©ì‹

#### ì „í†µì  í‰ê°€
- **ROUGE**: ìš”ì•½ í’ˆì§ˆ ì¸¡ì •
- **BLEU**: ë²ˆì—­ í’ˆì§ˆ ì¸¡ì •
- **BertScore**: ì˜ë¯¸ ìœ ì‚¬ë„ ì¸¡ì •

#### LLM ê¸°ë°˜ í‰ê°€
- ì‘ì§‘ì„±, ê´€ë ¨ì„±, ìœ ì°½ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨
- ì°¸ì¡° ë‹µë³€ì´ ì—†ëŠ” ê²½ìš°ì—ë„ í‰ê°€ ê°€ëŠ¥

## ğŸ¯ í‰ê°€ ë°ì´í„° ì¤€ë¹„

### 1. ìƒ˜í”Œ ë¬¸ì„œ ë°ì´í„° êµ¬ì„±

```python
# ì‹¤ì œ ë¬¸ì„œ (ì •ë‹µ)
actual_docs = [
    [  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ì˜ ì •ë‹µ ë¬¸ì„œë“¤
        Document(
            page_content="ê³ ê° ë¬¸ì˜: ì œí’ˆ ë°°ì†¡ ì§€ì—°\n...",
            metadata={"id": "doc1", "category": "ë°°ì†¡", "priority": "ë†’ìŒ"}
        ),
        Document(
            page_content="ê³ ê° ë¬¸ì˜: ê²°ì œ ì˜¤ë¥˜\n...",
            metadata={"id": "doc2", "category": "ê²°ì œ", "priority": "ë†’ìŒ"}
        ),
        # ... ì¶”ê°€ ë¬¸ì„œë“¤
    ],
    [  # ë‘ ë²ˆì§¸ ì¿¼ë¦¬ì˜ ì •ë‹µ ë¬¸ì„œë“¤
        Document(
            page_content="ê³ ê° ë¬¸ì˜: ì œí’ˆ êµí™˜ ìš”ì²­\n...",
            metadata={"id": "doc3", "category": "êµí™˜/ë°˜í’ˆ", "priority": "ì¤‘ê°„"}
        ),
        # ... ì¶”ê°€ ë¬¸ì„œë“¤
    ]
]

# ì˜ˆì¸¡ ë¬¸ì„œ (ê²€ìƒ‰ ê²°ê³¼)
predicted_docs = [
    [  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ì˜ ê²€ìƒ‰ ê²°ê³¼
        # ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë“¤
    ],
    [  # ë‘ ë²ˆì§¸ ì¿¼ë¦¬ì˜ ê²€ìƒ‰ ê²°ê³¼
        # ì¼ë¶€ ì˜¤ë‹µ í¬í•¨ëœ ê²°ê³¼ë“¤
    ]
]
```

## ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°

### 1. TP, FP, FN ê°œë…

```python
# True Positive: ì •í™•í•˜ê²Œ ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ
true_positives = [
    [doc.metadata["id"] for doc in actual if doc in predicted]
    for actual, predicted in zip(actual_docs, predicted_docs)
]

# False Positive: ì˜ëª» ê²€ìƒ‰ëœ ë¬´ê´€í•œ ë¬¸ì„œ
false_positives = [
    [doc.metadata["id"] for doc in predicted if doc not in actual]
    for actual, predicted in zip(actual_docs, predicted_docs)
]

# False Negative: ë†“ì¹œ ê´€ë ¨ ë¬¸ì„œ
false_negatives = [
    [doc.metadata["id"] for doc in actual if doc not in predicted]
    for actual, predicted in zip(actual_docs, predicted_docs)
]
```

### 2. Precision, Recall, F1 ê³„ì‚°

```python
# ê° ì¿¼ë¦¬ë³„ ì„±ëŠ¥ ê³„ì‚°
for i, (tp, fp, fn) in enumerate(zip(true_positives, false_positives, false_negatives)):
    precision = len(tp) / (len(tp) + len(fp)) if len(tp) + len(fp) > 0 else 0
    recall = len(tp) / (len(tp) + len(fn)) if len(tp) + len(fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0

    print(f"ì¿¼ë¦¬ {i+1}: Precision={precision:.2f}, Recall={recall:.2f}, F1={f1:.2f}")
```

### 3. Macro vs Micro Average

#### Macro Average (ê° í´ë˜ìŠ¤ ë™ë“± ê°€ì¤‘)
```python
def calculate_macro_metrics(true_positives, false_positives, false_negatives):
    n_classes = len(true_positives)
    precisions, recalls, f1_scores = [], [], []

    for i in range(n_classes):
        tp, fp, fn = len(true_positives[i]), len(false_positives[i]), len(false_negatives[i])

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        precisions.append(precision)
        recalls.append(recall)
        f1_scores.append(f1)

    return np.mean(precisions), np.mean(recalls), np.mean(f1_scores)
```

#### Micro Average (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
```python
def calculate_micro_metrics(true_positives, false_positives, false_negatives):
    # ì „ì²´ TP, FP, FN í•©ê³„
    total_tp = sum(len(tp) for tp in true_positives)
    total_fp = sum(len(fp) for fp in false_positives)
    total_fn = sum(len(fn) for fn in false_negatives)

    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
    micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) \
        if (micro_precision + micro_recall) > 0 else 0

    return micro_precision, micro_recall, micro_f1
```

## ğŸ” K-RAG íŒ¨í‚¤ì§€ í™œìš©

### 1. RAG í‰ê°€ì í´ë˜ìŠ¤ êµ¬ì„±

```python
class RAGEvaluator:
    def __init__(self, match_method="text", rouge_threshold=0.5):
        self.match_method = match_method  # "text", "rouge1", "rouge2", "rougeL"
        self.rouge_threshold = rouge_threshold
        self.evaluator = None

    def _initialize_evaluator(self, actual_docs, predicted_docs):
        if self.match_method in ["rouge1", "rouge2", "rougeL"]:
            self.evaluator = RougeOfflineRetrievalEvaluators(
                actual_docs, predicted_docs,
                match_method=self.match_method,
                threshold=self.rouge_threshold
            )
        else:
            self.evaluator = OfflineRetrievalEvaluators(
                actual_docs, predicted_docs,
                match_method=self.match_method,
            )
        return self.evaluator

    def evaluate_all(self, actual_docs, predicted_docs, k=10, visualize=False):
        if self.evaluator is None:
            self._initialize_evaluator(actual_docs, predicted_docs)

        results = {
            'hit_rate': self.evaluator.calculate_hit_rate(k=k),
            'mrr': self.evaluator.calculate_mrr(k=k),
            'recall': self.evaluator.calculate_recall(k=k),
            'precision': self.evaluator.calculate_precision(k=k),
            'f1_score': self.evaluator.calculate_f1_score(k=k),
            'map': self.evaluator.calculate_map(k=k),
            'ndcg': self.evaluator.calculate_ndcg(k=k)
        }

        return results
```

### 2. í‰ê°€ ì‹¤í–‰

```python
# í…ìŠ¤íŠ¸ ì¼ì¹˜ ê¸°ë°˜ í‰ê°€
evaluator = RAGEvaluator(match_method="text")
results = evaluator.evaluate_all(actual_docs, predicted_docs, k=10)

# ROUGE ê¸°ë°˜ í‰ê°€ (ì˜ë¯¸ì  ìœ ì‚¬ì„±)
rouge_evaluator = RAGEvaluator(match_method="rouge2", rouge_threshold=0.8)
rouge_results = rouge_evaluator.evaluate_all(actual_docs, predicted_docs, k=10)
```

## ğŸ“Š í•µì‹¬ í‰ê°€ ì§€í‘œ ìƒì„¸

### 1. Hit Rate (ì ì¤‘ë¥ )

**ê°œë…**: ê²€ìƒ‰ ê²°ê³¼ì— ì •ë‹µ ë¬¸ì„œê°€ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ì´ì§„ë²•ìœ¼ë¡œ í‰ê°€

**ê³„ì‚° ë°©ì‹**:
```python
# k=3ì¼ ë•Œ ì˜ˆì‹œ
# ì¿¼ë¦¬ 1: [doc1, doc2, doc5] ëª¨ë‘ ì •ë‹µ â†’ 1
# ì¿¼ë¦¬ 2: doc3 ëˆ„ë½, doc4ë§Œ ì°¾ìŒ â†’ 0
hit_rate = (1 + 0) / 2 = 0.5
```

**íŠ¹ì§•**:
- 0~1 ì‚¬ì´ ê°’, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìš°ìˆ˜
- ìˆœì„œ ê³ ë ¤í•˜ì§€ ì•ŠìŒ
- ê¸°ë³¸ì ì¸ ê²€ìƒ‰ ì„±ëŠ¥ ì§€í‘œ

### 2. MRR (Mean Reciprocal Rank)

**ê°œë…**: ì²« ë²ˆì§¸ ê´€ë ¨ ë¬¸ì„œì˜ ë“±ì¥ ìˆœìœ„ ì—­ìˆ˜ì˜ í‰ê· 

**ê³„ì‚° ë°©ì‹**:
```python
# ì¿¼ë¦¬ 1: doc1ì´ 1ìœ„ â†’ 1/1 = 1.0
# ì¿¼ë¦¬ 2: doc4ê°€ 2ìœ„ â†’ 1/2 = 0.5
MRR = (1.0 + 0.5) / 2 = 0.75
```

**íŠ¹ì§•**:
- ì‚¬ìš©ì ê²½í—˜ ê´€ì ì—ì„œ ì¤‘ìš”
- ì²« ë²ˆì§¸ ì •ë‹µì˜ ìœ„ì¹˜ë§Œ ê³ ë ¤
- ë¹ ë¥¸ ì •ë³´ ì ‘ê·¼ì„± ì¸¡ì •

### 3. mAP@k (Mean Average Precision)

**ê°œë…**: ìƒìœ„ kê°œ ê²°ê³¼ ë‚´ì—ì„œ ê´€ë ¨ ë¬¸ì„œë“¤ì˜ ì •í™•ë„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€

**ê³„ì‚° ë°©ì‹**:
```mathematica
AP = (ê° ì •ë‹µ ë¬¸ì„œ ìœ„ì¹˜ì—ì„œì˜ Precisionì˜ í•©) / (ì „ì²´ ì •ë‹µ ë¬¸ì„œ ìˆ˜)
mAP = ëª¨ë“  ì¿¼ë¦¬ì˜ AP í‰ê· 
```

**ì˜ˆì‹œ**:
```python
# ì¿¼ë¦¬ 1: [doc1, doc2, doc5] ëª¨ë‘ ì •ë‹µ
# P(1)=1/1, P(2)=2/2, P(3)=3/3
# AP = (1 + 1 + 1) / 3 = 1.0

# ì¿¼ë¦¬ 2: [doc6, doc4, doc5], ì •ë‹µì€ [doc3, doc4]
# P(2)=1/2 (doc4ë§Œ ì •ë‹µ)
# AP = (0.5) / 2 = 0.25

# mAP = (1.0 + 0.25) / 2 = 0.625
```

### 4. NDCG (Normalized Discounted Cumulative Gain)

**ê°œë…**: ë¬¸ì„œì˜ ê´€ë ¨ì„±ê³¼ ê²€ìƒ‰ ìˆœìœ„ë¥¼ ë™ì‹œì— ê³ ë ¤í•œ ì •ê·œí™” ì ìˆ˜

**ê³„ì‚° ë°©ì‹**:
```mathematica
DCG@k = Î£(i=1 to k) (2^rel_i - 1) / logâ‚‚(i+1)
NDCG@k = DCG@k / IDCG@k
```

**íŠ¹ì§•**:
- ìƒìœ„ ê²°ê³¼ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
- ì´ìƒì  ìˆœìœ„ì™€ ë¹„êµí•˜ì—¬ ì •ê·œí™”
- 0~1 ì‚¬ì´ ê°’, 1ì´ ì™„ë²½í•œ ìˆœìœ„

## ğŸ’¡ í‰ê°€ ì§€í‘œ ë¹„êµ

| ì§€í‘œ | ëª©ì  | ìˆœìœ„ ê³ ë ¤ | ê³„ì‚° ë³µì¡ë„ | í™œìš© ìƒí™© |
|------|------|-----------|-------------|-----------|
| **Hit Rate** | ê¸°ë³¸ í¬í•¨ ì—¬ë¶€ | âŒ | ë‚®ìŒ | ë¹ ë¥¸ ì„±ëŠ¥ ì²´í¬ |
| **MRR** | ì²« ì •ë‹µ ìœ„ì¹˜ | âœ… | ë‚®ìŒ | QA ì‹œìŠ¤í…œ |
| **mAP** | ì „ì²´ ì •í™•ë„ | âœ… | ì¤‘ê°„ | ì¼ë°˜ ê²€ìƒ‰ |
| **NDCG** | ìˆœìœ„ í’ˆì§ˆ | âœ… | ë†’ìŒ | ì¶”ì²œ ì‹œìŠ¤í…œ |

## ğŸ› ï¸ ì‹¤ë¬´ í™œìš© íŒ

### 1. ì§€í‘œ ì„ íƒ ê°€ì´ë“œ

```python
# ê¸°ë³¸ ì„±ëŠ¥ í™•ì¸
if quick_check:
    use_metrics = ['hit_rate']

# QA ì‹œìŠ¤í…œ
elif system_type == 'qa':
    use_metrics = ['mrr', 'hit_rate']

# ì¼ë°˜ ê²€ìƒ‰ ì—”ì§„
elif system_type == 'search':
    use_metrics = ['map', 'ndcg', 'mrr']

# ì¶”ì²œ ì‹œìŠ¤í…œ
elif system_type == 'recommendation':
    use_metrics = ['ndcg', 'map']
```

### 2. k ê°’ ì„¤ì •

```python
# ì‚¬ìš©ìê°€ ì¼ë°˜ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ìƒìœ„ ê²°ê³¼ ìˆ˜ì— ë§ì¶° ì„¤ì •
k_values = [1, 3, 5, 10]

for k in k_values:
    results = evaluator.evaluate_all(actual_docs, predicted_docs, k=k)
    print(f"@{k}: {results}")
```

### 3. ì„ê³„ê°’ ìµœì í™”

```python
# ROUGE ì„ê³„ê°’ ìµœì í™”
thresholds = [0.3, 0.5, 0.7, 0.9]
best_threshold = 0.5
best_score = 0

for threshold in thresholds:
    evaluator = RAGEvaluator(match_method="rouge2", rouge_threshold=threshold)
    results = evaluator.evaluate_all(actual_docs, predicted_docs)

    if results['f1_score']['macro_f1'] > best_score:
        best_score = results['f1_score']['macro_f1']
        best_threshold = threshold
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

1. **ë¹ˆ ê²€ìƒ‰ ê²°ê³¼**
   ```python
   # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆì„ ë•Œ ì²˜ë¦¬
   if not retrieved_docs:
       retrieved_docs = [Document(page_content="No results found")]
   ```

2. **ë©”íƒ€ë°ì´í„° ë¶ˆì¼ì¹˜**
   ```python
   # ID ê¸°ë°˜ ë§¤ì¹­ ì‹œ ë©”íƒ€ë°ì´í„° í˜•ì‹ í†µì¼
   for doc in docs:
       doc.metadata["id"] = str(doc.metadata["id"])
   ```

3. **ì„±ëŠ¥ ìµœì í™”**
   ```python
   # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ë°°ì¹˜ ì²˜ë¦¬
   batch_size = 100
   for i in range(0, len(queries), batch_size):
       batch = queries[i:i+batch_size]
       # ë°°ì¹˜ ë‹¨ìœ„ë¡œ í‰ê°€ ìˆ˜í–‰
   ```

## ğŸ“š ì°¸ê³  ìë£Œ

- [K-RAG íŒ¨í‚¤ì§€ ë¬¸ì„œ](https://github.com/your-repo/krag)
- [Information Retrieval í‰ê°€ ì§€í‘œ](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval))
- [ROUGE ë©”íŠ¸ë¦­ ì„¤ëª…](https://en.wikipedia.org/wiki/ROUGE_(metric))

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **PRJ02_W1_003**: í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¹„êµ
2. **PRJ02_W1_004**: ì¿¼ë¦¬ í™•ì¥ ê¸°ë²•ìœ¼ë¡œ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ
3. **ê³ ê¸‰ í‰ê°€**: ì‚¬ìš©ì ë§Œì¡±ë„, ë‹¤ì–‘ì„± ì§€í‘œ ì¶”ê°€ í•™ìŠµ

---

**ğŸ’¡ ì‹¤ë¬´ íŒ**: ë‹¨ì¼ ì§€í‘œì—ë§Œ ì˜ì¡´í•˜ì§€ ë§ê³ , ì‹œìŠ¤í…œì˜ ëª©ì ì— ë§ëŠ” ë³µìˆ˜ì˜ ì§€í‘œë¥¼ ì¡°í•©í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.