# W2_004 LLM ì„±ëŠ¥í‰ê°€ ê°œìš” - A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ë¹„êµ í‰ê°€

## í•™ìŠµ ëª©í‘œ
- A/B í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ì—¬ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ í‰ê°€ë¥¼ ì ìš©í•œë‹¤
- Reference-freeì™€ Reference-based í‰ê°€ ë°©ë²•ë¡ ì„ ì´í•´í•˜ê³  êµ¬í˜„í•œë‹¤
- ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ ë¹„êµ í‰ê°€í•˜ì—¬ ìµœì í™” ë°©ì•ˆì„ ë„ì¶œí•œë‹¤
- LangChain í‰ê°€ê¸°ë¥¼ í™œìš©í•œ ì²´ê³„ì ì¸ ì„±ëŠ¥ ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤

## í•µì‹¬ ê°œë…

### 1. LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í‰ê°€ì˜ í•µì‹¬ ìš”ì†Œ
- **ë°ì´í„°ì…‹**: í‰ê°€ë¥¼ ìœ„í•œ ê³ í’ˆì§ˆ ì˜ˆì œ (ì´ˆê¸° 10-20ê°œë¶€í„° ì‹œì‘)
- **í‰ê°€ì**: ì¸ê°„ í‰ê°€ì™€ ìë™í™” í‰ê°€ì˜ ì ì ˆí•œ ì¡°í•©
- **í‰ê°€ ë°©ë²•ë¡ **: ìƒí™©ì— ë§ëŠ” í‰ê°€ ê¸°ì¤€ê³¼ ì§€í‘œ ì„ íƒ

### 2. í‰ê°€ ë°©ì‹ì˜ ë¶„ë¥˜
- **ì¸ê°„ í‰ê°€**: ì£¼ê´€ì  íŒë‹¨ì´ í•„ìš”í•œ ì´ˆê¸° ë‹¨ê³„
- **ìë™í™” í‰ê°€**: í™•ì¥ì´ í•„ìš”í•œ ê²½ìš° íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í‰ê°€
- **ì˜¤í”„ë¼ì¸ í‰ê°€**: ë²¤ì¹˜ë§ˆí‚¹, í…ŒìŠ¤íŠ¸
- **ì˜¨ë¼ì¸ í‰ê°€**: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

### 3. A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ìœ í˜•
#### Reference-free í‰ê°€
- **íŠ¹ì§•**: ì°¸ì¡° ë‹µë³€ ì—†ì´ ë‘ RAG ë‹µë³€ ì§ì ‘ ë¹„êµ
- **í‰ê°€ ìš”ì†Œ**: ì‚¬ì‹¤ì„±, ê´€ë ¨ì„±, ì¼ê´€ì„± ë“± ìƒëŒ€ ë¹„êµ
- **ì¥ì **: ì ˆëŒ€ ê¸°ì¤€ ì—†ì´ë„ RAG ì‹œìŠ¤í…œ ê°„ ì„±ëŠ¥ ì°¨ì´ íŒë‹¨ ê°€ëŠ¥

#### Reference-based í‰ê°€
- **íŠ¹ì§•**: ì°¸ì¡° ë‹µì•ˆê³¼ RAG ì‘ë‹µì„ ë¹„êµ í‰ê°€
- **í‰ê°€ ë°©ì‹**: ìë™í™”ëœ A/B í…ŒìŠ¤íŠ¸ë¡œ ê°ê´€ì  ì„±ëŠ¥ ì¸¡ì •
- **ì£¼ìš” ì§€í‘œ**: ì •í™•ë„, ì™„ì„±ë„, ê´€ë ¨ì„± ë“± ì •ëŸ‰ì  í‰ê°€

## í™˜ê²½ ì„¤ì •

### 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
pip install langchain langchain-openai langchain-google-genai
pip install langchain-chroma langchain-community
pip install langfuse pandas openpyxl
pip install krag  # í•œêµ­ì–´ BM25 ê²€ìƒ‰ê¸°
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
# .env íŒŒì¼
OPENAI_API_KEY=your_openai_api_key
GOOGLE_API_KEY=your_google_api_key
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
LANGFUSE_HOST=https://cloud.langfuse.com
```

### 3. ê¸°ë³¸ ì„¤ì •
```python
from dotenv import load_dotenv
import os
import pandas as pd
import json
from pprint import pprint

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Langfuse ì½œë°± í•¸ë“¤ëŸ¬ ì„¤ì •
from langfuse.langchain import CallbackHandler
langfuse_handler = CallbackHandler()

print("í™˜ê²½ ì„¤ì • ì™„ë£Œ!")
```

## 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„

### í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
df_qa_test = pd.read_excel("data/testset.xlsx")
print(f"í…ŒìŠ¤íŠ¸ì…‹: {df_qa_test.shape[0]}ê°œ ë¬¸ì„œ")

# ë°ì´í„° êµ¬ì¡° í™•ì¸
df_qa_test.head(2)
```

### ë°ì´í„°ì…‹ êµ¬ì¡°
- `user_input`: ì‚¬ìš©ì ì§ˆë¬¸
- `reference`: ì°¸ì¡° ë‹µë³€
- `reference_contexts`: ì°¸ì¡° ë¬¸ë§¥
- `synthesizer_name`: ë°ì´í„° ìƒì„± ë°©ì‹

## 2ë‹¨ê³„: ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì„±

### 1. ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Chroma DB ë¡œë“œ
chroma_db = Chroma(
    collection_name="db_korean_cosine_metadata",
    embedding_function=embeddings,
    persist_directory="./chroma_db",
)

# ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„±
chroma_k = chroma_db.as_retriever(search_kwargs={'k': 4})

# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
query = "Elon MuskëŠ” Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ê³¼ ê²½ì˜ ë³€í™”ì— ì–´ë–»ê²Œ ê´€ì—¬í–ˆìœ¼ë©°, ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ë…¼ë€ì— ì§ë©´í–ˆë‚˜ìš”?"
retrieved_docs = chroma_k.invoke(query)

for doc in retrieved_docs:
    print(f"- {doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]")
```

### 2. BM25 ê²€ìƒ‰ê¸° êµ¬ì„±
```python
from krag.tokenizers import KiwiTokenizer
from krag.retrievers import KiWiBM25RetrieverWithScore
from langchain.schema import Document

# ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜
def load_jsonlines(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        docs = [json.loads(line) for line in f]
    return docs

# í•œêµ­ì–´ ë¬¸ì„œ ë¡œë“œ
korean_docs = load_jsonlines('data/korean_docs_final.jsonl')

# Document ê°ì²´ë¡œ ë³€í™˜
documents = []
for data in korean_docs:
    if isinstance(data, str):
        doc_data = json.loads(data)
    else:
        doc_data = data

    documents.append(Document(
        page_content=doc_data['page_content'],
        metadata=doc_data['metadata']
    ))

# BM25 ê²€ìƒ‰ê¸° ì„¤ì •
kiwi_tokenizer = KiwiTokenizer(model_type='knlm', typos='basic')
bm25_db = KiWiBM25RetrieverWithScore(
    documents=documents,
    kiwi_tokenizer=kiwi_tokenizer,
    k=4,
)

# BM25 ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
retrieved_docs = bm25_db.invoke(query)
for doc in retrieved_docs:
    print(f"BM25 ì ìˆ˜: {doc.metadata['bm25_score']:.2f}")
    print(f"{doc.page_content}")
```

### 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° êµ¬ì„±
```python
from langchain.retrievers import EnsembleRetriever

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
hybrid_retriever = EnsembleRetriever(
    retrievers=[bm25_db, chroma_k],
    weights=[0.5, 0.5],  # BM25ì™€ ë²¡í„° ê²€ìƒ‰ì˜ ê°€ì¤‘ì¹˜
)

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
retrieved_docs = hybrid_retriever.invoke(query)
for doc in retrieved_docs:
    print(f"{doc.page_content}\n[ì¶œì²˜: {doc.metadata['source']}]")
```

## 3ë‹¨ê³„: RAG ì²´ì¸ êµ¬í˜„

### RAG ë´‡ í•¨ìˆ˜ ì •ì˜
```python
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.runnables import RunnableConfig, RunnablePassthrough, RunnableParallel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from typing import List, Dict

def rag_bot(
    question: str,
    retriever: BaseRetriever,
    llm: BaseChatModel,
    config: RunnableConfig | None = None,
) -> Dict[str, str | List[Document]]:
    """
    ë¬¸ì„œ ê²€ìƒ‰ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰
    """
    # ë¬¸ì„œ ê²€ìƒ‰
    docs = retriever.invoke(question)
    context = "\n".join(doc.page_content for doc in docs)

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompt = f"""ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
- ì œê³µëœ ë¬¸ì„œë§Œ ì°¸ê³ í•˜ì—¬ ë‹µë³€
- ë¶ˆí™•ì‹¤í•  ê²½ìš° 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' ë¼ê³  ì‘ë‹µ
- 3ë¬¸ì¥ ì´ë‚´ë¡œ ë‹µë³€

[ë¬¸ì„œ]
{context}"""

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    prompt = ChatPromptTemplate.from_messages([
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": "\n\n[ì§ˆë¬¸]{question}\n\n[ë‹µë³€]\n"},
    ])

    # RAG ì²´ì¸ êµ¬ì„±
    docqa_chain = {
        "context": lambda x: context,
        "question": RunnablePassthrough(),
        "docs": lambda x: docs,
    } | RunnableParallel({
        "answer": prompt | llm | StrOutputParser(),
        "documents": lambda x: x["docs"],
    })

    return docqa_chain.invoke(question, config=config)

# ê¸°ë³¸ í…ŒìŠ¤íŠ¸
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)
result = rag_bot(
    question="Elon MuskëŠ” Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ê³¼ ê²½ì˜ ë³€í™”ì— ì–´ë–»ê²Œ ê´€ì—¬í–ˆìœ¼ë©°, ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ë…¼ë€ì— ì§ë©´í–ˆë‚˜ìš”?",
    retriever=hybrid_retriever,
    llm=llm,
    config={"callbacks": [langfuse_handler]},
)

print("ë‹µë³€:", result["answer"])
```

## 4ë‹¨ê³„: Reference-free A/B í…ŒìŠ¤íŠ¸

### 1. ê¸°ë³¸ ëª¨ë¸ ë¹„êµ
```python
from langchain.evaluation import load_evaluator
from langchain_google_genai import ChatGoogleGenerativeAI

# í‰ê°€ìš© LLM ì„¤ì •
evaluator_llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0,
    api_key=os.getenv("GOOGLE_API_KEY")
)

# ë¹„êµ í‰ê°€ì ë¡œë“œ
comparison_evaluator = load_evaluator(
    "pairwise_string",
    llm=evaluator_llm
)

# ë‘ ëª¨ë¸ ì„¤ì •
gpt_llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)
gemini_llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-001", temperature=0)

# ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•œ ë‘ ëª¨ë¸ì˜ ë‹µë³€ ìƒì„±
question = "Teslaì˜ ì£¼ìš” ì œí’ˆê³¼ ì„œë¹„ìŠ¤ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"

# GPT ë‹µë³€
gpt_response = rag_bot(
    question=question,
    retriever=chroma_k,
    llm=gpt_llm,
    config={"callbacks": [langfuse_handler]}
)

# Gemini ë‹µë³€
gemini_response = rag_bot(
    question=question,
    retriever=chroma_k,
    llm=gemini_llm,
    config={"callbacks": [langfuse_handler]}
)

# A/B í…ŒìŠ¤íŠ¸ í‰ê°€
evaluation_result = comparison_evaluator.evaluate_string_pairs(
    prediction=gpt_response["answer"],
    prediction_b=gemini_response["answer"],
    input=question
)

print("í‰ê°€ ê²°ê³¼:")
print(f"ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: {evaluation_result['value']}")
print(f"ì ìˆ˜: {evaluation_result['score']}")
print(f"í‰ê°€ ê·¼ê±°: {evaluation_result['reasoning']}")
```

### 2. í”„ë¡¬í”„íŠ¸ ë¹„êµ í‰ê°€
```python
# ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ RAG ë´‡ í•¨ìˆ˜ë“¤
def rag_bot_korean_style(question: str, retriever: BaseRetriever, llm: BaseChatModel, config=None):
    """í•œêµ­ì–´ ì¹œí™”ì  ìŠ¤íƒ€ì¼ì˜ RAG ë´‡"""
    docs = retriever.invoke(question)
    context = "\n".join(doc.page_content for doc in docs)

    system_prompt = f"""ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
- ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •ì¤‘í•˜ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤
- í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ì†”ì§í•˜ê²Œ ë§ì”€ë“œë¦½ë‹ˆë‹¤
- ì´í•´í•˜ê¸° ì‰½ê²Œ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤

[ì°¸ê³  ë¬¸ì„œ]
{context}"""

    prompt = ChatPromptTemplate.from_messages([
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"ì§ˆë¬¸: {question}\n\në‹µë³€ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."},
    ])

    chain = prompt | llm | StrOutputParser()
    return {"answer": chain.invoke({"question": question}, config=config)}

def rag_bot_business_style(question: str, retriever: BaseRetriever, llm: BaseChatModel, config=None):
    """ë¹„ì¦ˆë‹ˆìŠ¤ ì „ë¬¸ì  ìŠ¤íƒ€ì¼ì˜ RAG ë´‡"""
    docs = retriever.invoke(question)
    context = "\n".join(doc.page_content for doc in docs)

    system_prompt = f"""ì „ë¬¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨ì„¤í„´íŠ¸ë¡œì„œ ë‹µë³€í•©ë‹ˆë‹¤.
- ì œê³µëœ ë¬¸ì„œë¥¼ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤
- ë°ì´í„°ê°€ ë¶ˆì¶©ë¶„í•œ ê²½ìš° ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•©ë‹ˆë‹¤
- í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ëª…í™•í•˜ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤

[ë¶„ì„ ìë£Œ]
{context}"""

    prompt = ChatPromptTemplate.from_messages([
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"[Query] {question}\n\n[Analysis]"},
    ])

    chain = prompt | llm | StrOutputParser()
    return {"answer": chain.invoke({"question": question}, config=config)}

# í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ë¹„êµ
question = "Teslaì˜ ê²½ìŸ ìš°ìœ„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"

korean_response = rag_bot_korean_style(question, hybrid_retriever, gpt_llm, {"callbacks": [langfuse_handler]})
business_response = rag_bot_business_style(question, hybrid_retriever, gpt_llm, {"callbacks": [langfuse_handler]})

# í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ í‰ê°€
style_evaluation = comparison_evaluator.evaluate_string_pairs(
    prediction=korean_response["answer"],
    prediction_b=business_response["answer"],
    input=question
)

print("í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ë¹„êµ ê²°ê³¼:")
print(f"ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: {style_evaluation['value']}")
print(f"í‰ê°€ ê·¼ê±°: {style_evaluation['reasoning']}")
```

## 5ë‹¨ê³„: Reference-based A/B í…ŒìŠ¤íŠ¸

### 1. ì°¸ì¡° ë‹µë³€ ê¸°ë°˜ í‰ê°€
```python
# í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ ìƒ˜í”Œ ì„ íƒ
sample_idx = 0
sample = df_qa_test.iloc[sample_idx]

question = sample["user_input"]
reference = sample["reference"]

print(f"ì§ˆë¬¸: {question}")
print(f"ì°¸ì¡° ë‹µë³€: {reference}")

# ë‘ ëª¨ë¸ì˜ ë‹µë³€ ìƒì„±
gpt_response = rag_bot(question, hybrid_retriever, gpt_llm, {"callbacks": [langfuse_handler]})
gemini_response = rag_bot(question, hybrid_retriever, gemini_llm, {"callbacks": [langfuse_handler]})

# Reference-based í‰ê°€
ref_based_evaluator = load_evaluator(
    "labeled_pairwise_string",
    criteria="correctness",
    llm=evaluator_llm
)

ref_evaluation = ref_based_evaluator.evaluate_string_pairs(
    prediction=gpt_response["answer"],
    prediction_b=gemini_response["answer"],
    reference=reference,
    input=question
)

print("Reference-based í‰ê°€ ê²°ê³¼:")
print(f"ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: {ref_evaluation['value']}")
print(f"ì ìˆ˜: {ref_evaluation['score']}")
print(f"í‰ê°€ ê·¼ê±°: {ref_evaluation['reasoning']}")
```

### 2. ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ í‰ê°€
```python
from langchain_core.prompts import PromptTemplate

# ì‚¬ìš©ì ì •ì˜ í‰ê°€ ê¸°ì¤€
custom_criteria = {
    "conciseness": "ë‹µë³€ì´ ê°„ê²°í•˜ê³  í•µì‹¬ì„ ì˜ ì „ë‹¬í•˜ëŠ”ê°€? ë¶ˆí•„ìš”í•œ ë°˜ë³µì´ë‚˜ ì¥í™©í•¨ì´ ì—†ëŠ”ê°€?",
    "helpfulness": "ë‹µë³€ì´ ì‚¬ìš©ìì—ê²Œ ì‹¤ì§ˆì ì¸ ë„ì›€ì´ ë˜ëŠ”ê°€? ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ê°€?",
    "accuracy": "ë‹µë³€ì´ ì‚¬ì‹¤ì ìœ¼ë¡œ ì •í™•í•œê°€? ì œê³µëœ ë¬¸ì„œì™€ ì¼ì¹˜í•˜ëŠ”ê°€?"
}

# ì‚¬ìš©ì ì •ì˜ í‰ê°€ì ìƒì„±
def create_custom_evaluator(criteria_name, criteria_description):
    """ì‚¬ìš©ì ì •ì˜ í‰ê°€ì ìƒì„±"""

    # ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    custom_prompt = PromptTemplate(
        input_variables=["input", "prediction", "prediction_b", "reference"],
        template="""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‘ ê°œì˜ ë‹µë³€ì„ ë¹„êµ í‰ê°€í•´ì£¼ì„¸ìš”.

í‰ê°€ ê¸°ì¤€: {criteria_description}

ì§ˆë¬¸: {input}

ì°¸ì¡° ë‹µë³€: {reference}

ë‹µë³€ A: {prediction}

ë‹µë³€ B: {prediction_b}

í‰ê°€ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
- ì„ íƒ: A ë˜ëŠ” B (ë” ë‚˜ì€ ë‹µë³€ì„ ì„ íƒ)
- ì ìˆ˜: 0 (Bê°€ ë” ì¢‹ìŒ) ë˜ëŠ” 1 (Aê°€ ë” ì¢‹ìŒ)
- ê·¼ê±°: ìƒì„¸í•œ í‰ê°€ ê·¼ê±°

ì„ íƒ: [A/B]
ì ìˆ˜: [0/1]
ê·¼ê±°: [í‰ê°€ ê·¼ê±°]"""
    )

    # í‰ê°€ì ë¡œë“œ
    evaluator = load_evaluator(
        "labeled_pairwise_string",
        criteria=criteria_description,
        llm=evaluator_llm,
        prompt=custom_prompt
    )

    return evaluator

# ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€
for criteria_name, criteria_desc in custom_criteria.items():
    print(f"\n=== {criteria_name.upper()} í‰ê°€ ===")

    custom_evaluator = create_custom_evaluator(criteria_name, criteria_desc)

    custom_result = custom_evaluator.evaluate_string_pairs(
        prediction=gpt_response["answer"],
        prediction_b=gemini_response["answer"],
        reference=reference,
        input=question
    )

    print(f"ê¸°ì¤€: {criteria_desc}")
    print(f"ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: {custom_result['value']}")
    print(f"ì ìˆ˜: {custom_result['score']}")
    print(f"í‰ê°€ ê·¼ê±°: {custom_result['reasoning']}")
```

## 6ë‹¨ê³„: ëŒ€ê·œëª¨ A/B í…ŒìŠ¤íŠ¸ êµ¬í˜„

### ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ
```python
import time
import random
from typing import List, Dict, Any

class ComprehensiveABTester:
    def __init__(self, evaluator_llm, langfuse_handler):
        self.evaluator_llm = evaluator_llm
        self.langfuse_handler = langfuse_handler
        self.evaluation_results = []

    def run_comprehensive_evaluation(
        self,
        test_dataset: pd.DataFrame,
        model_a_config: Dict[str, Any],
        model_b_config: Dict[str, Any],
        retriever,
        evaluation_criteria: List[str] = ["conciseness", "helpfulness", "accuracy"],
        sample_size: int = None
    ) -> Dict[str, Any]:
        """
        ì¢…í•©ì ì¸ A/B í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
        """

        # ìƒ˜í”Œ ë°ì´í„° ì„ íƒ
        if sample_size and sample_size < len(test_dataset):
            sample_indices = random.sample(range(len(test_dataset)), sample_size)
            test_data = test_dataset.iloc[sample_indices]
        else:
            test_data = test_dataset

        print(f"ğŸš€ A/B í…ŒìŠ¤íŠ¸ ì‹œì‘: {len(test_data)}ê°œ ìƒ˜í”Œ")

        results = {
            "model_a_wins": 0,
            "model_b_wins": 0,
            "ties": 0,
            "detailed_results": [],
            "criteria_scores": {criterion: {"a_wins": 0, "b_wins": 0, "ties": 0}
                             for criterion in evaluation_criteria}
        }

        for idx, row in test_data.iterrows():
            try:
                print(f"\nğŸ“Š í‰ê°€ ì§„í–‰: {idx+1}/{len(test_data)}")

                question = row["user_input"]
                reference = row.get("reference", "")

                # ëª¨ë¸ A ë‹µë³€ ìƒì„±
                response_a = self._generate_response(
                    question, retriever,
                    model_a_config["llm"],
                    model_a_config.get("rag_function", rag_bot)
                )

                # ëª¨ë¸ B ë‹µë³€ ìƒì„±
                response_b = self._generate_response(
                    question, retriever,
                    model_b_config["llm"],
                    model_b_config.get("rag_function", rag_bot)
                )

                # ê° ê¸°ì¤€ë³„ í‰ê°€
                item_results = {
                    "question": question,
                    "reference": reference,
                    "response_a": response_a["answer"],
                    "response_b": response_b["answer"],
                    "evaluations": {}
                }

                overall_a_score = 0
                overall_b_score = 0

                for criterion in evaluation_criteria:
                    criterion_result = self._evaluate_responses(
                        question, response_a["answer"], response_b["answer"],
                        reference, criterion
                    )

                    item_results["evaluations"][criterion] = criterion_result

                    # ì ìˆ˜ ì§‘ê³„
                    if criterion_result["score"] == 1:  # Aê°€ ë” ì¢‹ìŒ
                        overall_a_score += 1
                        results["criteria_scores"][criterion]["a_wins"] += 1
                    elif criterion_result["score"] == 0:  # Bê°€ ë” ì¢‹ìŒ
                        overall_b_score += 1
                        results["criteria_scores"][criterion]["b_wins"] += 1
                    else:  # ë¬´ìŠ¹ë¶€
                        results["criteria_scores"][criterion]["ties"] += 1

                # ì „ì²´ ìŠ¹ë¶€ ê²°ì •
                if overall_a_score > overall_b_score:
                    results["model_a_wins"] += 1
                    item_results["winner"] = "A"
                elif overall_b_score > overall_a_score:
                    results["model_b_wins"] += 1
                    item_results["winner"] = "B"
                else:
                    results["ties"] += 1
                    item_results["winner"] = "Tie"

                results["detailed_results"].append(item_results)

                # ì§„í–‰ë¥  ì¶œë ¥
                if (idx + 1) % 5 == 0:
                    self._print_progress(results, idx + 1, len(test_data))

                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
                time.sleep(1)

            except Exception as e:
                print(f"âŒ í‰ê°€ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {idx}): {e}")
                continue

        # ìµœì¢… ê²°ê³¼ ê³„ì‚°
        total_evaluations = results["model_a_wins"] + results["model_b_wins"] + results["ties"]
        results["model_a_win_rate"] = results["model_a_wins"] / total_evaluations if total_evaluations > 0 else 0
        results["model_b_win_rate"] = results["model_b_wins"] / total_evaluations if total_evaluations > 0 else 0
        results["tie_rate"] = results["ties"] / total_evaluations if total_evaluations > 0 else 0

        return results

    def _generate_response(self, question: str, retriever, llm, rag_function):
        """ì‘ë‹µ ìƒì„±"""
        return rag_function(
            question=question,
            retriever=retriever,
            llm=llm,
            config={"callbacks": [self.langfuse_handler]}
        )

    def _evaluate_responses(self, question: str, response_a: str, response_b: str,
                          reference: str, criterion: str) -> Dict[str, Any]:
        """ì‘ë‹µ í‰ê°€"""
        try:
            evaluator = load_evaluator(
                "labeled_pairwise_string",
                criteria=criterion,
                llm=self.evaluator_llm
            )

            result = evaluator.evaluate_string_pairs(
                prediction=response_a,
                prediction_b=response_b,
                reference=reference,
                input=question
            )

            return {
                "criterion": criterion,
                "value": result["value"],
                "score": result["score"],
                "reasoning": result["reasoning"]
            }

        except Exception as e:
            print(f"âš ï¸ í‰ê°€ ì‹¤íŒ¨ ({criterion}): {e}")
            return {
                "criterion": criterion,
                "value": "C",
                "score": 0.5,
                "reasoning": f"í‰ê°€ ì‹¤íŒ¨: {e}"
            }

    def _print_progress(self, results: Dict, current: int, total: int):
        """ì§„í–‰ë¥  ì¶œë ¥"""
        print(f"\nğŸ“ˆ ì§„í–‰ë¥ : {current}/{total}")
        print(f"   ëª¨ë¸ A ìŠ¹ë¦¬: {results['model_a_wins']}")
        print(f"   ëª¨ë¸ B ìŠ¹ë¦¬: {results['model_b_wins']}")
        print(f"   ë¬´ìŠ¹ë¶€: {results['ties']}")

    def generate_evaluation_report(self, results: Dict, model_a_name: str, model_b_name: str) -> str:
        """í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("# A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ë¦¬í¬íŠ¸")
        report.append(f"**ëª¨ë¸ A**: {model_a_name}")
        report.append(f"**ëª¨ë¸ B**: {model_b_name}")
        report.append("")

        # ì „ì²´ ê²°ê³¼
        report.append("## ğŸ“Š ì „ì²´ ê²°ê³¼")
        report.append(f"- ëª¨ë¸ A ìŠ¹ë¦¬: {results['model_a_wins']} ({results['model_a_win_rate']:.1%})")
        report.append(f"- ëª¨ë¸ B ìŠ¹ë¦¬: {results['model_b_wins']} ({results['model_b_win_rate']:.1%})")
        report.append(f"- ë¬´ìŠ¹ë¶€: {results['ties']} ({results['tie_rate']:.1%})")
        report.append("")

        # ê¸°ì¤€ë³„ ê²°ê³¼
        report.append("## ğŸ¯ ê¸°ì¤€ë³„ ì„±ëŠ¥")
        for criterion, scores in results["criteria_scores"].items():
            total = scores["a_wins"] + scores["b_wins"] + scores["ties"]
            if total > 0:
                a_rate = scores["a_wins"] / total
                b_rate = scores["b_wins"] / total
                report.append(f"### {criterion.title()}")
                report.append(f"- ëª¨ë¸ A: {scores['a_wins']} ({a_rate:.1%})")
                report.append(f"- ëª¨ë¸ B: {scores['b_wins']} ({b_rate:.1%})")
                report.append(f"- ë¬´ìŠ¹ë¶€: {scores['ties']} ({(1-a_rate-b_rate):.1%})")
                report.append("")

        # ê¶Œì¥ì‚¬í•­
        report.append("## ğŸ’¡ ê¶Œì¥ì‚¬í•­")
        if results["model_a_win_rate"] > results["model_b_win_rate"]:
            report.append(f"- {model_a_name}ì´ ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤")
        elif results["model_b_win_rate"] > results["model_a_win_rate"]:
            report.append(f"- {model_b_name}ì´ ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤")
        else:
            report.append("- ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë¹„ìŠ·í•©ë‹ˆë‹¤")

        return "\n".join(report)

# ì¢…í•© í‰ê°€ ì‹¤í–‰
ab_tester = ComprehensiveABTester(evaluator_llm, langfuse_handler)

# ëª¨ë¸ ì„¤ì •
model_configs = {
    "gpt-4.1-mini": {
        "llm": ChatOpenAI(model="gpt-4.1-mini", temperature=0),
        "rag_function": rag_bot
    },
    "gemini-2.0-flash": {
        "llm": ChatGoogleGenerativeAI(model="gemini-2.0-flash-001", temperature=0),
        "rag_function": rag_bot
    }
}

# A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰
evaluation_results = ab_tester.run_comprehensive_evaluation(
    test_dataset=df_qa_test,
    model_a_config=model_configs["gpt-4.1-mini"],
    model_b_config=model_configs["gemini-2.0-flash"],
    retriever=hybrid_retriever,
    evaluation_criteria=["conciseness", "helpfulness", "accuracy"],
    sample_size=10  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ê°œ ìƒ˜í”Œë§Œ
)

# ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
report = ab_tester.generate_evaluation_report(
    evaluation_results,
    "GPT-4.1-mini",
    "Gemini-2.0-flash"
)

print(report)

# ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
with open("ab_test_report.md", "w", encoding="utf-8") as f:
    f.write(report)

print("\nğŸ“„ í‰ê°€ ë¦¬í¬íŠ¸ê°€ 'ab_test_report.md'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

## ì‹¤ìŠµ ê³¼ì œ

### ê¸°ë³¸ ì‹¤ìŠµ
1. **ë‹¨ì¼ ëª¨ë¸ ë¹„êµ**
   - GPT-4.1-miniì™€ Gemini-2.0-flash ëª¨ë¸ ë¹„êµ
   - Reference-free ë°©ì‹ìœ¼ë¡œ 5ê°œ ì§ˆë¬¸ í‰ê°€
   - ê²°ê³¼ë¥¼ Langfuse UIì—ì„œ í™•ì¸

2. **í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ë¹„êµ**
   - ì¹œê·¼í•œ ìŠ¤íƒ€ì¼ vs ì „ë¬¸ì  ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ì‘ì„±
   - ë™ì¼ ëª¨ë¸ë¡œ ë‘ í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¹„êµ
   - ì‚¬ìš©ì ì„ í˜¸ë„ ë¶„ì„

### ì‘ìš© ì‹¤ìŠµ
3. **ë‹¤ì¤‘ ê¸°ì¤€ í‰ê°€**
   - Conciseness, Helpfulness, Accuracy ê¸°ì¤€ìœ¼ë¡œ í‰ê°€
   - ê¸°ì¤€ë³„ ê°€ì¤‘ì¹˜ ì ìš©í•œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
   - ê¸°ì¤€ë³„ ì„±ëŠ¥ ì°¨ì´ ë¶„ì„

4. **ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ**
   - Ollamaì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ê³¼ ìƒìš© ëª¨ë¸ ë¹„êµ
   - ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹(df_qa_test)ìœ¼ë¡œ comprehensive evaluation
   - Reference-basedì™€ Reference-free í‰ê°€ ëª¨ë‘ ìˆ˜í–‰

### ì‹¬í™” ì‹¤ìŠµ
5. **ê²€ìƒ‰ê¸° ì„±ëŠ¥ ë¹„êµ**
   - Vector Search vs BM25 vs Hybrid Search ë¹„êµ
   - ê²€ìƒ‰ ì •í™•ë„ê°€ ìµœì¢… ë‹µë³€ í’ˆì§ˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„
   - ê²€ìƒ‰ê¸°ë³„ ìµœì  ëª¨ë¸ ì¡°í•© íƒìƒ‰

6. **ì‹¤ì‹œê°„ A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ**
   - ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í•œ ì§€ì†ì  í‰ê°€ ì‹œìŠ¤í…œ
   - ì„±ëŠ¥ ì €í•˜ ê°ì§€ ë° ìë™ ì•Œë¦¼ ê¸°ëŠ¥
   - ëª¨ë¸ ë°°í¬ ì „ ìë™ ê²€ì¦ íŒŒì´í”„ë¼ì¸

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜ë“¤
1. **API í˜¸ì¶œ ì œí•œ ì˜¤ë¥˜**
   ```python
   # ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€
   import time
   time.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
   ```

2. **Langfuse ì—°ê²° ì˜¤ë¥˜**
   ```python
   # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
   print("LANGFUSE_PUBLIC_KEY:", bool(os.getenv("LANGFUSE_PUBLIC_KEY")))
   print("LANGFUSE_SECRET_KEY:", bool(os.getenv("LANGFUSE_SECRET_KEY")))
   ```

3. **ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**
   ```python
   # ë°°ì¹˜ ì²˜ë¦¬ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
   import gc
   gc.collect()
   ```

## ì°¸ê³  ìë£Œ
- [LangChain í‰ê°€ ê°€ì´ë“œ](https://python.langchain.com/docs/guides/evaluation/)
- [Langfuse A/B í…ŒìŠ¤íŠ¸ ë¬¸ì„œ](https://langfuse.com/docs/evaluation)
- [LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í‰ê°€ Best Practices](https://docs.smith.langchain.com/evaluation)
- [Pairwise Evaluation ë°©ë²•ë¡ ](https://python.langchain.com/api_reference/langchain/evaluation.html)

ì´ ê°€ì´ë“œë¥¼ í†µí•´ ì²´ê³„ì ì¸ A/B í…ŒìŠ¤íŠ¸ë¥¼ êµ¬í˜„í•˜ì—¬ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.