# W3_003_Prompt_Engineering_CoT.md - Chain of Thought í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

## ğŸ¯ í•™ìŠµ ëª©í‘œ

- Chain-of-Thought (CoT) ì¶”ë¡  ê¸°ë²•ì„ í™œìš©í•œ ë³µì¡í•œ ë¬¸ì œ í•´ê²° ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤
- Zero-shot, One-shot, CoT í”„ë¡¬í”„íŒ… ë°©ì‹ì˜ ì°¨ì´ì ê³¼ ì ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì´í•´í•©ë‹ˆë‹¤
- Self-Consistency, Program-Aided Language (PAL), Reflexion ë“± ê³ ê¸‰ ì¶”ë¡  ê¸°ë²•ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤
- ë‹¤ì–‘í•œ AI ëª¨ë¸ì—ì„œì˜ ì¶”ë¡  ì„±ëŠ¥ì„ ë¹„êµí•˜ê³  ìµœì í™” ì „ëµì„ ê°œë°œí•©ë‹ˆë‹¤
- ë³µì¡í•œ ë…¼ë¦¬ ë¬¸ì œì™€ ìˆ˜í•™ ë¬¸ì œ í•´ê²°ì— CoTë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°œë°œí•©ë‹ˆë‹¤

## ğŸ“š í•µì‹¬ ê°œë…

### 1. Chain of Thought (CoT)ë€?

Chain of ThoughtëŠ” AI ëª¨ë¸ì´ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ë•Œ ê° ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ë³´ì—¬ì£¼ë„ë¡ í•˜ëŠ” í”„ë¡¬í”„íŒ… ê¸°ë²•ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì¶”ë¡  ê³¼ì •ì„ íˆ¬ëª…í•˜ê²Œ í™•ì¸í•  ìˆ˜ ìˆê³  ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### CoTì˜ í•µì‹¬ íŠ¹ì§•
```python
cot_structure = {
    "transparency": "ì¶”ë¡  ê³¼ì •ì˜ ëª…ì‹œì  í‘œí˜„",
    "accuracy": "ë‹¨ê³„ë³„ ê²€ì¦ì„ í†µí•œ ì •í™•ë„ í–¥ìƒ",
    "debuggability": "ì˜¤ë¥˜ ë°œìƒ ì§€ì ì˜ ëª…í™•í•œ ì‹ë³„",
    "interpretability": "ê²°ê³¼ì— ëŒ€í•œ ì´í•´ë„ ì¦ì§„"
}
```

#### CoT vs ê¸°ì¡´ ë°©ì‹ ë¹„êµ
```python
comparison = {
    "direct_answer": {
        "input": "2 + 3 Ã— 4 = ?",
        "output": "14",
        "pros": ["ë¹ ë¥¸ ì‘ë‹µ", "ê°„ê²°í•¨"],
        "cons": ["ê³¼ì • ë¶ˆíˆ¬ëª…", "ì˜¤ë¥˜ ê²€ì¦ ì–´ë ¤ì›€"]
    },
    "chain_of_thought": {
        "input": "2 + 3 Ã— 4 = ?",
        "output": """
        1ë‹¨ê³„: ì—°ì‚° ìˆœì„œ í™•ì¸ (ê³±ì…ˆ ìš°ì„ )
        2ë‹¨ê³„: 3 Ã— 4 = 12 ê³„ì‚°
        3ë‹¨ê³„: 2 + 12 = 14 ê³„ì‚°
        ë‹µ: 14
        """,
        "pros": ["íˆ¬ëª…í•œ ê³¼ì •", "ì˜¤ë¥˜ ê²€ì¦ ê°€ëŠ¥", "í•™ìŠµ íš¨ê³¼"],
        "cons": ["ê¸´ ì‘ë‹µ", "ë†’ì€ í† í° ì‚¬ìš©ëŸ‰"]
    }
}
```

### 2. í”„ë¡¬í”„íŒ… ê¸°ë²•ì˜ ì§„í™”

#### 2.1 Zero-Shot í”„ë¡¬í”„íŒ…
ì˜ˆì‹œ ì—†ì´ ì§ì ‘ì ì¸ ì§€ì‹œë§Œìœ¼ë¡œ ë‹µì„ êµ¬í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

**íŠ¹ì§•:**
- ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœ
- ë¹ ë¥¸ ì‘ë‹µ ì†ë„
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- ë‹¨ìˆœí•œ ë¬¸ì œì— ì í•©

**ì ìš© ì‚¬ë¡€:**
```python
zero_shot_examples = {
    "translation": "ë‹¤ìŒì„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”: ì•ˆë…•í•˜ì„¸ìš”",
    "classification": "ë‹¤ìŒ ê°ì •ì„ ë¶„ë¥˜í•˜ì„¸ìš”: ì˜¤ëŠ˜ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”",
    "simple_math": "15 Ã— 8ì€ ì–¼ë§ˆì…ë‹ˆê¹Œ?"
}
```

#### 2.2 One-Shot/Few-Shot í”„ë¡¬í”„íŒ…
í•˜ë‚˜ ì´ìƒì˜ ì˜ˆì‹œë¥¼ í†µí•´ ë¬¸ì œ í•´ê²° ë°©ì‹ì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

**íŠ¹ì§•:**
- ì˜ˆì‹œë¥¼ í†µí•œ íŒ¨í„´ í•™ìŠµ
- Zero-shotë³´ë‹¤ í–¥ìƒëœ ì„±ëŠ¥
- ì¤‘ê°„ ë³µì¡ë„ ë¬¸ì œì— íš¨ê³¼ì 
- í˜•ì‹í™”ëœ ì¶œë ¥ì— ìœ ë¦¬

**êµ¬ì¡°:**
```python
few_shot_structure = """
ì˜ˆì‹œ 1:
ë¬¸ì œ: [ë¬¸ì œ ì„¤ëª…]
í’€ì´: [ë‹¨ê³„ë³„ í•´ê²° ê³¼ì •]
ë‹µ: [ê²°ê³¼]

ì˜ˆì‹œ 2:
ë¬¸ì œ: [ë¬¸ì œ ì„¤ëª…]
í’€ì´: [ë‹¨ê³„ë³„ í•´ê²° ê³¼ì •]
ë‹µ: [ê²°ê³¼]

ìƒˆë¡œìš´ ë¬¸ì œ: {user_question}
í’€ì´:
"""
```

#### 2.3 Chain of Thought (CoT) í”„ë¡¬í”„íŒ…
ì²´ê³„ì ì´ê³  ëª…ì‹œì ì¸ ë‹¨ê³„ë³„ ì¶”ë¡ ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

**íŠ¹ì§•:**
- ê°€ì¥ ì²´ê³„ì ì¸ ë¬¸ì œ í•´ê²°
- ë³µì¡í•œ ì¶”ë¡ ì— ìµœì í™”
- ë†’ì€ ì •í™•ë„
- ê³¼ì • ê²€ì¦ ê°€ëŠ¥

**í…œí”Œë¦¿ êµ¬ì¡°:**
```python
cot_template = """
ë¬¸ì œ: {problem}

í•´ê²° ê³¼ì •:
1ë‹¨ê³„: ë¬¸ì œ ì´í•´
- ì£¼ì–´ì§„ ì •ë³´ íŒŒì•…
- êµ¬í•´ì•¼ í•  ê²ƒ ì •ë¦¬

2ë‹¨ê³„: í•´ê²° ì „ëµ ê³„íš
- ì‚¬ìš©í•  ë°©ë²• ì„ íƒ
- ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½

3ë‹¨ê³„: ê³„íš ì‹¤í–‰
- ê° ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰
- ì¤‘ê°„ ê²°ê³¼ í™•ì¸

4ë‹¨ê³„: ê²°ê³¼ ê²€ì¦
- ë‹µ í™•ì¸
- ëŒ€ì•ˆ ë°©ë²• ê²€í† 

ë‹µ: [ìµœì¢… ê²°ê³¼]
"""
```

### 3. ê³ ê¸‰ CoT ê¸°ë²•ë“¤

#### 3.1 Self-Consistency
í•˜ë‚˜ì˜ ë¬¸ì œë¥¼ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ í•´ê²°í•˜ì—¬ ê²°ê³¼ì˜ ì¼ê´€ì„±ì„ í™•ì¸í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

**ì›ë¦¬:**
- ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ìœ¼ë¡œ ë™ì¼í•œ ë¬¸ì œ í•´ê²°
- ê²°ê³¼ ê°„ ì¼ê´€ì„± ê²€ì¦
- ìµœì¢… ë‹µì•ˆì˜ ì‹ ë¢°ë„ í–¥ìƒ

**ì ìš© ë°©ë²•:**
```python
self_consistency_approach = {
    "method_1": "ì§ì ‘ ê³„ì‚°",
    "method_2": "ë¹„ìœ¨ í™œìš©",
    "method_3": "ë‹¨ê³„ë³„ ë¶„í•´",
    "verification": "ì„¸ ë°©ë²•ì˜ ê²°ê³¼ ì¼ì¹˜ì„± í™•ì¸",
    "confidence": "ì¼ì¹˜í•˜ëŠ” ë‹µì•ˆì˜ ì‹ ë¢°ë„ ì¦ê°€"
}
```

#### 3.2 Program-Aided Language (PAL)
ìì—°ì–´ ë¬¸ì œë¥¼ í”„ë¡œê·¸ë˜ë°ì  ì‚¬ê³ ë¡œ ì ‘ê·¼í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

**íŠ¹ì§•:**
- ì½”ë“œ êµ¬ì¡°ë¥¼ í™œìš©í•œ ë…¼ë¦¬ì  í•´ê²°
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ì‚¬ì½”ë“œ ìƒì„±
- ê²€ì¦ê³¼ ë””ë²„ê¹… ìš©ì´
- ìˆ˜í•™ì  ê³„ì‚°ì— íŠ¹íˆ íš¨ê³¼ì 

**êµ¬ì¡°:**
```python
pal_structure = """
def solve_problem():
    # 1. ë³€ìˆ˜ ì •ì˜
    [ì£¼ì–´ì§„ ê°’ë“¤ì„ ë³€ìˆ˜ë¡œ ì €ì¥]

    # 2. ê³„ì‚° ê³¼ì •
    [ë‹¨ê³„ë³„ ê³„ì‚° ìˆ˜í–‰]

    # 3. ê²°ê³¼ ë°˜í™˜
    [ìµœì¢… ê²°ê³¼ ê³„ì‚° ë° ë°˜í™˜]

    return result
"""
```

#### 3.3 Reflexion
ìê¸° í‰ê°€ì™€ ê°œì„ ì„ í†µí•œ ë©”íƒ€ì¸ì§€ì  ì ‘ê·¼ ê¸°ë²•ì…ë‹ˆë‹¤.

**ê³¼ì •:**
1. ì´ˆê¸° ë‹µì•ˆ ìƒì„±
2. ìì²´ í‰ê°€ ìˆ˜í–‰
3. ê°œì„ ì  ì‹ë³„
4. ìˆ˜ì •ëœ ë‹µì•ˆ ì‘ì„±

**ì¥ì :**
- ì§€ì†ì ì¸ í’ˆì§ˆ ê°œì„ 
- ì˜¤ë¥˜ ìê°€ ë°œê²¬
- ë©”íƒ€ì¸ì§€ì  ì‚¬ê³  ì´‰ì§„

## ğŸ”§ í™˜ê²½ ì„¤ì •

### 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
# uv ì‚¬ìš© (ê¶Œì¥)
uv add langchain langchain-openai langchain-ollama python-dotenv

# pip ì‚¬ìš©
pip install langchain langchain-openai langchain-ollama python-dotenv
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ì— ì¶”ê°€
OPENAI_API_KEY=your_openai_api_key_here
OLLAMA_HOST=http://localhost:11434  # Ollama ì„œë²„ ì£¼ì†Œ (ì„ íƒì‚¬í•­)
```

### 3. ê¸°ë³¸ ì„í¬íŠ¸

```python
import os
from dotenv import load_dotenv
from pprint import pprint
from typing import Dict, List, Any, Optional, Tuple

# LangChain ê´€ë ¨
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# LLM ì´ˆê¸°í™”
openai_llm = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=0.3,
    top_p=0.9
)

# Ollama LLMs (ì˜µì…˜)
phi3_llm = ChatOllama(
    model="phi3:mini",
    temperature=0.3,
    top_p=0.9
)

gemma_llm = ChatOllama(
    model="gemma2",
    temperature=0.3,
    top_p=0.9
)
```

## ğŸ’» ì½”ë“œ ì˜ˆì œ

### 1. í”„ë¡¬í”„íŒ… ê¸°ë²• ë¹„êµ ì‹œìŠ¤í…œ

```python
class PromptingMethodComparator:
    def __init__(self, models: Dict[str, Any]):
        """í”„ë¡¬í”„íŒ… ë°©ë²• ë¹„êµ ì‹œìŠ¤í…œ"""
        self.models = models

    def create_zero_shot_prompt(self, question: str) -> PromptTemplate:
        """Zero-shot í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = """
ë‹¤ìŒ ë¬¸ì œë¥¼ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {question}

ë‹µì•ˆ:
"""
        return PromptTemplate(
            input_variables=["question"],
            template=template
        )

    def create_one_shot_prompt(self, question: str) -> PromptTemplate:
        """One-shot í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = """
ë‹¤ìŒì€ ìˆ˜í•™ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤:

ì˜ˆì‹œ ë¬¸ì œ: í•œ í•™ê¸‰ì— 30ëª…ì˜ í•™ìƒì´ ìˆìŠµë‹ˆë‹¤. ì´ ì¤‘ 40%ê°€ ë‚¨í•™ìƒì´ë¼ë©´, ì—¬í•™ìƒì€ ëª‡ ëª…ì¸ê°€ìš”?

ì˜ˆì‹œ í’€ì´:
1) ë¨¼ì € ë‚¨í•™ìƒ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:
   - ì „ì²´ í•™ìƒì˜ 40% = 30 Ã— 0.4 = 12ëª…ì´ ë‚¨í•™ìƒ

2) ì—¬í•™ìƒ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:
   - ì „ì²´ í•™ìƒ ìˆ˜ - ë‚¨í•™ìƒ ìˆ˜ = 30 - 12 = 18ëª…ì´ ì—¬í•™ìƒ

ë”°ë¼ì„œ ì—¬í•™ìƒì€ 18ëª…ì…ë‹ˆë‹¤.

ì´ì œ ì•„ë˜ ë¬¸ì œë¥¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ í•´ê²°í•˜ì‹œì˜¤:

ìƒˆë¡œìš´ ë¬¸ì œ: {question}

ë‹µì•ˆ:
"""
        return PromptTemplate(
            input_variables=["question"],
            template=template
        )

    def create_cot_prompt(self, question: str) -> PromptTemplate:
        """Chain of Thought í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = """
ë‹¤ìŒ ë¬¸ì œë¥¼ ë…¼ë¦¬ì  ë‹¨ê³„ì— ë”°ë¼ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {question}

í•´ê²° ê³¼ì •:
1ë‹¨ê³„: ë¬¸ì œ ì´í•´í•˜ê¸°
- ì£¼ì–´ì§„ ì •ë³´ íŒŒì•…
- êµ¬í•´ì•¼ í•  ê²ƒ ì •ë¦¬

2ë‹¨ê³„: í•´ê²° ë°©ë²• ê³„íš
- ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì „ëµ ê²€í† 
- ìµœì ì˜ ë°©ë²• ì„ íƒ

3ë‹¨ê³„: ê³„íš ì‹¤í–‰
- ì„ íƒí•œ ë°©ë²• ì ìš©
- ì¤‘ê°„ ê²°ê³¼ í™•ì¸

4ë‹¨ê³„: ê²€í† 
- ë‹µì•ˆ í™•ì¸
- ë‹¤ë¥¸ ë°©ë²• ê°€ëŠ¥ì„± ê²€í† 

ë‹µì•ˆ:
"""
        return PromptTemplate(
            input_variables=["question"],
            template=template
        )

    def compare_methods(self, question: str, model_name: str = "openai") -> Dict[str, Any]:
        """ì„¸ ê°€ì§€ ë°©ë²• ì„±ëŠ¥ ë¹„êµ"""
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not available")

        model = self.models[model_name]
        results = {}

        # Zero-shot í…ŒìŠ¤íŠ¸
        zero_shot_prompt = self.create_zero_shot_prompt(question)
        zero_shot_chain = zero_shot_prompt | model | StrOutputParser()
        results['zero_shot'] = zero_shot_chain.invoke({"question": question})

        # One-shot í…ŒìŠ¤íŠ¸
        one_shot_prompt = self.create_one_shot_prompt(question)
        one_shot_chain = one_shot_prompt | model | StrOutputParser()
        results['one_shot'] = one_shot_chain.invoke({"question": question})

        # CoT í…ŒìŠ¤íŠ¸
        cot_prompt = self.create_cot_prompt(question)
        cot_chain = cot_prompt | model | StrOutputParser()
        results['cot'] = cot_chain.invoke({"question": question})

        return results

    def evaluate_response_quality(self, results: Dict[str, str]) -> Dict[str, Dict[str, Any]]:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        evaluation = {}

        for method, response in results.items():
            evaluation[method] = {
                "length": len(response),
                "has_calculation": any(char in response for char in "Ã—Ã·+-="),
                "step_by_step": "ë‹¨ê³„" in response or "step" in response.lower(),
                "shows_reasoning": len(response.split('\n')) > 3,
                "confidence_score": self._calculate_confidence(response)
            }

        return evaluation

    def _calculate_confidence(self, response: str) -> float:
        """ì‘ë‹µ ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""
        indicators = [
            "ë”°ë¼ì„œ" in response,
            "ê³„ì‚°" in response,
            "=" in response,
            len(response) > 100,  # ì¶©ë¶„í•œ ì„¤ëª…
            "ë‹¨ê³„" in response or "step" in response.lower()
        ]
        return sum(indicators) / len(indicators)

# ì‚¬ìš© ì˜ˆì‹œ
models = {
    "openai": openai_llm,
    "phi3": phi3_llm,
    "gemma": gemma_llm
}

comparator = PromptingMethodComparator(models)

# í…ŒìŠ¤íŠ¸ ë¬¸ì œ
test_question = """
í•™êµì—ì„œ 500ëª…ì˜ í•™ìƒì´ ìˆìŠµë‹ˆë‹¤. ì´ ì¤‘ 30%ëŠ” 5í•™ë…„ì´ê³ , 20%ëŠ” 6í•™ë…„ í•™ìƒì…ë‹ˆë‹¤.
5í•™ë…„ í•™ìƒë“¤ ì¤‘ 60%ëŠ” ìˆ˜í•™ ë™ì•„ë¦¬ì— ìˆê³ , ë‚˜ë¨¸ì§€ëŠ” ê³¼í•™ ë™ì•„ë¦¬ì— ìˆìŠµë‹ˆë‹¤.
6í•™ë…„ í•™ìƒë“¤ ì¤‘ 70%ëŠ” ìˆ˜í•™ ë™ì•„ë¦¬ì— ìˆê³ , ë‚˜ë¨¸ì§€ëŠ” ê³¼í•™ ë™ì•„ë¦¬ì— ìˆìŠµë‹ˆë‹¤.
ê³¼í•™ ë™ì•„ë¦¬ì—ëŠ” ëª‡ ëª…ì˜ í•™ìƒì´ ìˆë‚˜ìš”?
"""

# ë¹„êµ ì‹¤í–‰
comparison_results = comparator.compare_methods(test_question, "openai")
quality_evaluation = comparator.evaluate_response_quality(comparison_results)

print("=== í”„ë¡¬í”„íŒ… ë°©ë²• ë¹„êµ ê²°ê³¼ ===")
for method, result in comparison_results.items():
    print(f"\nã€{method.upper()}ã€‘")
    print(result[:300] + "..." if len(result) > 300 else result)

    quality = quality_evaluation[method]
    print(f"í‰ê°€: ê¸¸ì´={quality['length']}, ì¶”ë¡ ê³¼ì •={quality['shows_reasoning']}, ì‹ ë¢°ë„={quality['confidence_score']:.2f}")
```

### 2. Self-Consistency ì‹œìŠ¤í…œ

```python
class SelfConsistencySystem:
    def __init__(self, model: Any, num_attempts: int = 3):
        """Self-Consistency ì‹œìŠ¤í…œ"""
        self.model = model
        self.num_attempts = num_attempts

    def create_multi_method_prompt(self, question: str) -> PromptTemplate:
        """ë‹¤ì¤‘ ë°©ë²• í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = """
ë‹¤ìŒ ë¬¸ì œë¥¼ ì„¸ ê°€ì§€ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {question}

ì„¸ ê°€ì§€ í’€ì´ ë°©ë²•:
1) ì§ì ‘ ê³„ì‚° ë°©ë²•:
   - ì£¼ì–´ì§„ ìˆ«ìë¥¼ ì§ì ‘ ê³„ì‚°

2) ë¹„ìœ¨ í™œìš© ë°©ë²•:
   - ì „ì²´ì— ëŒ€í•œ ë¹„ìœ¨ë¡œ ê³„ì‚°

3) ë‹¨ê³„ë³„ ë¶„í•´ ë°©ë²•:
   - ë¬¸ì œë¥¼ ì‘ì€ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê³„ì‚°

ê° ë°©ë²•ì˜ ë‹µì•ˆì„ ì œì‹œí•˜ê³ , ê²°ê³¼ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì‹œì˜¤.

ë‹µì•ˆ:
"""
        return PromptTemplate(
            input_variables=["question"],
            template=template
        )

    def execute_self_consistency(self, question: str) -> Dict[str, Any]:
        """Self-Consistency ì‹¤í–‰"""
        prompt = self.create_multi_method_prompt(question)
        chain = prompt | self.model | StrOutputParser()

        # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ ì¼ê´€ì„± í™•ì¸
        responses = []
        for i in range(self.num_attempts):
            response = chain.invoke({"question": question})
            responses.append(response)

        # ì¼ê´€ì„± ë¶„ì„
        consistency_analysis = self._analyze_consistency(responses)

        return {
            "question": question,
            "responses": responses,
            "consistency_analysis": consistency_analysis,
            "final_answer": self._extract_final_answer(responses[0])  # ì²« ë²ˆì§¸ ì‘ë‹µì—ì„œ ë‹µ ì¶”ì¶œ
        }

    def _analyze_consistency(self, responses: List[str]) -> Dict[str, Any]:
        """ì¼ê´€ì„± ë¶„ì„"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
        answers = []
        for response in responses:
            answer = self._extract_final_answer(response)
            answers.append(answer)

        # ê°€ì¥ ë¹ˆë²ˆí•œ ë‹µ ì°¾ê¸°
        from collections import Counter
        answer_counts = Counter(answers)
        most_common = answer_counts.most_common(1)[0] if answer_counts else ("", 0)

        consistency_score = most_common[1] / len(answers) if answers else 0

        return {
            "all_answers": answers,
            "most_common_answer": most_common[0],
            "consistency_score": consistency_score,
            "agreement_level": "ë†’ìŒ" if consistency_score >= 0.8 else "ë³´í†µ" if consistency_score >= 0.6 else "ë‚®ìŒ"
        }

    def _extract_final_answer(self, response: str) -> str:
        """ì‘ë‹µì—ì„œ ìµœì¢… ë‹µ ì¶”ì¶œ"""
        import re

        # ìˆ«ì íŒ¨í„´ ì°¾ê¸°
        numbers = re.findall(r'\b\d+(?:\.\d+)?\b', response)

        # "ë‹µ:", "ë”°ë¼ì„œ", "ê²°ë¡ " ë“±ì˜ í‚¤ì›Œë“œ ê·¼ì²˜ì—ì„œ ìˆ«ì ì°¾ê¸°
        answer_indicators = ["ë‹µ:", "ë”°ë¼ì„œ", "ê²°ë¡ ", "ìµœì¢…", "ì´"]

        for indicator in answer_indicators:
            if indicator in response:
                # í•´ë‹¹ ë¶€ë¶„ ì´í›„ì˜ ì²« ë²ˆì§¸ ìˆ«ì ì°¾ê¸°
                parts = response.split(indicator, 1)
                if len(parts) > 1:
                    after_indicator = parts[1]
                    found_numbers = re.findall(r'\b\d+(?:\.\d+)?\b', after_indicator[:100])  # ì²˜ìŒ 100ì ë‚´ì—ì„œ
                    if found_numbers:
                        return found_numbers[0]

        # ë§ˆì§€ë§‰ ìˆ«ì ë°˜í™˜ (fallback)
        return numbers[-1] if numbers else "ì¶”ì¶œ ì‹¤íŒ¨"

# ì‚¬ìš© ì˜ˆì‹œ
self_consistency = SelfConsistencySystem(openai_llm, num_attempts=2)

consistency_result = self_consistency.execute_self_consistency(test_question)

print("=== Self-Consistency ê²°ê³¼ ===")
print(f"ì§ˆë¬¸: {consistency_result['question']}")
print(f"ìµœì¢… ë‹µ: {consistency_result['final_answer']}")
print(f"ì¼ê´€ì„± ì ìˆ˜: {consistency_result['consistency_analysis']['consistency_score']:.2f}")
print(f"ì¼ì¹˜ë„: {consistency_result['consistency_analysis']['agreement_level']}")

print("\n=== ì„¸ë¶€ ì‘ë‹µ ===")
for i, response in enumerate(consistency_result['responses'], 1):
    print(f"\nì‘ë‹µ {i}:")
    print(response[:400] + "..." if len(response) > 400 else response)
```

### 3. Program-Aided Language (PAL) ì‹œìŠ¤í…œ

```python
class PALSystem:
    def __init__(self, model: Any):
        """Program-Aided Language ì‹œìŠ¤í…œ"""
        self.model = model

    def create_pal_prompt(self, question: str) -> PromptTemplate:
        """PAL í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = """
ë‹¤ìŒ ë¬¸ì œë¥¼ Python í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {question}

# ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ Python ìŠ¤íƒ€ì¼ ì˜ì‚¬ì½”ë“œ:
def solve_problem():
    # 1. ë³€ìˆ˜ ì •ì˜
    # - ì£¼ì–´ì§„ ê°’ë“¤ì„ ë³€ìˆ˜ë¡œ ì €ì¥

    # 2. ê³„ì‚° ê³¼ì •
    # - í•„ìš”í•œ ê³„ì‚°ì„ ë‹¨ê³„ë³„ë¡œ ìˆ˜í–‰

    # 3. ê²°ê³¼ ë°˜í™˜
    # - ìµœì¢… ê²°ê³¼ ê³„ì‚° ë° ë°˜í™˜

    return result

# ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥
print(solve_problem())

ë‹µì•ˆ:
"""
        return PromptTemplate(
            input_variables=["question"],
            template=template
        )

    def execute_pal(self, question: str) -> Dict[str, Any]:
        """PAL ì‹¤í–‰"""
        prompt = self.create_pal_prompt(question)
        chain = prompt | self.model | StrOutputParser()

        response = chain.invoke({"question": question})

        # ì½”ë“œ ì¶”ì¶œ ë° ë¶„ì„
        code_analysis = self._analyze_code(response)

        return {
            "question": question,
            "response": response,
            "code_analysis": code_analysis
        }

    def _analyze_code(self, response: str) -> Dict[str, Any]:
        """ìƒì„±ëœ ì½”ë“œ ë¶„ì„"""
        import re

        # Python ì½”ë“œ ë¸”ë¡ ì°¾ê¸°
        code_blocks = re.findall(r'```python\n(.*?)\n```', response, re.DOTALL)
        if not code_blocks:
            # ì½”ë“œ ë¸”ë¡ì´ ì—†ìœ¼ë©´ defë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
            def_pattern = r'def\s+\w+.*?(?=\n[^\s]|$)'
            code_blocks = re.findall(def_pattern, response, re.DOTALL)

        analysis = {
            "has_function_definition": "def " in response,
            "has_variables": any(keyword in response for keyword in ["=", "total", "count", "num"]),
            "has_calculations": any(op in response for op in ["*", "+", "-", "/", "%"]),
            "has_return_statement": "return" in response,
            "code_blocks_found": len(code_blocks),
            "executable": self._check_executability(code_blocks[0] if code_blocks else "")
        }

        return analysis

    def _check_executability(self, code: str) -> bool:
        """ì½”ë“œ ì‹¤í–‰ ê°€ëŠ¥ì„± ê²€ì‚¬ (ê°„ë‹¨í•œ êµ¬ë¬¸ ê²€ì‚¬)"""
        try:
            compile(code, '<string>', 'exec')
            return True
        except SyntaxError:
            return False

    def create_logic_puzzle_pal_prompt(self, question: str) -> PromptTemplate:
        """ë…¼ë¦¬ í¼ì¦ìš© PAL í”„ë¡¬í”„íŠ¸"""
        template = """
ë‹¤ìŒ ë…¼ë¦¬ ë¬¸ì œë¥¼ Python í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {question}

# í•´ê²° ì ‘ê·¼ë²•:
def solve_puzzle():
    # 1. ìƒíƒœ ì •ì˜
    # - í˜„ì¬ ìƒíƒœë¥¼ ë³€ìˆ˜ë¡œ í‘œí˜„

    # 2. ì œì•½ì¡°ê±´ í•¨ìˆ˜
    # - ì•ˆì „ì„± ê²€ì‚¬ í•¨ìˆ˜ ì •ì˜

    # 3. ì´ë™ ì‹œë®¬ë ˆì´ì…˜
    # - ê°€ëŠ¥í•œ ì´ë™ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜

    # 4. ìµœì  í•´ íƒìƒ‰
    # - ìµœì†Œ ì´ë™ íšŸìˆ˜ ê³„ì‚°

    return moves_count

ë‹µì•ˆ:
"""
        return PromptTemplate(
            input_variables=["question"],
            template=template
        )

# ì‚¬ìš© ì˜ˆì‹œ
pal_system = PALSystem(openai_llm)

# ìˆ˜í•™ ë¬¸ì œ í…ŒìŠ¤íŠ¸
math_result = pal_system.execute_pal(test_question)

print("=== PAL ê²°ê³¼ (ìˆ˜í•™ ë¬¸ì œ) ===")
print("ìƒì„±ëœ ì½”ë“œ:")
print(math_result['response'])
print("\nì½”ë“œ ë¶„ì„:")
analysis = math_result['code_analysis']
for key, value in analysis.items():
    print(f"- {key}: {value}")

# ë…¼ë¦¬ í¼ì¦ í…ŒìŠ¤íŠ¸
puzzle_question = """
ë†ë¶€ê°€ ëŠ‘ëŒ€, ì–‘, ì–‘ë°°ì¶”ë¥¼ ë°ë¦¬ê³  ê°•ì„ ê±´ë„ˆì•¼ í•©ë‹ˆë‹¤.
ì œì•½ì¡°ê±´:
1. ë†ë¶€ê°€ ì—†ì„ ë•Œ ëŠ‘ëŒ€ì™€ ì–‘ì´ ê°™ì´ ìˆìœ¼ë©´ ëŠ‘ëŒ€ê°€ ì–‘ì„ ì¡ì•„ë¨¹ìŠµë‹ˆë‹¤
2. ë†ë¶€ê°€ ì—†ì„ ë•Œ ì–‘ê³¼ ì–‘ë°°ì¶”ê°€ ê°™ì´ ìˆìœ¼ë©´ ì–‘ì´ ì–‘ë°°ì¶”ë¥¼ ë¨¹ì–´ë²„ë¦½ë‹ˆë‹¤
3. ë³´íŠ¸ì—ëŠ” ë†ë¶€ì™€ í•œ ë¬¼ê±´ë§Œ ì‹¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
ëª¨ë‘ ì•ˆì „í•˜ê²Œ ê±´ë„ˆëŠ”ë° ëª‡ ë²ˆ ì´ë™ì´ í•„ìš”í• ê¹Œìš”?
"""

puzzle_prompt = pal_system.create_logic_puzzle_pal_prompt(puzzle_question)
puzzle_chain = puzzle_prompt | openai_llm | StrOutputParser()
puzzle_result = puzzle_chain.invoke({"question": puzzle_question})

print(f"\n=== PAL ê²°ê³¼ (ë…¼ë¦¬ í¼ì¦) ===")
print(puzzle_result)
```

### 4. Reflexion ì‹œìŠ¤í…œ

```python
class ReflexionSystem:
    def __init__(self, model: Any):
        """Reflexion ì‹œìŠ¤í…œ"""
        self.model = model

    def create_reflexion_prompt(self, question: str) -> PromptTemplate:
        """Reflexion í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = """
ë‹¤ìŒ ë¬¸ì œì— ëŒ€í•´ ë‹¨ê³„ì ìœ¼ë¡œ í•´ê²°í•˜ì—¬ ì´ˆê¸° ë‹µì•ˆì„ ì‘ì„±í•˜ê³ , ìì²´ í‰ê°€ í›„ ê°œì„ í•˜ì‹œì˜¤:

ë¬¸ì œ: {question}

1ë‹¨ê³„: ì´ˆê¸° ë‹µì•ˆ
---
[ì—¬ê¸°ì— ì²« ë²ˆì§¸ ë‹µì•ˆ ì‘ì„±]

2ë‹¨ê³„: ìì²´ í‰ê°€
---
- ì •í™•ì„± ê²€í† : ê³„ì‚°ê³¼ ë…¼ë¦¬ê°€ ì˜¬ë°”ë¥¸ê°€?
- ë…¼ë¦¬ì  ì˜¤ë¥˜ í™•ì¸: ì¶”ë¡  ê³¼ì •ì— ë¹ˆí‹ˆì€ ì—†ëŠ”ê°€?
- ì„¤ëª…ì˜ ëª…í™•ì„± í‰ê°€: ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í–ˆëŠ”ê°€?
- ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ ì‹ë³„: ì–´ë–¤ ë¶€ë¶„ì„ ë³´ì™„í•´ì•¼ í•˜ëŠ”ê°€?

3ë‹¨ê³„: ê°œì„ ëœ ë‹µì•ˆ
---
[í‰ê°€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ëœ ë‹µì•ˆ ì‘ì„±]

ë‹µì•ˆ:
"""
        return PromptTemplate(
            input_variables=["question"],
            template=template
        )

    def execute_reflexion(self, question: str) -> Dict[str, Any]:
        """Reflexion ì‹¤í–‰"""
        prompt = self.create_reflexion_prompt(question)
        chain = prompt | self.model | StrOutputParser()

        response = chain.invoke({"question": question})

        # ì‘ë‹µ ë¶„ì„
        reflection_analysis = self._analyze_reflection(response)

        return {
            "question": question,
            "full_response": response,
            "reflection_analysis": reflection_analysis
        }

    def _analyze_reflection(self, response: str) -> Dict[str, Any]:
        """Reflexion ì‘ë‹µ ë¶„ì„"""
        stages = ["1ë‹¨ê³„", "2ë‹¨ê³„", "3ë‹¨ê³„"]
        stage_content = {}

        for i, stage in enumerate(stages):
            if stage in response:
                # í˜„ì¬ ë‹¨ê³„ì™€ ë‹¤ìŒ ë‹¨ê³„ ì‚¬ì´ì˜ ë‚´ìš© ì¶”ì¶œ
                start_idx = response.find(stage)
                if i < len(stages) - 1:
                    next_stage_idx = response.find(stages[i + 1])
                    if next_stage_idx != -1:
                        stage_content[stage] = response[start_idx:next_stage_idx].strip()
                    else:
                        stage_content[stage] = response[start_idx:].strip()
                else:
                    stage_content[stage] = response[start_idx:].strip()

        analysis = {
            "has_initial_answer": "1ë‹¨ê³„" in response,
            "has_self_evaluation": "2ë‹¨ê³„" in response,
            "has_improved_answer": "3ë‹¨ê³„" in response,
            "stage_content": stage_content,
            "improvement_indicators": self._find_improvement_indicators(response),
            "self_correction_score": self._calculate_self_correction_score(response)
        }

        return analysis

    def _find_improvement_indicators(self, response: str) -> List[str]:
        """ê°œì„  ì§€í‘œ ì°¾ê¸°"""
        indicators = []
        improvement_keywords = [
            "ìˆ˜ì •", "ë³´ì™„", "ê°œì„ ", "ì •ì •", "ì¶”ê°€", "ëª…í™•íˆ", "ë” ì •í™•í•˜ê²Œ", "êµ¬ì²´ì ìœ¼ë¡œ"
        ]

        for keyword in improvement_keywords:
            if keyword in response:
                indicators.append(keyword)

        return indicators

    def _calculate_self_correction_score(self, response: str) -> float:
        """ìê¸° êµì • ì ìˆ˜ ê³„ì‚°"""
        score_factors = [
            "ì˜¤ë¥˜" in response or "í‹€ë¦°" in response,  # ì˜¤ë¥˜ ì¸ì‹
            "ìˆ˜ì •" in response or "ê°œì„ " in response,  # ìˆ˜ì • ì˜ë„
            len(response.split("3ë‹¨ê³„")) > 1,  # ê°œì„ ëœ ë‹µì•ˆ ì¡´ì¬
            "ë” ì •í™•" in response or "ë” ëª…í™•" in response,  # í’ˆì§ˆ í–¥ìƒ ì–¸ê¸‰
            "ê²€í† " in response or "í™•ì¸" in response  # ê²€ì¦ ê³¼ì •
        ]

        return sum(score_factors) / len(score_factors)

    def create_iterative_reflexion_prompt(self, question: str, previous_response: str) -> PromptTemplate:
        """ë°˜ë³µì  Reflexion í”„ë¡¬í”„íŠ¸"""
        template = """
ì´ì „ ë‹µì•ˆì„ ê²€í† í•˜ê³  ì¶”ê°€ë¡œ ê°œì„ í•´ë³´ì„¸ìš”:

ì›ë˜ ë¬¸ì œ: {question}

ì´ì „ ë‹µì•ˆ:
{previous_response}

ì¶”ê°€ ê°œì„ ì‚¬í•­:
1. ì´ì „ ë‹µì•ˆì˜ ê°•ì ê³¼ ì•½ì  ë¶„ì„
2. ë†“ì¹œ ë¶€ë¶„ì´ë‚˜ ì¶”ê°€ ê³ ë ¤ì‚¬í•­ ì‹ë³„
3. ë” ë‚˜ì€ ì„¤ëª… ë°©ì‹ì´ë‚˜ ì ‘ê·¼ë²• ì œì•ˆ
4. ìµœì¢… ê°œì„ ëœ ë‹µì•ˆ ì œì‹œ

ê°œì„ ëœ ë‹µì•ˆ:
"""
        return PromptTemplate(
            input_variables=["question", "previous_response"],
            template=template
        )

# ì‚¬ìš© ì˜ˆì‹œ
reflexion_system = ReflexionSystem(openai_llm)

# ê¸°ë³¸ Reflexion ì‹¤í–‰
reflexion_result = reflexion_system.execute_reflexion(test_question)

print("=== Reflexion ê²°ê³¼ ===")
print("ì „ì²´ ì‘ë‹µ:")
print(reflexion_result['full_response'])

print(f"\n=== ë¶„ì„ ê²°ê³¼ ===")
analysis = reflexion_result['reflection_analysis']
print(f"ì´ˆê¸° ë‹µì•ˆ ì¡´ì¬: {analysis['has_initial_answer']}")
print(f"ìì²´ í‰ê°€ ì¡´ì¬: {analysis['has_self_evaluation']}")
print(f"ê°œì„ ëœ ë‹µì•ˆ ì¡´ì¬: {analysis['has_improved_answer']}")
print(f"ìê¸° êµì • ì ìˆ˜: {analysis['self_correction_score']:.2f}")
print(f"ê°œì„  ì§€í‘œ: {', '.join(analysis['improvement_indicators'])}")

# ë°˜ë³µì  ê°œì„  (ì˜µì…˜)
if reflexion_result['reflection_analysis']['self_correction_score'] < 0.8:
    print("\n=== ì¶”ê°€ ê°œì„  ìˆ˜í–‰ ===")
    iterative_prompt = reflexion_system.create_iterative_reflexion_prompt(
        test_question,
        reflexion_result['full_response']
    )
    iterative_chain = iterative_prompt | openai_llm | StrOutputParser()
    improved_response = iterative_chain.invoke({
        "question": test_question,
        "previous_response": reflexion_result['full_response']
    })
    print("ê°œì„ ëœ ë‹µì•ˆ:")
    print(improved_response)
```

### 5. í†µí•© CoT ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ

```python
class CoTBenchmarkingSystem:
    def __init__(self, models: Dict[str, Any]):
        """CoT ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ"""
        self.models = models
        self.methods = {
            "zero_shot": self._create_zero_shot,
            "one_shot": self._create_one_shot,
            "cot": self._create_cot,
            "self_consistency": self._create_self_consistency,
            "pal": self._create_pal,
            "reflexion": self._create_reflexion
        }

    def _create_zero_shot(self, question: str) -> str:
        """Zero-shot í”„ë¡¬í”„íŠ¸"""
        return f"ë‹¤ìŒ ë¬¸ì œë¥¼ í•´ê²°í•˜ì‹œì˜¤:\n\në¬¸ì œ: {question}\n\në‹µì•ˆ:"

    def _create_one_shot(self, question: str) -> str:
        """One-shot í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒì€ ìˆ˜í•™ ë¬¸ì œ í•´ê²° ì˜ˆì‹œì…ë‹ˆë‹¤:

ì˜ˆì‹œ: 30ëª… ì¤‘ 40%ê°€ ë‚¨í•™ìƒì´ë©´ ì—¬í•™ìƒì€?
í’€ì´: 30 Ã— 0.4 = 12ëª…(ë‚¨í•™ìƒ), 30 - 12 = 18ëª…(ì—¬í•™ìƒ)

ë¬¸ì œ: {question}
ë‹µì•ˆ:
"""

    def _create_cot(self, question: str) -> str:
        """CoT í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {question}

1ë‹¨ê³„: ë¬¸ì œ ì´í•´ ë° ì •ë³´ ì •ë¦¬
2ë‹¨ê³„: í•´ê²° ì „ëµ ê³„íš
3ë‹¨ê³„: ë‹¨ê³„ë³„ ê³„ì‚° ì‹¤í–‰
4ë‹¨ê³„: ê²°ê³¼ ê²€ì¦

ë‹µì•ˆ:
"""

    def _create_self_consistency(self, question: str) -> str:
        """Self-Consistency í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë¬¸ì œë¥¼ ì„¸ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ í•´ê²°í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•˜ì‹œì˜¤:

ë¬¸ì œ: {question}

ë°©ë²•1: ì§ì ‘ ê³„ì‚°
ë°©ë²•2: ë¹„ìœ¨ í™œìš©
ë°©ë²•3: ë‹¨ê³„ë³„ ë¶„í•´

ê° ë°©ë²•ì˜ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì‹œì˜¤.

ë‹µì•ˆ:
"""

    def _create_pal(self, question: str) -> str:
        """PAL í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë¬¸ì œë¥¼ Python ì½”ë“œë¡œ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {question}

def solve():
    # ë³€ìˆ˜ ì •ì˜ ë° ê³„ì‚° ê³¼ì •
    return result

ë‹µì•ˆ:
"""

    def _create_reflexion(self, question: str) -> str:
        """Reflexion í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ìì²´ í‰ê°€ í›„ ê°œì„ í•˜ì‹œì˜¤:

ë¬¸ì œ: {question}

1ë‹¨ê³„: ì´ˆê¸° ë‹µì•ˆ
2ë‹¨ê³„: ìì²´ í‰ê°€ (ì •í™•ì„±, ë…¼ë¦¬ì„±, ëª…í™•ì„±)
3ë‹¨ê³„: ê°œì„ ëœ ë‹µì•ˆ

ë‹µì•ˆ:
"""

    def benchmark_single_question(
        self,
        question: str,
        correct_answer: str = None,
        model_name: str = "openai"
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¬¸ì œ ë²¤ì¹˜ë§ˆí‚¹"""
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not available")

        model = self.models[model_name]
        results = {}

        for method_name, method_func in self.methods.items():
            try:
                prompt_text = method_func(question)

                # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
                from langchain_core.prompts import PromptTemplate
                prompt = PromptTemplate(
                    input_variables=[],
                    template=prompt_text
                )

                chain = prompt | model | StrOutputParser()
                response = chain.invoke({})

                # ì‘ë‹µ ë¶„ì„
                analysis = self._analyze_response(response, correct_answer, method_name)

                results[method_name] = {
                    "response": response,
                    "analysis": analysis
                }

            except Exception as e:
                results[method_name] = {
                    "response": f"Error: {str(e)}",
                    "analysis": {"error": True}
                }

        return results

    def _analyze_response(
        self,
        response: str,
        correct_answer: str = None,
        method: str = None
    ) -> Dict[str, Any]:
        """ì‘ë‹µ ë¶„ì„"""
        analysis = {
            "length": len(response),
            "has_numbers": bool(re.findall(r'\d+', response)),
            "has_calculation": any(op in response for op in ["Ã—", "Ã·", "+", "-", "=", "*", "/"]),
            "step_count": len([line for line in response.split('\n') if line.strip() and any(c.isdigit() for c in line)]),
            "shows_process": "ë‹¨ê³„" in response or "step" in response.lower(),
            "confidence_indicators": sum([
                "ë”°ë¼ì„œ" in response,
                "ê²°ë¡ " in response,
                "ë‹µ:" in response,
                "=" in response
            ])
        }

        # ì •ë‹µê³¼ ë¹„êµ (ì œê³µëœ ê²½ìš°)
        if correct_answer:
            extracted_answer = self._extract_answer(response)
            analysis["extracted_answer"] = extracted_answer
            analysis["is_correct"] = extracted_answer == correct_answer

        return analysis

    def _extract_answer(self, response: str) -> str:
        """ì‘ë‹µì—ì„œ ë‹µ ì¶”ì¶œ"""
        import re

        # ì¼ë°˜ì ì¸ ë‹µ íŒ¨í„´ë“¤
        patterns = [
            r'ë‹µ[:ï¼š]\s*(\d+)',
            r'ê²°ë¡ [:ï¼š]\s*(\d+)',
            r'ë”°ë¼ì„œ\s*(\d+)',
            r'ì´\s*(\d+)',
            r'=\s*(\d+)',
            r'(\d+)ëª…',
            r'(\d+)ê°œ'
        ]

        for pattern in patterns:
            matches = re.findall(pattern, response)
            if matches:
                return matches[-1]  # ë§ˆì§€ë§‰ ë§¤ì¹˜ ë°˜í™˜

        # ìˆ«ìë§Œ ì¶”ì¶œ (fallback)
        numbers = re.findall(r'\d+', response)
        return numbers[-1] if numbers else "ì¶”ì¶œì‹¤íŒ¨"

    def run_comprehensive_benchmark(
        self,
        test_cases: List[Dict[str, str]],
        model_names: List[str] = None
    ) -> Dict[str, Any]:
        """ì¢…í•© ë²¤ì¹˜ë§ˆí‚¹"""
        if model_names is None:
            model_names = list(self.models.keys())

        all_results = {}

        for model_name in model_names:
            print(f"\n=== ëª¨ë¸: {model_name} ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘ ===")
            model_results = {}

            for i, test_case in enumerate(test_cases):
                question = test_case["question"]
                correct_answer = test_case.get("correct_answer")

                print(f"ë¬¸ì œ {i+1} ì²˜ë¦¬ ì¤‘...")

                results = self.benchmark_single_question(
                    question,
                    correct_answer,
                    model_name
                )
                model_results[f"question_{i+1}"] = results

            all_results[model_name] = model_results

        # í†µê³„ ìƒì„±
        stats = self._generate_statistics(all_results)

        return {
            "detailed_results": all_results,
            "statistics": stats
        }

    def _generate_statistics(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """í†µê³„ ìƒì„±"""
        stats = {}

        for model_name, model_results in results.items():
            model_stats = {
                "method_performance": {},
                "overall_metrics": {}
            }

            # ë°©ë²•ë³„ ì„±ëŠ¥
            for method in self.methods.keys():
                method_scores = []
                for question_key, question_results in model_results.items():
                    if method in question_results and "analysis" in question_results[method]:
                        analysis = question_results[method]["analysis"]
                        if not analysis.get("error", False):
                            # ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚°
                            score = 0
                            if analysis.get("has_calculation", False):
                                score += 1
                            if analysis.get("shows_process", False):
                                score += 1
                            if analysis.get("confidence_indicators", 0) > 0:
                                score += 1
                            if analysis.get("is_correct", False):
                                score += 2  # ì •ë‹µì¼ ê²½ìš° ê°€ì‚°ì 

                            method_scores.append(score / 5.0)  # 0-1 ì •ê·œí™”

                if method_scores:
                    model_stats["method_performance"][method] = {
                        "average_score": sum(method_scores) / len(method_scores),
                        "question_count": len(method_scores)
                    }

            stats[model_name] = model_stats

        return stats

# ì‚¬ìš© ì˜ˆì‹œ
models = {
    "openai": openai_llm,
    "phi3": phi3_llm if 'phi3_llm' in globals() else None,
    "gemma": gemma_llm if 'gemma_llm' in globals() else None
}

# None ê°’ ì œê±°
models = {k: v for k, v in models.items() if v is not None}

benchmark_system = CoTBenchmarkingSystem(models)

# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
test_cases = [
    {
        "question": "í•™êµì—ì„œ 500ëª…ì˜ í•™ìƒì´ ìˆìŠµë‹ˆë‹¤. ì´ ì¤‘ 30%ëŠ” 5í•™ë…„ì´ê³ , 20%ëŠ” 6í•™ë…„ í•™ìƒì…ë‹ˆë‹¤. 5í•™ë…„ í•™ìƒë“¤ ì¤‘ 60%ëŠ” ìˆ˜í•™ ë™ì•„ë¦¬ì— ìˆê³ , ë‚˜ë¨¸ì§€ëŠ” ê³¼í•™ ë™ì•„ë¦¬ì— ìˆìŠµë‹ˆë‹¤. 6í•™ë…„ í•™ìƒë“¤ ì¤‘ 70%ëŠ” ìˆ˜í•™ ë™ì•„ë¦¬ì— ìˆê³ , ë‚˜ë¨¸ì§€ëŠ” ê³¼í•™ ë™ì•„ë¦¬ì— ìˆìŠµë‹ˆë‹¤. ê³¼í•™ ë™ì•„ë¦¬ì—ëŠ” ëª‡ ëª…ì˜ í•™ìƒì´ ìˆë‚˜ìš”?",
        "correct_answer": "90"
    },
    {
        "question": "í•œ ìƒìì— ë¹¨ê°„ ê³µ 15ê°œì™€ íŒŒë€ ê³µ 25ê°œê°€ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ê³µì˜ 40%ë¥¼ ë½‘ì•˜ì„ ë•Œ, ë½‘ì€ ê³µ ì¤‘ ë¹¨ê°„ ê³µì´ 6ê°œì˜€ë‹¤ë©´ íŒŒë€ ê³µì€ ëª‡ ê°œë¥¼ ë½‘ì•˜ë‚˜ìš”?",
        "correct_answer": "10"
    }
]

# ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰ (OpenAI ëª¨ë¸ë§Œ ì‚¬ìš©)
benchmark_results = benchmark_system.run_comprehensive_benchmark(
    test_cases,
    model_names=["openai"]
)

# ê²°ê³¼ ì¶œë ¥
print("\n=== ë²¤ì¹˜ë§ˆí‚¹ ê²°ê³¼ ìš”ì•½ ===")
stats = benchmark_results["statistics"]

for model_name, model_stats in stats.items():
    print(f"\nã€{model_name.upper()} ëª¨ë¸ ì„±ëŠ¥ã€‘")
    method_perf = model_stats["method_performance"]

    # ë°©ë²•ë³„ ì ìˆ˜ ì •ë ¬
    sorted_methods = sorted(
        method_perf.items(),
        key=lambda x: x[1]["average_score"],
        reverse=True
    )

    for method, perf in sorted_methods:
        print(f"{method:15}: {perf['average_score']:.3f} (í…ŒìŠ¤íŠ¸ {perf['question_count']}ê°œ)")
```

## ğŸš€ ì‹¤ìŠµí•´ë³´ê¸°

### ì‹¤ìŠµ 1: ë…¼ë¦¬ í¼ì¦ í•´ê²° ì‹œìŠ¤í…œ

ë†ë¶€ì™€ ëŠ‘ëŒ€, ì–‘, ì–‘ë°°ì¶” ë¬¸ì œë¥¼ ë‹¤ì–‘í•œ CoT ê¸°ë²•ìœ¼ë¡œ í•´ê²°í•´ë³´ì„¸ìš”.

```python
# ë…¼ë¦¬ í¼ì¦ í•´ê²° ì‹œìŠ¤í…œ êµ¬í˜„
class LogicPuzzleSolver:
    def __init__(self):
        # ì´ˆê¸°í™” ì½”ë“œ ì‘ì„±
        pass

    def solve_with_cot(self, puzzle_description):
        # CoT ë°©ì‹ìœ¼ë¡œ í¼ì¦ í•´ê²°
        pass

    def solve_with_pal(self, puzzle_description):
        # PAL ë°©ì‹ìœ¼ë¡œ í¼ì¦ í•´ê²°
        pass

    def compare_approaches(self, puzzle_description):
        # ì—¬ëŸ¬ ì ‘ê·¼ ë°©ì‹ ë¹„êµ
        pass
```

### ì‹¤ìŠµ 2: ìˆ˜í•™ ë¬¸ì œ ìë™ í•´ê²° ì‹œìŠ¤í…œ

ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í•´ê²°í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
# ìˆ˜í•™ ë¬¸ì œ ìë™ í•´ê²° ì‹œìŠ¤í…œ êµ¬í˜„
class MathProblemSolver:
    def __init__(self):
        # ì´ˆê¸°í™” ì½”ë“œ ì‘ì„±
        pass

    def analyze_problem_type(self, problem):
        # ë¬¸ì œ ìœ í˜• ë¶„ì„
        pass

    def generate_solution_strategy(self, problem, problem_type):
        # í•´ê²° ì „ëµ ìƒì„±
        pass

    def execute_with_self_consistency(self, problem):
        # Self-Consistency ì ìš©
        pass
```

### ì‹¤ìŠµ 3: ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ì‹œìŠ¤í…œ

í…ìŠ¤íŠ¸ì™€ ìˆ˜ì‹ì„ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” ì¶”ë¡  ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
# ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬í˜„
class MultimodalReasoningSystem:
    def __init__(self):
        # ì´ˆê¸°í™” ì½”ë“œ ì‘ì„±
        pass

    def parse_mathematical_expressions(self, text):
        # ìˆ˜ì‹ íŒŒì‹±
        pass

    def create_visual_explanation(self, solution_steps):
        # ì‹œê°ì  ì„¤ëª… ìƒì„±
        pass

    def integrate_reasoning_modes(self, problem):
        # ì¶”ë¡  ëª¨ë“œ í†µí•©
        pass
```

## ğŸ“‹ í•´ë‹µ

### ì‹¤ìŠµ 1: ë…¼ë¦¬ í¼ì¦ í•´ê²° ì‹œìŠ¤í…œ

```python
class LogicPuzzleSolver:
    def __init__(self, models: Dict[str, Any]):
        """ë…¼ë¦¬ í¼ì¦ í•´ê²° ì‹œìŠ¤í…œ"""
        self.models = models

    def solve_with_cot(self, puzzle_description: str, model_name: str = "openai") -> Dict[str, Any]:
        """CoT ë°©ì‹ìœ¼ë¡œ í¼ì¦ í•´ê²°"""
        cot_template = """
ë‹¤ìŒ ë…¼ë¦¬ í¼ì¦ì„ ë‹¨ê³„ë³„ë¡œ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {puzzle}

í•´ê²° ê³¼ì •:
1ë‹¨ê³„: ë¬¸ì œ ìƒí™© ì´í•´
- ë“±ì¥ ì¸ë¬¼/ë¬¼ì²´ íŒŒì•…
- ì œì•½ ì¡°ê±´ ì •ë¦¬
- ëª©í‘œ ìƒíƒœ ì„¤ì •

2ë‹¨ê³„: í•´ê²° ì „ëµ ê³„íš
- ê°€ëŠ¥í•œ ì ‘ê·¼ ë°©ë²• ê²€í† 
- ìµœì  í•´ê²° ìˆœì„œ ê³„íš

3ë‹¨ê³„: ë‹¨ê³„ë³„ í•´ê²° ì‹¤í–‰
- ê° ì´ë™ ë‹¨ê³„ ì‹œë®¬ë ˆì´ì…˜
- ì œì•½ ì¡°ê±´ ì¤€ìˆ˜ í™•ì¸
- ì¤‘ê°„ ìƒíƒœ ê²€ì¦

4ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ ë° ìµœì í™”
- í•´ê²° ì™„ë£Œ í™•ì¸
- ìµœì†Œ ì´ë™ íšŸìˆ˜ ê²€ì¦
- ë‹¤ë¥¸ í•´ë²• ê°€ëŠ¥ì„± ê²€í† 

ë‹µì•ˆ:
"""

        prompt = PromptTemplate(
            input_variables=["puzzle"],
            template=cot_template
        )

        model = self.models[model_name]
        chain = prompt | model | StrOutputParser()

        response = chain.invoke({"puzzle": puzzle_description})

        # í•´ë²• ë¶„ì„
        analysis = self._analyze_solution(response)

        return {
            "method": "Chain of Thought",
            "response": response,
            "analysis": analysis
        }

    def solve_with_pal(self, puzzle_description: str, model_name: str = "openai") -> Dict[str, Any]:
        """PAL ë°©ì‹ìœ¼ë¡œ í¼ì¦ í•´ê²°"""
        pal_template = """
ë‹¤ìŒ ë…¼ë¦¬ í¼ì¦ì„ Python í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {puzzle}

# í”„ë¡œê·¸ë˜ë°ì  í•´ê²° ì ‘ê·¼:
def solve_river_crossing():
    # ìƒíƒœ í‘œí˜„: (farmer, wolf, sheep, cabbage) - 0: ì™¼ìª½, 1: ì˜¤ë¥¸ìª½
    initial_state = (0, 0, 0, 0)  # ëª¨ë‘ ì™¼ìª½ ì‹œì‘
    target_state = (1, 1, 1, 1)   # ëª¨ë‘ ì˜¤ë¥¸ìª½ ëª©í‘œ

    def is_safe_state(state):
        # ì•ˆì „ì„± ê²€ì‚¬ í•¨ìˆ˜
        farmer, wolf, sheep, cabbage = state

        # ë†ë¶€ê°€ ì—†ëŠ” ìª½ì—ì„œ ìœ„í—˜í•œ ì¡°í•© ì²´í¬
        if farmer == 0:  # ë†ë¶€ê°€ ì™¼ìª½ì— ìˆì„ ë•Œ, ì˜¤ë¥¸ìª½ ìƒíƒœ ì²´í¬
            if wolf == 1 and sheep == 1:  # ëŠ‘ëŒ€ì™€ ì–‘ì´ ê°™ì´
                return False
            if sheep == 1 and cabbage == 1:  # ì–‘ê³¼ ì–‘ë°°ì¶”ê°€ ê°™ì´
                return False
        else:  # ë†ë¶€ê°€ ì˜¤ë¥¸ìª½ì— ìˆì„ ë•Œ, ì™¼ìª½ ìƒíƒœ ì²´í¬
            if wolf == 0 and sheep == 0:
                return False
            if sheep == 0 and cabbage == 0:
                return False
        return True

    def get_possible_moves(state):
        # ê°€ëŠ¥í•œ ì´ë™ ìƒì„±
        moves = []
        farmer, wolf, sheep, cabbage = state

        # ë†ë¶€ í˜¼ì ì´ë™
        new_farmer = 1 - farmer
        new_state = (new_farmer, wolf, sheep, cabbage)
        if is_safe_state(new_state):
            moves.append(("farmer", new_state))

        # ë†ë¶€ + ëŠ‘ëŒ€
        if wolf == farmer:
            new_state = (new_farmer, new_farmer, sheep, cabbage)
            if is_safe_state(new_state):
                moves.append(("farmer_wolf", new_state))

        # ë†ë¶€ + ì–‘
        if sheep == farmer:
            new_state = (new_farmer, wolf, new_farmer, cabbage)
            if is_safe_state(new_state):
                moves.append(("farmer_sheep", new_state))

        # ë†ë¶€ + ì–‘ë°°ì¶”
        if cabbage == farmer:
            new_state = (new_farmer, wolf, sheep, new_farmer)
            if is_safe_state(new_state):
                moves.append(("farmer_cabbage", new_state))

        return moves

    def solve_bfs():
        # BFSë¡œ ìµœë‹¨ ê²½ë¡œ íƒìƒ‰
        from collections import deque

        queue = deque([(initial_state, [])])
        visited = {initial_state}

        while queue:
            current_state, path = queue.popleft()

            if current_state == target_state:
                return path

            for move_name, next_state in get_possible_moves(current_state):
                if next_state not in visited:
                    visited.add(next_state)
                    new_path = path + [(move_name, next_state)]
                    queue.append((next_state, new_path))

        return None  # í•´ê°€ ì—†ëŠ” ê²½ìš°

    # í•´ê²° ì‹¤í–‰
    solution_path = solve_bfs()

    if solution_path:
        print(f"ìµœì†Œ ì´ë™ íšŸìˆ˜: {len(solution_path)}")
        print("ì´ë™ ìˆœì„œ:")
        for i, (move, state) in enumerate(solution_path, 1):
            print(f"{i}. {move}: {state}")
        return len(solution_path)
    else:
        return -1  # í•´ê°€ ì—†ìŒ

# ì‹¤í–‰
result = solve_river_crossing()
print(f"ë‹µ: {result}ë²ˆ ì´ë™")

ë‹µì•ˆ:
"""

        prompt = PromptTemplate(
            input_variables=["puzzle"],
            template=pal_template
        )

        model = self.models[model_name]
        chain = prompt | model | StrOutputParser()

        response = chain.invoke({"puzzle": puzzle_description})

        # ì½”ë“œ ë¶„ì„
        code_analysis = self._analyze_generated_code(response)

        return {
            "method": "Program-Aided Language",
            "response": response,
            "code_analysis": code_analysis
        }

    def solve_with_reflexion(self, puzzle_description: str, model_name: str = "openai") -> Dict[str, Any]:
        """Reflexion ë°©ì‹ìœ¼ë¡œ í¼ì¦ í•´ê²°"""
        reflexion_template = """
ë‹¤ìŒ ë…¼ë¦¬ í¼ì¦ì„ í•´ê²°í•˜ê³  ìì²´ ê²€í† ë¥¼ í†µí•´ ê°œì„ í•˜ì‹œì˜¤:

ë¬¸ì œ: {puzzle}

1ë‹¨ê³„: ì´ˆê¸° í•´ê²° ì‹œë„
---
[ì²« ë²ˆì§¸ í•´ê²° ê³¼ì •ê³¼ ë‹µì•ˆ]

2ë‹¨ê³„: ìì²´ í‰ê°€
---
- ë…¼ë¦¬ì  ì •í™•ì„±: ê° ë‹¨ê³„ê°€ ì œì•½ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ê°€?
- ì™„ì „ì„±: ëª¨ë“  ìš”ì†Œê°€ ìµœì¢… ëª©í‘œì— ë„ë‹¬í–ˆëŠ”ê°€?
- ìµœì ì„±: ë” ì ì€ ì´ë™ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥í•œê°€?
- ê²€ì¦: ê° ë‹¨ê³„ì—ì„œ ì•ˆì „ì„±ì´ ë³´ì¥ë˜ëŠ”ê°€?

3ë‹¨ê³„: ê°œì„ ëœ í•´ê²°ì±…
---
[í‰ê°€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ëœ ë‹µì•ˆ]

ë‹µì•ˆ:
"""

        prompt = PromptTemplate(
            input_variables=["puzzle"],
            template=reflexion_template
        )

        model = self.models[model_name]
        chain = prompt | model | StrOutputParser()

        response = chain.invoke({"puzzle": puzzle_description})

        # Reflexion ë¶„ì„
        reflexion_analysis = self._analyze_reflexion_quality(response)

        return {
            "method": "Reflexion",
            "response": response,
            "reflexion_analysis": reflexion_analysis
        }

    def compare_approaches(self, puzzle_description: str, model_name: str = "openai") -> Dict[str, Any]:
        """ì—¬ëŸ¬ ì ‘ê·¼ ë°©ì‹ ë¹„êµ"""
        results = {}

        # ê° ë°©ë²•ìœ¼ë¡œ í•´ê²°
        print("CoT ë°©ì‹ìœ¼ë¡œ í•´ê²° ì¤‘...")
        results["cot"] = self.solve_with_cot(puzzle_description, model_name)

        print("PAL ë°©ì‹ìœ¼ë¡œ í•´ê²° ì¤‘...")
        results["pal"] = self.solve_with_pal(puzzle_description, model_name)

        print("Reflexion ë°©ì‹ìœ¼ë¡œ í•´ê²° ì¤‘...")
        results["reflexion"] = self.solve_with_reflexion(puzzle_description, model_name)

        # ë¹„êµ ë¶„ì„
        comparison = self._compare_solution_quality(results)

        return {
            "individual_results": results,
            "comparison_analysis": comparison
        }

    def _analyze_solution(self, response: str) -> Dict[str, Any]:
        """í•´ë²• ë¶„ì„"""
        import re

        # ì´ë™ íšŸìˆ˜ ì¶”ì¶œ
        move_patterns = [
            r'(\d+)ë²ˆ ì´ë™',
            r'(\d+)íšŒ ì´ë™',
            r'ì´ (\d+)',
            r'ìµœì†Œ (\d+)'
        ]

        move_count = None
        for pattern in move_patterns:
            matches = re.findall(pattern, response)
            if matches:
                move_count = int(matches[0])
                break

        return {
            "extracted_move_count": move_count,
            "has_step_by_step": "ë‹¨ê³„" in response,
            "has_constraint_check": "ì œì•½" in response or "ì¡°ê±´" in response,
            "has_verification": "ê²€ì¦" in response or "í™•ì¸" in response,
            "response_length": len(response),
            "shows_reasoning": "ë”°ë¼ì„œ" in response or "ê·¸ëŸ¬ë¯€ë¡œ" in response
        }

    def _analyze_generated_code(self, response: str) -> Dict[str, Any]:
        """ìƒì„±ëœ ì½”ë“œ ë¶„ì„"""
        code_analysis = {
            "has_function_definition": "def " in response,
            "has_state_representation": "state" in response,
            "has_safety_check": "is_safe" in response or "safe" in response,
            "has_bfs_or_search": "bfs" in response.lower() or "search" in response,
            "has_queue_or_stack": "queue" in response or "stack" in response,
            "has_visited_tracking": "visited" in response,
            "code_complexity": response.count("def ") + response.count("if ") + response.count("for ")
        }

        return code_analysis

    def _analyze_reflexion_quality(self, response: str) -> Dict[str, Any]:
        """Reflexion í’ˆì§ˆ ë¶„ì„"""
        stages = ["1ë‹¨ê³„", "2ë‹¨ê³„", "3ë‹¨ê³„"]
        stage_presence = {stage: stage in response for stage in stages}

        improvement_keywords = [
            "ìˆ˜ì •", "ê°œì„ ", "ë³´ì™„", "ìµœì í™”", "ë” ë‚˜ì€", "íš¨ìœ¨ì ", "ì •í™•í•œ"
        ]

        improvement_count = sum(1 for keyword in improvement_keywords if keyword in response)

        return {
            "all_stages_present": all(stage_presence.values()),
            "stage_presence": stage_presence,
            "improvement_mentions": improvement_count,
            "self_critique_quality": "í‰ê°€" in response and "ì•½ì " in response,
            "iterative_improvement": improvement_count >= 2
        }

    def _compare_solution_quality(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ì†”ë£¨ì…˜ í’ˆì§ˆ ë¹„êµ"""
        comparison = {
            "method_scores": {},
            "best_method": None,
            "consistency_check": {}
        }

        # ê° ë°©ë²•ë³„ ì ìˆ˜ ê³„ì‚°
        for method, result in results.items():
            score = 0

            if method == "cot":
                analysis = result.get("analysis", {})
                score += 2 if analysis.get("has_step_by_step") else 0
                score += 1 if analysis.get("has_constraint_check") else 0
                score += 1 if analysis.get("shows_reasoning") else 0

            elif method == "pal":
                analysis = result.get("code_analysis", {})
                score += 3 if analysis.get("has_function_definition") else 0
                score += 2 if analysis.get("has_safety_check") else 0
                score += 2 if analysis.get("has_bfs_or_search") else 0

            elif method == "reflexion":
                analysis = result.get("reflexion_analysis", {})
                score += 3 if analysis.get("all_stages_present") else 0
                score += 2 if analysis.get("iterative_improvement") else 0
                score += 1 if analysis.get("self_critique_quality") else 0

            comparison["method_scores"][method] = score

        # ìµœê³  ë°©ë²• ì„ ì •
        if comparison["method_scores"]:
            best_method = max(comparison["method_scores"], key=comparison["method_scores"].get)
            comparison["best_method"] = best_method

        # ë‹µì•ˆ ì¼ê´€ì„± ì²´í¬ (ì´ë™ íšŸìˆ˜ ê¸°ì¤€)
        extracted_answers = {}
        for method, result in results.items():
            if method == "cot" and "analysis" in result:
                move_count = result["analysis"].get("extracted_move_count")
                if move_count:
                    extracted_answers[method] = move_count

        if len(set(extracted_answers.values())) <= 1:
            comparison["consistency_check"]["consistent"] = True
        else:
            comparison["consistency_check"]["consistent"] = False
            comparison["consistency_check"]["answers"] = extracted_answers

        return comparison

# ì‹¤ìŠµ 1 í…ŒìŠ¤íŠ¸
models = {"openai": openai_llm}
puzzle_solver = LogicPuzzleSolver(models)

river_crossing_puzzle = """
ë†ë¶€ê°€ ëŠ‘ëŒ€, ì–‘, ì–‘ë°°ì¶”ë¥¼ ë°ë¦¬ê³  ê°•ì„ ê±´ë„ˆì•¼ í•©ë‹ˆë‹¤.

ì œì•½ì¡°ê±´:
1. ë†ë¶€ê°€ ì—†ì„ ë•Œ ëŠ‘ëŒ€ì™€ ì–‘ì´ ê°™ì´ ìˆìœ¼ë©´ ëŠ‘ëŒ€ê°€ ì–‘ì„ ì¡ì•„ë¨¹ìŠµë‹ˆë‹¤
2. ë†ë¶€ê°€ ì—†ì„ ë•Œ ì–‘ê³¼ ì–‘ë°°ì¶”ê°€ ê°™ì´ ìˆìœ¼ë©´ ì–‘ì´ ì–‘ë°°ì¶”ë¥¼ ë¨¹ì–´ë²„ë¦½ë‹ˆë‹¤
3. ë³´íŠ¸ì—ëŠ” ë†ë¶€ì™€ í•œ ë¬¼ê±´ë§Œ ì‹¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤

ëª¨ë‘ ì•ˆì „í•˜ê²Œ ê±´ë„ˆëŠ”ë° ëª‡ ë²ˆ ì´ë™ì´ í•„ìš”í• ê¹Œìš”?
"""

print("=== ë…¼ë¦¬ í¼ì¦ í•´ê²° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
comparison_results = puzzle_solver.compare_approaches(river_crossing_puzzle)

# ê²°ê³¼ ì¶œë ¥
for method, result in comparison_results["individual_results"].items():
    print(f"\nã€{method.upper()} ë°©ì‹ ê²°ê³¼ã€‘")
    print(result["response"][:500] + "..." if len(result["response"]) > 500 else result["response"])

# ë¹„êµ ë¶„ì„ ì¶œë ¥
print(f"\n=== ë°©ë²•ë³„ ë¹„êµ ë¶„ì„ ===")
comparison_analysis = comparison_results["comparison_analysis"]
print("ë°©ë²•ë³„ ì ìˆ˜:")
for method, score in comparison_analysis["method_scores"].items():
    print(f"- {method}: {score}ì ")

if comparison_analysis["best_method"]:
    print(f"ìµœê³  ì„±ëŠ¥ ë°©ë²•: {comparison_analysis['best_method']}")

consistency = comparison_analysis["consistency_check"]
if consistency.get("consistent", False):
    print("ë‹µì•ˆ ì¼ê´€ì„±: ì¼ì¹˜")
else:
    print(f"ë‹µì•ˆ ì¼ê´€ì„±: ë¶ˆì¼ì¹˜ - {consistency.get('answers', {})}")
```

### ì‹¤ìŠµ 2: ìˆ˜í•™ ë¬¸ì œ ìë™ í•´ê²° ì‹œìŠ¤í…œ

```python
class MathProblemSolver:
    def __init__(self, models: Dict[str, Any]):
        """ìˆ˜í•™ ë¬¸ì œ ìë™ í•´ê²° ì‹œìŠ¤í…œ"""
        self.models = models
        self.problem_types = {
            "percentage": ["í¼ì„¼íŠ¸", "%", "ë¹„ìœ¨"],
            "arithmetic": ["ë”í•˜ê¸°", "ë¹¼ê¸°", "ê³±í•˜ê¸°", "ë‚˜ëˆ„ê¸°", "+", "-", "Ã—", "Ã·"],
            "word_problem": ["í•™ìƒ", "ëª…", "ê°œ", "ë§ˆë¦¬", "ê·¸ë£¹"],
            "geometry": ["ë„“ì´", "ë‘˜ë ˆ", "ë¶€í”¼", "ê°ë„", "ì‚¼ê°í˜•", "ì‚¬ê°í˜•"],
            "algebra": ["ë°©ì •ì‹", "x", "y", "ë³€ìˆ˜", "ë¯¸ì§€ìˆ˜"]
        }

    def analyze_problem_type(self, problem: str) -> Dict[str, Any]:
        """ë¬¸ì œ ìœ í˜• ë¶„ì„"""
        problem_lower = problem.lower()
        type_scores = {}

        for prob_type, keywords in self.problem_types.items():
            score = sum(1 for keyword in keywords if keyword in problem)
            if score > 0:
                type_scores[prob_type] = score

        # ìˆ«ìì™€ ì—°ì‚°ì ë¶„ì„
        import re
        numbers = re.findall(r'\d+(?:\.\d+)?', problem)
        operators = re.findall(r'[+\-Ã—Ã·%]', problem)

        analysis = {
            "detected_types": type_scores,
            "primary_type": max(type_scores, key=type_scores.get) if type_scores else "general",
            "numbers_found": len(numbers),
            "operators_found": len(operators),
            "complexity_score": len(numbers) + len(operators) + len(type_scores),
            "extracted_numbers": numbers
        }

        return analysis

    def generate_solution_strategy(self, problem: str, problem_type: str) -> str:
        """í•´ê²° ì „ëµ ìƒì„±"""
        strategies = {
            "percentage": """
1ë‹¨ê³„: ì „ì²´ ê°’ê³¼ í¼ì„¼íŠ¸ ê°’ ì‹ë³„
2ë‹¨ê³„: í¼ì„¼íŠ¸ë¥¼ ì†Œìˆ˜ë¡œ ë³€í™˜ (ì˜ˆ: 30% â†’ 0.3)
3ë‹¨ê³„: ì „ì²´ Ã— í¼ì„¼íŠ¸ ë˜ëŠ” ë¶€ë¶„ Ã· í¼ì„¼íŠ¸ ê³„ì‚°
4ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ ë° ë‹¨ìœ„ í™•ì¸
""",
            "word_problem": """
1ë‹¨ê³„: ë¬¸ì œì—ì„œ ì£¼ì–´ì§„ ì •ë³´ ì •ë¦¬
2ë‹¨ê³„: êµ¬í•´ì•¼ í•  ê²ƒ ëª…í™•íˆ íŒŒì•…
3ë‹¨ê³„: ì ì ˆí•œ ì—°ì‚° ë°©ë²• ì„ íƒ
4ë‹¨ê³„: ë‹¨ê³„ë³„ ê³„ì‚° ìˆ˜í–‰
5ë‹¨ê³„: ë‹µì´ í•©ë¦¬ì ì¸ì§€ ê²€í† 
""",
            "arithmetic": """
1ë‹¨ê³„: ì—°ì‚° ìˆœì„œ í™•ì¸ (ê´„í˜¸ â†’ ê³±ì…ˆ/ë‚˜ëˆ—ì…ˆ â†’ ë§ì…ˆ/ëº„ì…ˆ)
2ë‹¨ê³„: ê° ì—°ì‚°ì„ ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰
3ë‹¨ê³„: ì¤‘ê°„ ê²°ê³¼ í™•ì¸
4ë‹¨ê³„: ìµœì¢… ë‹µ ê²€ì¦
""",
            "geometry": """
1ë‹¨ê³„: ì£¼ì–´ì§„ ë„í˜•ê³¼ ì¹˜ìˆ˜ íŒŒì•…
2ë‹¨ê³„: ì ìš©í•  ê³µì‹ ì„ íƒ
3ë‹¨ê³„: ê³µì‹ì— ê°’ ëŒ€ì…
4ë‹¨ê³„: ê³„ì‚° ìˆ˜í–‰ ë° ë‹¨ìœ„ í™•ì¸
""",
            "general": """
1ë‹¨ê³„: ë¬¸ì œ ìƒí™© ì´í•´
2ë‹¨ê³„: ì£¼ì–´ì§„ ì¡°ê±´ê³¼ êµ¬í•  ê²ƒ íŒŒì•…
3ë‹¨ê³„: í•´ê²° ë°©ë²• ê³„íš
4ë‹¨ê³„: ë‹¨ê³„ë³„ ì‹¤í–‰
5ë‹¨ê³„: ê²°ê³¼ ê²€ì¦
"""
        }

        return strategies.get(problem_type, strategies["general"])

    def solve_with_strategy(self, problem: str, model_name: str = "openai") -> Dict[str, Any]:
        """ì „ëµ ê¸°ë°˜ ë¬¸ì œ í•´ê²°"""
        # ë¬¸ì œ ìœ í˜• ë¶„ì„
        type_analysis = self.analyze_problem_type(problem)
        primary_type = type_analysis["primary_type"]

        # í•´ê²° ì „ëµ ìƒì„±
        strategy = self.generate_solution_strategy(problem, primary_type)

        # ì „ëµì  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        strategic_template = """
ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ ë¶„ì„ëœ ìœ í˜•ê³¼ ì „ëµì— ë”°ë¼ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {problem}

ë¬¸ì œ ìœ í˜•: {problem_type}
ë³µì¡ë„: {complexity}/10

í•´ê²° ì „ëµ:
{strategy}

ìœ„ ì „ëµì„ ë”°ë¼ ë‹¨ê³„ë³„ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ì‹œì˜¤:

ë‹µì•ˆ:
"""

        prompt = PromptTemplate(
            input_variables=["problem", "problem_type", "complexity", "strategy"],
            template=strategic_template
        )

        model = self.models[model_name]
        chain = prompt | model | StrOutputParser()

        response = chain.invoke({
            "problem": problem,
            "problem_type": primary_type,
            "complexity": min(type_analysis["complexity_score"], 10),
            "strategy": strategy
        })

        return {
            "problem": problem,
            "type_analysis": type_analysis,
            "applied_strategy": strategy,
            "response": response
        }

    def execute_with_self_consistency(self, problem: str, model_name: str = "openai", num_attempts: int = 3) -> Dict[str, Any]:
        """Self-Consistency ì ìš©"""
        consistency_template = """
ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ ì„¸ ê°€ì§€ ì„œë¡œ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {problem}

ë°©ë²• 1: ì§ì ‘ ê³„ì‚° ì ‘ê·¼ë²•
- ì£¼ì–´ì§„ ìˆ˜ì¹˜ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë‹¨ê³„ë³„ë¡œ ê³„ì‚°

ë°©ë²• 2: ë¹„ë¡€ì‹ í™œìš© ì ‘ê·¼ë²•
- ë¹„ìœ¨ê³¼ ë¹„ë¡€ ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬ í•´ê²°

ë°©ë²• 3: ê²€ì‚° ì¤‘ì‹¬ ì ‘ê·¼ë²•
- ì—­ì‚°ì´ë‚˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ê²€ì¦í•˜ë©´ì„œ í•´ê²°

ê° ë°©ë²•ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê³  ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì‹œì˜¤.

ë‹µì•ˆ:
"""

        prompt = PromptTemplate(
            input_variables=["problem"],
            template=consistency_template
        )

        model = self.models[model_name]
        chain = prompt | model | StrOutputParser()

        responses = []
        for attempt in range(num_attempts):
            response = chain.invoke({"problem": problem})
            responses.append(response)

        # ì¼ê´€ì„± ë¶„ì„
        consistency_analysis = self._analyze_math_consistency(responses)

        return {
            "problem": problem,
            "responses": responses,
            "consistency_analysis": consistency_analysis
        }

    def create_step_by_step_explanation(self, problem: str, solution: str, model_name: str = "openai") -> str:
        """ë‹¨ê³„ë³„ ì„¤ëª… ìƒì„±"""
        explanation_template = """
ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œì™€ í•´ë‹µì„ ë°”íƒ•ìœ¼ë¡œ ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë‹¨ê³„ë³„ ì„¤ëª…ì„ ì‘ì„±í•˜ì‹œì˜¤:

ë¬¸ì œ: {problem}

í•´ë‹µ: {solution}

ìš”êµ¬ì‚¬í•­:
1. ê° ê³„ì‚° ë‹¨ê³„ë¥¼ ëª…í™•íˆ êµ¬ë¶„
2. ì™œ ê·¸ ë°©ë²•ì„ ì„ íƒí–ˆëŠ”ì§€ ì„¤ëª…
3. ì¤‘ê°„ ê³„ì‚° ê³¼ì •ì„ ëª¨ë‘ í‘œì‹œ
4. ìµœì¢… ë‹µì´ ì˜¬ë°”ë¥¸ì§€ ê²€ì¦ ê³¼ì • í¬í•¨
5. ë¹„ìŠ·í•œ ë¬¸ì œë¥¼ í’€ ë•Œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì¼ë°˜ì  ì›ì¹™ ì œì‹œ

ë‹¨ê³„ë³„ ì„¤ëª…:
"""

        prompt = PromptTemplate(
            input_variables=["problem", "solution"],
            template=explanation_template
        )

        model = self.models[model_name]
        chain = prompt | model | StrOutputParser()

        return chain.invoke({
            "problem": problem,
            "solution": solution
        })

    def _analyze_math_consistency(self, responses: List[str]) -> Dict[str, Any]:
        """ìˆ˜í•™ ë¬¸ì œ ì¼ê´€ì„± ë¶„ì„"""
        import re

        extracted_answers = []
        for response in responses:
            # ë‹¤ì–‘í•œ ë‹µ íŒ¨í„´ìœ¼ë¡œ ìˆ«ì ì¶”ì¶œ
            answer_patterns = [
                r'ë‹µ[:ï¼š]\s*(\d+(?:\.\d+)?)',
                r'ê²°ê³¼[:ï¼š]\s*(\d+(?:\.\d+)?)',
                r'ìµœì¢….*?(\d+(?:\.\d+)?)',
                r'ë”°ë¼ì„œ.*?(\d+(?:\.\d+)?)',
                r'=\s*(\d+(?:\.\d+)?)',
                r'(\d+(?:\.\d+)?)ëª…',
                r'(\d+(?:\.\d+)?)ê°œ'
            ]

            found_answer = None
            for pattern in answer_patterns:
                matches = re.findall(pattern, response)
                if matches:
                    found_answer = matches[-1]  # ë§ˆì§€ë§‰ ë§¤ì¹˜ ì‚¬ìš©
                    break

            if found_answer:
                extracted_answers.append(float(found_answer))

        # ì¼ê´€ì„± ê³„ì‚°
        if extracted_answers:
            unique_answers = list(set(extracted_answers))
            consistency_score = extracted_answers.count(max(set(extracted_answers), key=extracted_answers.count)) / len(extracted_answers)

            return {
                "extracted_answers": extracted_answers,
                "unique_answers": unique_answers,
                "consistency_score": consistency_score,
                "most_common_answer": max(set(extracted_answers), key=extracted_answers.count),
                "is_consistent": len(unique_answers) == 1,
                "agreement_level": "ë†’ìŒ" if consistency_score >= 0.8 else "ë³´í†µ" if consistency_score >= 0.6 else "ë‚®ìŒ"
            }
        else:
            return {
                "extracted_answers": [],
                "consistency_score": 0,
                "is_consistent": False,
                "error": "ë‹µì•ˆ ì¶”ì¶œ ì‹¤íŒ¨"
            }

    def comprehensive_solve(self, problem: str, model_name: str = "openai") -> Dict[str, Any]:
        """ì¢…í•©ì  ë¬¸ì œ í•´ê²°"""
        print(f"ë¬¸ì œ ë¶„ì„ ì¤‘: {problem[:50]}...")

        # 1. ì „ëµ ê¸°ë°˜ í•´ê²°
        strategic_result = self.solve_with_strategy(problem, model_name)
        print("ì „ëµ ê¸°ë°˜ í•´ê²° ì™„ë£Œ")

        # 2. Self-Consistency í•´ê²°
        consistency_result = self.execute_with_self_consistency(problem, model_name)
        print("Self-Consistency í•´ê²° ì™„ë£Œ")

        # 3. ë‹¨ê³„ë³„ ì„¤ëª… ìƒì„±
        explanation = self.create_step_by_step_explanation(
            problem,
            strategic_result["response"],
            model_name
        )
        print("ë‹¨ê³„ë³„ ì„¤ëª… ìƒì„± ì™„ë£Œ")

        return {
            "problem": problem,
            "strategic_solution": strategic_result,
            "consistency_solution": consistency_result,
            "detailed_explanation": explanation,
            "overall_assessment": self._assess_solution_quality(strategic_result, consistency_result)
        }

    def _assess_solution_quality(self, strategic_result: Dict, consistency_result: Dict) -> Dict[str, Any]:
        """ì†”ë£¨ì…˜ í’ˆì§ˆ í‰ê°€"""
        assessment = {
            "strategy_effectiveness": 0,
            "consistency_reliability": 0,
            "overall_confidence": 0
        }

        # ì „ëµ íš¨ê³¼ì„± í‰ê°€
        type_analysis = strategic_result["type_analysis"]
        if type_analysis["primary_type"] != "general":
            assessment["strategy_effectiveness"] += 0.3
        if type_analysis["complexity_score"] > 0:
            assessment["strategy_effectiveness"] += 0.2

        response_quality_indicators = [
            "ë‹¨ê³„" in strategic_result["response"],
            "ê³„ì‚°" in strategic_result["response"],
            "ë”°ë¼ì„œ" in strategic_result["response"],
            len(strategic_result["response"]) > 200
        ]
        assessment["strategy_effectiveness"] += sum(response_quality_indicators) * 0.125

        # ì¼ê´€ì„± ì‹ ë¢°ë„ í‰ê°€
        consistency_analysis = consistency_result["consistency_analysis"]
        if not consistency_analysis.get("error"):
            assessment["consistency_reliability"] = consistency_analysis.get("consistency_score", 0)

        # ì „ì²´ ì‹ ë¢°ë„
        assessment["overall_confidence"] = (assessment["strategy_effectiveness"] + assessment["consistency_reliability"]) / 2

        # ì‹ ë¢°ë„ ë ˆë²¨
        confidence_level = assessment["overall_confidence"]
        if confidence_level >= 0.8:
            assessment["confidence_level"] = "ë§¤ìš° ë†’ìŒ"
        elif confidence_level >= 0.6:
            assessment["confidence_level"] = "ë†’ìŒ"
        elif confidence_level >= 0.4:
            assessment["confidence_level"] = "ë³´í†µ"
        else:
            assessment["confidence_level"] = "ë‚®ìŒ"

        return assessment

# ì‹¤ìŠµ 2 í…ŒìŠ¤íŠ¸
models = {"openai": openai_llm}
math_solver = MathProblemSolver(models)

test_problems = [
    "í•™êµì—ì„œ 500ëª…ì˜ í•™ìƒì´ ìˆìŠµë‹ˆë‹¤. ì´ ì¤‘ 30%ëŠ” 5í•™ë…„ì´ê³ , 20%ëŠ” 6í•™ë…„ í•™ìƒì…ë‹ˆë‹¤. 5í•™ë…„ í•™ìƒë“¤ ì¤‘ 60%ëŠ” ìˆ˜í•™ ë™ì•„ë¦¬ì— ìˆê³ , ë‚˜ë¨¸ì§€ëŠ” ê³¼í•™ ë™ì•„ë¦¬ì— ìˆìŠµë‹ˆë‹¤. 6í•™ë…„ í•™ìƒë“¤ ì¤‘ 70%ëŠ” ìˆ˜í•™ ë™ì•„ë¦¬ì— ìˆê³ , ë‚˜ë¨¸ì§€ëŠ” ê³¼í•™ ë™ì•„ë¦¬ì— ìˆìŠµë‹ˆë‹¤. ê³¼í•™ ë™ì•„ë¦¬ì—ëŠ” ëª‡ ëª…ì˜ í•™ìƒì´ ìˆë‚˜ìš”?",
    "í•œ ìƒìì— ë¹¨ê°„ ê³µ 24ê°œì™€ íŒŒë€ ê³µ 36ê°œê°€ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ê³µì˜ 25%ë¥¼ ë¬´ì‘ìœ„ë¡œ ë½‘ì•˜ì„ ë•Œ, ë½‘ì€ ê³µ ì¤‘ì— ë¹¨ê°„ ê³µì´ 8ê°œê°€ ìˆì—ˆë‹¤ë©´ íŒŒë€ ê³µì€ ëª‡ ê°œë¥¼ ë½‘ì•˜ë‚˜ìš”?",
    "ì§ì‚¬ê°í˜• ëª¨ì–‘ì˜ í™”ë‹¨ì´ ìˆìŠµë‹ˆë‹¤. ê°€ë¡œê°€ 12m, ì„¸ë¡œê°€ 8mì…ë‹ˆë‹¤. ì´ í™”ë‹¨ ë‘˜ë ˆì— 1m ê°„ê²©ìœ¼ë¡œ ë‚˜ë¬´ë¥¼ ì‹¬ìœ¼ë ¤ê³  í•©ë‹ˆë‹¤. ëª¨ì„œë¦¬ì—ë„ ë‚˜ë¬´ë¥¼ ì‹¬ëŠ”ë‹¤ë©´ ì´ ëª‡ ê·¸ë£¨ì˜ ë‚˜ë¬´ê°€ í•„ìš”í•œê°€ìš”?"
]

print("=== ìˆ˜í•™ ë¬¸ì œ ìë™ í•´ê²° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")

for i, problem in enumerate(test_problems, 1):
    print(f"\n{'='*20} ë¬¸ì œ {i} {'='*20}")
    result = math_solver.comprehensive_solve(problem)

    print(f"ë¬¸ì œ: {problem}")
    print(f"\nã€ë¬¸ì œ ë¶„ì„ã€‘")
    type_analysis = result["strategic_solution"]["type_analysis"]
    print(f"ì£¼ìš” ìœ í˜•: {type_analysis['primary_type']}")
    print(f"ë³µì¡ë„: {type_analysis['complexity_score']}/10")
    print(f"ì¶”ì¶œëœ ìˆ«ì: {type_analysis['extracted_numbers']}")

    print(f"\nã€ì „ëµ ê¸°ë°˜ í•´ê²°ã€‘")
    print(result["strategic_solution"]["response"][:300] + "...")

    print(f"\nã€ì¼ê´€ì„± ë¶„ì„ã€‘")
    consistency = result["consistency_solution"]["consistency_analysis"]
    if not consistency.get("error"):
        print(f"ì¼ê´€ì„± ì ìˆ˜: {consistency['consistency_score']:.2f}")
        print(f"ì¼ì¹˜ë„: {consistency['agreement_level']}")
        print(f"ì¶”ì¶œëœ ë‹µë“¤: {consistency['extracted_answers']}")

    print(f"\nã€í’ˆì§ˆ í‰ê°€ã€‘")
    assessment = result["overall_assessment"]
    print(f"ì „ëµ íš¨ê³¼ì„±: {assessment['strategy_effectiveness']:.2f}")
    print(f"ì¼ê´€ì„± ì‹ ë¢°ë„: {assessment['consistency_reliability']:.2f}")
    print(f"ì „ì²´ ì‹ ë¢°ë„: {assessment['confidence_level']}")
```

### ì‹¤ìŠµ 3: ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ì‹œìŠ¤í…œ

```python
class MultimodalReasoningSystem:
    def __init__(self, models: Dict[str, Any]):
        """ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ì‹œìŠ¤í…œ"""
        self.models = models
        self.math_symbols = {
            "Ã—": "*", "Ã·": "/", "Â²": "**2", "Â³": "**3",
            "âˆš": "sqrt", "âˆ‘": "sum", "âˆ": "product",
            "â‰¤": "<=", "â‰¥": ">=", "â‰ ": "!=", "â‰ˆ": "â‰ˆ"
        }

    def parse_mathematical_expressions(self, text: str) -> Dict[str, Any]:
        """ìˆ˜ì‹ íŒŒì‹±"""
        import re

        # ìˆ˜ì‹ íŒ¨í„´ë“¤
        patterns = {
            "fractions": r'\d+/\d+',
            "percentages": r'\d+(?:\.\d+)?%',
            "equations": r'[a-zA-Z]\s*=\s*[\d\w\+\-\*/\(\)]+',
            "formulas": r'[A-Za-z]+\s*=\s*[^,\n]+',
            "numbers": r'\d+(?:\.\d+)?',
            "operations": r'[+\-Ã—Ã·*/=]',
            "variables": r'\b[a-zA-Z]\b',
            "parentheses": r'[\(\)]'
        }

        extracted = {}
        for pattern_name, pattern in patterns.items():
            matches = re.findall(pattern, text)
            extracted[pattern_name] = matches

        # ìˆ˜ì‹ ë³µì¡ë„ ê³„ì‚°
        complexity_score = (
            len(extracted["numbers"]) * 1 +
            len(extracted["operations"]) * 2 +
            len(extracted["variables"]) * 3 +
            len(extracted["formulas"]) * 4 +
            len(extracted["equations"]) * 5
        )

        return {
            "extracted_patterns": extracted,
            "complexity_score": complexity_score,
            "has_algebra": len(extracted["variables"]) > 0 or len(extracted["equations"]) > 0,
            "has_geometry": any(keyword in text.lower() for keyword in ["ë„“ì´", "ë‘˜ë ˆ", "ë¶€í”¼", "ê°ë„", "ë°˜ì§€ë¦„"]),
            "expression_count": sum(len(v) for v in extracted.values())
        }

    def create_visual_explanation(self, solution_steps: List[str], model_name: str = "openai") -> Dict[str, Any]:
        """ì‹œê°ì  ì„¤ëª… ìƒì„±"""
        visual_template = """
ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œ í•´ê²° ë‹¨ê³„ë“¤ì„ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ì„¤ëª…ì„ ì‘ì„±í•˜ì‹œì˜¤:

í•´ê²° ë‹¨ê³„ë“¤:
{steps}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‹œê°ì  ì„¤ëª…ì„ ì œê³µí•˜ì‹œì˜¤:

**1. ë¬¸ì œ ìƒí™© ë‹¤ì´ì–´ê·¸ë¨**
- ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë„í‘œë‚˜ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„
- ê´€ê³„ë¥¼ í™”ì‚´í‘œë‚˜ ì—°ê²°ì„ ìœ¼ë¡œ í‘œì‹œ

**2. ê³„ì‚° ê³¼ì • ì‹œê°í™”**
- ê° ê³„ì‚° ë‹¨ê³„ë¥¼ ë°•ìŠ¤ë‚˜ í”Œë¡œìš°ì°¨íŠ¸ë¡œ í‘œí˜„
- ì¤‘ê°„ ê²°ê³¼ë¥¼ ëª…í™•íˆ í‘œì‹œ

**3. ê²€ì¦ ê³¼ì • í‘œí˜„**
- ë‹µì•ˆ í™•ì¸ ê³¼ì •ì„ ì—­ì‚°ì´ë‚˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì‹œê°í™”
- ê²°ê³¼ì˜ íƒ€ë‹¹ì„±ì„ ê·¸ë˜í”„ë‚˜ ë¹„êµí‘œë¡œ í‘œí˜„

**4. ì¼ë°˜í™” íŒ¨í„´**
- ì´ ë¬¸ì œ ìœ í˜•ì˜ ì¼ë°˜ì  í•´ê²° íŒ¨í„´ì„ í”Œë¡œìš°ì°¨íŠ¸ë¡œ í‘œí˜„
- ë¹„ìŠ·í•œ ë¬¸ì œì— ì ìš©í•  ìˆ˜ ìˆëŠ” í…œí”Œë¦¿ ì œì‹œ

ì‹œê°ì  ì„¤ëª…:
"""

        steps_text = "\n".join(f"{i+1}. {step}" for i, step in enumerate(solution_steps))

        prompt = PromptTemplate(
            input_variables=["steps"],
            template=visual_template
        )

        model = self.models[model_name]
        chain = prompt | model | StrOutputParser()

        visual_explanation = chain.invoke({"steps": steps_text})

        # ì‹œê°ì  ìš”ì†Œ ë¶„ì„
        visual_analysis = self._analyze_visual_content(visual_explanation)

        return {
            "visual_explanation": visual_explanation,
            "visual_analysis": visual_analysis
        }

    def integrate_reasoning_modes(self, problem: str, model_name: str = "openai") -> Dict[str, Any]:
        """ì¶”ë¡  ëª¨ë“œ í†µí•©"""

        # 1. ìˆ˜ì‹ íŒŒì‹±
        math_parsing = self.parse_mathematical_expressions(problem)

        # 2. ì¶”ë¡  ëª¨ë“œ ê²°ì •
        reasoning_modes = self._determine_reasoning_modes(problem, math_parsing)

        # 3. í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„±
        integrated_template = """
ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¤ì¤‘ ì¶”ë¡  ëª¨ë“œë¡œ í•´ê²°í•˜ì‹œì˜¤:

ë¬¸ì œ: {problem}

ìˆ˜ì‹ ë¶„ì„ ê²°ê³¼:
- ë³µì¡ë„ ì ìˆ˜: {complexity_score}/20
- ëŒ€ìˆ˜ì  ìš”ì†Œ: {has_algebra}
- ê¸°í•˜í•™ì  ìš”ì†Œ: {has_geometry}
- ì¶”ì¶œëœ ìˆ«ì: {numbers}

ê¶Œì¥ ì¶”ë¡  ëª¨ë“œ: {reasoning_modes}

**ë‹¨ê³„ 1: ë…¼ë¦¬ì  ë¶„í•´**
- ë¬¸ì œë¥¼ ë…¼ë¦¬ì  êµ¬ì„±ìš”ì†Œë¡œ ë¶„í•´
- ê° ìš”ì†Œ ê°„ì˜ ê´€ê³„ íŒŒì•…

**ë‹¨ê³„ 2: ìˆ˜í•™ì  ëª¨ë¸ë§**
- ë¬¸ì œë¥¼ ìˆ˜í•™ì  ëª¨ë¸ë¡œ ë³€í™˜
- ì ì ˆí•œ ê³µì‹ì´ë‚˜ ë°©ì •ì‹ ì„¤ì •

**ë‹¨ê³„ 3: ë‹¨ê³„ë³„ ê³„ì‚°**
- ì²´ê³„ì ì¸ ê³„ì‚° ìˆ˜í–‰
- ì¤‘ê°„ ê²°ê³¼ ê²€ì¦

**ë‹¨ê³„ 4: ì‹œê°ì  ê²€í† **
- ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„
- ì§ê´€ì  íƒ€ë‹¹ì„± í™•ì¸

**ë‹¨ê³„ 5: ë‹¤ì¤‘ ë°©ë²• ê²€ì¦**
- ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì¬ê²€ì¦
- ì¼ê´€ì„± í™•ì¸

í†µí•© í•´ê²°:
"""

        prompt = PromptTemplate(
            input_variables=[
                "problem", "complexity_score", "has_algebra",
                "has_geometry", "numbers", "reasoning_modes"
            ],
            template=integrated_template
        )

        model = self.models[model_name]
        chain = prompt | model | StrOutputParser()

        response = chain.invoke({
            "problem": problem,
            "complexity_score": math_parsing["complexity_score"],
            "has_algebra": math_parsing["has_algebra"],
            "has_geometry": math_parsing["has_geometry"],
            "numbers": ", ".join(math_parsing["extracted_patterns"]["numbers"]),
            "reasoning_modes": ", ".join(reasoning_modes)
        })

        # í•´ê²° ë‹¨ê³„ ì¶”ì¶œ
        solution_steps = self._extract_solution_steps(response)

        # ì‹œê°ì  ì„¤ëª… ìƒì„±
        visual_result = self.create_visual_explanation(solution_steps, model_name)

        return {
            "problem": problem,
            "math_parsing": math_parsing,
            "reasoning_modes": reasoning_modes,
            "integrated_solution": response,
            "solution_steps": solution_steps,
            "visual_explanation": visual_result,
            "integration_quality": self._assess_integration_quality(response, visual_result)
        }

    def _determine_reasoning_modes(self, problem: str, math_parsing: Dict[str, Any]) -> List[str]:
        """ì¶”ë¡  ëª¨ë“œ ê²°ì •"""
        modes = []

        # ë³µì¡ë„ ê¸°ë°˜
        if math_parsing["complexity_score"] > 10:
            modes.append("ë‹¨ê³„ì  ë¶„í•´")

        # ëŒ€ìˆ˜ì  ìš”ì†Œ
        if math_parsing["has_algebra"]:
            modes.extend(["ë°©ì •ì‹ ëª¨ë¸ë§", "ë³€ìˆ˜ ì¶”ë¡ "])

        # ê¸°í•˜í•™ì  ìš”ì†Œ
        if math_parsing["has_geometry"]:
            modes.extend(["ê³µê°„ì  ì‹œê°í™”", "ê³µì‹ ì ìš©"])

        # ìˆ˜ì¹˜ ê³„ì‚°
        if len(math_parsing["extracted_patterns"]["numbers"]) > 3:
            modes.append("ë‹¤ë‹¨ê³„ ê³„ì‚°")

        # í¼ì„¼íŠ¸ë‚˜ ë¹„ìœ¨
        if math_parsing["extracted_patterns"]["percentages"]:
            modes.append("ë¹„ë¡€ ì¶”ë¡ ")

        # ê¸°ë³¸ ëª¨ë“œ
        if not modes:
            modes = ["ë…¼ë¦¬ì  ì¶”ë¡ ", "ì‚°ìˆ  ê³„ì‚°"]

        return modes

    def _extract_solution_steps(self, response: str) -> List[str]:
        """í•´ê²° ë‹¨ê³„ ì¶”ì¶œ"""
        import re

        # ë‹¨ê³„ íŒ¨í„´ë“¤
        step_patterns = [
            r'ë‹¨ê³„\s*\d+[:ï¼š]\s*([^\n]+)',
            r'\d+\.\s*([^\n]+)',
            r'Step\s*\d+[:ï¼š]\s*([^\n]+)',
            r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*([^\n]+)'
        ]

        steps = []
        for pattern in step_patterns:
            matches = re.findall(pattern, response)
            if matches:
                steps.extend(matches)
                break  # ì²« ë²ˆì§¸ë¡œ ë§¤ì¹˜ëœ íŒ¨í„´ ì‚¬ìš©

        # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        if not steps:
            sentences = [s.strip() for s in response.split('.') if len(s.strip()) > 10]
            steps = sentences[:8]  # ìµœëŒ€ 8ë‹¨ê³„

        return steps

    def _analyze_visual_content(self, visual_explanation: str) -> Dict[str, Any]:
        """ì‹œê°ì  ë‚´ìš© ë¶„ì„"""
        visual_indicators = {
            "diagram_mentions": ["ë‹¤ì´ì–´ê·¸ë¨", "ë„í‘œ", "ê·¸ë¦¼", "chart", "diagram"],
            "flowchart_mentions": ["í”Œë¡œìš°ì°¨íŠ¸", "íë¦„ë„", "flowchart", "flow"],
            "graph_mentions": ["ê·¸ë˜í”„", "graph", "ì°¨íŠ¸", "chart"],
            "table_mentions": ["í‘œ", "í…Œì´ë¸”", "table"],
            "visual_elements": ["í™”ì‚´í‘œ", "ë°•ìŠ¤", "ì—°ê²°ì„ ", "ìƒ‰ê¹”", "í•˜ì´ë¼ì´íŠ¸"]
        }

        analysis = {}
        for category, indicators in visual_indicators.items():
            count = sum(1 for indicator in indicators if indicator in visual_explanation)
            analysis[category] = count

        # ì‹œê°í™” í’ˆì§ˆ ì ìˆ˜
        total_visual_elements = sum(analysis.values())
        analysis["visual_richness_score"] = min(total_visual_elements / 5.0, 1.0)  # 0-1 ì •ê·œí™”

        # êµ¬ì¡°í™” ìˆ˜ì¤€
        structure_indicators = ["**", "##", "1.", "2.", "3.", "â€¢", "-"]
        structure_count = sum(1 for indicator in structure_indicators if indicator in visual_explanation)
        analysis["structure_level"] = min(structure_count / 10.0, 1.0)

        return analysis

    def _assess_integration_quality(self, integrated_solution: str, visual_result: Dict[str, Any]) -> Dict[str, Any]:
        """í†µí•© í’ˆì§ˆ í‰ê°€"""

        # í†µí•© ì†”ë£¨ì…˜ í’ˆì§ˆ
        integration_indicators = [
            "ë‹¨ê³„" in integrated_solution,
            "ëª¨ë¸ë§" in integrated_solution,
            "ê²€ì¦" in integrated_solution,
            "ì‹œê°ì " in integrated_solution,
            len(integrated_solution) > 500,  # ì¶©ë¶„í•œ ìƒì„¸ë„
            "ë”°ë¼ì„œ" in integrated_solution or "ê²°ë¡ " in integrated_solution
        ]

        integration_score = sum(integration_indicators) / len(integration_indicators)

        # ì‹œê°ì  ì„¤ëª… í’ˆì§ˆ
        visual_analysis = visual_result["visual_analysis"]
        visual_score = (
            visual_analysis["visual_richness_score"] * 0.4 +
            visual_analysis["structure_level"] * 0.6
        )

        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        overall_quality = (integration_score * 0.7 + visual_score * 0.3)

        return {
            "integration_score": integration_score,
            "visual_score": visual_score,
            "overall_quality": overall_quality,
            "quality_level": (
                "ìš°ìˆ˜" if overall_quality >= 0.8 else
                "ì–‘í˜¸" if overall_quality >= 0.6 else
                "ë³´í†µ" if overall_quality >= 0.4 else "ê°œì„ í•„ìš”"
            ),
            "strengths": self._identify_strengths(integration_indicators, visual_analysis),
            "improvement_areas": self._identify_improvements(integration_indicators, visual_analysis)
        }

    def _identify_strengths(self, integration_indicators: List[bool], visual_analysis: Dict[str, Any]) -> List[str]:
        """ê°•ì  ì‹ë³„"""
        strengths = []

        if integration_indicators[0]:  # ë‹¨ê³„ì  í•´ê²°
            strengths.append("ì²´ê³„ì ì¸ ë‹¨ê³„ë³„ ì ‘ê·¼")
        if integration_indicators[2]:  # ê²€ì¦ í¬í•¨
            strengths.append("ê²€ì¦ ê³¼ì • í¬í•¨")
        if visual_analysis["visual_richness_score"] > 0.6:
            strengths.append("í’ë¶€í•œ ì‹œê°ì  ì„¤ëª…")
        if visual_analysis["structure_level"] > 0.7:
            strengths.append("ëª…í™•í•œ êµ¬ì¡°í™”")

        return strengths

    def _identify_improvements(self, integration_indicators: List[bool], visual_analysis: Dict[str, Any]) -> List[str]:
        """ê°œì„ ì  ì‹ë³„"""
        improvements = []

        if not integration_indicators[1]:  # ëª¨ë¸ë§ ë¶€ì¡±
            improvements.append("ìˆ˜í•™ì  ëª¨ë¸ë§ ê°•í™” í•„ìš”")
        if not integration_indicators[3]:  # ì‹œê°ì  ìš”ì†Œ ë¶€ì¡±
            improvements.append("ì‹œê°ì  í‘œí˜„ ì¶”ê°€ í•„ìš”")
        if visual_analysis["visual_richness_score"] < 0.4:
            improvements.append("ë” ë‹¤ì–‘í•œ ì‹œê°ì  ìš”ì†Œ í™œìš©")
        if visual_analysis["structure_level"] < 0.5:
            improvements.append("êµ¬ì¡°í™”ëœ ì„¤ëª… ë°©ì‹ ê°œì„ ")

        return improvements

# ì‹¤ìŠµ 3 í…ŒìŠ¤íŠ¸
models = {"openai": openai_llm}
multimodal_system = MultimodalReasoningSystem(models)

complex_problem = """
í•œ íšŒì‚¬ì—ì„œ ì§ì›ë“¤ì˜ ê¸‰ì—¬ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •í–ˆìŠµë‹ˆë‹¤:
- ê¸°ë³¸ê¸‰: ì›” 300ë§Œì›
- ì„±ê³¼ê¸‰: ê¸°ë³¸ê¸‰ì˜ a% (aëŠ” ê°œì¸ ì„±ê³¼ì— ë”°ë¼ 10~50% ë²”ìœ„)
- íŒ€ ë³´ë„ˆìŠ¤: ì „ì²´ ê¸‰ì—¬ì˜ 15%
- ì„¸ê¸ˆ: ì „ì²´ ê¸‰ì—¬ì˜ 22%

ë§Œì•½ í•œ ì§ì›ì´ ì„±ê³¼ê¸‰ì„ 30% ë°›ì•˜ë‹¤ë©´, ì‹¤ì œ ë°›ëŠ” ê¸‰ì—¬(ì„¸í›„)ëŠ” ì–¼ë§ˆì¸ê°€ìš”?
ë˜í•œ ì´ ì§ì›ì´ ì—°ê°„ ë°›ëŠ” ì´ ê¸‰ì—¬ëŠ” ì–¼ë§ˆì¸ê°€ìš”?
"""

print("=== ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
multimodal_result = multimodal_system.integrate_reasoning_modes(complex_problem)

print(f"ë¬¸ì œ: {complex_problem}")

print(f"\nã€ìˆ˜ì‹ ë¶„ì„ã€‘")
math_parsing = multimodal_result["math_parsing"]
print(f"ë³µì¡ë„ ì ìˆ˜: {math_parsing['complexity_score']}/20")
print(f"ëŒ€ìˆ˜ì  ìš”ì†Œ: {math_parsing['has_algebra']}")
print(f"ê¸°í•˜í•™ì  ìš”ì†Œ: {math_parsing['has_geometry']}")
print(f"ì¶”ì¶œëœ ìˆ«ì: {math_parsing['extracted_patterns']['numbers']}")
print(f"ì¶”ì¶œëœ í¼ì„¼íŠ¸: {math_parsing['extracted_patterns']['percentages']}")

print(f"\nã€ê¶Œì¥ ì¶”ë¡  ëª¨ë“œã€‘")
print(f"ì„ íƒëœ ëª¨ë“œ: {', '.join(multimodal_result['reasoning_modes'])}")

print(f"\nã€í†µí•© í•´ê²° ê³¼ì •ã€‘")
print(multimodal_result["integrated_solution"][:800] + "...")

print(f"\nã€ì¶”ì¶œëœ í•´ê²° ë‹¨ê³„ã€‘")
for i, step in enumerate(multimodal_result["solution_steps"], 1):
    print(f"{i}. {step}")

print(f"\nã€ì‹œê°ì  ì„¤ëª…ã€‘")
visual_explanation = multimodal_result["visual_explanation"]["visual_explanation"]
print(visual_explanation[:600] + "...")

print(f"\nã€í’ˆì§ˆ í‰ê°€ã€‘")
quality = multimodal_result["integration_quality"]
print(f"í†µí•© ì ìˆ˜: {quality['integration_score']:.2f}")
print(f"ì‹œê°ì  ì ìˆ˜: {quality['visual_score']:.2f}")
print(f"ì „ì²´ í’ˆì§ˆ: {quality['overall_quality']:.2f}")
print(f"í’ˆì§ˆ ìˆ˜ì¤€: {quality['quality_level']}")

if quality["strengths"]:
    print(f"ê°•ì : {', '.join(quality['strengths'])}")
if quality["improvement_areas"]:
    print(f"ê°œì„ ì : {', '.join(quality['improvement_areas'])}")
```

## ğŸ” ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LangChain Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/)
- [OpenAI GPT Best Practices](https://platform.openai.com/docs/guides/gpt-best-practices)
- [Chain of Thought Prompting Guide](https://www.promptingguide.ai/techniques/cot)

### í•™ìˆ  ìë£Œ
- Wei, J., et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
- Wang, X., et al. (2022). "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
- Gao, L., et al. (2023). "PAL: Program-aided Language Models"
- Shinn, N., et al. (2023). "Reflexion: Language Agents with Verbal Reinforcement Learning"

### ì‹¤ë¬´ ê°€ì´ë“œ
- [Advanced Prompting Techniques](https://example.com/advanced-prompting)
- [Chain of Thought Implementation Guide](https://example.com/cot-implementation)
- [Mathematical Reasoning with LLMs](https://example.com/math-reasoning)

### ë„êµ¬ ë° ë¦¬ì†ŒìŠ¤
- [Prompt Engineering Toolkit](https://example.com/prompt-toolkit)
- [CoT Benchmarking Datasets](https://example.com/cot-benchmarks)
- [Mathematical Problem Collections](https://example.com/math-problems)

---

**ë‹¤ìŒ í•™ìŠµ**: W3_004_Chat_History.md - ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬ì™€ ëŒ€í™” ë§¥ë½ ìœ ì§€ ê¸°ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.