# W2_002_Simple_RAG_Pipeline.md - ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- RAG(Retrieval-Augmented Generation)ì˜ ê°œë…ê³¼ ì•„í‚¤í…ì²˜ ì´í•´
- ë¬¸ì„œ ì „ì²˜ë¦¬ë¶€í„° ì‘ë‹µ ìƒì„±ê¹Œì§€ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ëŠ¥ë ¥ ìŠµë“
- LangChainì„ í™œìš©í•œ ì‹¤ì „ RAG ì‹œìŠ¤í…œ êµ¬í˜„ ê¸°ë²• í•™ìŠµ
- ê²€ìƒ‰ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ í‰ê°€ ë°©ë²• ì´í•´

## ğŸ“š í•µì‹¬ ê°œë…

### RAG(Retrieval-Augmented Generation)ë€?
- **ê°œë…**: ê²€ìƒ‰ ì¦ê°• ìƒì„±, ì™¸ë¶€ ì§€ì‹ì„ ë™ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ì‘ë‹µ ìƒì„±ì— í™œìš©
- **ë°°ê²½**: ê¸°ì¡´ LLMì˜ ê³ ì •ëœ í›ˆë ¨ ë°ì´í„° í•œê³„ë¥¼ ê·¹ë³µ
- **ì¥ì **: ìµœì‹  ì •ë³´, ë„ë©”ì¸ íŠ¹í™” ì§€ì‹, ì‚¬ì‹¤ ê¸°ë°˜ ì‘ë‹µ ê°€ëŠ¥
- **êµ¬ì„±**: Retrieval(ê²€ìƒ‰) + Augmentation(ì¦ê°•) + Generation(ìƒì„±)

### RAG vs ê¸°ì¡´ ì ‘ê·¼ë²• ë¹„êµ

| íŠ¹ì„± | ê¸°ì¡´ LLM | íŒŒì¸íŠœë‹ | RAG |
|------|----------|----------|-----|
| ìµœì‹  ì •ë³´ | âŒ | âŒ | âœ… |
| êµ¬í˜„ ë³µì¡ë„ | ë‚®ìŒ | ë†’ìŒ | ì¤‘ê°„ |
| ê³„ì‚° ë¹„ìš© | ë‚®ìŒ | ë†’ìŒ | ì¤‘ê°„ |
| ì†ŒìŠ¤ ì¶”ì  | âŒ | âŒ | âœ… |
| í™˜ê° ë°©ì§€ | âŒ | â–³ | âœ… |

### RAG ì•„í‚¤í…ì²˜
```mermaid
graph TB
    A[ì‚¬ìš©ì ì¿¼ë¦¬] --> B[ë¬¸ì„œ ê²€ìƒ‰<br/>Retrieval]
    B --> C[ê´€ë ¨ ë¬¸ì„œ]
    C --> D[ì»¨í…ìŠ¤íŠ¸ ì¦ê°•<br/>Augmentation]
    A --> D
    D --> E[LLM ìƒì„±<br/>Generation]
    E --> F[ìµœì¢… ì‘ë‹µ]
```

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# ê¸°ë³¸ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install langchain langchain-community langchain-openai

# ë¬¸ì„œ ì²˜ë¦¬ ë° ì›¹ ìŠ¤í¬ë˜í•‘
pip install beautifulsoup4 langchain_text_splitters

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
pip install langchain-chroma faiss-cpu

# UV íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì‚¬ìš© ì‹œ
uv add langchain langchain-community langchain-openai beautifulsoup4 langchain_text_splitters langchain-chroma faiss-cpu
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
from dotenv import load_dotenv
load_dotenv()
```

## ğŸ’» RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„

### Step 1: Indexing (ì¸ë±ì‹±)

RAG ì‹œìŠ¤í…œì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•ì…ë‹ˆë‹¤.

#### 1.1 ë¬¸ì„œ ë°ì´í„° ë¡œë“œ (Load Data)

```python
from langchain_community.document_loaders import WebBaseLoader

# ì›¹í˜ì´ì§€ì—ì„œ ë°ì´í„° ë¡œë“œ
url = 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8'
loader = WebBaseLoader(url)

# ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
docs = loader.load()

print(f"Document ê°œìˆ˜: {len(docs)}")
print(f"Document ê¸¸ì´: {len(docs[0].page_content)}")
print(f"Document ë‚´ìš© ìƒ˜í”Œ: {docs[0].page_content[5000:5500]}")

# Document ë©”íƒ€ë°ì´í„° í™•ì¸
print(f"ë©”íƒ€ë°ì´í„°: {docs[0].metadata}")
```

#### 1.2 ë¬¸ì„œ ì²­í¬ ë¶„í•  (Split Texts)

```python
from langchain_text_splitters import CharacterTextSplitter

# ì²­í¬ ë¶„í•  ì „ëµ
text_splitter = CharacterTextSplitter(
    separator="\n\n",      # ë¬¸ë‹¨ êµ¬ë¶„ì
    chunk_size=1000,       # ì²­í¬ í¬ê¸°
    chunk_overlap=200,     # ê²¹ì¹˜ëŠ” ì˜ì—­
    length_function=len,   # ê¸¸ì´ ì¸¡ì • í•¨ìˆ˜
    is_separator_regex=False
)

splitted_docs = text_splitter.split_documents(docs)

print(f"ë¶„í• ëœ Document ê°œìˆ˜: {len(splitted_docs)}")

# ê° ì²­í¬ í™•ì¸
for i, doc in enumerate(splitted_docs[:3]):
    print(f"\nDocument {i} ê¸¸ì´: {len(doc.page_content)}")
    print(f"Document {i} ë‚´ìš©: {doc.page_content[:100]}...")
    print("-" * 50)
```

#### ì²­í¬ ë¶„í•  ì „ëµ ë¹„êµ

```python
# ê· ë“± ë¶„í•  ë°©ì‹
text_splitter_equal = CharacterTextSplitter(
    separator="",          # ë¬¸ì ë‹¨ìœ„ ë¶„í• 
    chunk_size=1000,       # ì—„ê²©í•œ 1000ì ì œí•œ
    length_function=len,
    is_separator_regex=False
)

equally_splitted_docs = text_splitter_equal.split_documents(docs)

print(f"ê· ë“± ë¶„í•  Document ê°œìˆ˜: {len(equally_splitted_docs)}")

# ê¸¸ì´ ë¶„í¬ í™•ì¸
for i, doc in enumerate(equally_splitted_docs):
    print(f"Document {i} ê¸¸ì´: {len(doc.page_content)}")
```

#### 1.3 ë¬¸ì„œ ì„ë² ë”© ìƒì„± (Document Embeddings)

```python
from langchain_openai import OpenAIEmbeddings

# OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-small"  # ì„±ëŠ¥ê³¼ ë¹„ìš©ì˜ ê· í˜•
)

# ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì„ë² ë”© í…ŒìŠ¤íŠ¸
sample_text = "ìœ„í‚¤í”¼ë””ì•„ ì •ì±… ë³€ê²½ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
embedding_vector = embedding_model.embed_query(sample_text)

print(f"ì„ë² ë”© ë²¡í„° ì°¨ì›: {len(embedding_vector)}")
print(f"ì„ë² ë”© ë²¡í„° ìƒ˜í”Œ: {embedding_vector[:10]}...")
```

#### 1.4 ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• (Vectorstores)

```python
from langchain_chroma import Chroma

# Chroma ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
vector_store = Chroma(embedding_function=embedding_model)

# Documentë“¤ì„ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
document_ids = vector_store.add_documents(splitted_docs)

print(f"ì €ì¥ëœ Document ê°œìˆ˜: {len(document_ids)}")
print(f"Document ID ìƒ˜í”Œ: {document_ids[:3]}")

# ì €ì¥ì†Œ ìƒíƒœ í™•ì¸
print(f"ë²¡í„° ì €ì¥ì†Œ ì´ Document ìˆ˜: {vector_store._collection.count()}")
```

### Step 2: Retrieval and Generation (ê²€ìƒ‰ ë° ìƒì„±)

#### 2.1 ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰

```python
# ì§ì ‘ì ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
search_query = "ìœ„í‚¤í”¼ë””ì•„ ì •ì±… ë³€ê²½ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"

results = vector_store.similarity_search(query=search_query, k=2)

print("ê²€ìƒ‰ ê²°ê³¼:")
for i, doc in enumerate(results):
    print(f"\n{i+1}. {doc.page_content[:200]}...")
    print(f"   ë©”íƒ€ë°ì´í„°: {doc.metadata}")
    print("-" * 50)
```

#### 2.2 Retriever ì„¤ì •

```python
# ê²€ìƒ‰ê¸° ì„¤ì •
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 2}  # ìƒìœ„ 2ê°œ ë¬¸ì„œ ê²€ìƒ‰
)

# ê²€ìƒ‰ê¸°ë¥¼ í†µí•œ ê²€ìƒ‰
retrieved_docs = retriever.invoke(input=search_query)

print("ê²€ìƒ‰ê¸° ê²°ê³¼:")
for doc in retrieved_docs:
    print(f"* {doc.page_content[:100]}...")
    print(f"  [{doc.metadata}]")
    print("-" * 50)
```

#### 2.3 RAG ì²´ì¸ êµ¬ì„±

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
system_prompt = (
    "ë‹¤ìŒ ê²€ìƒ‰ëœ ë§¥ë½ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. "
    "ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”. "
    "ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
    "{context}"
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}")
])

# LLM ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

# ë¬¸ì„œ ì²˜ë¦¬ ì²´ì¸ ìƒì„±
question_answer_chain = create_stuff_documents_chain(llm, prompt)

# ì „ì²´ RAG ì²´ì¸ ìƒì„±
rag_chain = create_retrieval_chain(retriever, question_answer_chain)
```

#### 2.4 RAG ì²´ì¸ ì‹¤í–‰

```python
# ì§ˆì˜ ì‹¤í–‰
query = "ìœ„í‚¤í”¼ë””ì•„ ì •ì±… ë³€ê²½ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
response = rag_chain.invoke({"input": query})

# ì‘ë‹µ ë¶„ì„
print("=== RAG ì‹œìŠ¤í…œ ì‘ë‹µ ===")
print(response['answer'])

print(f"\n=== ì‚¬ìš©ëœ ë¬¸ì„œ ({len(response['context'])}ê°œ) ===")
for i, doc in enumerate(response['context'], 1):
    print(f"{i}. {doc.page_content[:150]}...")
    print(f"   ì¶œì²˜: {doc.metadata['source']}")
    print("-" * 50)
```

## ğŸš€ ì‹¤ìŠµí•´ë³´ê¸°

### ì‹¤ìŠµ: ë‰´ìŠ¤ ê¸°ì‚¬ RAG ì‹œìŠ¤í…œ êµ¬ì¶•

**ëª©í‘œ**: ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§ˆë¬¸ ë‹µë³€ ì‹œìŠ¤í…œ êµ¬í˜„

#### ë‹¨ê³„ë³„ êµ¬í˜„

```python
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# 1ë‹¨ê³„: ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
web_urls = [
    "https://n.news.naver.com/mnews/article/029/0002927209",
    "https://n.news.naver.com/mnews/article/092/0002358620",
    "https://n.news.naver.com/mnews/article/008/0005136824",
]

# 2ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ
loader = WebBaseLoader(web_urls)
docs = loader.load()
print(f"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

# 3ë‹¨ê³„: ë¬¸ì„œ ë¶„í• 
text_splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)
splitted_docs = text_splitter.split_documents(docs)
print(f"ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(splitted_docs)}")

# 4ë‹¨ê³„: ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
vector_store = Chroma(embedding_function=embedding_model)
document_ids = vector_store.add_documents(splitted_docs)
print(f"ë²¡í„° ì €ì¥ì†Œ ë¬¸ì„œ ìˆ˜: {len(document_ids)}")

# 5ë‹¨ê³„: RAG ì²´ì¸ êµ¬ì„±
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 2}
)

system_prompt = (
    "ë‹¤ìŒ ê²€ìƒ‰ëœ ë§¥ë½ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. "
    "ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”. "
    "ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}")
])

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

# 6ë‹¨ê³„: ì§ˆë¬¸ ë‹µë³€ í…ŒìŠ¤íŠ¸
query = "ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”"
response = rag_chain.invoke({"input": query})

print("=== RAG ì‹œìŠ¤í…œ ì‘ë‹µ ===")
print(response['answer'])
```

### ê³ ê¸‰ ì‹¤ìŠµ: RAG ì„±ëŠ¥ ê°œì„ 

#### ê²€ìƒ‰ íŒŒë¼ë¯¸í„° íŠœë‹

```python
class AdvancedRAGSystem:
    def __init__(self, documents):
        self.documents = documents
        self.vector_store = None
        self.retriever = None
        self.rag_chain = None

    def setup_vectorstore(self, chunk_size=1000, chunk_overlap=200):
        """ë²¡í„° ì €ì¥ì†Œ ì„¤ì •"""
        text_splitter = CharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len
        )

        splitted_docs = text_splitter.split_documents(self.documents)

        embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
        self.vector_store = Chroma(embedding_function=embedding_model)
        self.vector_store.add_documents(splitted_docs)

        return len(splitted_docs)

    def setup_retriever(self, search_type="similarity", k=3):
        """ê²€ìƒ‰ê¸° ì„¤ì •"""
        if not self.vector_store:
            raise ValueError("Vector store not initialized")

        self.retriever = self.vector_store.as_retriever(
            search_type=search_type,
            search_kwargs={"k": k}
        )

    def setup_rag_chain(self, model_name="gpt-4.1-mini", temperature=0):
        """RAG ì²´ì¸ ì„¤ì •"""
        if not self.retriever:
            raise ValueError("Retriever not initialized")

        system_prompt = (
            "ë‹¤ìŒ ê²€ìƒ‰ëœ ë§¥ë½ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. "
            "ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”. "
            "ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
            "{context}"
        )

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}")
        ])

        llm = ChatOpenAI(model=model_name, temperature=temperature)

        question_answer_chain = create_stuff_documents_chain(llm, prompt)
        self.rag_chain = create_retrieval_chain(self.retriever, question_answer_chain)

    def query(self, question):
        """ì§ˆë¬¸ ì²˜ë¦¬"""
        if not self.rag_chain:
            raise ValueError("RAG chain not initialized")

        response = self.rag_chain.invoke({"input": question})
        return {
            'answer': response['answer'],
            'source_documents': response['context'],
            'source_count': len(response['context'])
        }

# ì‚¬ìš© ì˜ˆì œ
rag_system = AdvancedRAGSystem(docs)

# ì‹œìŠ¤í…œ ì„¤ì •
chunk_count = rag_system.setup_vectorstore(chunk_size=800, chunk_overlap=150)
rag_system.setup_retriever(search_type="similarity", k=3)
rag_system.setup_rag_chain(model_name="gpt-4.1-mini")

print(f"ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ - ì´ {chunk_count}ê°œ ì²­í¬ ìƒì„±")

# ì§ˆë¬¸ ì²˜ë¦¬
result = rag_system.query("ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì •ì±…ì„ ì–´ë–»ê²Œ ë³€ê²½í•˜ë‚˜ìš”?")

print("=== í–¥ìƒëœ RAG ì‹œìŠ¤í…œ ì‘ë‹µ ===")
print(result['answer'])
print(f"\nì‚¬ìš©ëœ ì†ŒìŠ¤ ë¬¸ì„œ: {result['source_count']}ê°œ")
```

## ğŸ“‹ í•´ë‹µ

### ì™„ì „í•œ RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„

```python
import os
from typing import List, Dict, Any
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.documents import Document

class ComprehensiveRAGPipeline:
    """í¬ê´„ì ì¸ RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„"""

    def __init__(self):
        self.docs = []
        self.splitted_docs = []
        self.vector_store = None
        self.retriever = None
        self.rag_chain = None
        self.embedding_model = None

    def load_documents(self, sources: List[str]) -> int:
        """ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë¬¸ì„œ ë¡œë“œ"""
        all_docs = []

        # ì›¹ URL ì²˜ë¦¬
        web_urls = [s for s in sources if s.startswith('http')]
        if web_urls:
            web_loader = WebBaseLoader(web_urls)
            web_docs = web_loader.load()
            all_docs.extend(web_docs)

        # í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ ì²˜ë¦¬
        text_sources = [s for s in sources if not s.startswith('http')]
        for text in text_sources:
            doc = Document(page_content=text, metadata={"source": "direct_input"})
            all_docs.append(doc)

        self.docs = all_docs
        return len(self.docs)

    def split_documents(self, chunk_size: int = 1000, chunk_overlap: int = 200) -> int:
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if not self.docs:
            raise ValueError("ë¬¸ì„œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        text_splitter = CharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False
        )

        self.splitted_docs = text_splitter.split_documents(self.docs)
        return len(self.splitted_docs)

    def create_vectorstore(self, embedding_model_name: str = "text-embedding-3-small"):
        """ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
        if not self.splitted_docs:
            raise ValueError("ë¬¸ì„œê°€ ë¶„í• ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.embedding_model = OpenAIEmbeddings(model=embedding_model_name)
        self.vector_store = Chroma(embedding_function=self.embedding_model)

        # ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
        document_ids = self.vector_store.add_documents(self.splitted_docs)
        return len(document_ids)

    def setup_retriever(self, search_type: str = "similarity", k: int = 3):
        """ê²€ìƒ‰ê¸° ì„¤ì •"""
        if not self.vector_store:
            raise ValueError("ë²¡í„° ì €ì¥ì†Œê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.retriever = self.vector_store.as_retriever(
            search_type=search_type,
            search_kwargs={"k": k}
        )

    def create_rag_chain(self,
                        model_name: str = "gpt-4.1-mini",
                        temperature: float = 0,
                        custom_prompt: str = None):
        """RAG ì²´ì¸ ìƒì„±"""
        if not self.retriever:
            raise ValueError("ê²€ìƒ‰ê¸°ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        if custom_prompt is None:
            system_prompt = (
                "ë‹¤ìŒ ê²€ìƒ‰ëœ ë§¥ë½ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. "
                "ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”. "
                "ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. "
                "ê°€ëŠ¥í•œ ê²½ìš° ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”.\n\n"
                "{context}"
            )
        else:
            system_prompt = custom_prompt

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}")
        ])

        llm = ChatOpenAI(model=model_name, temperature=temperature)

        question_answer_chain = create_stuff_documents_chain(llm, prompt)
        self.rag_chain = create_retrieval_chain(self.retriever, question_answer_chain)

    def query(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        if not self.rag_chain:
            raise ValueError("RAG ì²´ì¸ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        response = self.rag_chain.invoke({"input": question})

        return {
            'question': question,
            'answer': response['answer'],
            'source_documents': response['context'],
            'source_count': len(response['context']),
            'sources': list(set([doc.metadata.get('source', 'unknown')
                               for doc in response['context']]))
        }

    def batch_query(self, questions: List[str]) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ì§ˆë¬¸ì— ëŒ€í•œ ë°°ì¹˜ ì²˜ë¦¬"""
        results = []
        for question in questions:
            try:
                result = self.query(question)
                results.append(result)
            except Exception as e:
                results.append({
                    'question': question,
                    'answer': f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                    'error': True
                })
        return results

    def get_pipeline_info(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì •ë³´"""
        return {
            'total_documents': len(self.docs),
            'total_chunks': len(self.splitted_docs),
            'vector_store_count': self.vector_store._collection.count() if self.vector_store else 0,
            'embedding_model': self.embedding_model.model if self.embedding_model else None,
            'retriever_configured': self.retriever is not None,
            'rag_chain_configured': self.rag_chain is not None
        }

# ì‚¬ìš© ì˜ˆì œ
def demonstrate_rag_pipeline():
    """RAG íŒŒì´í”„ë¼ì¸ ë°ëª¨"""
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    rag = ComprehensiveRAGPipeline()

    # 1. ë¬¸ì„œ ë¡œë“œ
    sources = [
        'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EC%B3%90:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8'
    ]
    doc_count = rag.load_documents(sources)
    print(f"âœ… {doc_count}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

    # 2. ë¬¸ì„œ ë¶„í• 
    chunk_count = rag.split_documents(chunk_size=800, chunk_overlap=150)
    print(f"âœ… {chunk_count}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

    # 3. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    vector_count = rag.create_vectorstore()
    print(f"âœ… {vector_count}ê°œ ë²¡í„° ì €ì¥ ì™„ë£Œ")

    # 4. ê²€ìƒ‰ê¸° ì„¤ì •
    rag.setup_retriever(k=3)
    print("âœ… ê²€ìƒ‰ê¸° ì„¤ì • ì™„ë£Œ")

    # 5. RAG ì²´ì¸ ìƒì„±
    rag.create_rag_chain()
    print("âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")

    # 6. íŒŒì´í”„ë¼ì¸ ì •ë³´ ì¶œë ¥
    info = rag.get_pipeline_info()
    print(f"\nğŸ“Š íŒŒì´í”„ë¼ì¸ ì •ë³´: {info}")

    # 7. ì§ˆë¬¸ ë‹µë³€ í…ŒìŠ¤íŠ¸
    test_questions = [
        "ìœ„í‚¤í”¼ë””ì•„ ì •ì±…ì€ ì–´ë–»ê²Œ ë³€ê²½í•˜ë‚˜ìš”?",
        "ìƒˆë¡œìš´ ì •ì±…ì„ ì œì•ˆí•˜ëŠ” ê³¼ì •ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì •ì±… ìœ„ë°˜ ì‹œ ì–´ë–¤ ì¡°ì¹˜ê°€ ì·¨í•´ì§€ë‚˜ìš”?"
    ]

    print("\nğŸ” ì§ˆë¬¸ ë‹µë³€ í…ŒìŠ¤íŠ¸:")
    results = rag.batch_query(test_questions)

    for i, result in enumerate(results, 1):
        print(f"\n{i}. ì§ˆë¬¸: {result['question']}")
        print(f"   ë‹µë³€: {result['answer'][:200]}...")
        if 'source_count' in result:
            print(f"   ì‚¬ìš©ëœ ì†ŒìŠ¤: {result['source_count']}ê°œ")
            print(f"   ì¶œì²˜: {', '.join(result['sources'])}")
        print("-" * 80)

    return rag

# ì‹¤í–‰
if __name__ == "__main__":
    rag_pipeline = demonstrate_rag_pipeline()
```

## ğŸ” ì„±ëŠ¥ ìµœì í™” íŒ

### ì²­í¬ í¬ê¸° ìµœì í™”

```python
def optimize_chunk_size(documents, test_questions, chunk_sizes=[500, 800, 1000, 1200]):
    """ì²­í¬ í¬ê¸°ë³„ ì„±ëŠ¥ ë¹„êµ"""
    results = {}

    for chunk_size in chunk_sizes:
        rag = ComprehensiveRAGPipeline()
        rag.docs = documents

        chunk_count = rag.split_documents(chunk_size=chunk_size)
        rag.create_vectorstore()
        rag.setup_retriever(k=2)
        rag.create_rag_chain()

        # ì‘ë‹µ ì‹œê°„ ì¸¡ì •
        import time
        start_time = time.time()

        responses = []
        for question in test_questions:
            result = rag.query(question)
            responses.append(result)

        end_time = time.time()

        results[chunk_size] = {
            'chunk_count': chunk_count,
            'response_time': end_time - start_time,
            'avg_response_length': sum(len(r['answer']) for r in responses) / len(responses)
        }

    return results

# ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
def print_optimization_results(results):
    print("ì²­í¬ í¬ê¸°ë³„ ì„±ëŠ¥ ë¹„êµ:")
    print(f"{'ì²­í¬ í¬ê¸°':<10} {'ì²­í¬ ìˆ˜':<10} {'ì‘ë‹µ ì‹œê°„':<12} {'í‰ê·  ì‘ë‹µ ê¸¸ì´':<15}")
    print("-" * 50)

    for chunk_size, metrics in results.items():
        print(f"{chunk_size:<10} {metrics['chunk_count']:<10} "
              f"{metrics['response_time']:.2f}s{'':<6} "
              f"{metrics['avg_response_length']:.1f}{'':<10}")
```

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/) - ê³µì‹ RAG íŠœí† ë¦¬ì–¼
- [LangChain Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/) - ë¬¸ì„œ ë¡œë” ì¢…ë¥˜
- [LangChain Text Splitters](https://python.langchain.com/docs/concepts/text_splitters/) - í…ìŠ¤íŠ¸ ë¶„í•  ì „ëµ
- [LangChain Vector Stores](https://python.langchain.com/docs/integrations/vectorstores/) - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤

### í•™ìŠµ ìë£Œ
- [RAG ì•„í‚¤í…ì²˜ ê°€ì´ë“œ](https://blog.langchain.dev/rag-from-scratch/) - RAG ê°œë…ê³¼ êµ¬í˜„
- [Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤](https://docs.trychroma.com/) - ë²¡í„° ì €ì¥ì†Œ í™œìš©ë²•
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings) - ì„ë² ë”© ëª¨ë¸ ê°€ì´ë“œ

### ê°œë°œ ë„êµ¬
- [LangSmith](https://smith.langchain.com/) - RAG ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- [Chroma](https://www.trychroma.com/) - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
- [FAISS](https://github.com/facebookresearch/faiss) - ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰

### ì¶”ê°€ í•™ìŠµ
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì˜ë¯¸ ê²€ìƒ‰)
- RAG ì„±ëŠ¥ í‰ê°€ ë©”íŠ¸ë¦­
- ì‹¤ì‹œê°„ ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì „ëµ
- ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œ