# Langfuseë¥¼ í™œìš©í•œ RAG ì‹œìŠ¤í…œ í‰ê°€ - ì²´ê³„ì  ëª¨ë‹ˆí„°ë§ê³¼ í‰ê°€ ìë™í™” ê°€ì´ë“œ

## ğŸ“š í•™ìŠµ ëª©í‘œ
- Langfuseì˜ í•µì‹¬ ê¸°ëŠ¥ê³¼ RAG ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë°©ë²•ì„ ì´í•´í•œë‹¤
- ë°ì´í„°ì…‹ ê¸°ë°˜ì˜ ì²´ê³„ì ì¸ í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤
- CallbackHandlerë¥¼ í™œìš©í•œ ìë™ ì¶”ì  ë° ë¡œê¹… ì‹œìŠ¤í…œì„ êµ¬í˜„í•œë‹¤
- ROUGE ì ìˆ˜ì™€ LLM-as-Judge ë°©ì‹ì˜ í†µí•© í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤
- Langfuse ëŒ€ì‹œë³´ë“œë¥¼ í†µí•œ ì„±ëŠ¥ ë¶„ì„ê³¼ ê°œì„ ì  ë„ì¶œ ë°©ë²•ì„ ìŠµë“í•œë‹¤

## ğŸ”‘ í•µì‹¬ ê°œë…

### Langfuseë€?
- **ì •ì˜**: LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ ê´€ì¸¡ì„± ë° ë¶„ì„ í”Œë«í¼
- **ëª©ì **: í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ê°œì„ 
- **íŠ¹ì§•**: ì‹¤ì‹œê°„ ì¶”ì , ìë™ í‰ê°€, ë¹„ìš© ë¶„ì„, ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ì œê³µ

### ì£¼ìš” ê¸°ëŠ¥
1. **íŠ¸ë ˆì´ì‹±(Tracing)**: LLM í˜¸ì¶œê³¼ ì²´ì¸ ì‹¤í–‰ ê³¼ì •ì˜ ìƒì„¸ ì¶”ì 
2. **í‰ê°€(Evaluation)**: ìë™í™”ëœ í’ˆì§ˆ í‰ê°€ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
3. **ë°ì´í„°ì…‹ ê´€ë¦¬**: í‰ê°€ìš© ë°ì´í„°ì…‹ ìƒì„± ë° ë²„ì „ ê´€ë¦¬
4. **ëŒ€ì‹œë³´ë“œ**: ì‹œê°ì  ì„±ëŠ¥ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ì œê³µ
5. **ë¹„ìš© ì¶”ì **: API ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ëª¨ë‹ˆí„°ë§

### Langfuseì˜ ì¥ì 
- **ğŸ”„ ìë™í™”ëœ ì¶”ì **: CallbackHandlerë¥¼ í†µí•œ íˆ¬ëª…í•œ ë¡œê¹…
- **ğŸ“Š ì‹œê°ì  ëŒ€ì‹œë³´ë“œ**: ì§ê´€ì ì¸ ì„±ëŠ¥ ë¶„ì„ ì¸í„°í˜ì´ìŠ¤
- **ğŸ” ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ**: ROUGE, LLM-as-Judge ë“± í†µí•© ì§€ì›
- **ğŸš€ í™•ì¥ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸**: ëŒ€ê·œëª¨ í‰ê°€ ìë™í™” ê°€ëŠ¥

## ğŸ›  í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# Langfuse ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install langfuse langfuse-langchain

# ê¸°ë³¸ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install langchain langchain-openai langchain-chroma

# í‰ê°€ ë©”íŠ¸ë¦­ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install korouge-score
pip install krag  # í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ë° ê²€ìƒ‰ê¸°

# ë°ì´í„° ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install pandas numpy openpyxl
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
# .env íŒŒì¼
OPENAI_API_KEY=your_openai_api_key
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
LANGFUSE_HOST=https://cloud.langfuse.com  # ë˜ëŠ” ìì²´ í˜¸ìŠ¤íŒ… URL

# LangSmith ì„¤ì • (ì„ íƒì‚¬í•­)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key
```

### ê¸°ë³¸ ì„¤ì •
```python
import os
from dotenv import load_dotenv
import pandas as pd
import numpy as np
import warnings
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
warnings.filterwarnings("ignore")

# Langfuse ë¼ì´ë¸ŒëŸ¬ë¦¬
from langfuse import get_client
from langfuse.langchain import CallbackHandler

# LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.evaluation import load_evaluator
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# í‰ê°€ ë©”íŠ¸ë¦­
from korouge_score import rouge_scorer
from krag.tokenizers import KiwiTokenizer
```

## ğŸ’» ë‹¨ê³„ë³„ êµ¬í˜„

### 1ë‹¨ê³„: Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì¸ì¦

```python
class LangfuseManager:
    def __init__(self):
        """Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.client = get_client()
        self.callback_handler = CallbackHandler()

        # ì¸ì¦ ìƒíƒœ í™•ì¸
        auth_status = self.client.auth_check()
        if auth_status:
            print("âœ… Langfuse ì¸ì¦ ì„±ê³µ")
        else:
            raise Exception("âŒ Langfuse ì¸ì¦ ì‹¤íŒ¨ - API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")

    def get_client(self):
        """Langfuse í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        return self.client

    def get_callback_handler(self):
        """CallbackHandler ë°˜í™˜"""
        return self.callback_handler

    def list_datasets(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ë°˜í™˜"""
        try:
            # Langfuse APIë¡œ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ
            # ì‹¤ì œ êµ¬í˜„ì€ Langfuse API ë¬¸ì„œ ì°¸ê³ 
            return []
        except Exception as e:
            print(f"ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def flush(self):
        """ë³´ë¥˜ ì¤‘ì¸ ì‘ì—…ì„ Langfuse ì„œë²„ì— ì „ì†¡"""
        self.client.flush()

# Langfuse ë§¤ë‹ˆì € ì´ˆê¸°í™”
langfuse_manager = LangfuseManager()
langfuse_client = langfuse_manager.get_client()
langfuse_handler = langfuse_manager.get_callback_handler()

print("Langfuse ì´ˆê¸°í™” ì™„ë£Œ!")
```

### 2ë‹¨ê³„: í‰ê°€ìš© ë°ì´í„°ì…‹ ìƒì„± ë° ê´€ë¦¬

```python
class DatasetManager:
    def __init__(self, langfuse_client):
        self.client = langfuse_client

    def create_dataset_from_excel(self, excel_path: str, dataset_name: str) -> str:
        """
        Excel íŒŒì¼ë¡œë¶€í„° Langfuse ë°ì´í„°ì…‹ ìƒì„±

        Args:
            excel_path: Excel íŒŒì¼ ê²½ë¡œ
            dataset_name: ìƒì„±í•  ë°ì´í„°ì…‹ ì´ë¦„

        Returns:
            ìƒì„±ëœ ë°ì´í„°ì…‹ ì´ë¦„
        """
        # Excel íŒŒì¼ ì½ê¸°
        df_qa_test = pd.read_excel(excel_path)
        print(f"ğŸ“Š Excel ë°ì´í„° ë¡œë“œ: {df_qa_test.shape[0]}ê°œ í•­ëª©")

        # ë°ì´í„°ì…‹ ìƒì„±
        try:
            dataset = self.client.create_dataset(name=dataset_name)
            print(f"ğŸ“‚ ë°ì´í„°ì…‹ ìƒì„±: {dataset_name}")
        except Exception as e:
            print(f"âš ï¸ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {e}")

        # ë°ì´í„° ë³€í™˜ ë° ì¶”ê°€
        successful_items = 0
        failed_items = 0

        for idx, row in df_qa_test.iterrows():
            try:
                self.client.create_dataset_item(
                    dataset_name=dataset_name,
                    input=row["user_input"],
                    expected_output=row["reference"],
                    metadata={
                        "reference_contexts": row.get("reference_contexts", ""),
                        "synthesizer_name": row.get("synthesizer_name", ""),
                        "item_index": idx
                    }
                )
                successful_items += 1

                if (idx + 1) % 10 == 0:
                    print(f"ğŸ“ ë°ì´í„° ì¶”ê°€ ì§„í–‰: {idx + 1}/{len(df_qa_test)}")

            except Exception as e:
                failed_items += 1
                print(f"âŒ í•­ëª© {idx} ì¶”ê°€ ì‹¤íŒ¨: {e}")

        # ë³€ê²½ì‚¬í•­ ì €ì¥
        self.client.flush()

        print(f"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ:")
        print(f"   - ì„±ê³µ: {successful_items}ê°œ")
        print(f"   - ì‹¤íŒ¨: {failed_items}ê°œ")

        return dataset_name

    def get_dataset_info(self, dataset_name: str) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ ì •ë³´ ì¡°íšŒ"""
        try:
            dataset = self.client.get_dataset(name=dataset_name)

            return {
                "name": dataset.name,
                "item_count": len(dataset.items),
                "created_at": getattr(dataset, 'created_at', 'Unknown'),
                "items_preview": dataset.items[:3] if dataset.items else []
            }
        except Exception as e:
            print(f"ë°ì´í„°ì…‹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def validate_dataset(self, dataset_name: str) -> bool:
        """ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì¦"""
        try:
            dataset = self.client.get_dataset(name=dataset_name)

            if not dataset:
                print(f"âŒ ë°ì´í„°ì…‹ '{dataset_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return False

            if len(dataset.items) == 0:
                print(f"âŒ ë°ì´í„°ì…‹ '{dataset_name}'ì— í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤")
                return False

            # ìƒ˜í”Œ í•­ëª© ê²€ì¦
            sample_item = dataset.items[0]
            required_fields = ['input', 'expected_output']

            for field in required_fields:
                if not hasattr(sample_item, field) or not getattr(sample_item, field):
                    print(f"âŒ í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤")
                    return False

            print(f"âœ… ë°ì´í„°ì…‹ '{dataset_name}' ê²€ì¦ ì™„ë£Œ")
            return True

        except Exception as e:
            print(f"âŒ ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

# ë°ì´í„°ì…‹ ë§¤ë‹ˆì € ì‚¬ìš© ì˜ˆì‹œ
dataset_manager = DatasetManager(langfuse_client)

# Excel íŒŒì¼ë¡œë¶€í„° ë°ì´í„°ì…‹ ìƒì„±
dataset_name = dataset_manager.create_dataset_from_excel(
    excel_path="data/testset.xlsx",
    dataset_name="RAG_Evaluation_Dataset_Tesla"
)

# ë°ì´í„°ì…‹ ì •ë³´ í™•ì¸
dataset_info = dataset_manager.get_dataset_info(dataset_name)
print(f"\nğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:")
for key, value in dataset_info.items():
    if key != 'items_preview':
        print(f"   - {key}: {value}")
```

### 3ë‹¨ê³„: RAG ì‹œìŠ¤í…œ êµ¬ì„±

```python
class RAGSystemBuilder:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.1)

    def setup_retrievers(self, chroma_path: str = "./chroma_db",
                        documents_path: str = "data/korean_docs_final.jsonl"):
        """ê²€ìƒ‰ê¸° ì„¤ì •"""
        # ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
        chroma_db = Chroma(
            collection_name="db_korean_cosine_metadata",
            embedding_function=self.embeddings,
            persist_directory=chroma_path,
        )

        vector_retriever = chroma_db.as_retriever(search_kwargs={'k': 4})

        # BM25 ê²€ìƒ‰ê¸° ì„¤ì • (í•œêµ­ì–´ ë¬¸ì„œìš©)
        try:
            from krag.tokenizers import KiwiTokenizer
            from krag.retrievers import KiWiBM25RetrieverWithScore
            import json
            from langchain.schema import Document

            # í•œêµ­ì–´ ë¬¸ì„œ ë¡œë“œ
            with open(documents_path, 'r', encoding='utf-8') as f:
                korean_docs = [json.loads(line) for line in f]

            # Document ê°ì²´ë¡œ ë³€í™˜
            documents = []
            for data in korean_docs:
                if isinstance(data, str):
                    doc_data = json.loads(data)
                else:
                    doc_data = data

                documents.append(Document(
                    page_content=doc_data['page_content'],
                    metadata=doc_data['metadata']
                ))

            # BM25 ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
            kiwi_tokenizer = KiwiTokenizer(model_type='knlm', typos='basic')
            bm25_retriever = KiWiBM25RetrieverWithScore(
                documents=documents,
                kiwi_tokenizer=kiwi_tokenizer,
                k=4
            )

            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° (Ensemble)
            from langchain.retrievers import EnsembleRetriever

            hybrid_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, vector_retriever],
                weights=[0.5, 0.5]
            )

            print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì„¤ì • ì™„ë£Œ")
            return hybrid_retriever

        except ImportError:
            print("âš ï¸ BM25 ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê¸°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return vector_retriever

    def create_rag_chain(self, retriever):
        """RAG ì²´ì¸ ìƒì„±"""
        template = """ë‹¤ìŒ ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
ë§¥ë½ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ë‹¤ë©´ 'ë‹µë³€ì— í•„ìš”í•œ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.

[ë§¥ë½]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]
"""

        prompt = ChatPromptTemplate.from_template(template)

        def format_docs(docs):
            return "\n\n".join([doc.page_content for doc in docs])

        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return rag_chain

# RAG ì‹œìŠ¤í…œ êµ¬ì¶•
rag_builder = RAGSystemBuilder()
retriever = rag_builder.setup_retrievers()
rag_chain = rag_builder.create_rag_chain(retriever)

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
test_question = "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"
test_answer = rag_chain.invoke(test_question)
print(f"\nğŸ§ª RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸:")
print(f"ì§ˆë¬¸: {test_question}")
print(f"ë‹µë³€: {test_answer}")
```

### 4ë‹¨ê³„: ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ êµ¬í˜„

```python
@dataclass
class EvaluationResult:
    """í‰ê°€ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    item_id: str
    input: str
    output: str
    expected_output: str
    scores: Dict[str, float]
    details: Dict[str, Any]
    trace_id: Optional[str] = None
    execution_time: Optional[float] = None
    error: Optional[str] = None

class ComprehensiveEvaluator:
    def __init__(self, llm):
        self.llm = llm
        self.setup_evaluators()

    def setup_evaluators(self):
        """í‰ê°€ì ì´ˆê¸°í™”"""
        # ROUGE í‰ê°€ì
        from korouge_score import rouge_scorer
        from krag.tokenizers import KiwiTokenizer

        class CustomKiwiTokenizer(KiwiTokenizer):
            def tokenize(self, text):
                return [t.form for t in super().tokenize(text)]

        self.kiwi_tokenizer = CustomKiwiTokenizer(model_type='knlm', typos='basic')
        self.rouge_scorer = rouge_scorer.RougeScorer(
            ["rouge1", "rouge2", "rougeL"],
            tokenizer=self.kiwi_tokenizer
        )

        # LLM-as-Judge í‰ê°€ìë“¤
        self.evaluators = {
            "relevance": load_evaluator(
                evaluator="labeled_criteria",
                criteria="relevance",
                llm=self.llm
            ),
            "helpfulness": load_evaluator(
                evaluator="labeled_criteria",
                criteria="helpfulness",
                llm=self.llm
            ),
            "conciseness": load_evaluator(
                evaluator="labeled_criteria",
                criteria="conciseness",
                llm=self.llm
            ),
            "correctness": load_evaluator(
                evaluator="labeled_criteria",
                criteria="correctness",
                llm=self.llm
            )
        }

        print("âœ… í‰ê°€ì ì´ˆê¸°í™” ì™„ë£Œ")

    def evaluate_rouge_scores(self, prediction: str, reference: str) -> Dict[str, float]:
        """ROUGE ì ìˆ˜ ê³„ì‚°"""
        try:
            rouge_results = self.rouge_scorer.score(reference, prediction)
            return {
                "rouge1": rouge_results['rouge1'].fmeasure,
                "rouge2": rouge_results['rouge2'].fmeasure,
                "rougeL": rouge_results['rougeL'].fmeasure
            }
        except Exception as e:
            print(f"ROUGE í‰ê°€ ì‹¤íŒ¨: {e}")
            return {"rouge1": 0.0, "rouge2": 0.0, "rougeL": 0.0}

    def evaluate_llm_criteria(self, input_text: str, prediction: str,
                             reference: str) -> Dict[str, Dict[str, Any]]:
        """LLM-as-Judge í‰ê°€"""
        results = {}

        for criterion_name, evaluator in self.evaluators.items():
            try:
                evaluation_result = evaluator.evaluate_strings(
                    input=input_text,
                    prediction=prediction,
                    reference=reference
                )

                results[criterion_name] = {
                    "score": float(evaluation_result.get('score', 0)),
                    "reasoning": evaluation_result.get('reasoning', ''),
                    "status": "success"
                }

            except Exception as e:
                results[criterion_name] = {
                    "score": 0.0,
                    "reasoning": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}",
                    "status": "error"
                }

        return results

    def calculate_overall_score(self, scores: Dict[str, float]) -> float:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        if not scores:
            return 0.0

        # ê°€ì¤‘ì¹˜ ì„¤ì •
        weights = {
            "rouge1": 0.2,
            "rouge2": 0.1,
            "rougeL": 0.1,
            "relevance": 0.25,
            "helpfulness": 0.15,
            "conciseness": 0.1,
            "correctness": 0.1
        }

        weighted_sum = 0.0
        total_weight = 0.0

        for metric, score in scores.items():
            if metric in weights:
                weight = weights[metric]
                weighted_sum += score * weight
                total_weight += weight

        return weighted_sum / total_weight if total_weight > 0 else 0.0

class LangfuseEvaluationPipeline:
    def __init__(self, langfuse_client, rag_chain, evaluator):
        self.client = langfuse_client
        self.rag_chain = rag_chain
        self.evaluator = evaluator

    def run_evaluation(self, dataset_name: str, run_name: str,
                      max_items: Optional[int] = None) -> List[EvaluationResult]:
        """ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

        # ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°
        dataset = self.client.get_dataset(name=dataset_name)
        if not dataset:
            raise ValueError(f"ë°ì´í„°ì…‹ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        items_to_process = dataset.items
        if max_items:
            items_to_process = items_to_process[:max_items]

        print(f"ğŸš€ í‰ê°€ ì‹œì‘: {len(items_to_process)}ê°œ í•­ëª©")

        results = []
        successful = 0
        failed = 0

        for idx, item in enumerate(items_to_process, 1):
            try:
                print(f"\nğŸ“Š í•­ëª© {idx}/{len(items_to_process)} ì²˜ë¦¬ ì¤‘...")

                result = self._evaluate_single_item(item, run_name)
                results.append(result)

                if result.error:
                    failed += 1
                    print(f"   âŒ ì‹¤íŒ¨: {result.error}")
                else:
                    successful += 1
                    overall_score = self.evaluator.calculate_overall_score(result.scores)
                    print(f"   âœ… ì™„ë£Œ (ì¢…í•© ì ìˆ˜: {overall_score:.3f})")

                # ì§„í–‰ë¥  ì¶œë ¥
                if idx % 10 == 0 or idx == len(items_to_process):
                    print(f"\nğŸ“ˆ ì§„í–‰ë¥ : {idx}/{len(items_to_process)} "
                          f"(ì„±ê³µ: {successful}, ì‹¤íŒ¨: {failed})")

            except Exception as e:
                failed += 1
                print(f"   âŒ ì˜ˆì™¸ ë°œìƒ: {e}")

                # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë„ ê²°ê³¼ì— í¬í•¨
                results.append(EvaluationResult(
                    item_id=item.id,
                    input=str(item.input),
                    output="",
                    expected_output=str(item.expected_output) if item.expected_output else "",
                    scores={},
                    details={},
                    error=str(e)
                ))

        print(f"\nğŸ¯ í‰ê°€ ì™„ë£Œ:")
        print(f"   - ì´ í•­ëª©: {len(items_to_process)}")
        print(f"   - ì„±ê³µ: {successful}")
        print(f"   - ì‹¤íŒ¨: {failed}")
        print(f"   - ì„±ê³µë¥ : {successful/len(items_to_process)*100:.1f}%")

        return results

    def _evaluate_single_item(self, item, run_name: str) -> EvaluationResult:
        """ë‹¨ì¼ í•­ëª© í‰ê°€"""
        import time

        start_time = time.time()

        # Langfuse íŠ¸ë ˆì´ì‹±ê³¼ í•¨ê»˜ RAG ì‹¤í–‰
        with item.run(run_name=run_name) as root_span:
            try:
                # RAG ì²´ì¸ ì‹¤í–‰
                output = self.rag_chain.invoke(
                    item.input,
                    config={"callbacks": [CallbackHandler()]}
                )

                # í‰ê°€ ìˆ˜í–‰
                rouge_scores = self.evaluator.evaluate_rouge_scores(
                    str(output), str(item.expected_output)
                )

                llm_criteria_results = self.evaluator.evaluate_llm_criteria(
                    str(item.input), str(output), str(item.expected_output)
                )

                # ì ìˆ˜ í†µí•©
                all_scores = rouge_scores.copy()
                for criterion, result in llm_criteria_results.items():
                    all_scores[criterion] = result["score"]

                # ì¢…í•© ì ìˆ˜ ê³„ì‚°
                overall_score = self.evaluator.calculate_overall_score(all_scores)

                # Langfuseì— ì ìˆ˜ ê¸°ë¡
                root_span.score(name="overall", value=overall_score)
                for score_name, score_value in all_scores.items():
                    root_span.score(name=score_name, value=score_value)

                execution_time = time.time() - start_time

                return EvaluationResult(
                    item_id=item.id,
                    input=str(item.input),
                    output=str(output),
                    expected_output=str(item.expected_output) if item.expected_output else "",
                    scores=all_scores,
                    details={
                        "rouge": rouge_scores,
                        "llm_criteria": llm_criteria_results,
                        "overall_score": overall_score
                    },
                    trace_id=getattr(root_span, 'trace_id', None),
                    execution_time=execution_time
                )

            except Exception as e:
                execution_time = time.time() - start_time

                # ì‹¤íŒ¨í•´ë„ Langfuseì— ê¸°ë¡
                root_span.score(name="overall", value=0.0)

                return EvaluationResult(
                    item_id=item.id,
                    input=str(item.input),
                    output="",
                    expected_output=str(item.expected_output) if item.expected_output else "",
                    scores={},
                    details={},
                    execution_time=execution_time,
                    error=str(e)
                )

# ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì„±
evaluator = ComprehensiveEvaluator(ChatOpenAI(model="gpt-4.1-mini", temperature=0.1))
evaluation_pipeline = LangfuseEvaluationPipeline(langfuse_client, rag_chain, evaluator)

# í‰ê°€ ì‹¤í–‰
results = evaluation_pipeline.run_evaluation(
    dataset_name="RAG_Evaluation_Dataset_Tesla",
    run_name="comprehensive_evaluation_v1",
    max_items=10  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ìŒ 10ê°œë§Œ
)

print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:")
successful_results = [r for r in results if not r.error]
if successful_results:
    avg_overall_score = np.mean([
        evaluator.calculate_overall_score(r.scores)
        for r in successful_results
    ])
    avg_execution_time = np.mean([r.execution_time for r in successful_results])

    print(f"   - í‰ê·  ì¢…í•© ì ìˆ˜: {avg_overall_score:.3f}")
    print(f"   - í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_execution_time:.2f}ì´ˆ")
```

## ğŸ¯ ì‹¤ìŠµ ë¬¸ì œ

### ê¸°ì´ˆ ì‹¤ìŠµ
1. **Langfuse ì„¤ì • ë° ì—°ê²°**
   - Langfuse ê³„ì • ìƒì„± í›„ API í‚¤ ì„¤ì •
   - ê°„ë‹¨í•œ CallbackHandlerë¥¼ ì‚¬ìš©í•œ LLM í˜¸ì¶œ ì¶”ì 
   - ëŒ€ì‹œë³´ë“œì—ì„œ í˜¸ì¶œ ë¡œê·¸ í™•ì¸

2. **ê¸°ë³¸ ë°ì´í„°ì…‹ ìƒì„±**
   - 5ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒìœ¼ë¡œ ì†Œê·œëª¨ ë°ì´í„°ì…‹ ìƒì„±
   - RAG ì²´ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„± í›„ Langfuseì— ê¸°ë¡

### ì‘ìš© ì‹¤ìŠµ
3. **ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­ êµ¬í˜„**
   - í•œêµ­ì–´ íŠ¹í™” í‰ê°€ ê¸°ì¤€ ê°œë°œ
   - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì (ì˜ˆ: ê¸°ìˆ  ì •í™•ì„±) êµ¬í˜„
   - ê¸°ì¡´ ë©”íŠ¸ë¦­ê³¼ ì„±ëŠ¥ ë¹„êµ

4. **A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ**
   - ì„œë¡œ ë‹¤ë¥¸ RAG ì„¤ì •ìœ¼ë¡œ ë‘ ì‹œìŠ¤í…œ êµ¬ì„±
   - ë™ì¼ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ ë¹„êµ í‰ê°€
   - Langfuseì—ì„œ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”

### ì‹¬í™” ì‹¤ìŠµ
5. **í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ**
   - ì‹¤ì‹œê°„ ì„±ëŠ¥ ì„ê³„ê°’ ëª¨ë‹ˆí„°ë§
   - ì„±ëŠ¥ ì €í•˜ ì‹œ ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ
   - ì§€ì†ì  ê°œì„ ì„ ìœ„í•œ í”¼ë“œë°± ë£¨í”„ êµ¬í˜„

## âœ… ì†”ë£¨ì…˜ ì˜ˆì‹œ

### ì‹¤ìŠµ 1: ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
```python
class RealTimeMonitor:
    def __init__(self, langfuse_client, threshold_scores: Dict[str, float]):
        self.client = langfuse_client
        self.thresholds = threshold_scores
        self.alerts = []

    def monitor_performance(self, run_name: str,
                          check_interval: int = 100) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""

        # ìµœê·¼ ì‹¤í–‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œ êµ¬í˜„ ì‹œ Langfuse API í™œìš©)
        recent_traces = self._get_recent_traces(run_name, check_interval)

        if not recent_traces:
            return {"status": "no_data", "message": "ëª¨ë‹ˆí„°ë§í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        performance_metrics = self._calculate_performance_metrics(recent_traces)

        # ì„ê³„ê°’ ì²´í¬
        alerts = self._check_thresholds(performance_metrics)

        # ì•Œë¦¼ ìƒì„±
        if alerts:
            self._generate_alerts(alerts, performance_metrics)

        return {
            "status": "monitored",
            "metrics": performance_metrics,
            "alerts": alerts,
            "trace_count": len(recent_traces)
        }

    def _get_recent_traces(self, run_name: str, limit: int) -> List[Dict]:
        """ìµœê·¼ íŠ¸ë ˆì´ìŠ¤ ë°ì´í„° ì¡°íšŒ (ëª¨ì˜ êµ¬í˜„)"""
        # ì‹¤ì œë¡œëŠ” Langfuse APIë¥¼ í†µí•´ ë°ì´í„° ì¡°íšŒ
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ ë°ì´í„° ë°˜í™˜
        return [
            {
                "id": f"trace_{i}",
                "scores": {
                    "overall": np.random.uniform(0.6, 0.9),
                    "relevance": np.random.uniform(0.7, 0.95),
                    "correctness": np.random.uniform(0.65, 0.85)
                },
                "execution_time": np.random.uniform(1.0, 3.0),
                "timestamp": "2024-01-01T00:00:00Z"
            }
            for i in range(limit)
        ]

    def _calculate_performance_metrics(self, traces: List[Dict]) -> Dict[str, float]:
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        if not traces:
            return {}

        metrics = {}

        # ì ìˆ˜ë³„ í‰ê·  ê³„ì‚°
        score_names = ["overall", "relevance", "correctness"]
        for score_name in score_names:
            scores = [
                trace["scores"].get(score_name, 0)
                for trace in traces
                if "scores" in trace
            ]
            if scores:
                metrics[f"avg_{score_name}"] = np.mean(scores)
                metrics[f"min_{score_name}"] = np.min(scores)

        # ì‹¤í–‰ ì‹œê°„ í†µê³„
        execution_times = [trace.get("execution_time", 0) for trace in traces]
        if execution_times:
            metrics["avg_execution_time"] = np.mean(execution_times)
            metrics["max_execution_time"] = np.max(execution_times)

        return metrics

    def _check_thresholds(self, metrics: Dict[str, float]) -> List[Dict]:
        """ì„ê³„ê°’ í™•ì¸"""
        alerts = []

        for metric_name, value in metrics.items():
            if metric_name in self.thresholds:
                threshold = self.thresholds[metric_name]

                # ì ìˆ˜ëŠ” ì„ê³„ê°’ë³´ë‹¤ ë†’ì•„ì•¼ í•¨
                if "score" in metric_name or "avg_" in metric_name:
                    if value < threshold:
                        alerts.append({
                            "type": "low_performance",
                            "metric": metric_name,
                            "value": value,
                            "threshold": threshold,
                            "severity": "high" if value < threshold * 0.8 else "medium"
                        })

                # ì‹¤í–‰ ì‹œê°„ì€ ì„ê³„ê°’ë³´ë‹¤ ë‚®ì•„ì•¼ í•¨
                elif "time" in metric_name:
                    if value > threshold:
                        alerts.append({
                            "type": "slow_performance",
                            "metric": metric_name,
                            "value": value,
                            "threshold": threshold,
                            "severity": "high" if value > threshold * 1.5 else "medium"
                        })

        return alerts

    def _generate_alerts(self, alerts: List[Dict], metrics: Dict[str, float]):
        """ì•Œë¦¼ ìƒì„± ë° ì €ì¥"""
        for alert in alerts:
            alert_message = self._format_alert_message(alert)

            self.alerts.append({
                "timestamp": pd.Timestamp.now(),
                "message": alert_message,
                "alert_data": alert,
                "metrics_snapshot": metrics.copy()
            })

            print(f"ğŸš¨ ì•Œë¦¼: {alert_message}")

    def _format_alert_message(self, alert: Dict) -> str:
        """ì•Œë¦¼ ë©”ì‹œì§€ í¬ë§·íŒ…"""
        metric = alert["metric"]
        value = alert["value"]
        threshold = alert["threshold"]
        severity = alert["severity"]

        severity_emoji = "ğŸ”´" if severity == "high" else "ğŸŸ¡"

        if alert["type"] == "low_performance":
            return (f"{severity_emoji} {metric} ì„±ëŠ¥ ì €í•˜ ê°ì§€: "
                   f"{value:.3f} < {threshold:.3f} (ì„ê³„ê°’)")
        else:
            return (f"{severity_emoji} {metric} ì‘ë‹µ ì§€ì—° ê°ì§€: "
                   f"{value:.2f}ì´ˆ > {threshold:.2f}ì´ˆ (ì„ê³„ê°’)")

    def get_alert_summary(self, hours: int = 24) -> Dict[str, Any]:
        """ì•Œë¦¼ ìš”ì•½ ì¡°íšŒ"""
        cutoff_time = pd.Timestamp.now() - pd.Timedelta(hours=hours)
        recent_alerts = [
            alert for alert in self.alerts
            if alert["timestamp"] > cutoff_time
        ]

        if not recent_alerts:
            return {"period": f"ìµœê·¼ {hours}ì‹œê°„", "alert_count": 0}

        # ì•Œë¦¼ ìœ í˜•ë³„ ì§‘ê³„
        alert_types = {}
        severity_counts = {"high": 0, "medium": 0}

        for alert in recent_alerts:
            alert_type = alert["alert_data"]["type"]
            severity = alert["alert_data"]["severity"]

            alert_types[alert_type] = alert_types.get(alert_type, 0) + 1
            severity_counts[severity] += 1

        return {
            "period": f"ìµœê·¼ {hours}ì‹œê°„",
            "alert_count": len(recent_alerts),
            "alert_types": alert_types,
            "severity_distribution": severity_counts,
            "latest_alerts": recent_alerts[-5:]  # ìµœê·¼ 5ê°œ
        }

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ
monitor = RealTimeMonitor(
    langfuse_client=langfuse_client,
    threshold_scores={
        "avg_overall": 0.75,      # ì¢…í•© ì ìˆ˜ 75% ì´ìƒ
        "avg_relevance": 0.80,    # ê´€ë ¨ì„± 80% ì´ìƒ
        "avg_correctness": 0.70,  # ì •í™•ì„± 70% ì´ìƒ
        "avg_execution_time": 5.0, # í‰ê·  ì‹¤í–‰ ì‹œê°„ 5ì´ˆ ì´í•˜
        "max_execution_time": 10.0 # ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ 10ì´ˆ ì´í•˜
    }
)

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
monitoring_result = monitor.monitor_performance("comprehensive_evaluation_v1")
print("ëª¨ë‹ˆí„°ë§ ê²°ê³¼:", monitoring_result)

# ì•Œë¦¼ ìš”ì•½ í™•ì¸
alert_summary = monitor.get_alert_summary(hours=24)
print("ì•Œë¦¼ ìš”ì•½:", alert_summary)
```

### ì‹¤ìŠµ 2: ìë™í™”ëœ ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ
```python
class AutomatedComparisonSystem:
    def __init__(self, langfuse_client):
        self.client = langfuse_client
        self.comparison_results = []

    def compare_rag_systems(self, systems_config: Dict[str, Dict],
                           dataset_name: str,
                           comparison_name: str) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ RAG ì‹œìŠ¤í…œ ìë™ ë¹„êµ

        Args:
            systems_config: {"system_name": {"rag_chain": chain, "description": str}, ...}
            dataset_name: ë¹„êµì— ì‚¬ìš©í•  ë°ì´í„°ì…‹
            comparison_name: ë¹„êµ ì‹¤í—˜ ì´ë¦„
        """

        print(f"ğŸ”„ RAG ì‹œìŠ¤í…œ ë¹„êµ ì‹œì‘: {len(systems_config)}ê°œ ì‹œìŠ¤í…œ")

        # ê° ì‹œìŠ¤í…œë³„ í‰ê°€ ì‹¤í–‰
        system_results = {}

        for system_name, config in systems_config.items():
            print(f"\nğŸ“Š {system_name} ì‹œìŠ¤í…œ í‰ê°€ ì¤‘...")

            # í‰ê°€ íŒŒì´í”„ë¼ì¸ ìƒì„±
            evaluator = ComprehensiveEvaluator(
                ChatOpenAI(model="gpt-4.1-mini", temperature=0.1)
            )
            pipeline = LangfuseEvaluationPipeline(
                self.client, config["rag_chain"], evaluator
            )

            # í‰ê°€ ì‹¤í–‰
            run_name = f"{comparison_name}_{system_name}"
            results = pipeline.run_evaluation(
                dataset_name=dataset_name,
                run_name=run_name,
                max_items=20  # ë¹„êµìš©ìœ¼ë¡œ 20ê°œ í•­ëª©
            )

            # ê²°ê³¼ ë¶„ì„
            system_analysis = self._analyze_system_results(results, system_name)
            system_results[system_name] = {
                "config": config,
                "results": results,
                "analysis": system_analysis
            }

        # ì „ì²´ ë¹„êµ ë¶„ì„
        comparison_analysis = self._compare_systems(system_results)

        # ê²°ê³¼ ì €ì¥
        comparison_result = {
            "comparison_name": comparison_name,
            "dataset_name": dataset_name,
            "systems": system_results,
            "comparison_analysis": comparison_analysis,
            "timestamp": pd.Timestamp.now()
        }

        self.comparison_results.append(comparison_result)

        return comparison_result

    def _analyze_system_results(self, results: List[EvaluationResult],
                               system_name: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‹œìŠ¤í…œ ê²°ê³¼ ë¶„ì„"""
        successful_results = [r for r in results if not r.error]

        if not successful_results:
            return {"error": "ì„±ê³µí•œ í‰ê°€ê°€ ì—†ìŠµë‹ˆë‹¤"}

        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        evaluator = ComprehensiveEvaluator(
            ChatOpenAI(model="gpt-4.1-mini", temperature=0.1)
        )

        overall_scores = [
            evaluator.calculate_overall_score(r.scores)
            for r in successful_results
        ]

        # ë©”íŠ¸ë¦­ë³„ ì ìˆ˜
        metric_scores = {}
        metric_names = ["rouge1", "rouge2", "rougeL", "relevance", "helpfulness", "correctness", "conciseness"]

        for metric in metric_names:
            scores = [r.scores.get(metric, 0) for r in successful_results]
            if scores:
                metric_scores[metric] = {
                    "mean": np.mean(scores),
                    "std": np.std(scores),
                    "min": np.min(scores),
                    "max": np.max(scores)
                }

        # ì‹¤í–‰ ì‹œê°„ ë¶„ì„
        execution_times = [r.execution_time for r in successful_results if r.execution_time]
        time_analysis = {}
        if execution_times:
            time_analysis = {
                "mean": np.mean(execution_times),
                "std": np.std(execution_times),
                "min": np.min(execution_times),
                "max": np.max(execution_times)
            }

        return {
            "system_name": system_name,
            "total_evaluations": len(results),
            "successful_evaluations": len(successful_results),
            "success_rate": len(successful_results) / len(results),
            "overall_score": {
                "mean": np.mean(overall_scores),
                "std": np.std(overall_scores),
                "scores": overall_scores
            },
            "metric_scores": metric_scores,
            "execution_time": time_analysis
        }

    def _compare_systems(self, system_results: Dict[str, Dict]) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ê°„ ë¹„êµ ë¶„ì„"""
        comparison = {
            "ranking": [],
            "performance_comparison": {},
            "statistical_significance": {},
            "recommendations": []
        }

        # ì „ì²´ ì ìˆ˜ ê¸°ì¤€ ìˆœìœ„
        system_scores = []
        for system_name, data in system_results.items():
            analysis = data["analysis"]
            if "overall_score" in analysis:
                system_scores.append({
                    "system": system_name,
                    "score": analysis["overall_score"]["mean"],
                    "std": analysis["overall_score"]["std"]
                })

        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        system_scores.sort(key=lambda x: x["score"], reverse=True)
        comparison["ranking"] = system_scores

        # ë©”íŠ¸ë¦­ë³„ ìµœê³  ì„±ëŠ¥ ì‹œìŠ¤í…œ
        metric_names = ["rouge1", "relevance", "helpfulness", "correctness"]
        for metric in metric_names:
            best_system = None
            best_score = -1

            for system_name, data in system_results.items():
                analysis = data["analysis"]
                if metric in analysis.get("metric_scores", {}):
                    score = analysis["metric_scores"][metric]["mean"]
                    if score > best_score:
                        best_score = score
                        best_system = system_name

            if best_system:
                comparison["performance_comparison"][metric] = {
                    "best_system": best_system,
                    "score": best_score
                }

        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        if system_scores:
            best_system = system_scores[0]
            comparison["recommendations"].append({
                "type": "best_overall",
                "recommendation": f"{best_system['system']} ì‹œìŠ¤í…œì´ ì¢…í•© ì„±ëŠ¥ì´ ê°€ì¥ ìš°ìˆ˜í•©ë‹ˆë‹¤ (ì ìˆ˜: {best_system['score']:.3f})"
            })

            # ì‹¤í–‰ ì‹œê°„ ê³ ë ¤
            fastest_system = None
            fastest_time = float('inf')

            for system_name, data in system_results.items():
                analysis = data["analysis"]
                if "execution_time" in analysis and "mean" in analysis["execution_time"]:
                    time = analysis["execution_time"]["mean"]
                    if time < fastest_time:
                        fastest_time = time
                        fastest_system = system_name

            if fastest_system:
                comparison["recommendations"].append({
                    "type": "fastest",
                    "recommendation": f"{fastest_system} ì‹œìŠ¤í…œì´ ê°€ì¥ ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„ì„ ë³´ì…ë‹ˆë‹¤ ({fastest_time:.2f}ì´ˆ)"
                })

        return comparison

    def generate_comparison_report(self, comparison_result: Dict[str, Any]) -> str:
        """ë¹„êµ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append(f"# RAG ì‹œìŠ¤í…œ ë¹„êµ ë¦¬í¬íŠ¸")
        report.append(f"**ì‹¤í—˜ëª…**: {comparison_result['comparison_name']}")
        report.append(f"**ë°ì´í„°ì…‹**: {comparison_result['dataset_name']}")
        report.append(f"**ì‹¤í–‰ ì‹œê°„**: {comparison_result['timestamp']}")
        report.append("")

        # ìˆœìœ„ ì •ë³´
        ranking = comparison_result["comparison_analysis"]["ranking"]
        report.append("## ğŸ“Š ì¢…í•© ì„±ëŠ¥ ìˆœìœ„")
        for i, system in enumerate(ranking, 1):
            report.append(f"{i}. **{system['system']}**: {system['score']:.3f} (Â±{system['std']:.3f})")
        report.append("")

        # ë©”íŠ¸ë¦­ë³„ ìµœê³  ì„±ëŠ¥
        performance = comparison_result["comparison_analysis"]["performance_comparison"]
        if performance:
            report.append("## ğŸ† ë©”íŠ¸ë¦­ë³„ ìµœê³  ì„±ëŠ¥")
            for metric, data in performance.items():
                report.append(f"- **{metric}**: {data['best_system']} ({data['score']:.3f})")
            report.append("")

        # ê¶Œì¥ì‚¬í•­
        recommendations = comparison_result["comparison_analysis"]["recommendations"]
        if recommendations:
            report.append("## ğŸ’¡ ê¶Œì¥ì‚¬í•­")
            for rec in recommendations:
                report.append(f"- {rec['recommendation']}")
            report.append("")

        # ìƒì„¸ ì‹œìŠ¤í…œ ë¶„ì„
        report.append("## ğŸ“ˆ ìƒì„¸ ë¶„ì„")
        for system_name, data in comparison_result["systems"].items():
            analysis = data["analysis"]
            report.append(f"### {system_name}")
            report.append(f"- ì„±ê³µë¥ : {analysis['success_rate']:.1%}")

            if "overall_score" in analysis:
                report.append(f"- í‰ê·  ì ìˆ˜: {analysis['overall_score']['mean']:.3f}")

            if "execution_time" in analysis and "mean" in analysis["execution_time"]:
                report.append(f"- í‰ê·  ì‹¤í–‰ ì‹œê°„: {analysis['execution_time']['mean']:.2f}ì´ˆ")

            report.append("")

        return "\n".join(report)

# ë¹„êµ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ
comparison_system = AutomatedComparisonSystem(langfuse_client)

# ì—¬ëŸ¬ RAG ì‹œìŠ¤í…œ êµ¬ì„±
systems_to_compare = {
    "Standard_RAG": {
        "rag_chain": rag_chain,  # ê¸°ë³¸ RAG ì‹œìŠ¤í…œ
        "description": "ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ RAG"
    },
    "High_Temp_RAG": {
        "rag_chain": rag_builder.create_rag_chain(retriever), # ë‹¤ë¥¸ ì„¤ì •ì˜ RAG
        "description": "ë†’ì€ ì°½ì˜ì„± ì„¤ì • RAG"
    }
}

# ë¹„êµ ì‹¤í–‰
comparison_result = comparison_system.compare_rag_systems(
    systems_config=systems_to_compare,
    dataset_name="RAG_Evaluation_Dataset_Tesla",
    comparison_name="rag_system_comparison_v1"
)

# ë¦¬í¬íŠ¸ ìƒì„±
report = comparison_system.generate_comparison_report(comparison_result)
print(report)

# ë¦¬í¬íŠ¸ íŒŒì¼ë¡œ ì €ì¥
with open("rag_comparison_report.md", "w", encoding="utf-8") as f:
    f.write(report)

print("\nğŸ“„ ë¹„êµ ë¦¬í¬íŠ¸ê°€ 'rag_comparison_report.md'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

## ğŸš€ ì‹¤ë¬´ í™œìš© ì˜ˆì‹œ

### 1. ì§€ì†ì  í†µí•©(CI) í‰ê°€ ì‹œìŠ¤í…œ

```python
class ContinuousEvaluationSystem:
    def __init__(self, langfuse_client, config_path: str = "evaluation_config.json"):
        self.client = langfuse_client
        self.config = self._load_config(config_path)
        self.baseline_scores = {}

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """í‰ê°€ ì„¤ì • ë¡œë“œ"""
        default_config = {
            "evaluation_frequency": "daily",  # daily, weekly, on_commit
            "datasets": ["RAG_Evaluation_Dataset_Tesla"],
            "quality_thresholds": {
                "overall": 0.75,
                "relevance": 0.80,
                "correctness": 0.70
            },
            "regression_threshold": 0.05,  # 5% ì„±ëŠ¥ ì €í•˜ì‹œ ì‹¤íŒ¨
            "notification_channels": ["console", "email"]
        }

        try:
            with open(config_path, 'r') as f:
                custom_config = json.load(f)
            default_config.update(custom_config)
        except FileNotFoundError:
            print(f"ì„¤ì • íŒŒì¼ '{config_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        return default_config

    def run_ci_evaluation(self, rag_chain, commit_id: str = None) -> Dict[str, Any]:
        """CI í™˜ê²½ì—ì„œ í‰ê°€ ì‹¤í–‰"""
        evaluation_id = f"ci_eval_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}"

        if commit_id:
            evaluation_id += f"_{commit_id[:8]}"

        print(f"ğŸ”„ CI í‰ê°€ ì‹œì‘: {evaluation_id}")

        results = {
            "evaluation_id": evaluation_id,
            "commit_id": commit_id,
            "timestamp": pd.Timestamp.now(),
            "dataset_results": {},
            "overall_status": "unknown",
            "recommendations": []
        }

        # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ í‰ê°€ ì‹¤í–‰
        for dataset_name in self.config["datasets"]:
            print(f"\nğŸ“Š ë°ì´í„°ì…‹ '{dataset_name}' í‰ê°€ ì¤‘...")

            try:
                dataset_result = self._evaluate_dataset(
                    rag_chain, dataset_name, evaluation_id
                )
                results["dataset_results"][dataset_name] = dataset_result

            except Exception as e:
                print(f"âŒ ë°ì´í„°ì…‹ '{dataset_name}' í‰ê°€ ì‹¤íŒ¨: {e}")
                results["dataset_results"][dataset_name] = {
                    "status": "failed",
                    "error": str(e)
                }

        # ì „ì²´ ê²°ê³¼ ë¶„ì„
        overall_analysis = self._analyze_ci_results(results)
        results.update(overall_analysis)

        # ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµ
        regression_analysis = self._check_regression(results)
        results["regression_analysis"] = regression_analysis

        # ìµœì¢… ìƒíƒœ ê²°ì •
        results["overall_status"] = self._determine_ci_status(results)

        # ì•Œë¦¼ ë°œì†¡
        if results["overall_status"] == "failed":
            self._send_notifications(results)

        print(f"\nğŸ¯ CI í‰ê°€ ì™„ë£Œ: {results['overall_status']}")
        return results

    def _evaluate_dataset(self, rag_chain, dataset_name: str,
                         evaluation_id: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ë°ì´í„°ì…‹ í‰ê°€"""
        evaluator = ComprehensiveEvaluator(
            ChatOpenAI(model="gpt-4.1-mini", temperature=0.1)
        )
        pipeline = LangfuseEvaluationPipeline(self.client, rag_chain, evaluator)

        # í‰ê°€ ì‹¤í–‰ (CIì—ì„œëŠ” ìƒ˜í”Œë§)
        results = pipeline.run_evaluation(
            dataset_name=dataset_name,
            run_name=f"{evaluation_id}_{dataset_name}",
            max_items=30  # CIì—ì„œëŠ” 30ê°œ ìƒ˜í”Œë§Œ
        )

        # ê²°ê³¼ ë¶„ì„
        successful_results = [r for r in results if not r.error]

        if not successful_results:
            return {"status": "failed", "reason": "ëª¨ë“  í‰ê°€ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"}

        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        overall_scores = [
            evaluator.calculate_overall_score(r.scores)
            for r in successful_results
        ]

        # ë©”íŠ¸ë¦­ë³„ í‰ê· 
        metric_averages = {}
        for metric in ["relevance", "correctness", "helpfulness"]:
            scores = [r.scores.get(metric, 0) for r in successful_results]
            if scores:
                metric_averages[metric] = np.mean(scores)

        # í’ˆì§ˆ ì„ê³„ê°’ ì²´í¬
        quality_checks = {}
        thresholds = self.config["quality_thresholds"]

        overall_avg = np.mean(overall_scores)
        quality_checks["overall"] = {
            "score": overall_avg,
            "threshold": thresholds.get("overall", 0.75),
            "passed": overall_avg >= thresholds.get("overall", 0.75)
        }

        for metric, avg_score in metric_averages.items():
            threshold = thresholds.get(metric, 0.70)
            quality_checks[metric] = {
                "score": avg_score,
                "threshold": threshold,
                "passed": avg_score >= threshold
            }

        # ì „ì²´ í†µê³¼ ì—¬ë¶€
        all_passed = all(check["passed"] for check in quality_checks.values())

        return {
            "status": "passed" if all_passed else "failed",
            "total_evaluations": len(results),
            "successful_evaluations": len(successful_results),
            "success_rate": len(successful_results) / len(results),
            "overall_score": overall_avg,
            "metric_scores": metric_averages,
            "quality_checks": quality_checks,
            "detailed_results": results
        }

    def _analyze_ci_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """CI ê²°ê³¼ ì „ì²´ ë¶„ì„"""
        dataset_results = results["dataset_results"]
        successful_datasets = [
            name for name, result in dataset_results.items()
            if result.get("status") == "passed"
        ]

        failed_datasets = [
            name for name, result in dataset_results.items()
            if result.get("status") == "failed"
        ]

        # ì „ì²´ í‰ê·  ì ìˆ˜ ê³„ì‚°
        overall_scores = []
        for dataset_result in dataset_results.values():
            if "overall_score" in dataset_result:
                overall_scores.append(dataset_result["overall_score"])

        analysis = {
            "total_datasets": len(dataset_results),
            "passed_datasets": len(successful_datasets),
            "failed_datasets": len(failed_datasets),
            "dataset_pass_rate": len(successful_datasets) / len(dataset_results) if dataset_results else 0,
        }

        if overall_scores:
            analysis["aggregate_score"] = np.mean(overall_scores)

        return analysis

    def _check_regression(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ íšŒê·€ ì²´í¬"""
        regression_analysis = {
            "baseline_available": False,
            "regression_detected": False,
            "performance_change": {}
        }

        # ë² ì´ìŠ¤ë¼ì¸ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ë¹„êµ
        if self.baseline_scores and "aggregate_score" in results:
            current_score = results["aggregate_score"]
            baseline_score = self.baseline_scores.get("aggregate_score")

            if baseline_score:
                regression_analysis["baseline_available"] = True
                performance_change = (current_score - baseline_score) / baseline_score
                regression_analysis["performance_change"] = {
                    "absolute": current_score - baseline_score,
                    "relative": performance_change,
                    "current_score": current_score,
                    "baseline_score": baseline_score
                }

                # íšŒê·€ ì„ê³„ê°’ ì²´í¬
                regression_threshold = self.config["regression_threshold"]
                if performance_change < -regression_threshold:  # 5% ì´ìƒ ì„±ëŠ¥ ì €í•˜
                    regression_analysis["regression_detected"] = True

        return regression_analysis

    def _determine_ci_status(self, results: Dict[str, Any]) -> str:
        """ìµœì¢… CI ìƒíƒœ ê²°ì •"""
        # íšŒê·€ ê°ì§€ì‹œ ì‹¤íŒ¨
        if results.get("regression_analysis", {}).get("regression_detected", False):
            return "failed"

        # ë°ì´í„°ì…‹ í†µê³¼ìœ¨ í™•ì¸
        pass_rate = results.get("dataset_pass_rate", 0)
        if pass_rate < 0.8:  # 80% ë¯¸ë§Œ í†µê³¼ì‹œ ì‹¤íŒ¨
            return "failed"

        # ëª¨ë“  ë°ì´í„°ì…‹ì´ ì‹¤íŒ¨í•œ ê²½ìš°
        if results.get("passed_datasets", 0) == 0:
            return "failed"

        return "passed"

    def _send_notifications(self, results: Dict[str, Any]):
        """ì‹¤íŒ¨ ì‹œ ì•Œë¦¼ ë°œì†¡"""
        channels = self.config.get("notification_channels", ["console"])

        failure_message = self._format_failure_message(results)

        for channel in channels:
            if channel == "console":
                print(f"\nğŸš¨ CI í‰ê°€ ì‹¤íŒ¨ ì•Œë¦¼:\n{failure_message}")
            elif channel == "email":
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ë©”ì¼ ë°œì†¡ ë¡œì§ ì¶”ê°€
                print(f"ğŸ“§ ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡ (êµ¬í˜„ í•„ìš”): {failure_message}")

    def _format_failure_message(self, results: Dict[str, Any]) -> str:
        """ì‹¤íŒ¨ ë©”ì‹œì§€ í¬ë§·íŒ…"""
        message = [
            f"CI í‰ê°€ ì‹¤íŒ¨: {results['evaluation_id']}",
            f"ì‹œê°„: {results['timestamp']}",
            f"ìƒíƒœ: {results['overall_status']}"
        ]

        if results.get("commit_id"):
            message.append(f"ì»¤ë°‹: {results['commit_id']}")

        # ì‹¤íŒ¨ ì›ì¸
        if results.get("regression_analysis", {}).get("regression_detected"):
            change = results["regression_analysis"]["performance_change"]
            message.append(f"ì„±ëŠ¥ íšŒê·€ ê°ì§€: {change['relative']:.1%} ê°ì†Œ")

        # ì‹¤íŒ¨í•œ ë°ì´í„°ì…‹
        failed_datasets = [
            name for name, result in results["dataset_results"].items()
            if result.get("status") == "failed"
        ]

        if failed_datasets:
            message.append(f"ì‹¤íŒ¨í•œ ë°ì´í„°ì…‹: {', '.join(failed_datasets)}")

        return "\n".join(message)

    def update_baseline(self, results: Dict[str, Any]):
        """ë² ì´ìŠ¤ë¼ì¸ ì ìˆ˜ ì—…ë°ì´íŠ¸"""
        if results.get("overall_status") == "passed" and "aggregate_score" in results:
            self.baseline_scores["aggregate_score"] = results["aggregate_score"]
            self.baseline_scores["updated_at"] = results["timestamp"]
            print(f"âœ… ë² ì´ìŠ¤ë¼ì¸ ì—…ë°ì´íŠ¸: {results['aggregate_score']:.3f}")

# CI í‰ê°€ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ
ci_system = ContinuousEvaluationSystem(langfuse_client)

# CI í‰ê°€ ì‹¤í–‰ (ì˜ˆ: GitHub Actionsì—ì„œ)
ci_results = ci_system.run_ci_evaluation(
    rag_chain=rag_chain,
    commit_id="abc123def"  # Git ì»¤ë°‹ ID
)

# ì„±ê³µí•œ ê²½ìš° ë² ì´ìŠ¤ë¼ì¸ ì—…ë°ì´íŠ¸
if ci_results["overall_status"] == "passed":
    ci_system.update_baseline(ci_results)

print(f"\nCI í‰ê°€ ê²°ê³¼: {ci_results['overall_status']}")
if ci_results["overall_status"] == "failed":
    print("âŒ ë¹Œë“œ ì‹¤íŒ¨ - ì„±ëŠ¥ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
    exit(1)  # CI íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨
else:
    print("âœ… ë¹Œë“œ ì„±ê³µ - ëª¨ë“  í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í–ˆìŠµë‹ˆë‹¤")
```

## ğŸ“– ì°¸ê³  ìë£Œ

### Langfuse ê´€ë ¨
- [Langfuse ê³µì‹ ë¬¸ì„œ](https://langfuse.com/docs)
- [Langfuse LangChain í†µí•© ê°€ì´ë“œ](https://langfuse.com/docs/integrations/langchain)
- [Langfuse Python SDK](https://langfuse.com/docs/sdk/python)

### í‰ê°€ ë°©ë²•ë¡ 
- [LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í‰ê°€ best practices](https://langfuse.com/docs/evaluation)
- [í”„ë¡œë•ì…˜ LLM ëª¨ë‹ˆí„°ë§ ì „ëµ](https://docs.smith.langchain.com/monitoring)

### ë©”íŠ¸ë¦­ê³¼ í‰ê°€ ë„êµ¬
- [ROUGE ì ìˆ˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/neural-dialogue-metrics/rouge)
- [í•œêµ­ì–´ ROUGE êµ¬í˜„](https://github.com/gucci-j/korouge-score)
- [LangChain í‰ê°€ ë„êµ¬](https://python.langchain.com/docs/guides/evaluation/)

### ì‹¤ë¬´ ëª¨ë‹ˆí„°ë§
- [MLOps for LLM Applications](https://neptune.ai/blog/mlops-for-llm)
- [LLM ì• í”Œë¦¬ì¼€ì´ì…˜ CI/CD](https://docs.smith.langchain.com/evaluation)

ì´ ê°€ì´ë“œë¥¼ í†µí•´ Langfuseë¥¼ í™œìš©í•œ ì²´ê³„ì ì¸ RAG ì‹œìŠ¤í…œ í‰ê°€ì™€ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œëŠ” ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ ê°œì„ ì„ í†µí•´ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í’ˆì§ˆì„ ìœ ì§€í•˜ê³  ë°œì „ì‹œí‚¤ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.