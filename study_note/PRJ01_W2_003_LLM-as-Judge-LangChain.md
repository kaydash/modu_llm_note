# LLM-as-Judge í‰ê°€ ì‹œìŠ¤í…œ - ì–¸ì–´ëª¨ë¸ ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ ê°€ì´ë“œ

## ğŸ“š í•™ìŠµ ëª©í‘œ
- LLM-as-Judgeì˜ ê°œë…ê³¼ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤
- Reference-freeì™€ Reference-based í‰ê°€ ë°©ì‹ì˜ ì°¨ì´ì ì„ íŒŒì•…í•œë‹¤
- LangChainì„ í™œìš©í•œ ì²´ê³„ì ì¸ LLM í‰ê°€ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤
- ë‹¤ì–‘í•œ í‰ê°€ ê¸°ì¤€ê³¼ í”„ë¡¬í”„íŠ¸ ì„¤ê³„ ê¸°ë²•ì„ ìŠµë“í•œë‹¤
- ì‹¤ë¬´ì—ì„œ ìë™í™”ëœ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤

## ğŸ”‘ í•µì‹¬ ê°œë…

### LLM-as-Judgeë€?
- **ì •ì˜**: ëŒ€í˜• ì–¸ì–´ëª¨ë¸ì„ í‰ê°€ì(Judge)ë¡œ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ ìë™ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë°©ë²•ë¡ 
- **ë°°ê²½**: ì „í†µì ì¸ ë©”íŠ¸ë¦­(ROUGE, BLEU)ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , ì¸ê°„ì˜ ì£¼ê´€ì  í‰ê°€ë¥¼ ëª¨ë°©
- **ì¥ì **: ë¬¸ë§¥ ì´í•´ë ¥, ë‹¤ì°¨ì›ì  í‰ê°€, ìœ ì—°í•œ ê¸°ì¤€ ì ìš© ê°€ëŠ¥
- **í™œìš© ë¶„ì•¼**: RAG ì‹œìŠ¤í…œ, ì±—ë´‡, ìš”ì•½, ë²ˆì—­, ì°½ì‘ ë“± ë‹¤ì–‘í•œ ìì—°ì–´ ìƒì„± ì‘ì—…

### í‰ê°€ ë°©ì‹ ë¶„ë¥˜
1. **Reference-free í‰ê°€**: ì°¸ì¡° ë‹µì•ˆ ì—†ì´ ë…ë¦½ì  í’ˆì§ˆ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€
2. **Reference-based í‰ê°€**: ì°¸ì¡° ë‹µì•ˆê³¼ ë¹„êµí•˜ì—¬ ìƒëŒ€ì  í’ˆì§ˆ í‰ê°€
3. **Pairwise í‰ê°€**: ë‘ ë‹µë³€ì„ ì§ì ‘ ë¹„êµí•˜ì—¬ ìš°ì—´ íŒë‹¨
4. **Scoring í‰ê°€**: ì ˆëŒ€ì  ì ìˆ˜ë¡œ í’ˆì§ˆì„ ìˆ˜ì¹˜í™”

### í‰ê°€ ê¸°ì¤€ ìš”ì†Œ
- **ì •í™•ì„±(Accuracy)**: ì‚¬ì‹¤ì  ì •ë³´ì˜ ì •í™•ë„
- **ê´€ë ¨ì„±(Relevance)**: ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„±
- **ì™„ì „ì„±(Completeness)**: ë‹µë³€ì˜ ì¶©ë¶„ì„±
- **ëª…í™•ì„±(Clarity)**: ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„
- **ì¼ê´€ì„±(Consistency)**: ë…¼ë¦¬ì  ëª¨ìˆœ ì—†ìŒ
- **ìœ ìš©ì„±(Helpfulness)**: ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ì •ë„

## ğŸ›  í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# ê¸°ë³¸ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install langchain langchain-openai langchain-chroma
pip install langsmith  # í‰ê°€ ì¶”ì  ë° ê´€ë¦¬

# ì¶”ê°€ í‰ê°€ ë„êµ¬
pip install pandas numpy matplotlib
pip install scikit-learn  # ë©”íŠ¸ë¦­ ê³„ì‚°ìš©
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
# .env íŒŒì¼
OPENAI_API_KEY=your_openai_api_key
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key
LANGCHAIN_PROJECT=LLM_Judge_Evaluation
```

### ê¸°ë³¸ ì„¤ì •
```python
import os
from dotenv import load_dotenv
import warnings
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
warnings.filterwarnings("ignore")

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.evaluation import load_evaluator

# ê¸°ë³¸ LLM ì„¤ì • (í‰ê°€ìš©)
judge_llm = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=0.1,  # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ ë‚®ì€ temperature
    max_tokens=1000
)
```

## ğŸ’» ë‹¨ê³„ë³„ êµ¬í˜„

### 1ë‹¨ê³„: Reference-free í‰ê°€ ì‹œìŠ¤í…œ

```python
class ReferenceFreeJudge:
    def __init__(self, llm, evaluation_criteria: List[str] = None):
        self.llm = llm
        self.criteria = evaluation_criteria or [
            "ì •í™•ì„±", "ëª…í™•ì„±", "ì™„ì „ì„±", "ìœ ìš©ì„±"
        ]

    def create_evaluation_prompt(self) -> ChatPromptTemplate:
        """í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        criteria_text = "\n".join([f"- {criterion}" for criterion in self.criteria])

        template = """ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:

{criteria}

ê° ê¸°ì¤€ì— ëŒ€í•´ 1-5ì ìœ¼ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ê°„ë‹¨í•œ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”.

[ì§ˆë¬¸]
{question}

[ë‹µë³€]
{answer}

[í‰ê°€ ê²°ê³¼]
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ì •í™•ì„±: X/5 - ì„¤ëª…
ëª…í™•ì„±: X/5 - ì„¤ëª…
ì™„ì „ì„±: X/5 - ì„¤ëª…
ìœ ìš©ì„±: X/5 - ì„¤ëª…

ì „ì²´ ì ìˆ˜: X/20
ì¢…í•© ì˜ê²¬: (ê°„ë‹¨í•œ í‰ê°€ ì˜ê²¬)
"""

        return ChatPromptTemplate.from_template(template)

    def evaluate(self, question: str, answer: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ë‹µë³€ í‰ê°€"""
        prompt = self.create_evaluation_prompt()

        criteria_text = "\n".join([f"- {criterion}" for criterion in self.criteria])

        chain = prompt | self.llm | StrOutputParser()

        try:
            evaluation_result = chain.invoke({
                "criteria": criteria_text,
                "question": question,
                "answer": answer
            })

            # ê²°ê³¼ íŒŒì‹±
            parsed_result = self._parse_evaluation(evaluation_result)
            parsed_result.update({
                "question": question,
                "answer": answer,
                "raw_evaluation": evaluation_result,
                "success": True
            })

            return parsed_result

        except Exception as e:
            return {
                "question": question,
                "answer": answer,
                "error": str(e),
                "success": False,
                "total_score": 0.0
            }

    def _parse_evaluation(self, evaluation_text: str) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ íŒŒì‹±"""
        lines = evaluation_text.split('\n')
        scores = {}
        total_score = 0.0
        overall_comment = ""

        for line in lines:
            line = line.strip()
            if ':' in line:
                if '/' in line and any(criterion in line for criterion in self.criteria):
                    # ì ìˆ˜ ë¼ì¸ íŒŒì‹±
                    parts = line.split(':')
                    if len(parts) >= 2:
                        criterion = parts[0].strip()
                        score_part = parts[1].strip()
                        try:
                            if '/' in score_part:
                                score = float(score_part.split('/')[0])
                                scores[criterion] = score
                        except ValueError:
                            continue
                elif "ì „ì²´ ì ìˆ˜" in line or "ì´ì " in line:
                    try:
                        score_part = line.split(':')[1].strip()
                        if '/' in score_part:
                            total_score = float(score_part.split('/')[0])
                    except (ValueError, IndexError):
                        continue
                elif "ì¢…í•© ì˜ê²¬" in line:
                    try:
                        overall_comment = line.split(':')[1].strip()
                    except IndexError:
                        continue

        return {
            "individual_scores": scores,
            "total_score": total_score,
            "max_score": len(self.criteria) * 5,
            "overall_comment": overall_comment
        }

# ì‚¬ìš© ì˜ˆì‹œ
judge = ReferenceFreeJudge(judge_llm)

question = "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"
answer = "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤. ê·¸ëŠ” 2008ë…„ë¶€í„° í…ŒìŠ¬ë¼ë¥¼ ì´ëŒê³  ìˆìœ¼ë©°, ì „ê¸°ì°¨ ì‚°ì—…ì˜ í˜ì‹ ì„ ì£¼ë„í•˜ê³  ìˆìŠµë‹ˆë‹¤."

evaluation_result = judge.evaluate(question, answer)
print("Reference-free í‰ê°€ ê²°ê³¼:")
print(f"ì´ì : {evaluation_result['total_score']}/{evaluation_result['max_score']}")
print(f"ê°œë³„ ì ìˆ˜: {evaluation_result['individual_scores']}")
print(f"ì¢…í•© ì˜ê²¬: {evaluation_result['overall_comment']}")
```

### 2ë‹¨ê³„: Reference-based í‰ê°€ ì‹œìŠ¤í…œ

```python
class ReferenceBasedJudge:
    def __init__(self, llm):
        self.llm = llm

    def create_comparison_prompt(self) -> ChatPromptTemplate:
        """ì°¸ì¡° ë‹µì•ˆ ë¹„êµ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = """ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ì°¸ì¡° ë‹µì•ˆê³¼ ìƒì„±ëœ ë‹µë³€ì„ ë¹„êµí•˜ì—¬ í‰ê°€í•´ì£¼ì„¸ìš”.

[ì§ˆë¬¸]
{question}

[ì°¸ì¡° ë‹µì•ˆ]
{reference_answer}

[ìƒì„±ëœ ë‹µë³€]
{generated_answer}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”:

1. ì •í™•ì„±: ì°¸ì¡° ë‹µì•ˆê³¼ ë¹„êµí•œ ì‚¬ì‹¤ì  ì •í™•ë„ (1-5ì )
2. ì™„ì „ì„±: ì°¸ì¡° ë‹µì•ˆì˜ í•µì‹¬ ë‚´ìš©ì„ ì–¼ë§ˆë‚˜ í¬í•¨í•˜ëŠ”ê°€ (1-5ì )
3. ìœ ì‚¬ì„±: ì°¸ì¡° ë‹µì•ˆê³¼ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„± (1-5ì )
4. ë¶€ê°€ê°€ì¹˜: ì°¸ì¡° ë‹µì•ˆì— ì—†ëŠ” ìœ ìš©í•œ ì •ë³´ ì œê³µ ì—¬ë¶€ (1-5ì )

[í‰ê°€ ê²°ê³¼]
ì •í™•ì„±: X/5 - ì„¤ëª…
ì™„ì „ì„±: X/5 - ì„¤ëª…
ìœ ì‚¬ì„±: X/5 - ì„¤ëª…
ë¶€ê°€ê°€ì¹˜: X/5 - ì„¤ëª…

ì „ì²´ ì ìˆ˜: X/20
ë¹„êµ ì˜ê²¬: (ì°¸ì¡° ë‹µì•ˆ ëŒ€ë¹„ ìƒì„± ë‹µë³€ì˜ ì¥ë‹¨ì )
"""

        return ChatPromptTemplate.from_template(template)

    def evaluate_with_reference(self, question: str, reference_answer: str,
                               generated_answer: str) -> Dict[str, Any]:
        """ì°¸ì¡° ë‹µì•ˆê³¼ ë¹„êµ í‰ê°€"""
        prompt = self.create_comparison_prompt()
        chain = prompt | self.llm | StrOutputParser()

        try:
            evaluation_result = chain.invoke({
                "question": question,
                "reference_answer": reference_answer,
                "generated_answer": generated_answer
            })

            # ê²°ê³¼ íŒŒì‹± (Reference-freeì™€ ìœ ì‚¬í•œ ë°©ì‹)
            parsed_result = self._parse_comparison_evaluation(evaluation_result)
            parsed_result.update({
                "question": question,
                "reference_answer": reference_answer,
                "generated_answer": generated_answer,
                "raw_evaluation": evaluation_result,
                "success": True
            })

            return parsed_result

        except Exception as e:
            return {
                "question": question,
                "reference_answer": reference_answer,
                "generated_answer": generated_answer,
                "error": str(e),
                "success": False,
                "total_score": 0.0
            }

    def _parse_comparison_evaluation(self, evaluation_text: str) -> Dict[str, Any]:
        """ë¹„êµ í‰ê°€ ê²°ê³¼ íŒŒì‹±"""
        lines = evaluation_text.split('\n')
        scores = {}
        total_score = 0.0
        comparison_comment = ""

        criteria = ["ì •í™•ì„±", "ì™„ì „ì„±", "ìœ ì‚¬ì„±", "ë¶€ê°€ê°€ì¹˜"]

        for line in lines:
            line = line.strip()
            if ':' in line:
                if '/' in line and any(criterion in line for criterion in criteria):
                    parts = line.split(':')
                    if len(parts) >= 2:
                        criterion = parts[0].strip()
                        score_part = parts[1].strip()
                        try:
                            if '/' in score_part:
                                score = float(score_part.split('/')[0])
                                scores[criterion] = score
                        except ValueError:
                            continue
                elif "ì „ì²´ ì ìˆ˜" in line or "ì´ì " in line:
                    try:
                        score_part = line.split(':')[1].strip()
                        if '/' in score_part:
                            total_score = float(score_part.split('/')[0])
                    except (ValueError, IndexError):
                        continue
                elif "ë¹„êµ ì˜ê²¬" in line:
                    try:
                        comparison_comment = line.split(':')[1].strip()
                    except IndexError:
                        continue

        return {
            "individual_scores": scores,
            "total_score": total_score,
            "max_score": 20,
            "comparison_comment": comparison_comment
        }

# ì‚¬ìš© ì˜ˆì‹œ
ref_judge = ReferenceBasedJudge(judge_llm)

reference = "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤."
generated = "ì¼ë¡  ë¨¸ìŠ¤í¬ê°€ í…ŒìŠ¬ë¼ì˜ ìµœê³ ê²½ì˜ìë¡œ í™œë™í•˜ê³  ìˆìŠµë‹ˆë‹¤."

comparison_result = ref_judge.evaluate_with_reference(question, reference, generated)
print("\nReference-based í‰ê°€ ê²°ê³¼:")
print(f"ì´ì : {comparison_result['total_score']}/{comparison_result['max_score']}")
print(f"ê°œë³„ ì ìˆ˜: {comparison_result['individual_scores']}")
print(f"ë¹„êµ ì˜ê²¬: {comparison_result['comparison_comment']}")
```

### 3ë‹¨ê³„: Pairwise ë¹„êµ í‰ê°€ ì‹œìŠ¤í…œ

```python
class PairwiseJudge:
    def __init__(self, llm):
        self.llm = llm

    def create_pairwise_prompt(self) -> ChatPromptTemplate:
        """ìŒë³„ ë¹„êµ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = """ë‹¹ì‹ ì€ ë‘ ë‹µë³€ì˜ í’ˆì§ˆì„ ë¹„êµí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ë‘ ë‹µë³€ ì¤‘ ì–´ëŠ ê²ƒì´ ë” ë‚˜ì€ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

[ì§ˆë¬¸]
{question}

[ë‹µë³€ A]
{answer_a}

[ë‹µë³€ B]
{answer_b}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¹„êµí•´ì£¼ì„¸ìš”:
1. ì •í™•ì„±: ì–´ëŠ ë‹µë³€ì´ ë” ì •í™•í•œê°€?
2. ì™„ì „ì„±: ì–´ëŠ ë‹µë³€ì´ ë” ì™„ì „í•œê°€?
3. ëª…í™•ì„±: ì–´ëŠ ë‹µë³€ì´ ë” ëª…í™•í•œê°€?
4. ìœ ìš©ì„±: ì–´ëŠ ë‹µë³€ì´ ë” ìœ ìš©í•œê°€?

[ë¹„êµ ê²°ê³¼]
ìš°ìˆ˜í•œ ë‹µë³€: A ë˜ëŠ” B ë˜ëŠ” ë™ë“±í•¨
ì´ìœ : (êµ¬ì²´ì ì¸ ê·¼ê±° ì œì‹œ)

ì ìˆ˜ ì°¨ì´: Xì  (1-5ì  ì²™ë„ì—ì„œ ì°¨ì´)
ê° ê¸°ì¤€ë³„ ë¶„ì„:
- ì •í™•ì„±: Aê°€ ìš°ìˆ˜/Bê°€ ìš°ìˆ˜/ë™ë“±í•¨ - ì´ìœ 
- ì™„ì „ì„±: Aê°€ ìš°ìˆ˜/Bê°€ ìš°ìˆ˜/ë™ë“±í•¨ - ì´ìœ 
- ëª…í™•ì„±: Aê°€ ìš°ìˆ˜/Bê°€ ìš°ìˆ˜/ë™ë“±í•¨ - ì´ìœ 
- ìœ ìš©ì„±: Aê°€ ìš°ìˆ˜/Bê°€ ìš°ìˆ˜/ë™ë“±í•¨ - ì´ìœ 
"""

        return ChatPromptTemplate.from_template(template)

    def compare_answers(self, question: str, answer_a: str,
                       answer_b: str) -> Dict[str, Any]:
        """ë‘ ë‹µë³€ ë¹„êµ í‰ê°€"""
        prompt = self.create_pairwise_prompt()
        chain = prompt | self.llm | StrOutputParser()

        try:
            comparison_result = chain.invoke({
                "question": question,
                "answer_a": answer_a,
                "answer_b": answer_b
            })

            parsed_result = self._parse_pairwise_evaluation(comparison_result)
            parsed_result.update({
                "question": question,
                "answer_a": answer_a,
                "answer_b": answer_b,
                "raw_comparison": comparison_result,
                "success": True
            })

            return parsed_result

        except Exception as e:
            return {
                "question": question,
                "answer_a": answer_a,
                "answer_b": answer_b,
                "error": str(e),
                "success": False,
                "winner": "error"
            }

    def _parse_pairwise_evaluation(self, evaluation_text: str) -> Dict[str, Any]:
        """ìŒë³„ ë¹„êµ ê²°ê³¼ íŒŒì‹±"""
        lines = evaluation_text.split('\n')
        winner = "ë™ë“±í•¨"
        reasoning = ""
        score_difference = 0
        criteria_analysis = {}

        for line in lines:
            line = line.strip()
            if "ìš°ìˆ˜í•œ ë‹µë³€:" in line:
                try:
                    winner = line.split(':')[1].strip()
                except IndexError:
                    continue
            elif "ì´ìœ :" in line:
                try:
                    reasoning = line.split(':')[1].strip()
                except IndexError:
                    continue
            elif "ì ìˆ˜ ì°¨ì´:" in line:
                try:
                    score_part = line.split(':')[1].strip()
                    score_difference = float(score_part.split('ì ')[0])
                except (ValueError, IndexError):
                    continue
            elif any(criterion in line for criterion in ["ì •í™•ì„±", "ì™„ì „ì„±", "ëª…í™•ì„±", "ìœ ìš©ì„±"]):
                if ':' in line:
                    try:
                        criterion = line.split(':')[0].strip()
                        analysis = line.split(':')[1].strip()
                        criteria_analysis[criterion] = analysis
                    except IndexError:
                        continue

        return {
            "winner": winner,
            "reasoning": reasoning,
            "score_difference": score_difference,
            "criteria_analysis": criteria_analysis
        }

# ì‚¬ìš© ì˜ˆì‹œ
pairwise_judge = PairwiseJudge(judge_llm)

answer_a = "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤."
answer_b = "ì¼ë¡  ë¨¸ìŠ¤í¬ê°€ í…ŒìŠ¬ë¼ì˜ ìµœê³ ê²½ì˜ìì…ë‹ˆë‹¤. ê·¸ëŠ” 2008ë…„ë¶€í„° í…ŒìŠ¬ë¼ë¥¼ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤."

pairwise_result = pairwise_judge.compare_answers(question, answer_a, answer_b)
print("\nPairwise ë¹„êµ ê²°ê³¼:")
print(f"ìš°ìŠ¹ì: {pairwise_result['winner']}")
print(f"ì´ìœ : {pairwise_result['reasoning']}")
print(f"ì ìˆ˜ ì°¨ì´: {pairwise_result['score_difference']}")
```

### 4ë‹¨ê³„: ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ

```python
class ComprehensiveJudge:
    def __init__(self, llm):
        self.llm = llm
        self.reference_free_judge = ReferenceFreeJudge(llm)
        self.reference_based_judge = ReferenceBasedJudge(llm)
        self.pairwise_judge = PairwiseJudge(llm)

    def comprehensive_evaluation(self, question: str, generated_answer: str,
                               reference_answer: str = None,
                               comparison_answer: str = None) -> Dict[str, Any]:
        """ì¢…í•©ì ì¸ í‰ê°€ ìˆ˜í–‰"""
        results = {
            "question": question,
            "generated_answer": generated_answer,
            "timestamp": pd.Timestamp.now()
        }

        # 1. Reference-free í‰ê°€
        ref_free_result = self.reference_free_judge.evaluate(question, generated_answer)
        results["reference_free"] = ref_free_result

        # 2. Reference-based í‰ê°€ (ì°¸ì¡° ë‹µì•ˆì´ ìˆëŠ” ê²½ìš°)
        if reference_answer:
            ref_based_result = self.reference_based_judge.evaluate_with_reference(
                question, reference_answer, generated_answer
            )
            results["reference_based"] = ref_based_result

        # 3. Pairwise ë¹„êµ (ë¹„êµ ë‹µë³€ì´ ìˆëŠ” ê²½ìš°)
        if comparison_answer:
            pairwise_result = self.pairwise_judge.compare_answers(
                question, generated_answer, comparison_answer
            )
            results["pairwise"] = pairwise_result

        # 4. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_score(results)
        results["overall_assessment"] = overall_score

        return results

    def _calculate_overall_score(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        scores = []
        weights = []

        # Reference-free ì ìˆ˜
        if "reference_free" in results and results["reference_free"]["success"]:
            ref_free_score = results["reference_free"]["total_score"] / results["reference_free"]["max_score"]
            scores.append(ref_free_score)
            weights.append(0.4)  # 40% ê°€ì¤‘ì¹˜

        # Reference-based ì ìˆ˜
        if "reference_based" in results and results["reference_based"]["success"]:
            ref_based_score = results["reference_based"]["total_score"] / results["reference_based"]["max_score"]
            scores.append(ref_based_score)
            weights.append(0.6)  # 60% ê°€ì¤‘ì¹˜ (ì°¸ì¡° ë‹µì•ˆì´ ìˆì„ ë•Œ ë” ì‹ ë¢°)

        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        if scores:
            overall_score = sum(score * weight for score, weight in zip(scores, weights)) / sum(weights)
        else:
            overall_score = 0.0

        # ë“±ê¸‰ ë¶€ì—¬
        if overall_score >= 0.9:
            grade = "A+"
        elif overall_score >= 0.8:
            grade = "A"
        elif overall_score >= 0.7:
            grade = "B"
        elif overall_score >= 0.6:
            grade = "C"
        else:
            grade = "D"

        return {
            "overall_score": overall_score,
            "grade": grade,
            "individual_scores": scores,
            "weights_used": weights
        }

# ì‚¬ìš© ì˜ˆì‹œ
comprehensive_judge = ComprehensiveJudge(judge_llm)

# ì¢…í•© í‰ê°€ ì‹¤í–‰
comprehensive_result = comprehensive_judge.comprehensive_evaluation(
    question=question,
    generated_answer=answer,
    reference_answer=reference,
    comparison_answer="í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤."
)

print("\n=== ì¢…í•© í‰ê°€ ê²°ê³¼ ===")
print(f"ì „ì²´ ì ìˆ˜: {comprehensive_result['overall_assessment']['overall_score']:.3f}")
print(f"ë“±ê¸‰: {comprehensive_result['overall_assessment']['grade']}")

if "reference_free" in comprehensive_result:
    print(f"Reference-free ì ìˆ˜: {comprehensive_result['reference_free']['total_score']}/20")

if "reference_based" in comprehensive_result:
    print(f"Reference-based ì ìˆ˜: {comprehensive_result['reference_based']['total_score']}/20")
```

## ğŸ¯ ì‹¤ìŠµ ë¬¸ì œ

### ê¸°ì´ˆ ì‹¤ìŠµ
1. **ë‹¨ìˆœ í‰ê°€ê¸° êµ¬í˜„**
   - í•˜ë‚˜ì˜ í‰ê°€ ê¸°ì¤€(ì˜ˆ: ì •í™•ì„±)ìœ¼ë¡œ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ê°„ë‹¨í•œ ì‹œìŠ¤í…œ êµ¬í˜„
   - 5ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒìœ¼ë¡œ í…ŒìŠ¤íŠ¸

2. **í‰ê°€ ê¸°ì¤€ ì»¤ìŠ¤í„°ë§ˆì´ì§•**
   - ë³¸ì¸ë§Œì˜ í‰ê°€ ê¸°ì¤€ì„ 3ê°œ ì •ì˜í•˜ê³  í‰ê°€ ì‹œìŠ¤í…œì— ì ìš©
   - ê° ê¸°ì¤€ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ë¥´ê²Œ ì„¤ì •

### ì‘ìš© ì‹¤ìŠµ
3. **ë‹¤ì¤‘ ëª¨ë¸ í’ˆì§ˆ ë¹„êµ**
   - OpenAI, Gemini, Ollama ëª¨ë¸ì˜ ë‹µë³€ì„ LLM-as-Judgeë¡œ í‰ê°€
   - ì–´ë–¤ ëª¨ë¸ì´ ì–´ë–¤ ê¸°ì¤€ì—ì„œ ìš°ìˆ˜í•œì§€ ë¶„ì„

4. **í‰ê°€ì ê°„ ì¼ì¹˜ë„ ë¶„ì„**
   - ê°™ì€ ë‹µë³€ì„ ì„œë¡œ ë‹¤ë¥¸ í‰ê°€ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€
   - í‰ê°€ ê²°ê³¼ì˜ ì¼ê´€ì„±ê³¼ ì‹ ë¢°ì„± ë¶„ì„

### ì‹¬í™” ì‹¤ìŠµ
5. **ìë™ í‰ê°€ íŒŒì´í”„ë¼ì¸**
   - ë°°ì¹˜ë¡œ ì—¬ëŸ¬ ë‹µë³€ì„ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
   - ì‹œê°„ë³„/ëª¨ë¸ë³„ ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ êµ¬í˜„

## âœ… ì†”ë£¨ì…˜ ì˜ˆì‹œ

### ì‹¤ìŠµ 1: ë°°ì¹˜ í‰ê°€ ì‹œìŠ¤í…œ
```python
def batch_llm_evaluation(questions: List[str], answers: List[str],
                        references: List[str] = None) -> pd.DataFrame:
    """ë°°ì¹˜ë¡œ LLM í‰ê°€ ìˆ˜í–‰"""
    judge = ComprehensiveJudge(judge_llm)
    results = []

    for i, (question, answer) in enumerate(zip(questions, answers)):
        print(f"í‰ê°€ ì¤‘... {i+1}/{len(questions)}")

        reference = references[i] if references else None

        evaluation = judge.comprehensive_evaluation(
            question=question,
            generated_answer=answer,
            reference_answer=reference
        )

        # ê²°ê³¼ë¥¼ í”Œë«í•œ êµ¬ì¡°ë¡œ ë³€í™˜
        flat_result = {
            'question_id': i,
            'question': question,
            'answer': answer,
            'reference': reference,
            'overall_score': evaluation['overall_assessment']['overall_score'],
            'grade': evaluation['overall_assessment']['grade'],
        }

        # Reference-free ê²°ê³¼ ì¶”ê°€
        if 'reference_free' in evaluation and evaluation['reference_free']['success']:
            ref_free = evaluation['reference_free']
            flat_result.update({
                'ref_free_total': ref_free['total_score'],
                'ref_free_max': ref_free['max_score'],
                'ref_free_comment': ref_free['overall_comment']
            })

        # Reference-based ê²°ê³¼ ì¶”ê°€
        if 'reference_based' in evaluation and evaluation['reference_based']['success']:
            ref_based = evaluation['reference_based']
            flat_result.update({
                'ref_based_total': ref_based['total_score'],
                'ref_based_max': ref_based['max_score'],
                'ref_based_comment': ref_based['comparison_comment']
            })

        results.append(flat_result)

    return pd.DataFrame(results)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
test_questions = [
    "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
    "ì „ê¸°ì°¨ì˜ ì£¼ìš” ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ììœ¨ì£¼í–‰ ê¸°ìˆ ì˜ í˜„ì¬ ìˆ˜ì¤€ì€?"
]

test_answers = [
    "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤. ê·¸ëŠ” 2008ë…„ë¶€í„° í…ŒìŠ¬ë¼ë¥¼ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.",
    "ì „ê¸°ì°¨ëŠ” í™˜ê²½ì¹œí™”ì ì´ê³  ì—°ë£Œë¹„ê°€ ì ˆì•½ë˜ë©° ì¡°ìš©í•©ë‹ˆë‹¤.",
    "ììœ¨ì£¼í–‰ ê¸°ìˆ ì€ í˜„ì¬ ë ˆë²¨ 2-3 ë‹¨ê³„ì— ìˆìœ¼ë©° ì§€ì† ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤."
]

test_references = [
    "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤.",
    "ì „ê¸°ì°¨ì˜ ì¥ì ìœ¼ë¡œëŠ” í™˜ê²½ ë³´í˜¸, ê²½ì œì„±, ì •ìˆ™ì„± ë“±ì´ ìˆìŠµë‹ˆë‹¤.",
    "ììœ¨ì£¼í–‰ ê¸°ìˆ ì€ ë ˆë²¨ 2-3 ìˆ˜ì¤€ìœ¼ë¡œ ë¶€ë¶„ ìë™í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤."
]

# ë°°ì¹˜ í‰ê°€ ì‹¤í–‰
batch_results = batch_llm_evaluation(test_questions, test_answers, test_references)
print("ë°°ì¹˜ í‰ê°€ ì™„ë£Œ!")
print(batch_results[['question_id', 'overall_score', 'grade']].head())
```

### ì‹¤ìŠµ 2: í‰ê°€ì ì‹ ë¢°ì„± ë¶„ì„
```python
class EvaluatorReliabilityAnalyzer:
    def __init__(self):
        self.evaluation_history = []

    def test_evaluator_consistency(self, question: str, answer: str,
                                 num_trials: int = 5) -> Dict[str, Any]:
        """í‰ê°€ì ì¼ê´€ì„± í…ŒìŠ¤íŠ¸"""
        judge = ReferenceFreeJudge(judge_llm)
        trial_results = []

        for i in range(num_trials):
            print(f"í‰ê°€ ì‹œí–‰ {i+1}/{num_trials}")
            result = judge.evaluate(question, answer)
            if result['success']:
                trial_results.append(result)

        if not trial_results:
            return {"error": "ëª¨ë“  í‰ê°€ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

        # ì ìˆ˜ ë¶„ì„
        scores = [r['total_score'] for r in trial_results]

        analysis = {
            "num_trials": len(trial_results),
            "scores": scores,
            "mean_score": np.mean(scores),
            "std_score": np.std(scores),
            "min_score": min(scores),
            "max_score": max(scores),
            "coefficient_of_variation": np.std(scores) / np.mean(scores) if np.mean(scores) > 0 else 0,
            "consistency_rating": self._rate_consistency(np.std(scores))
        }

        return analysis

    def _rate_consistency(self, std_dev: float) -> str:
        """ì¼ê´€ì„± ë“±ê¸‰ ë¶€ì—¬"""
        if std_dev < 0.5:
            return "ë§¤ìš° ì¼ê´€ë¨"
        elif std_dev < 1.0:
            return "ì¼ê´€ë¨"
        elif std_dev < 2.0:
            return "ë³´í†µ"
        else:
            return "ë¶ˆì¼ì¹˜í•¨"

# ì‹ ë¢°ì„± ë¶„ì„ ì‹¤í–‰
analyzer = EvaluatorReliabilityAnalyzer()
reliability_result = analyzer.test_evaluator_consistency(
    question="í…ŒìŠ¬ë¼ì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
    answer="í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤.",
    num_trials=5
)

print("í‰ê°€ì ì‹ ë¢°ì„± ë¶„ì„ ê²°ê³¼:")
print(f"í‰ê·  ì ìˆ˜: {reliability_result['mean_score']:.2f}")
print(f"í‘œì¤€í¸ì°¨: {reliability_result['std_score']:.2f}")
print(f"ì¼ê´€ì„± ë“±ê¸‰: {reliability_result['consistency_rating']}")
```

## ğŸš€ ì‹¤ë¬´ í™œìš© ì˜ˆì‹œ

### 1. ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

```python
import asyncio
from datetime import datetime, timedelta
import json

class RealTimeQualityMonitor:
    def __init__(self, judge_llm, quality_threshold: float = 0.7):
        self.judge = ComprehensiveJudge(judge_llm)
        self.quality_threshold = quality_threshold
        self.monitoring_data = []
        self.alerts = []

    async def monitor_response(self, question: str, answer: str,
                             reference: str = None) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ì‘ë‹µ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""
        start_time = datetime.now()

        # í‰ê°€ ì‹¤í–‰
        evaluation = self.judge.comprehensive_evaluation(
            question=question,
            generated_answer=answer,
            reference_answer=reference
        )

        end_time = datetime.now()
        evaluation_time = (end_time - start_time).total_seconds()

        # ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì €ì¥
        monitoring_record = {
            "timestamp": start_time,
            "question": question,
            "answer": answer,
            "reference": reference,
            "overall_score": evaluation["overall_assessment"]["overall_score"],
            "grade": evaluation["overall_assessment"]["grade"],
            "evaluation_time": evaluation_time
        }

        self.monitoring_data.append(monitoring_record)

        # í’ˆì§ˆ ì„ê³„ê°’ ì²´í¬
        if evaluation["overall_assessment"]["overall_score"] < self.quality_threshold:
            alert = {
                "timestamp": start_time,
                "alert_type": "LOW_QUALITY",
                "score": evaluation["overall_assessment"]["overall_score"],
                "threshold": self.quality_threshold,
                "question": question[:100] + "..." if len(question) > 100 else question
            }
            self.alerts.append(alert)

        return monitoring_record

    def get_quality_report(self, hours: int = 24) -> Dict[str, Any]:
        """í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_data = [
            record for record in self.monitoring_data
            if record["timestamp"] > cutoff_time
        ]

        if not recent_data:
            return {"message": f"ìµœê·¼ {hours}ì‹œê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        scores = [record["overall_score"] for record in recent_data]

        report = {
            "period": f"ìµœê·¼ {hours}ì‹œê°„",
            "total_evaluations": len(recent_data),
            "average_score": np.mean(scores),
            "score_std": np.std(scores),
            "min_score": min(scores),
            "max_score": max(scores),
            "quality_pass_rate": sum(1 for score in scores if score >= self.quality_threshold) / len(scores),
            "alerts_count": len([
                alert for alert in self.alerts
                if alert["timestamp"] > cutoff_time
            ]),
            "grade_distribution": self._calculate_grade_distribution(recent_data)
        }

        return report

    def _calculate_grade_distribution(self, data: List[Dict]) -> Dict[str, int]:
        """ë“±ê¸‰ë³„ ë¶„í¬ ê³„ì‚°"""
        grade_counts = {}
        for record in data:
            grade = record["grade"]
            grade_counts[grade] = grade_counts.get(grade, 0) + 1
        return grade_counts

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ
monitor = RealTimeQualityMonitor(judge_llm, quality_threshold=0.75)

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (ë¹„ë™ê¸° ì‹¤í–‰)
async def simulate_real_time_monitoring():
    test_cases = [
        ("í…ŒìŠ¬ë¼ì˜ CEOëŠ”?", "ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤.", "í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤."),
        ("ì „ê¸°ì°¨ ì¥ì ì€?", "í™˜ê²½ì¹œí™”ì ì…ë‹ˆë‹¤.", "ì „ê¸°ì°¨ëŠ” í™˜ê²½ì¹œí™”ì ì´ê³  ê²½ì œì ì…ë‹ˆë‹¤."),
        ("ì˜ëª»ëœ ì§ˆë¬¸", "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.", "ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")
    ]

    for question, answer, reference in test_cases:
        result = await monitor.monitor_response(question, answer, reference)
        print(f"ëª¨ë‹ˆí„°ë§: {result['grade']} (ì ìˆ˜: {result['overall_score']:.3f})")

        # 1ì´ˆ ëŒ€ê¸° (ì‹¤ì œë¡œëŠ” ì‹¤ì‹œê°„ ì²˜ë¦¬)
        await asyncio.sleep(1)

# ì‹¤í–‰
# asyncio.run(simulate_real_time_monitoring())
```

### 2. ë‹¤ì¤‘ í‰ê°€ì íˆ¬í‘œ ì‹œìŠ¤í…œ

```python
class MultiJudgeVotingSystem:
    def __init__(self, judge_llms: List, voting_method: str = "majority"):
        self.judges = [ReferenceFreeJudge(llm) for llm in judge_llms]
        self.voting_method = voting_method

    def evaluate_with_voting(self, question: str, answer: str) -> Dict[str, Any]:
        """ë‹¤ì¤‘ í‰ê°€ì íˆ¬í‘œ ì‹œìŠ¤í…œ"""
        all_evaluations = []

        for i, judge in enumerate(self.judges):
            print(f"í‰ê°€ì {i+1} í‰ê°€ ì¤‘...")
            evaluation = judge.evaluate(question, answer)

            if evaluation['success']:
                evaluation['judge_id'] = i
                all_evaluations.append(evaluation)

        if not all_evaluations:
            return {"error": "ëª¨ë“  í‰ê°€ìì˜ í‰ê°€ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

        # íˆ¬í‘œ ê²°ê³¼ ê³„ì‚°
        if self.voting_method == "majority":
            final_result = self._majority_voting(all_evaluations)
        elif self.voting_method == "average":
            final_result = self._average_voting(all_evaluations)
        elif self.voting_method == "weighted":
            final_result = self._weighted_voting(all_evaluations)
        else:
            final_result = self._average_voting(all_evaluations)  # ê¸°ë³¸ê°’

        final_result.update({
            "question": question,
            "answer": answer,
            "num_judges": len(all_evaluations),
            "voting_method": self.voting_method,
            "individual_evaluations": all_evaluations
        })

        return final_result

    def _majority_voting(self, evaluations: List[Dict]) -> Dict[str, Any]:
        """ë‹¤ìˆ˜ê²° íˆ¬í‘œ"""
        # ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë‹¤ìˆ˜ê²°
        grades = []
        for eval_result in evaluations:
            score = eval_result['total_score'] / eval_result['max_score']
            if score >= 0.8:
                grades.append("ìš°ìˆ˜")
            elif score >= 0.6:
                grades.append("ì–‘í˜¸")
            else:
                grades.append("ë³´ì™„í•„ìš”")

        from collections import Counter
        grade_counts = Counter(grades)
        majority_grade = grade_counts.most_common(1)[0][0]

        # ë‹¤ìˆ˜ê²° ë“±ê¸‰ì— í•´ë‹¹í•˜ëŠ” í‰ê°€ë“¤ì˜ í‰ê·  ê³„ì‚°
        selected_evaluations = [
            eval_result for eval_result, grade in zip(evaluations, grades)
            if grade == majority_grade
        ]

        avg_score = np.mean([e['total_score'] for e in selected_evaluations])

        return {
            "voting_result": "majority",
            "final_grade": majority_grade,
            "final_score": avg_score,
            "vote_distribution": dict(grade_counts),
            "confidence": grade_counts.most_common(1)[0][1] / len(grades)
        }

    def _average_voting(self, evaluations: List[Dict]) -> Dict[str, Any]:
        """í‰ê·  íˆ¬í‘œ"""
        scores = [eval_result['total_score'] for eval_result in evaluations]
        max_scores = [eval_result['max_score'] for eval_result in evaluations]

        avg_score = np.mean(scores)
        avg_max_score = np.mean(max_scores)
        normalized_score = avg_score / avg_max_score

        # ë“±ê¸‰ ë¶€ì—¬
        if normalized_score >= 0.8:
            final_grade = "ìš°ìˆ˜"
        elif normalized_score >= 0.6:
            final_grade = "ì–‘í˜¸"
        else:
            final_grade = "ë³´ì™„í•„ìš”"

        return {
            "voting_result": "average",
            "final_grade": final_grade,
            "final_score": avg_score,
            "normalized_score": normalized_score,
            "score_std": np.std(scores),
            "agreement_level": 1 - (np.std(scores) / np.mean(scores)) if np.mean(scores) > 0 else 0
        }

    def _weighted_voting(self, evaluations: List[Dict]) -> Dict[str, Any]:
        """ê°€ì¤‘ íˆ¬í‘œ (ì‹ ë¢°ë„ ê¸°ë°˜)"""
        # ê° í‰ê°€ìì˜ ì‹ ë¢°ë„ ê³„ì‚° (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì¼ê´€ì„± ê¸°ë°˜)
        weights = []
        for evaluation in evaluations:
            # ê°œë³„ ì ìˆ˜ë“¤ì˜ ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
            individual_scores = list(evaluation.get('individual_scores', {}).values())
            if individual_scores:
                consistency = 1 / (1 + np.std(individual_scores))
            else:
                consistency = 1.0
            weights.append(consistency)

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(weights)
        weights = [w / total_weight for w in weights]

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_score = sum(
            eval_result['total_score'] * weight
            for eval_result, weight in zip(evaluations, weights)
        )

        weighted_max_score = sum(
            eval_result['max_score'] * weight
            for eval_result, weight in zip(evaluations, weights)
        )

        normalized_score = weighted_score / weighted_max_score

        if normalized_score >= 0.8:
            final_grade = "ìš°ìˆ˜"
        elif normalized_score >= 0.6:
            final_grade = "ì–‘í˜¸"
        else:
            final_grade = "ë³´ì™„í•„ìš”"

        return {
            "voting_result": "weighted",
            "final_grade": final_grade,
            "final_score": weighted_score,
            "normalized_score": normalized_score,
            "weights_used": weights,
            "confidence": max(weights)  # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì‹ ë¢°ë„ë¡œ ì‚¬ìš©
        }

# ë‹¤ì¤‘ í‰ê°€ì ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ
# ì—¬ëŸ¬ LLM ëª¨ë¸ì„ í‰ê°€ìë¡œ ì‚¬ìš©
judge_models = [
    ChatOpenAI(model="gpt-4.1-mini", temperature=0.1),
    ChatOpenAI(model="gpt-3.5-turbo", temperature=0.1),
    ChatOpenAI(model="gpt-4.1-mini", temperature=0.2)  # ë‹¤ë¥¸ ì„¤ì •
]

voting_system = MultiJudgeVotingSystem(judge_models, voting_method="average")

voting_result = voting_system.evaluate_with_voting(
    question="í…ŒìŠ¬ë¼ì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
    answer="í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤. ê·¸ëŠ” í˜ì‹ ì ì¸ ê¸°ì—…ê°€ë¡œ í‰ê°€ë°›ê³  ìˆìŠµë‹ˆë‹¤."
)

print("ë‹¤ì¤‘ í‰ê°€ì íˆ¬í‘œ ê²°ê³¼:")
print(f"ìµœì¢… ë“±ê¸‰: {voting_result['final_grade']}")
print(f"ìµœì¢… ì ìˆ˜: {voting_result['final_score']:.2f}")
print(f"í‰ê°€ì ìˆ˜: {voting_result['num_judges']}")
print(f"í•©ì˜ ìˆ˜ì¤€: {voting_result.get('agreement_level', 'N/A'):.3f}")
```

## ğŸ“– ì°¸ê³  ìë£Œ

### LLM-as-Judge ê´€ë ¨ ì—°êµ¬
- [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)
- [LLM-as-a-Judge: A Comprehensive Study](https://arxiv.org/abs/2403.16950)
- [Can Large Language Models Be Good Judges?](https://arxiv.org/abs/2308.02312)

### LangChain í‰ê°€ ë„êµ¬
- [LangChain Evaluation Documentation](https://python.langchain.com/docs/guides/evaluation/)
- [LangSmith Evaluation Guide](https://docs.smith.langchain.com/evaluation)
- [Custom Evaluator êµ¬í˜„ ê°€ì´ë“œ](https://python.langchain.com/docs/guides/evaluation/string/custom)

### í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- [Prompt Engineering for LLM-as-Judge](https://platform.openai.com/docs/guides/prompt-engineering)
- [Constitutional AIì™€ AI í”¼ë“œë°±](https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback)

### ì¶”ê°€ í•™ìŠµ ìë£Œ
- [ìë™ í‰ê°€ ì‹œìŠ¤í…œ ì„¤ê³„ ì›ì¹™](https://research.google/pubs/pub48671/)
- [RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ ë°©ë²•ë¡ ](https://python.langchain.com/docs/use_cases/question_answering/evaluation/)
- [ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í‰ê°€ ì‹œìŠ¤í…œ](https://python.langchain.com/docs/use_cases/agent_simulations/)

ì´ ê°€ì´ë“œë¥¼ í†µí•´ LLM-as-Judge ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„± í’ˆì§ˆì„ ìë™ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê¸°ë¥´ì‹œê¸° ë°”ëë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œëŠ” ì—¬ëŸ¬ í‰ê°€ ë°©ì‹ì„ ì¡°í•©í•˜ì—¬ ë”ìš± ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.