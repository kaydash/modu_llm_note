# W2_003 Document Loader ì‚¬ìš©ë²•

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- LangChainì˜ ë‹¤ì–‘í•œ Document Loader ì´í•´í•˜ê³  í™œìš©í•˜ê¸°
- PDF, ì›¹, JSON, CSV, ë””ë ‰í† ë¦¬ ë“± ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ ì²˜ë¦¬ ë°©ë²• í•™ìŠµí•˜ê¸°
- ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ íš¨ìœ¨ì ì¸ ë¬¸ì„œ ë¡œë”© ì „ëµ ìˆ˜ë¦½í•˜ê¸°

## ğŸ“š í•µì‹¬ ê°œë…

### Document Loaderë€?
Document LoaderëŠ” ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ LangChainì˜ í‘œì¤€ Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- BaseLoader ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ì¼ê´€ëœ êµ¬í˜„
- `.load()` (ë™ê¸°) ë˜ëŠ” `.lazy_load()` (ë¹„ë™ê¸°) ë©”ì„œë“œ ì œê³µ
- ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì˜ ê²½ìš° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ `.lazy_load()` ê¶Œì¥
- ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ì§€ì› (PDF, ì›¹, JSON, CSV, í…ìŠ¤íŠ¸, ë””ë ‰í† ë¦¬ ë“±)

### Document ê°ì²´ êµ¬ì¡°
```python
Document(
    page_content="ë¬¸ì„œì˜ ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©",
    metadata={
        "source": "íŒŒì¼ ê²½ë¡œ ë˜ëŠ” URL",
        "page": "í˜ì´ì§€ ë²ˆí˜¸",
        # ê¸°íƒ€ ë©”íƒ€ë°ì´í„°
    }
)
```

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install langchain-community

# PDF ì²˜ë¦¬ìš©
pip install pypdf

# JSON ì²˜ë¦¬ìš©
pip install jq

# ì›¹ í˜ì´ì§€ ì²˜ë¦¬ìš©
pip install beautifulsoup4 requests
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
from dotenv import load_dotenv
import os
from typing import List, Dict, Any, Optional
from pathlib import Path

load_dotenv()

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from glob import glob
from pprint import pprint
import json
```

## ğŸ’» ì½”ë“œ ì˜ˆì œ

### 1. PDF íŒŒì¼ ë¡œë”

#### ê¸°ë³¸ ì‚¬ìš©ë²•
```python
from langchain_community.document_loaders import PyPDFLoader
from typing import List
from langchain_core.documents import Document

def load_pdf_documents(file_path: str) -> List[Document]:
    """PDF íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    pdf_loader = PyPDFLoader(file_path)
    documents = pdf_loader.load()

    print(f'PDF ë¬¸ì„œ ê°œìˆ˜: {len(documents)}')
    return documents

# ì‚¬ìš© ì˜ˆì‹œ
pdf_docs = load_pdf_documents('./data/transformer.pdf')

# ì²« ë²ˆì§¸ ë¬¸ì„œ í™•ì¸
print("ì²« ë²ˆì§¸ í˜ì´ì§€ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
print(pdf_docs[0].page_content[:200])
print("\në©”íƒ€ë°ì´í„°:")
print(pdf_docs[0].metadata)
```

#### ë¹„ë™ê¸° ë¡œë”© (ëŒ€ìš©ëŸ‰ íŒŒì¼ ê¶Œì¥)
```python
async def load_pdf_async(file_path: str) -> None:
    """PDFë¥¼ ë¹„ë™ê¸°ë¡œ í˜ì´ì§€ë³„ ì²˜ë¦¬"""
    pdf_loader = PyPDFLoader(file_path)

    async for page in pdf_loader.alazy_load():
        print(f"í˜ì´ì§€ {page.metadata.get('page', 0) + 1}:")
        print(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(page.page_content)}")
        print("-" * 80)
```

### 2. ì›¹ ë¬¸ì„œ ë¡œë”

#### ê¸°ë³¸ ì›¹ ë¡œë”©
```python
from langchain_community.document_loaders import WebBaseLoader
from typing import List, Optional

def load_web_documents(urls: List[str]) -> List[Document]:
    """ì›¹ í˜ì´ì§€ë¥¼ ë¡œë“œí•˜ì—¬ Document ê°ì²´ë¡œ ë³€í™˜"""
    web_loader = WebBaseLoader(web_paths=urls)
    documents = web_loader.load()

    print(f"ë¡œë“œëœ ì›¹ ë¬¸ì„œ ê°œìˆ˜: {len(documents)}")
    return documents

# ì‚¬ìš© ì˜ˆì‹œ
web_docs = load_web_documents([
    "https://python.langchain.com/",
    "https://js.langchain.com/"
])

# ë©”íƒ€ë°ì´í„° í™•ì¸
for doc in web_docs:
    print(f"ì œëª©: {doc.metadata.get('title', 'N/A')}")
    print(f"ì–¸ì–´: {doc.metadata.get('language', 'N/A')}")
    print(f"ì„¤ëª…: {doc.metadata.get('description', 'N/A')}")
    print("-" * 50)
```

#### BeautifulSoupì„ í™œìš©í•œ ì„ íƒì  íŒŒì‹±
```python
import bs4

def load_web_selective(url: str, target_class: str) -> List[Document]:
    """íŠ¹ì • HTML ìš”ì†Œë§Œ ì„ íƒì ìœ¼ë¡œ íŒŒì‹±"""
    web_loader = WebBaseLoader(
        web_paths=[url],
        bs_kwargs={
            "parse_only": bs4.SoupStrainer(class_=target_class),
        },
        bs_get_text_kwargs={
            "separator": " | ",    # êµ¬ë¶„ì
            "strip": True          # ê³µë°± ì œê±°
        }
    )

    return web_loader.load()

# ì‚¬ìš© ì˜ˆì‹œ
selective_docs = load_web_selective(
    "https://python.langchain.com/",
    "theme-doc-markdown markdown"
)
```

### 3. JSON íŒŒì¼ ë¡œë”

#### JSON êµ¬ì¡°í™”ëœ ë°ì´í„° ì²˜ë¦¬
```python
from langchain_community.document_loaders import JSONLoader
from langchain_core.documents import Document

def load_json_content(file_path: str, jq_schema: str, text_content: bool = True) -> List[Document]:
    """JSON íŒŒì¼ì—ì„œ íŠ¹ì • í•„ë“œ ì¶”ì¶œ"""
    json_loader = JSONLoader(
        file_path=file_path,
        jq_schema=jq_schema,      # JQ ìŠ¤í‚¤ë§ˆë¡œ í•„ë“œ ì„ íƒ
        text_content=text_content, # í…ìŠ¤íŠ¸ ë‚´ìš© ì—¬ë¶€
    )

    return json_loader.load()

# ë©”ì‹œì§€ ë‚´ìš©ë§Œ ì¶”ì¶œ
message_docs = load_json_content(
    "./data/kakao_chat.json",
    ".messages[].content"
)

# ì „ì²´ ë©”ì‹œì§€ ê°ì²´ ì¶”ì¶œ
full_message_docs = load_json_content(
    "./data/kakao_chat.json",
    ".messages[]",
    text_content=False
)
```

#### JSONL íŒŒì¼ ì²˜ë¦¬
```python
def load_jsonl_file(file_path: str, content_key: str) -> List[Document]:
    """JSONL(JSON Lines) íŒŒì¼ ë¡œë“œ"""
    json_loader = JSONLoader(
        file_path=file_path,
        jq_schema=".",
        content_key=content_key,
        json_lines=True,  # JSONL í˜•ì‹ ì§€ì •
    )

    return json_loader.load()

# JSONL íŒŒì¼ ë¡œë“œ
jsonl_docs = load_jsonl_file("./data/kakao_chat.jsonl", "content")
```

#### í•œê¸€ ìœ ë‹ˆì½”ë“œ ì²˜ë¦¬
```python
def decode_unicode_json(documents: List[Document]) -> List[Document]:
    """ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ ë””ì½”ë”©"""
    decoded_docs = []

    for doc in documents:
        try:
            decoded_data = json.loads(doc.page_content)
            decoded_content = json.dumps(decoded_data, ensure_ascii=False)

            decoded_doc = Document(
                page_content=decoded_content,
                metadata=doc.metadata
            )
            decoded_docs.append(decoded_doc)

        except json.JSONDecodeError as e:
            print(f"JSON ë””ì½”ë”© ì˜¤ë¥˜: {e}")
            decoded_docs.append(doc)  # ì›ë³¸ ìœ ì§€

    return decoded_docs
```

### 4. CSV íŒŒì¼ ë¡œë”

#### ê¸°ë³¸ CSV ë¡œë”©
```python
from langchain_community.document_loaders.csv_loader import CSVLoader

def load_csv_documents(file_path: str, source_column: Optional[str] = None) -> List[Document]:
    """CSV íŒŒì¼ì„ Document ê°ì²´ë¡œ ë³€í™˜"""
    csv_loader = CSVLoader(
        file_path=file_path,
        source_column=source_column,  # ë©”íƒ€ë°ì´í„° source í•„ë“œë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼
    )

    documents = csv_loader.load()
    print(f"ë¡œë“œëœ CSV í–‰ ìˆ˜: {len(documents)}")

    return documents

# ê¸°ë³¸ ì‚¬ìš©ë²•
csv_docs = load_csv_documents("./data/kbo_teams_2023.csv")

# íŠ¹ì • ì»¬ëŸ¼ì„ ì†ŒìŠ¤ë¡œ ì‚¬ìš©
csv_docs_with_source = load_csv_documents("./data/kbo_teams_2023.csv", "Team")
```

#### CSV íŒŒì‹± ì˜µì…˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•
```python
def load_csv_custom(file_path: str, delimiter: str = ",", quotechar: str = '"') -> List[Document]:
    """CSV íŒŒì‹± ì˜µì…˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•"""
    csv_loader = CSVLoader(
        file_path=file_path,
        csv_args={
            "delimiter": delimiter,     # êµ¬ë¶„ì ì§€ì •
            "quotechar": quotechar,     # ë”°ì˜´í‘œ ë¬¸ì ì§€ì •
        }
    )

    return csv_loader.load()
```

### 5. ë””ë ‰í† ë¦¬ ë¡œë”

#### ë””ë ‰í† ë¦¬ ì¼ê´„ ë¡œë”©
```python
from langchain_community.document_loaders import DirectoryLoader, TextLoader

def load_directory_documents(
    path: str,
    glob_pattern: str = "**/*.txt",
    loader_class=TextLoader,
    show_progress: bool = True
) -> List[Document]:
    """ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ë“¤ì„ ì¼ê´„ ë¡œë”©"""
    dir_loader = DirectoryLoader(
        path=path,
        glob=glob_pattern,           # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´
        loader_cls=loader_class,     # ì‚¬ìš©í•  ë¡œë” í´ë˜ìŠ¤
        show_progress=show_progress, # ì§„í–‰ìƒíƒœ í‘œì‹œ
    )

    documents = dir_loader.load()
    print(f"ë¡œë“œëœ íŒŒì¼ ìˆ˜: {len(documents)}")

    return documents

# íŠ¹ì • íŒ¨í„´ì˜ í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ ë¡œë“œ
dir_docs = load_directory_documents(
    path="./",
    glob_pattern="**/*_KR.txt",
    loader_class=TextLoader
)

# ë¡œë“œëœ íŒŒì¼ ì •ë³´ ì¶œë ¥
for doc in dir_docs:
    print(f"íŒŒì¼: {doc.metadata['source']}")
    print(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(doc.page_content)}")
    print("-" * 50)
```

### 6. ë¬¸ì„œ ë¡œë”© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

#### ë¬¸ì„œ ì •ë³´ ë¶„ì„ í•¨ìˆ˜
```python
def analyze_documents(documents: List[Document]) -> Dict[str, Any]:
    """ë¡œë“œëœ ë¬¸ì„œë“¤ì˜ í†µê³„ ì •ë³´ ë¶„ì„"""
    if not documents:
        return {"count": 0, "total_length": 0, "avg_length": 0}

    total_length = sum(len(doc.page_content) for doc in documents)
    avg_length = total_length / len(documents)

    # ë©”íƒ€ë°ì´í„° í‚¤ ë¶„ì„
    metadata_keys = set()
    for doc in documents:
        metadata_keys.update(doc.metadata.keys())

    return {
        "count": len(documents),
        "total_length": total_length,
        "avg_length": avg_length,
        "metadata_keys": list(metadata_keys),
        "sources": [doc.metadata.get("source", "Unknown") for doc in documents]
    }

# ì‚¬ìš© ì˜ˆì‹œ
analysis = analyze_documents(pdf_docs)
pprint(analysis)
```

#### ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° í•¨ìˆ˜
```python
def preview_documents(documents: List[Document], max_preview: int = 200) -> None:
    """ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°"""
    for i, doc in enumerate(documents):
        print(f"=== ë¬¸ì„œ {i+1} ===")
        print(f"ì†ŒìŠ¤: {doc.metadata.get('source', 'Unknown')}")
        print(f"ê¸¸ì´: {len(doc.page_content)} characters")

        # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
        content_preview = doc.page_content[:max_preview]
        if len(doc.page_content) > max_preview:
            content_preview += "..."

        print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\n{content_preview}")
        print("-" * 80)

# ì‚¬ìš© ì˜ˆì‹œ
preview_documents(csv_docs, max_preview=150)
```

## ğŸš€ ì‹¤ìŠµí•´ë³´ê¸°

### ì‹¤ìŠµ 1: PDF ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œ
PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê° í˜ì´ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
def pdf_analysis_system(pdf_path: str) -> Dict[str, Any]:
    """PDF ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œ"""
    # TODO: PDF ë¡œë”ë¡œ ë¬¸ì„œ ë¡œë“œ
    # TODO: ê° í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
    # TODO: ì „ì²´ í†µê³„ ì •ë³´ ë°˜í™˜
    pass

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
result = pdf_analysis_system("./articles/notionai.pdf")
print(f"ë¶„ì„ ê²°ê³¼: {result}")
```

### ì‹¤ìŠµ 2: ë©€í‹° í¬ë§· ë¬¸ì„œ í†µí•© ë¡œë”
ì—¬ëŸ¬ í˜•ì‹ì˜ ë¬¸ì„œë¥¼ í•œ ë²ˆì— ë¡œë“œí•˜ëŠ” í†µí•© ë¡œë”ë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
def multi_format_loader(file_configs: List[Dict[str, Any]]) -> List[Document]:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ íŒŒì¼ë“¤ì„ í†µí•© ë¡œë”©

    Args:
        file_configs: íŒŒì¼ ì„¤ì • ë¦¬ìŠ¤íŠ¸
            [
                {"type": "pdf", "path": "doc.pdf"},
                {"type": "csv", "path": "data.csv", "source_column": "id"},
                {"type": "json", "path": "data.json", "jq_schema": ".content"}
            ]
    """
    # TODO: íŒŒì¼ íƒ€ì…ë³„ ì ì ˆí•œ ë¡œë” ì„ íƒ
    # TODO: ê° íŒŒì¼ ë¡œë“œ í›„ ê²°ê³¼ ë³‘í•©
    # TODO: í†µí•©ëœ Document ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    pass

# í…ŒìŠ¤íŠ¸ ì„¤ì •
configs = [
    {"type": "pdf", "path": "./data/transformer.pdf"},
    {"type": "csv", "path": "./data/kbo_teams_2023.csv"},
    {"type": "json", "path": "./data/kakao_chat.json", "jq_schema": ".messages[].content"}
]

documents = multi_format_loader(configs)
print(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œë¨")
```

### ì‹¤ìŠµ 3: ì›¹ ì»¨í…ì¸  ìˆ˜ì§‘ê¸°
ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ íŠ¹ì • ë‚´ìš©ë§Œ ì„ íƒì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
def web_content_collector(
    url_configs: List[Dict[str, Any]]
) -> List[Document]:
    """ì›¹ ì»¨í…ì¸  ì„ íƒì  ìˆ˜ì§‘ê¸°

    Args:
        url_configs: URL ì„¤ì • ë¦¬ìŠ¤íŠ¸
            [
                {
                    "url": "https://example.com",
                    "target_class": "content",
                    "separator": " | "
                }
            ]
    """
    # TODO: URLë³„ BeautifulSoup ì„¤ì • ì ìš©
    # TODO: ì„ íƒì  ì»¨í…ì¸  ì¶”ì¶œ
    # TODO: Document ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
    pass
```

## ğŸ“‹ í•´ë‹µ

### ì‹¤ìŠµ 1 í•´ë‹µ: PDF ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œ
```python
from langchain_community.document_loaders import PyPDFLoader
from pathlib import Path
from typing import Dict, Any, List

def pdf_analysis_system(pdf_path: str) -> Dict[str, Any]:
    """PDF ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œ"""
    # PDF íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(pdf_path).exists():
        # ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ íŒŒì¼ ê²€ìƒ‰
        cwd = Path.cwd()
        candidates = [
            cwd / "articles" / Path(pdf_path).name,
            cwd / "data" / Path(pdf_path).name
        ]
        candidates += list(cwd.rglob(Path(pdf_path).name))

        found_path = None
        for candidate in candidates:
            if candidate.exists():
                found_path = str(candidate)
                break

        if not found_path:
            return {"error": f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}"}

        pdf_path = found_path

    # PDF ë¡œë”ë¡œ ë¬¸ì„œ ë¡œë“œ
    pdf_loader = PyPDFLoader(pdf_path)
    documents = pdf_loader.load()

    # ê° í˜ì´ì§€ ë¶„ì„
    page_lengths = []
    for i, doc in enumerate(documents):
        page_length = len(doc.page_content)
        page_lengths.append(page_length)
        print(f"í˜ì´ì§€ {i+1}: {page_length} characters")

    # ì „ì²´ í†µê³„ ê³„ì‚°
    total_pages = len(documents)
    total_characters = sum(page_lengths)
    avg_length = total_characters / total_pages if total_pages > 0 else 0

    return {
        "file_path": pdf_path,
        "total_pages": total_pages,
        "total_characters": total_characters,
        "average_length_per_page": round(avg_length, 2),
        "page_lengths": page_lengths,
        "longest_page": max(page_lengths) if page_lengths else 0,
        "shortest_page": min(page_lengths) if page_lengths else 0
    }

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
result = pdf_analysis_system("notionai.pdf")
pprint(result)
```

### ì‹¤ìŠµ 2 í•´ë‹µ: ë©€í‹° í¬ë§· ë¬¸ì„œ í†µí•© ë¡œë”
```python
from langchain_community.document_loaders import (
    PyPDFLoader, CSVLoader, JSONLoader, TextLoader
)

def multi_format_loader(file_configs: List[Dict[str, Any]]) -> List[Document]:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ íŒŒì¼ë“¤ì„ í†µí•© ë¡œë”©"""
    all_documents = []

    for config in file_configs:
        file_type = config.get("type", "").lower()
        file_path = config.get("path", "")

        if not Path(file_path).exists():
            print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            continue

        try:
            if file_type == "pdf":
                loader = PyPDFLoader(file_path)

            elif file_type == "csv":
                source_column = config.get("source_column")
                loader = CSVLoader(
                    file_path=file_path,
                    source_column=source_column
                )

            elif file_type == "json":
                jq_schema = config.get("jq_schema", ".")
                text_content = config.get("text_content", True)
                json_lines = config.get("json_lines", False)

                loader = JSONLoader(
                    file_path=file_path,
                    jq_schema=jq_schema,
                    text_content=text_content,
                    json_lines=json_lines
                )

            elif file_type == "txt" or file_type == "text":
                loader = TextLoader(file_path)

            else:
                print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_type}")
                continue

            # ë¬¸ì„œ ë¡œë“œ
            documents = loader.load()
            all_documents.extend(documents)
            print(f"{file_type.upper()} ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")

        except Exception as e:
            print(f"{file_path} ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

    return all_documents

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
configs = [
    {"type": "pdf", "path": "./data/transformer.pdf"},
    {"type": "csv", "path": "./data/kbo_teams_2023.csv", "source_column": "Team"},
    {
        "type": "json",
        "path": "./data/kakao_chat.json",
        "jq_schema": ".messages[].content",
        "text_content": True
    }
]

documents = multi_format_loader(configs)
print(f"\nì´ {len(documents)}ê°œ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë¡œë“œëœ ë¬¸ì„œë“¤ì˜ ì†ŒìŠ¤ í™•ì¸
sources = {}
for doc in documents:
    source = doc.metadata.get("source", "Unknown")
    source_ext = Path(source).suffix if source != "Unknown" else "Unknown"
    sources[source_ext] = sources.get(source_ext, 0) + 1

print("\níŒŒì¼ í˜•ì‹ë³„ ë¬¸ì„œ ìˆ˜:")
for ext, count in sources.items():
    print(f"{ext}: {count}ê°œ")
```

### ì‹¤ìŠµ 3 í•´ë‹µ: ì›¹ ì»¨í…ì¸  ìˆ˜ì§‘ê¸°
```python
import bs4
from langchain_community.document_loaders import WebBaseLoader

def web_content_collector(url_configs: List[Dict[str, Any]]) -> List[Document]:
    """ì›¹ ì»¨í…ì¸  ì„ íƒì  ìˆ˜ì§‘ê¸°"""
    all_documents = []

    for config in url_configs:
        url = config.get("url", "")
        target_class = config.get("target_class")
        target_id = config.get("target_id")
        target_tag = config.get("target_tag")
        separator = config.get("separator", " ")

        if not url:
            print("URLì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            continue

        try:
            # BeautifulSoup ì„¤ì • êµ¬ì„±
            bs_kwargs = {}
            bs_get_text_kwargs = {
                "separator": separator,
                "strip": True
            }

            # ì„ íƒì ì„¤ì •
            if target_class:
                bs_kwargs["parse_only"] = bs4.SoupStrainer(class_=target_class)
            elif target_id:
                bs_kwargs["parse_only"] = bs4.SoupStrainer(id=target_id)
            elif target_tag:
                bs_kwargs["parse_only"] = bs4.SoupStrainer(target_tag)

            # ì›¹ ë¡œë” ìƒì„±
            web_loader = WebBaseLoader(
                web_paths=[url],
                bs_kwargs=bs_kwargs,
                bs_get_text_kwargs=bs_get_text_kwargs
            )

            # ë¬¸ì„œ ë¡œë“œ
            documents = web_loader.load()
            all_documents.extend(documents)

            print(f"ì›¹ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {url}")
            print(f"ì¶”ì¶œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")

        except Exception as e:
            print(f"{url} ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

    return all_documents

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
url_configs = [
    {
        "url": "https://python.langchain.com/",
        "target_class": "theme-doc-markdown markdown",
        "separator": " | "
    }
]

web_documents = web_content_collector(url_configs)
print(f"\nì´ {len(web_documents)}ê°œì˜ ì›¹ ë¬¸ì„œê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ìˆ˜ì§‘ëœ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
for i, doc in enumerate(web_documents):
    print(f"\n=== ë¬¸ì„œ {i+1} ===")
    print(f"ì œëª©: {doc.metadata.get('title', 'N/A')}")
    print(f"URL: {doc.metadata.get('source', 'N/A')}")
    print(f"ë‚´ìš© ê¸¸ì´: {len(doc.page_content)}")
    print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:200]}...")
```

## ğŸ” ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LangChain Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)
- [PyPDFLoader Documentation](https://python.langchain.com/docs/integrations/document_loaders/pypdf/)
- [WebBaseLoader Documentation](https://python.langchain.com/docs/integrations/document_loaders/web_base/)

### ì¶”ê°€ í•™ìŠµ ìë£Œ
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [jq Manual](https://stedolan.github.io/jq/manual/) - JSON ì¿¼ë¦¬ ì–¸ì–´
- [Python CSV Module](https://docs.python.org/3/library/csv.html)

### ê´€ë ¨ íŒ¨í‚¤ì§€
```python
# í•µì‹¬ íŒ¨í‚¤ì§€
langchain-community     # ì»¤ë®¤ë‹ˆí‹° ë¡œë”ë“¤
langchain-core         # ê¸°ë³¸ Document í´ë˜ìŠ¤

# íŠ¹í™” íŒ¨í‚¤ì§€
pypdf                  # PDF ì²˜ë¦¬
beautifulsoup4         # HTML/XML íŒŒì‹±
jq                     # JSON ì¿¼ë¦¬
python-dotenv          # í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬
```

### ì„±ëŠ¥ ìµœì í™” íŒ
- ëŒ€ìš©ëŸ‰ íŒŒì¼: `.lazy_load()` ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
- ì›¹ í˜ì´ì§€: BeautifulSoup íŒŒì‹± ì˜µì…˜ìœ¼ë¡œ í•„ìš”í•œ ìš”ì†Œë§Œ ì¶”ì¶œ
- JSON íŒŒì¼: jq ìŠ¤í‚¤ë§ˆë¡œ í•„ìš”í•œ í•„ë“œë§Œ ì„ íƒì  ì¶”ì¶œ
- ë””ë ‰í† ë¦¬ ë¡œë”©: glob íŒ¨í„´ì„ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì •í•˜ì—¬ ë¶ˆí•„ìš”í•œ íŒŒì¼ ì œì™¸