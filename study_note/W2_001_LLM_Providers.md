# LLM ê³µê¸‰ì ë¹„êµ ë° í™œìš© - ì£¼ìš” ì–¸ì–´ ëª¨ë¸ ì„œë¹„ìŠ¤ ê°€ì´ë“œ

## ğŸ“š í•™ìŠµ ëª©í‘œ
- ì£¼ìš” LLM ê³µê¸‰ìë“¤(Google Gemini, Groq, Ollama, OpenAI)ì˜ íŠ¹ì§•ê³¼ ì¥ë‹¨ì ì„ ì´í•´í•œë‹¤
- ê° ê³µê¸‰ìë³„ API ì„¤ì • ë°©ë²•ê³¼ ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ìŠµë“í•œë‹¤
- RAG ì‹œìŠ¤í…œì— ë‹¤ì–‘í•œ LLM ëª¨ë¸ì„ ì ìš©í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•  ìˆ˜ ìˆë‹¤
- í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ì ì ˆí•œ LLM ì„ íƒ ê¸°ì¤€ì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆë‹¤
- ì‹¤ë¬´ì—ì„œ ë¹„ìš© íš¨ìœ¨ì ì´ê³  ì„±ëŠ¥ ìµœì í™”ëœ LLM í™œìš© ì „ëµì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤

## ğŸ”‘ í•µì‹¬ ê°œë…

### LLM ê³µê¸‰ìë€?
- **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(Large Language Model)**ì„ ê°œë°œí•˜ê³  API ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ê¸°ì—…
- ê°ê¸° ë‹¤ë¥¸ **ê¸°ìˆ ì  íŠ¹ì„±, ê°€ê²© ì •ì±…, ì„±ëŠ¥ íŠ¹ì§•**ì„ ê°€ì§
- **í´ë¼ìš°ë“œ ê¸°ë°˜ API**ì™€ **ë¡œì»¬ ì‹¤í–‰ í™˜ê²½** ë“± ë‹¤ì–‘í•œ ì œê³µ ë°©ì‹ ì¡´ì¬

### ì£¼ìš” ë¹„êµ ìš”ì†Œ
- **ì‘ë‹µ ì†ë„**: ì¶”ë¡  ì‹œê°„ê³¼ ì²˜ë¦¬ëŸ‰
- **ë¹„ìš©**: í† í°ë‹¹ ê°€ê²©ê³¼ ë¬´ë£Œ í• ë‹¹ëŸ‰
- **ì„±ëŠ¥**: ë‹µë³€ í’ˆì§ˆê³¼ ì •í™•ë„
- **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´ ì²˜ë¦¬ ëŠ¥ë ¥
- **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ì²˜ë¦¬
- **ì ‘ê·¼ì„±**: API ì œí•œê³¼ ê°€ìš©ì„±

## ğŸ›  í™˜ê²½ ì„¤ì •

### ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install langchain langchain-openai langchain-chroma
pip install python-dotenv pandas numpy

# ê° ê³µê¸‰ìë³„ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install langchain-google-genai  # Google Gemini
pip install langchain-groq          # Groq
pip install langchain-ollama        # Ollama
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
# .env íŒŒì¼ ìƒì„±
OPENAI_API_KEY=your_openai_api_key
GOOGLE_API_KEY=your_google_api_key
GROQ_API_KEY=your_groq_api_key
```

### ê¸°ë³¸ ì„¤ì • ì½”ë“œ
```python
import os
from dotenv import load_dotenv
import warnings

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
warnings.filterwarnings("ignore")

# ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
chroma_db = Chroma(
    collection_name="db_korean_cosine_metadata",
    embedding_function=embeddings,
    persist_directory="./chroma_db",
)

# ê²€ìƒ‰ê¸° ìƒì„±
retriever = chroma_db.as_retriever(search_kwargs={"k": 4})
```

## ğŸ’» ë‹¨ê³„ë³„ êµ¬í˜„

### 1ë‹¨ê³„: RAG ì²´ì¸ ê¸°ë³¸ êµ¬ì¡° ìƒì„±

```python
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

def create_rag_chain(retriever, llm):
    """
    RAG ì²´ì¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    - retriever: ë¬¸ì„œ ê²€ìƒ‰ê¸°
    - llm: ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸
    """

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    template = """ë‹¤ìŒ ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    ë§¥ë½ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ë‹¤ë©´ 'ë‹µë³€ì— í•„ìš”í•œ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.

    [ë§¥ë½]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ë‹µë³€]
    """

    prompt = ChatPromptTemplate.from_template(template)

    # ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜
    def format_docs(docs):
        return "\n\n".join([doc.page_content for doc in docs])

    # RAG ì²´ì¸ êµ¬ì„±
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain
```

### 2ë‹¨ê³„: Google Gemini API í™œìš©

```python
from langchain_google_genai import ChatGoogleGenerativeAI

# Gemini ëª¨ë¸ ì„¤ì •
gemini_llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.1,  # ì°½ì˜ì„± ì¡°ì ˆ (0: ë³´ìˆ˜ì , 1: ì°½ì˜ì )
    max_tokens=1000   # ìµœëŒ€ ì‘ë‹µ ê¸¸ì´
)

# RAG ì²´ì¸ ìƒì„±
gemini_rag_chain = create_rag_chain(retriever, gemini_llm)

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
query = "í…ŒìŠ¬ë¼ì˜ ììœ¨ì£¼í–‰ ê¸°ìˆ ì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?"
answer = gemini_rag_chain.invoke(query)
print("Gemini ì‘ë‹µ:", answer)
```

**Gemini íŠ¹ì§•:**
- êµ¬ê¸€ì˜ ìµœì‹  ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸
- ë¬´ë£Œ í• ë‹¹ëŸ‰ì´ ìƒë‹¹íˆ ê´€ëŒ€í•¨
- í•œêµ­ì–´ ì²˜ë¦¬ ì„±ëŠ¥ì´ ìš°ìˆ˜í•¨
- í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì½”ë“œ ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì²˜ë¦¬ ê°€ëŠ¥

### 3ë‹¨ê³„: Groq API í™œìš©

```python
from langchain_groq import ChatGroq

# Groq ëª¨ë¸ ì„¤ì •
groq_llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.0,
    max_tokens=500,
    timeout=60  # ì‘ë‹µ ëŒ€ê¸° ì‹œê°„
)

# RAG ì²´ì¸ ìƒì„±
groq_rag_chain = create_rag_chain(retriever, groq_llm)

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
answer = groq_rag_chain.invoke(query)
print("Groq ì‘ë‹µ:", answer)
```

**Groq íŠ¹ì§•:**
- ì´ˆê³ ì† ì¶”ë¡  ì†ë„ (LPU ê¸°ìˆ  í™œìš©)
- ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ê¸°ë°˜ (Llama, Mixtral ë“±)
- 1ì´ˆ ë¯¸ë§Œì˜ ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„
- í•©ë¦¬ì ì¸ ê°€ê²© ì •ì±…

### 4ë‹¨ê³„: Ollama ë¡œì»¬ ëª¨ë¸ í™œìš©

```python
from langchain_ollama import ChatOllama

# Ollama ëª¨ë¸ ì„¤ì • (ë¡œì»¬ ì‹¤í–‰)
ollama_llm = ChatOllama(
    model="qwen2.5:latest",  # ollama pull qwen2.5:latestë¡œ ë‹¤ìš´ë¡œë“œ í•„ìš”
    temperature=0,
    num_predict=200,  # ìƒì„±í•  í† í° ìˆ˜
    base_url="http://localhost:11434"  # Ollama ì„œë²„ ì£¼ì†Œ
)

# RAG ì²´ì¸ ìƒì„±
ollama_rag_chain = create_rag_chain(retriever, ollama_llm)

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
answer = ollama_rag_chain.invoke(query)
print("Ollama ì‘ë‹µ:", answer)
```

**Ollama íŠ¹ì§•:**
- ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ (ì¸í„°ë„· ë¶ˆí•„ìš”)
- ë°ì´í„° í”„ë¼ì´ë²„ì‹œ ë³´ì¥
- ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì§€ì›
- ë¬´ë£Œ ì‚¬ìš© (í•˜ë“œì›¨ì–´ ì„±ëŠ¥ì— ë”°ë¼ ì†ë„ ê²°ì •)

### 5ë‹¨ê³„: ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ êµ¬í˜„

```python
import time
from typing import Dict, List

def compare_llm_performance(models: Dict, queries: List[str]) -> Dict:
    """
    ì—¬ëŸ¬ LLM ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜
    """
    results = {}

    for model_name, rag_chain in models.items():
        print(f"\n=== {model_name} í…ŒìŠ¤íŠ¸ ì¤‘ ===")
        model_results = []

        for query in queries:
            start_time = time.time()

            try:
                answer = rag_chain.invoke(query)
                end_time = time.time()

                model_results.append({
                    "query": query,
                    "answer": answer,
                    "response_time": round(end_time - start_time, 2),
                    "status": "ì„±ê³µ"
                })

            except Exception as e:
                model_results.append({
                    "query": query,
                    "answer": f"ì˜¤ë¥˜: {str(e)}",
                    "response_time": None,
                    "status": "ì‹¤íŒ¨"
                })

        results[model_name] = model_results

    return results

# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì¤€ë¹„
test_queries = [
    "í…ŒìŠ¬ë¼ì˜ ììœ¨ì£¼í–‰ ê¸°ìˆ ì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ì „ê¸°ì°¨ ë°°í„°ë¦¬ ê¸°ìˆ ì˜ ìµœì‹  ë™í–¥ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "ìë™ì°¨ ì‚°ì—…ì˜ ë¯¸ë˜ ì „ë§ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
]

# ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ ì¤€ë¹„
models_to_compare = {
    "OpenAI": openai_rag_chain,
    "Gemini": gemini_rag_chain,
    "Groq": groq_rag_chain,
    "Ollama": ollama_rag_chain
}

# ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
performance_results = compare_llm_performance(models_to_compare, test_queries)
```

## ğŸ¯ ì‹¤ìŠµ ë¬¸ì œ

### ê¸°ì´ˆ ì‹¤ìŠµ
1. **API í‚¤ ì„¤ì • ë° ì—°ê²° í…ŒìŠ¤íŠ¸**
   - ê° ê³µê¸‰ìì˜ API í‚¤ë¥¼ ì„¤ì •í•˜ê³  ì •ìƒ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”
   - ê°„ë‹¨í•œ "ì•ˆë…•í•˜ì„¸ìš”" ë©”ì‹œì§€ë¡œ ì‘ë‹µ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³´ì„¸ìš”

2. **ëª¨ë¸ ë³€ê²½ ì‹¤ìŠµ**
   - Geminiì—ì„œ ë‹¤ë¥¸ ëª¨ë¸(`gemini-1.5-pro`)ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”
   - Groqì—ì„œ ë‹¤ë¥¸ ëª¨ë¸(`mixtral-8x7b-32768`)ì„ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”

### ì‘ìš© ì‹¤ìŠµ
3. **ë§¤ê°œë³€ìˆ˜ íŠœë‹**
   - ê° ëª¨ë¸ì˜ `temperature` ê°’ì„ 0, 0.5, 1.0ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì‘ë‹µ ì°¨ì´ë¥¼ ë¹„êµí•˜ì„¸ìš”
   - `max_tokens` ê°’ì„ ì¡°ì •í•˜ì—¬ ì‘ë‹µ ê¸¸ì´ì˜ ë³€í™”ë¥¼ ê´€ì°°í•˜ì„¸ìš”

4. **ë¹„ìš© ë¶„ì„**
   - ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•´ ê° ëª¨ë¸ì˜ í† í° ì‚¬ìš©ëŸ‰ì„ ê³„ì‚°í•´ë³´ì„¸ìš”
   - ê³µê¸‰ìë³„ ê°€ê²© ì •ì±…ì„ ì¡°ì‚¬í•˜ê³  ë¹„ìš©ì„ ì‚°ì¶œí•´ë³´ì„¸ìš”

### ì‹¬í™” ì‹¤ìŠµ
5. **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹**
   - 10ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ êµ¬ì„±ëœ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë§Œë“¤ì–´ ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•© í‰ê°€í•˜ì„¸ìš”
   - ì‘ë‹µ ì†ë„, ì •í™•ë„, í•œêµ­ì–´ í’ˆì§ˆì„ ê¸°ì¤€ìœ¼ë¡œ ì ìˆ˜í™”í•´ë³´ì„¸ìš”

## âœ… ì†”ë£¨ì…˜ ì˜ˆì‹œ

### ì‹¤ìŠµ 1: API ì—°ê²° í…ŒìŠ¤íŠ¸
```python
def test_api_connection():
    models = {
        "OpenAI": ChatOpenAI(model="gpt-3.5-turbo"),
        "Gemini": ChatGoogleGenerativeAI(model="gemini-1.5-flash"),
        "Groq": ChatGroq(model="llama-3.3-70b-versatile")
    }

    test_message = "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ì¸ì‚¬ë§ë¡œ ë‹µí•´ì£¼ì„¸ìš”."

    for name, model in models.items():
        try:
            response = model.invoke(test_message)
            print(f"{name}: âœ… ì—°ê²° ì„±ê³µ - {response.content[:50]}...")
        except Exception as e:
            print(f"{name}: âŒ ì—°ê²° ì‹¤íŒ¨ - {e}")

# ì‹¤í–‰
test_api_connection()
```

### ì‹¤ìŠµ 2: ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ ìƒì„±
```python
import pandas as pd

def create_performance_comparison():
    comparison_data = {
        "ê³µê¸‰ì": ["OpenAI", "Google Gemini", "Groq", "Ollama"],
        "ëª¨ë¸": ["gpt-4o-mini", "gemini-1.5-flash", "llama-3.3-70b", "qwen2.5:latest"],
        "í‰ê· ì‘ë‹µì‹œê°„(ì´ˆ)": [2.3, 1.8, 0.6, 4.2],
        "í•œêµ­ì–´í’ˆì§ˆ": ["ìš°ìˆ˜", "ìš°ìˆ˜", "ì–‘í˜¸", "ì–‘í˜¸"],
        "ë¹„ìš©ìˆ˜ì¤€": ["ì¤‘ê°„", "ë‚®ìŒ", "ë‚®ìŒ", "ë¬´ë£Œ"],
        "íŠ¹ì§•": ["ë²”ìš©ì„±", "ë©€í‹°ëª¨ë‹¬", "ê³ ì†ì²˜ë¦¬", "í”„ë¼ì´ë²„ì‹œ"]
    }

    df = pd.DataFrame(comparison_data)
    return df

# ì‹¤í–‰ ë° ì¶œë ¥
comparison_df = create_performance_comparison()
print(comparison_df.to_string(index=False))
```

## ğŸš€ ì‹¤ë¬´ í™œìš© ì˜ˆì‹œ

### 1. ë¹„ìš© íš¨ìœ¨ì ì¸ LLM ë¼ìš°íŒ… ì‹œìŠ¤í…œ

```python
class SmartLLMRouter:
    def __init__(self):
        self.models = {
            "fast": ChatGroq(model="llama-3.3-70b-versatile"),  # ë¹ ë¥¸ ì‘ë‹µ
            "accurate": ChatGoogleGenerativeAI(model="gemini-1.5-pro"),  # ì •í™•í•œ ë‹µë³€
            "free": ChatOllama(model="qwen2.5:latest")  # ë¬´ë£Œ ì‚¬ìš©
        }

    def route_query(self, query: str, priority: str = "balanced"):
        """
        ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ì„ ì„ íƒ
        priority: "speed", "accuracy", "cost"
        """
        if priority == "speed":
            return self.models["fast"].invoke(query)
        elif priority == "accuracy":
            return self.models["accurate"].invoke(query)
        elif priority == "cost":
            return self.models["free"].invoke(query)
        else:
            # ê· í˜•ì¡íŒ ì„ íƒ (ê¸°ë³¸ê°’)
            return self.models["fast"].invoke(query)

# ì‚¬ìš© ì˜ˆì‹œ
router = SmartLLMRouter()

# ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°
quick_answer = router.route_query("ê°„ë‹¨í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤", priority="speed")

# ì •í™•í•œ ë‹µë³€ì´ í•„ìš”í•œ ê²½ìš°
accurate_answer = router.route_query("ë³µì¡í•œ ê¸°ìˆ  ë¬¸ì œì…ë‹ˆë‹¤", priority="accuracy")
```

### 2. ë‹¤ì¤‘ ëª¨ë¸ íˆ¬í‘œ ì‹œìŠ¤í…œ

```python
def multi_model_consensus(query: str, models: list, threshold: float = 0.7):
    """
    ì—¬ëŸ¬ ëª¨ë¸ì˜ ì‘ë‹µì„ ë¹„êµí•˜ì—¬ ì¼ì¹˜ë„ê°€ ë†’ì€ ë‹µë³€ì„ ì„ íƒ
    """
    responses = []

    # ê° ëª¨ë¸ì—ì„œ ì‘ë‹µ ìˆ˜ì§‘
    for model in models:
        response = model.invoke(query)
        responses.append(response.content)

    # ì‘ë‹µ ìœ ì‚¬ë„ ë¶„ì„ (ì‹¤ì œë¡œëŠ” ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°)
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ê¸¸ì´ ê¸°ë°˜ ë¹„êµ
    avg_length = sum(len(r) for r in responses) / len(responses)

    # í‰ê·  ê¸¸ì´ì— ê°€ì¥ ê°€ê¹Œìš´ ì‘ë‹µì„ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì„ íƒ
    best_response = min(responses, key=lambda x: abs(len(x) - avg_length))

    return {
        "final_answer": best_response,
        "all_responses": responses,
        "consensus_score": threshold  # ì‹¤ì œë¡œëŠ” ìœ ì‚¬ë„ ì ìˆ˜
    }

# ì‚¬ìš© ì˜ˆì‹œ
models = [gemini_llm, groq_llm, ollama_llm]
result = multi_model_consensus("AIì˜ ë¯¸ë˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œìš”?", models)
print("ìµœì¢… í•©ì˜ ë‹µë³€:", result["final_answer"])
```

### 3. ì‹¤ì‹œê°„ ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
import logging
from datetime import datetime

class LLMPerformanceMonitor:
    def __init__(self):
        self.performance_log = []

    def monitor_request(self, model_name: str, query: str, response_time: float,
                       token_count: int, success: bool):
        """
        ê° ìš”ì²­ì˜ ì„±ëŠ¥ ì§€í‘œë¥¼ ê¸°ë¡
        """
        log_entry = {
            "timestamp": datetime.now(),
            "model": model_name,
            "query_length": len(query),
            "response_time": response_time,
            "token_count": token_count,
            "success": success,
            "tokens_per_second": token_count / response_time if response_time > 0 else 0
        }

        self.performance_log.append(log_entry)

        # ì„±ëŠ¥ ì„ê³„ê°’ ì²´í¬
        if response_time > 10.0:  # 10ì´ˆ ì´ˆê³¼ì‹œ ê²½ê³ 
            logging.warning(f"{model_name} ì‘ë‹µ ì‹œê°„ ì§€ì—°: {response_time}ì´ˆ")

    def get_performance_report(self, model_name: str = None):
        """
        ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
        """
        if model_name:
            logs = [log for log in self.performance_log if log["model"] == model_name]
        else:
            logs = self.performance_log

        if not logs:
            return "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        avg_response_time = sum(log["response_time"] for log in logs) / len(logs)
        success_rate = sum(1 for log in logs if log["success"]) / len(logs)

        return {
            "ì´_ìš”ì²­ìˆ˜": len(logs),
            "í‰ê· _ì‘ë‹µì‹œê°„": round(avg_response_time, 2),
            "ì„±ê³µë¥ ": f"{success_rate:.1%}",
            "í‰ê· _í† í°_ì†ë„": round(sum(log["tokens_per_second"] for log in logs) / len(logs), 2)
        }

# ì‚¬ìš© ì˜ˆì‹œ
monitor = LLMPerformanceMonitor()

# ìš”ì²­ ëª¨ë‹ˆí„°ë§
start_time = time.time()
response = gemini_llm.invoke("í…ŒìŠ¤íŠ¸ ì§ˆë¬¸")
end_time = time.time()

monitor.monitor_request(
    model_name="Gemini",
    query="í…ŒìŠ¤íŠ¸ ì§ˆë¬¸",
    response_time=end_time - start_time,
    token_count=len(response.content.split()),
    success=True
)

# ì„±ëŠ¥ ë¦¬í¬íŠ¸ í™•ì¸
report = monitor.get_performance_report("Gemini")
print("ì„±ëŠ¥ ë¦¬í¬íŠ¸:", report)
```

### 4. ìë™ ëª¨ë¸ ì„ íƒ ìµœì í™”

```python
class AdaptiveLLMSelector:
    def __init__(self):
        self.model_stats = {
            "gemini": {"success_rate": 0.95, "avg_time": 2.1, "cost_per_token": 0.0001},
            "groq": {"success_rate": 0.90, "avg_time": 0.8, "cost_per_token": 0.0002},
            "ollama": {"success_rate": 0.85, "avg_time": 3.5, "cost_per_token": 0.0}
        }

    def select_optimal_model(self, query_complexity: str, budget_constraint: float):
        """
        ì¿¼ë¦¬ ë³µì¡ë„ì™€ ì˜ˆì‚° ì œì•½ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ
        """
        if budget_constraint == 0:  # ë¬´ë£Œ ì‚¬ìš©ë§Œ ê°€ëŠ¥
            return "ollama"

        if query_complexity == "simple":
            # ë‹¨ìˆœí•œ ì¿¼ë¦¬ëŠ” ë¹ ë¥¸ ëª¨ë¸ ìš°ì„ 
            return "groq"
        elif query_complexity == "complex":
            # ë³µì¡í•œ ì¿¼ë¦¬ëŠ” ì •í™•ë„ ìš°ì„ 
            return "gemini"
        else:
            # ê· í˜•ì¡íŒ ì„ íƒ
            scores = {}
            for model, stats in self.model_stats.items():
                # ì„±ê³µë¥ , ì†ë„, ë¹„ìš©ì„ ì¢…í•©í•œ ì ìˆ˜ ê³„ì‚°
                score = (stats["success_rate"] * 0.4 +
                        (1/stats["avg_time"]) * 0.3 +
                        (1-stats["cost_per_token"]*10000) * 0.3)
                scores[model] = score

            return max(scores, key=scores.get)

# ì‚¬ìš© ì˜ˆì‹œ
selector = AdaptiveLLMSelector()
optimal_model = selector.select_optimal_model("complex", 0.01)
print(f"ì¶”ì²œ ëª¨ë¸: {optimal_model}")
```

## ğŸ“– ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [OpenAI API Documentation](https://platform.openai.com/docs)
- [Google AI Studio](https://ai.google.dev/)
- [Groq API Reference](https://console.groq.com/docs)
- [Ollama Documentation](https://ollama.com/)
- [LangChain LLM Integration](https://python.langchain.com/docs/integrations/llms/)

### ì„±ëŠ¥ ë¹„êµ ë° ë²¤ì¹˜ë§ˆí¬
- [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [LLM Arena Chatbot](https://lmsys.org/blog/2023-05-03-arena/)

### ì¶”ê°€ í•™ìŠµ ìë£Œ
- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/)
- [RAG ì‹œìŠ¤í…œ ìµœì í™” ê°€ì´ë“œ](https://python.langchain.com/docs/use_cases/question_answering/)
- [í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤](https://platform.openai.com/docs/guides/prompt-engineering)

ì´ ê°€ì´ë“œë¥¼ í†µí•´ ë‹¤ì–‘í•œ LLM ê³µê¸‰ìì˜ íŠ¹ì„±ì„ ì´í•´í•˜ê³ , í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê¸°ë¥´ì‹œê¸° ë°”ëë‹ˆë‹¤.