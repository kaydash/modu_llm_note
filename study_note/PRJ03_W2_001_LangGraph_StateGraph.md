# LangGraph StateGraph í™œìš© - ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° êµ¬í˜„

## ğŸ“š í•™ìŠµ ëª©í‘œ

- **StateGraphì˜ ê¸°ë³¸ êµ¬ì¡°**ë¥¼ ì´í•´í•˜ê³  State, Node, Edgeë¥¼ í™œìš©í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤
- **ì¡°ê±´ë¶€ ì—£ì§€(Conditional Edge)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  ë¼ìš°íŒ… ì‹œìŠ¤í…œì„ ì„¤ê³„í•  ìˆ˜ ìˆë‹¤
- **Command ê°ì²´**ë¥¼ í™œìš©í•˜ì—¬ ìƒíƒœ ì—…ë°ì´íŠ¸ì™€ íë¦„ ì œì–´ë¥¼ ë™ì‹œì— ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤
- **invoke vs stream** ì‹¤í–‰ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ì´í•´í•˜ê³  ìƒí™©ì— ë§ê²Œ ì„ íƒí•  ìˆ˜ ìˆë‹¤
- **ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œ**ì—ì„œ StateGraphë¥¼ í™œìš©í•œ ë¼ìš°íŒ… ë¡œì§ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤

## ğŸ”‘ í•µì‹¬ ê°œë…

### StateGraphë€?

**StateGraph**ëŠ” LangGraphì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¡œ, **ìƒíƒœ ê¸°ë°˜ì˜ ê·¸ë˜í”„ êµ¬ì¡°**ë¥¼ í†µí•´ ë³µì¡í•œ ëŒ€í™” íë¦„ê³¼ ë°ì´í„° ì²˜ë¦¬ ê³¼ì •ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

#### ì£¼ìš” êµ¬ì„± ìš”ì†Œ

1. **State (ìƒíƒœ)**
   - ê·¸ë˜í”„ì—ì„œ ì²˜ë¦¬í•˜ëŠ” ë°ì´í„°ì˜ ê¸°ë³¸ êµ¬ì¡°
   - TypedDictë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…í™•í•œ íƒ€ì… ì •ì˜
   - ê° ë…¸ë“œê°€ ìƒíƒœë¥¼ ì½ê³  ì—…ë°ì´íŠ¸

2. **Node (ë…¸ë“œ)**
   - ë…ë¦½ì ì¸ ì‘ì—… ë‹¨ìœ„ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
   - ìƒíƒœë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì²˜ë¦¬í•˜ê³  ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ë°˜í™˜
   - ê° ë…¸ë“œëŠ” íŠ¹ì • ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ìº¡ìŠí™”

3. **Edge (ì—£ì§€)**
   - ë…¸ë“œ ê°„ì˜ ì—°ê²° ê²½ë¡œë¥¼ ì •ì˜
   - ë‹¨ìˆœ ì—£ì§€: ë¬´ì¡°ê±´ì ì¸ íë¦„ ì „í™˜
   - ì¡°ê±´ë¶€ ì—£ì§€: ìƒíƒœì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê²½ë¡œ ê²°ì •

4. **Command ê°ì²´**
   - ìƒíƒœ ì—…ë°ì´íŠ¸ì™€ íë¦„ ì œì–´ë¥¼ ë™ì‹œì— ìˆ˜í–‰
   - `goto`ë¡œ ë‹¤ìŒ ë…¸ë“œ ì§€ì •, `update`ë¡œ ìƒíƒœ ë³€ê²½
   - ì¡°ê±´ë¶€ ì—£ì§€ì˜ ëŒ€ì•ˆìœ¼ë¡œ ë” ê°„ê²°í•œ ì½”ë“œ ì‘ì„± ê°€ëŠ¥

### StateGraph vs ê¸°ì¡´ ì²´ì¸ ë°©ì‹

| êµ¬ë¶„ | ê¸°ì¡´ ì²´ì¸ | StateGraph |
|------|----------|------------|
| íë¦„ ì œì–´ | ì„ í˜•ì , ê³ ì •ì  | ë™ì , ì¡°ê±´ë¶€ ë¶„ê¸° ê°€ëŠ¥ |
| ìƒíƒœ ê´€ë¦¬ | ì•”ë¬µì  | ëª…ì‹œì , TypedDictë¡œ ì •ì˜ |
| ë³µì¡ë„ | ë‹¨ìˆœí•œ íŒŒì´í”„ë¼ì¸ | ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° |
| ë””ë²„ê¹… | ì–´ë ¤ì›€ | ì‹œê°í™” ë° ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìš©ì´ |
| ì¬ì‚¬ìš©ì„± | ë‚®ìŒ | ë†’ìŒ (ë…¸ë“œ ë‹¨ìœ„ ì¬ì‚¬ìš©) |

### ì–¸ì œ StateGraphë¥¼ ì‚¬ìš©í•˜ë‚˜?

- **ì¡°ê±´ë¶€ ë¶„ê¸°ê°€ í•„ìš”í•œ ê²½ìš°**: ì‚¬ìš©ì ì…ë ¥ì´ë‚˜ ìƒíƒœì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬ ê²½ë¡œ í•„ìš”
- **ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°**: ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì²˜ë¦¬ì™€ ê²€ì¦ì´ í•„ìš”í•œ ì‹œìŠ¤í…œ
- **ìƒíƒœ ì¶”ì ì´ ì¤‘ìš”í•œ ê²½ìš°**: ê° ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬í•´ì•¼ í•˜ëŠ” ìƒí™©
- **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ**: ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤

## ğŸ›  í™˜ê²½ ì„¤ì •

### 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
# LangGraph ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install langgraph langchain-openai langchain-chroma langchain-core

# í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
pip install python-dotenv

# ì–¸ì–´ ê°ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì‹¤ìŠµìš©)
pip install langdetect
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  OpenAI API í‚¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤:

```bash
OPENAI_API_KEY=your_openai_api_key_here
```

### 3. ê¸°ë³¸ ì„¤ì • ì½”ë“œ

```python
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
from pprint import pprint
from typing import TypedDict, Literal, List

# LangChain ë° LangGraph
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langgraph.graph import StateGraph, START, END
from langgraph.types import Command

# ì‹œê°í™”
from IPython.display import Image, display
```

## ğŸ’» ë‹¨ê³„ë³„ êµ¬í˜„

### 1ë‹¨ê³„: ê¸°ë³¸ StateGraph êµ¬ì„±

#### State ì •ì˜

StateëŠ” ê·¸ë˜í”„ì—ì„œ ì²˜ë¦¬í•˜ëŠ” ë°ì´í„°ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ì •ì˜í•©ë‹ˆë‹¤. TypedDictë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…í™•í•œ íƒ€ì…ì„ ì§€ì •í•©ë‹ˆë‹¤.

```python
from typing import TypedDict

# ìƒíƒœ ì •ì˜ - ë¬¸ì„œ ìš”ì•½ ì‹œìŠ¤í…œ ì˜ˆì œ
class State(TypedDict):
    original_text: str   # ì›ë³¸ í…ìŠ¤íŠ¸
    summary: str         # ìš”ì•½ë³¸
    final_summary: str   # ìµœì¢… ìš”ì•½ë³¸
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- TypedDictë¡œ íƒ€ì… ì•ˆì •ì„± í™•ë³´
- ê° í•„ë“œëŠ” ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
- ë‹¤ë¥¸ ë…¸ë“œ ê°„ ë°ì´í„° ê³µìœ ì˜ ì¤‘ì‹¬

#### Node í•¨ìˆ˜ ì‘ì„±

ë…¸ë“œëŠ” ìƒíƒœë¥¼ ì…ë ¥ë°›ì•„ ì²˜ë¦¬í•˜ê³  ì—…ë°ì´íŠ¸ëœ ìƒíƒœë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

```python
from langchain_openai import ChatOpenAI

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm = ChatOpenAI(model="gpt-4.1-mini")

def generate_summary(state: State):
    """ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” ë…¸ë“œ"""
    prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•µì‹¬ ë‚´ìš© ì¤‘ì‹¬ìœ¼ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:

    [í…ìŠ¤íŠ¸]
    {state['original_text']}

    [ìš”ì•½]
    """
    response = llm.invoke(prompt)

    # ìƒíƒœ ì—…ë°ì´íŠ¸ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    return {"summary": response.content}
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- ë…¸ë“œ í•¨ìˆ˜ëŠ” `state` íŒŒë¼ë¯¸í„°ë¥¼ ë°›ìŒ
- ë°˜í™˜ê°’ì€ ì—…ë°ì´íŠ¸í•  ìƒíƒœ í•„ë“œì˜ ë”•ì…”ë„ˆë¦¬
- ê¸°ì¡´ ìƒíƒœëŠ” ìë™ìœ¼ë¡œ ë³‘í•©ë¨

#### Graph êµ¬ì„± ë° ì»´íŒŒì¼

```python
from langgraph.graph import StateGraph, START, END

# StateGraph ê°ì²´ ìƒì„±
workflow = StateGraph(State)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("generate_summary", generate_summary)

# ì—£ì§€ ì¶”ê°€: START -> generate_summary -> END
workflow.add_edge(START, "generate_summary")
workflow.add_edge("generate_summary", END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
graph = workflow.compile()

# ê·¸ë˜í”„ ì‹œê°í™”
display(Image(graph.get_graph().draw_mermaid_png()))
```

**ê·¸ë˜í”„ êµ¬ì¡°:**
```
START â†’ generate_summary â†’ END
```

#### ì‹¤í–‰: invoke ë°©ì‹

`invoke`ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ì‹¤í–‰ ë°©ì‹ìœ¼ë¡œ, ì „ì²´ ì²˜ë¦¬ê°€ ì™„ë£Œëœ í›„ ìµœì¢… ê²°ê³¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
# ì´ˆê¸° ìƒíƒœ ì„¤ì •
text = """
ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„° ê³¼í•™ì˜ í•œ ë¶„ì•¼ë¡œ, ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥ê³¼ ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥,
ìì—°ì–¸ì–´ì˜ ì´í•´ëŠ¥ë ¥ ë“±ì„ ì»´í“¨í„° í”„ë¡œê·¸ë¨ìœ¼ë¡œ ì‹¤í˜„í•œ ê¸°ìˆ ì´ë‹¤.
ìµœê·¼ì—ëŠ” ê¸°ê³„í•™ìŠµê³¼ ë”¥ëŸ¬ë‹ì˜ ë°œì „ìœ¼ë¡œ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆë‹¤.
"""

initial_state = {
    "original_text": text,
}

# ê·¸ë˜í”„ ì‹¤í–‰
final_state = graph.invoke(initial_state)

# ê²°ê³¼ ì¶œë ¥
for key, value in final_state.items():
    print(f"{key}: {value}")
```

**ì˜ˆìƒ ì¶œë ¥:**
```
original_text: ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„° ê³¼í•™ì˜ í•œ ë¶„ì•¼ë¡œ...
summary: ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµ, ì¶”ë¡ , ì§€ê°, ìì—°ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ì„ ì»´í“¨í„°ë¡œ êµ¬í˜„í•œ ê¸°ìˆ ë¡œ, ìµœê·¼ ê¸°ê³„í•™ìŠµê³¼ ë”¥ëŸ¬ë‹ì˜ ë°œì „ìœ¼ë¡œ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆë‹¤.
```

### 2ë‹¨ê³„: ì¡°ê±´ë¶€ ì—£ì§€ í™œìš©

ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì‚¬ìš©í•˜ë©´ ìƒíƒœì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë‹¤ìŒ ë…¸ë“œë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### í’ˆì§ˆ ê²€ì‚¬ í•¨ìˆ˜ ì‘ì„±

```python
from typing import Literal

def check_summary_quality(state: State) -> Literal["needs_improvement", "good"]:
    """ìš”ì•½ì˜ í’ˆì§ˆì„ ì²´í¬í•˜ê³  ê°œì„ ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜"""
    prompt = f"""ë‹¤ìŒ ìš”ì•½ì˜ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.
    ìš”ì•½ì´ ëª…í™•í•˜ê³  í•µì‹¬ì„ ì˜ ì „ë‹¬í•˜ë©´ 'good'ì„,
    ê°œì„ ì´ í•„ìš”í•˜ë©´ 'needs_improvement'ë¥¼ ì‘ë‹µí•´ì£¼ì„¸ìš”.

    ìš”ì•½ë³¸: {state['summary']}
    """
    response = llm.invoke(prompt).content.lower().strip()

    if "good" in response:
        print("âœ… í’ˆì§ˆ ê²€ì‚¬ í†µê³¼")
        return "good"
    else:
        print("âš ï¸ ê°œì„  í•„ìš”")
        return "needs_improvement"
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- ë°˜í™˜ íƒ€ì…ì€ `Literal`ë¡œ ê°€ëŠ¥í•œ ê²½ë¡œë¥¼ ëª…ì‹œ
- ë°˜í™˜ê°’ì€ ì¡°ê±´ë¶€ ì—£ì§€ì˜ ë§¤í•‘ í‚¤ì™€ ì¼ì¹˜í•´ì•¼ í•¨

#### ì¶”ê°€ ë…¸ë“œ ì‘ì„±

```python
def improve_summary(state: State):
    """ìš”ì•½ì„ ê°œì„ í•˜ê³  ë‹¤ë“¬ëŠ” ë…¸ë“œ"""
    prompt = f"""ë‹¤ìŒ ìš”ì•½ì„ ë” ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ê°œì„ í•´ì£¼ì„¸ìš”:

    ìš”ì•½ë³¸: {state['summary']}
    """
    response = llm.invoke(prompt)

    return {"final_summary": response.content}

def finalize_summary(state: State):
    """í˜„ì¬ ìš”ì•½ì„ ìµœì¢… ìš”ì•½ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ë…¸ë“œ"""
    return {"final_summary": state["summary"]}
```

#### ì¡°ê±´ë¶€ ì—£ì§€ê°€ í¬í•¨ëœ Graph êµ¬ì„±

```python
# ì›Œí¬í”Œë¡œìš° êµ¬ì„±
workflow = StateGraph(State)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("generate_summary", generate_summary)
workflow.add_node("improve_summary", improve_summary)
workflow.add_node("finalize_summary", finalize_summary)

# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
workflow.add_conditional_edges(
    "generate_summary",          # ì‹œì‘ ë…¸ë“œ
    check_summary_quality,       # ì¡°ê±´ íŒë‹¨ í•¨ìˆ˜
    {
        "needs_improvement": "improve_summary",  # ê°œì„  í•„ìš” ì‹œ
        "good": "finalize_summary"               # í’ˆì§ˆ í†µê³¼ ì‹œ
    }
)

# ê¸°ë³¸ ì—£ì§€ ì¶”ê°€
workflow.add_edge(START, "generate_summary")
workflow.add_edge("improve_summary", END)
workflow.add_edge("finalize_summary", END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
graph = workflow.compile()

# ê·¸ë˜í”„ ì‹œê°í™”
display(Image(graph.get_graph().draw_mermaid_png()))
```

**ê·¸ë˜í”„ êµ¬ì¡°:**
```
                    â”Œâ”€ check_summary_quality â”€â”
START â†’ generate_summary                       â†“
                    â”œâ”€â†’ improve_summary â†’ END
                    â””â”€â†’ finalize_summary â†’ END
```

### 3ë‹¨ê³„: Stream ì‹¤í–‰ ë°©ì‹

Stream ë°©ì‹ì€ ê·¸ë˜í”„ ì‹¤í–‰ì˜ ì¤‘ê°„ ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆì–´ ë””ë²„ê¹…ê³¼ ëª¨ë‹ˆí„°ë§ì— ìœ ìš©í•©ë‹ˆë‹¤.

#### stream_mode="values"

ê° ë‹¨ê³„ì—ì„œì˜ ì „ì²´ ìƒíƒœ ê°’ì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

```python
# values ëª¨ë“œ: ì „ì²´ ìƒíƒœ ê°’ í™•ì¸
for chunk in graph.stream(initial_state, stream_mode="values"):
    print("=== í˜„ì¬ ìƒíƒœ ===")
    pprint(chunk)
    print()
```

**ì˜ˆìƒ ì¶œë ¥:**
```
=== í˜„ì¬ ìƒíƒœ ===
{'original_text': 'ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„° ê³¼í•™ì˜...'}

=== í˜„ì¬ ìƒíƒœ ===
{'original_text': 'ì¸ê³µì§€ëŠ¥(AI)ì€...', 'summary': 'ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµ...'}

=== í˜„ì¬ ìƒíƒœ ===
{'original_text': '...', 'summary': '...', 'final_summary': 'ì¸ê³µì§€ëŠ¥(AI)ì€...'}
```

#### stream_mode="updates"

ì–´ë–¤ ë…¸ë“œê°€ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í–ˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë””ë²„ê¹…ìš©).

```python
# updates ëª¨ë“œ: ë…¸ë“œë³„ ì—…ë°ì´íŠ¸ ë‚´ì—­ í™•ì¸
for chunk in graph.stream(initial_state, stream_mode="updates"):
    print("=== ë…¸ë“œ ì—…ë°ì´íŠ¸ ===")
    pprint(chunk)
    print()
```

**ì˜ˆìƒ ì¶œë ¥:**
```
=== ë…¸ë“œ ì—…ë°ì´íŠ¸ ===
{'generate_summary': {'summary': 'ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµ...'}}

=== ë…¸ë“œ ì—…ë°ì´íŠ¸ ===
{'finalize_summary': {'final_summary': 'ì¸ê³µì§€ëŠ¥(AI)ì€...'}}
```

**invoke vs stream ì„ íƒ ê¸°ì¤€:**

| ìƒí™© | ê¶Œì¥ ë°©ì‹ | ì´ìœ  |
|------|----------|------|
| ë‹¨ìˆœ ê²°ê³¼ë§Œ í•„ìš” | invoke | ê°„ê²°í•˜ê³  ë¹ ë¦„ |
| ì§„í–‰ ìƒí™© í‘œì‹œ | stream (values) | ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ |
| ë””ë²„ê¹… | stream (updates) | ë…¸ë“œë³„ ì¶”ì  ê°€ëŠ¥ |
| ì‹¤ì‹œê°„ ì‘ë‹µ | stream | ì ì§„ì  í”¼ë“œë°± |

### 4ë‹¨ê³„: Command ê°ì²´ í™œìš©

Command ê°ì²´ëŠ” ìƒíƒœ ì—…ë°ì´íŠ¸ì™€ íë¦„ ì œì–´ë¥¼ ë™ì‹œì— ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ ì¡°ê±´ë¶€ ì—£ì§€ë³´ë‹¤ ê°„ê²°í•œ ì½”ë“œ ì‘ì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

#### Command ê¸°ë°˜ ë…¸ë“œ ì‘ì„±

```python
from langgraph.types import Command

def generate_summary_with_command(state: State) -> Command[Literal["improve_summary", "finalize_summary"]]:
    """ìš”ì•½ ìƒì„± ë° í’ˆì§ˆ í‰ê°€ë¥¼ í•œ ë²ˆì— ìˆ˜í–‰í•˜ëŠ” ë…¸ë“œ"""

    # 1. ìš”ì•½ ìƒì„±
    summary_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•µì‹¬ ë‚´ìš© ì¤‘ì‹¬ìœ¼ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:
    [í…ìŠ¤íŠ¸]
    {state['original_text']}
    [ìš”ì•½]
    """
    summary = llm.invoke(summary_prompt).content

    # 2. í’ˆì§ˆ í‰ê°€
    eval_prompt = f"""ë‹¤ìŒ ìš”ì•½ì˜ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.
    ìš”ì•½ì´ ëª…í™•í•˜ê³  í•µì‹¬ì„ ì˜ ì „ë‹¬í•˜ë©´ 'good'ì„,
    ê°œì„ ì´ í•„ìš”í•˜ë©´ 'needs_improvement'ë¥¼ ì‘ë‹µí•´ì£¼ì„¸ìš”.

    ìš”ì•½ë³¸: {summary}
    """
    quality = llm.invoke(eval_prompt).content.lower().strip()

    # 3. Commandë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸ì™€ ë¼ìš°íŒ…ì„ ë™ì‹œì— ìˆ˜í–‰
    return Command(
        goto="finalize_summary" if "good" in quality else "improve_summary",
        update={"summary": summary}
    )

def improve_summary_with_command(state: State) -> Command[Literal[END]]:
    """ìš”ì•½ì„ ê°œì„ í•˜ëŠ” ë…¸ë“œ (Command ì‚¬ìš©)"""
    prompt = f"""ë‹¤ìŒ ìš”ì•½ì„ ë” ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ê°œì„ í•´ì£¼ì„¸ìš”:
    [ê¸°ì¡´ ìš”ì•½]
    {state['summary']}
    [ê°œì„  ìš”ì•½]
    """
    improved_summary = llm.invoke(prompt).content

    return Command(
        goto=END,
        update={"final_summary": improved_summary}
    )

def finalize_summary_with_command(state: State) -> Command[Literal[END]]:
    """í˜„ì¬ ìš”ì•½ì„ ìµœì¢… ìš”ì•½ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ë…¸ë“œ (Command ì‚¬ìš©)"""
    return Command(
        goto=END,
        update={"final_summary": state["summary"]}
    )
```

#### Command ê¸°ë°˜ Graph êµ¬ì„±

```python
# ì›Œí¬í”Œë¡œìš° êµ¬ì„±
workflow = StateGraph(State)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("generate_summary", generate_summary_with_command)
workflow.add_node("improve_summary", improve_summary_with_command)
workflow.add_node("finalize_summary", finalize_summary_with_command)

# ê¸°ë³¸ ì—£ì§€ë§Œ ì¶”ê°€ (Commandê°€ ë¼ìš°íŒ… ë‹´ë‹¹)
workflow.add_edge(START, "generate_summary")

# ê·¸ë˜í”„ ì»´íŒŒì¼
graph = workflow.compile()
```

**Command vs ì¡°ê±´ë¶€ ì—£ì§€ ë¹„êµ:**

| êµ¬ë¶„ | ì¡°ê±´ë¶€ ì—£ì§€ | Command ê°ì²´ |
|------|-----------|-------------|
| ë¡œì§ ìœ„ì¹˜ | ë…¸ë“œ ì™¸ë¶€ (ë³„ë„ í•¨ìˆ˜) | ë…¸ë“œ ë‚´ë¶€ |
| ì½”ë“œ ê°„ê²°ì„± | ë‚®ìŒ (ë¶„ë¦¬ëœ í•¨ìˆ˜) | ë†’ìŒ (í†µí•©) |
| ìƒíƒœ ì—…ë°ì´íŠ¸ | ë…¸ë“œ ë°˜í™˜ + ì—£ì§€ í•¨ìˆ˜ | Command í•œ ë²ˆì— |
| ì •ë³´ ì „ë‹¬ | ì œí•œì  | ìœ ì—°í•¨ |
| ì‚¬ìš© ì‹œê¸° | ë‹¨ìˆœ ë¶„ê¸° | ë³µì¡í•œ ë¡œì§ + ìƒíƒœ ì—…ë°ì´íŠ¸ |

## ğŸ¯ ì‹¤ìŠµ ë¬¸ì œ

### ë¬¸ì œ 1: ê¸°ë³¸ StateGraph - ë¬¸ì„œ ë²ˆì—­ ì‹œìŠ¤í…œ (ë‚œì´ë„: â­â­)

**ìš”êµ¬ì‚¬í•­:**
ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œ ë²ˆì—­ ì‹œìŠ¤í…œì„ StateGraphë¡œ êµ¬í˜„í•˜ì„¸ìš”.

1. State ì •ì˜:
   - `original_text`: ì›ë³¸ í…ìŠ¤íŠ¸
   - `detected_language`: ê°ì§€ëœ ì–¸ì–´
   - `translated_text`: ë²ˆì—­ëœ í…ìŠ¤íŠ¸

2. ë…¸ë“œ êµ¬ì„±:
   - `detect_language`: ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ ê°ì§€
   - `translate_to_english`: ì˜ì–´ë¡œ ë²ˆì—­
   - `translate_to_korean`: í•œêµ­ì–´ë¡œ ë²ˆì—­

3. íë¦„:
   - ì–¸ì–´ ê°ì§€ â†’ ì˜ì–´ë©´ í•œêµ­ì–´ë¡œ ë²ˆì—­, í•œêµ­ì–´ë©´ ì˜ì–´ë¡œ ë²ˆì—­

**íŒíŠ¸:**
```python
# State ì •ì˜ ì˜ˆì‹œ
class TranslationState(TypedDict):
    original_text: str
    detected_language: str
    translated_text: str
```

---

### ë¬¸ì œ 2: ì¡°ê±´ë¶€ ì—£ì§€ - ë‹¤ë‹¨ê³„ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ (ë‚œì´ë„: â­â­â­)

**ìš”êµ¬ì‚¬í•­:**
ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”.

1. State ì •ì˜:
   - `document`: ë¬¸ì„œ ë‚´ìš©
   - `grammar_score`: ë¬¸ë²• ì ìˆ˜ (0-100)
   - `clarity_score`: ëª…í™•ì„± ì ìˆ˜ (0-100)
   - `final_document`: ìµœì¢… ë¬¸ì„œ
   - `revision_count`: ìˆ˜ì • íšŸìˆ˜

2. ë…¸ë“œ êµ¬ì„±:
   - `check_grammar`: ë¬¸ë²• ì ìˆ˜ ê³„ì‚°
   - `check_clarity`: ëª…í™•ì„± ì ìˆ˜ ê³„ì‚°
   - `improve_grammar`: ë¬¸ë²• ê°œì„ 
   - `improve_clarity`: ëª…í™•ì„± ê°œì„ 
   - `finalize`: ìµœì¢… ìŠ¹ì¸

3. íë¦„:
   - ë¬¸ë²• ê²€ì‚¬ â†’ 70ì  ë¯¸ë§Œì´ë©´ ë¬¸ë²• ê°œì„ , 70ì  ì´ìƒì´ë©´ ëª…í™•ì„± ê²€ì‚¬
   - ëª…í™•ì„± ê²€ì‚¬ â†’ 70ì  ë¯¸ë§Œì´ë©´ ëª…í™•ì„± ê°œì„ , 70ì  ì´ìƒì´ë©´ ìµœì¢… ìŠ¹ì¸
   - ê°œì„  í›„ ë‹¤ì‹œ í•´ë‹¹ ê²€ì‚¬ë¡œ ëŒì•„ê°€ê¸° (ìµœëŒ€ 3íšŒ)

**íŒíŠ¸:**
```python
def check_grammar(state: TranslationState) -> Literal["improve_grammar", "check_clarity"]:
    # ë¬¸ë²• ì ìˆ˜ ê³„ì‚° ë¡œì§
    if state['grammar_score'] < 70:
        return "improve_grammar"
    return "check_clarity"
```

---

### ë¬¸ì œ 3: Command í™œìš© - ìŠ¤ë§ˆíŠ¸ ê³ ê° ì§€ì› ë¼ìš°í„° (ë‚œì´ë„: â­â­â­)

**ìš”êµ¬ì‚¬í•­:**
Command ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ê° ë¬¸ì˜ë¥¼ ì ì ˆí•œ ë¶€ì„œë¡œ ë¼ìš°íŒ…í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”.

1. State ì •ì˜:
   - `customer_query`: ê³ ê° ë¬¸ì˜
   - `category`: ë¬¸ì˜ ì¹´í…Œê³ ë¦¬ (ê¸°ìˆ ì§€ì›/ê²°ì œ/ì¼ë°˜)
   - `priority`: ìš°ì„ ìˆœìœ„ (ê¸´ê¸‰/ë³´í†µ/ë‚®ìŒ)
   - `assigned_department`: ë°°ì •ëœ ë¶€ì„œ
   - `response`: ì‘ë‹µ

2. ë…¸ë“œ êµ¬ì„±:
   - `categorize_query`: ë¬¸ì˜ ë¶„ë¥˜ ë° ìš°ì„ ìˆœìœ„ íŒë‹¨
   - `technical_support`: ê¸°ìˆ  ì§€ì› ì‘ë‹µ
   - `billing_support`: ê²°ì œ ì§€ì› ì‘ë‹µ
   - `general_support`: ì¼ë°˜ ì§€ì› ì‘ë‹µ

3. íë¦„:
   - categorize_queryì—ì„œ Commandë¡œ ì¹´í…Œê³ ë¦¬ì™€ ìš°ì„ ìˆœìœ„ë¥¼ ë™ì‹œì— ì—…ë°ì´íŠ¸í•˜ê³  ì ì ˆí•œ ë¶€ì„œë¡œ ë¼ìš°íŒ…
   - ê° ë¶€ì„œ ë…¸ë“œëŠ” ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë‹¤ë¥¸ ì‘ë‹µ ìƒì„±

**íŒíŠ¸:**
```python
def categorize_query(state: SupportState) -> Command[Literal["technical_support", "billing_support", "general_support"]]:
    # LLMìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ì™€ ìš°ì„ ìˆœìœ„ íŒë‹¨
    analysis = llm.invoke(f"ë¶„ì„: {state['customer_query']}")

    return Command(
        goto=determine_department(analysis),
        update={
            "category": extract_category(analysis),
            "priority": extract_priority(analysis)
        }
    )
```

---

### ë¬¸ì œ 4: ì‹¤ì „ í”„ë¡œì íŠ¸ - ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œ ê°œì„  (ë‚œì´ë„: â­â­â­â­)

**ìš”êµ¬ì‚¬í•­:**
ë…¸íŠ¸ë¶ì˜ ì‹¤ìŠµ ë¬¸ì œë¥¼ í™•ì¥í•˜ì—¬ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì„¸ìš”.

1. ì§€ì› ì–¸ì–´ í™•ì¥:
   - í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ 3ê°œ ì–¸ì–´ ì§€ì›
   - ê° ì–¸ì–´ë³„ ë²¡í„° DB êµ¬ì„±

2. í˜¼í•© ì–¸ì–´ ì²˜ë¦¬:
   - í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ ì–¸ì–´ê°€ ì„ì—¬ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
   - ì£¼ ì–¸ì–´ë¥¼ ê°ì§€í•˜ì—¬ í•´ë‹¹ DB ì‚¬ìš©

3. í´ë°±(Fallback) ë©”ì»¤ë‹ˆì¦˜:
   - í•´ë‹¹ ì–¸ì–´ DBì—ì„œ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì–¸ì–´ DBë„ ê²€ìƒ‰
   - ë²ˆì—­ì„ í†µí•œ í¬ë¡œìŠ¤ ì–¸ì–´ ê²€ìƒ‰

4. ì‘ë‹µ í’ˆì§ˆ í–¥ìƒ:
   - ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ê²€ìƒ‰ ìˆ˜í–‰
   - ì—¬ëŸ¬ DBì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±

**íŒíŠ¸:**
```python
class MultilingualRAGState(TypedDict):
    user_query: str
    detected_languages: List[str]  # ê°ì§€ëœ ì–¸ì–´ ëª©ë¡
    primary_language: str
    search_results_ko: List[str]
    search_results_en: List[str]
    search_results_ja: List[str]
    confidence_score: float
    final_answer: str
    fallback_used: bool
```

## âœ… ì†”ë£¨ì…˜ ì˜ˆì‹œ

### ë¬¸ì œ 1 ì†”ë£¨ì…˜: ë¬¸ì„œ ë²ˆì—­ ì‹œìŠ¤í…œ

```python
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI

# State ì •ì˜
class TranslationState(TypedDict):
    original_text: str
    detected_language: str
    translated_text: str

# LLM ì¸ìŠ¤í„´ìŠ¤
llm = ChatOpenAI(model="gpt-4.1-mini")

# ë…¸ë“œ í•¨ìˆ˜ë“¤
def detect_language(state: TranslationState):
    """í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•˜ëŠ” ë…¸ë“œ"""
    prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•˜ì„¸ìš”. 'korean' ë˜ëŠ” 'english'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

    í…ìŠ¤íŠ¸: {state['original_text']}

    ì–¸ì–´:"""

    language = llm.invoke(prompt).content.strip().lower()
    return {"detected_language": language}

def translate_to_english(state: TranslationState):
    """í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ë…¸ë“œ"""
    prompt = f"""ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.

    í•œêµ­ì–´: {state['original_text']}

    ì˜ì–´:"""

    translation = llm.invoke(prompt).content
    return {"translated_text": translation}

def translate_to_korean(state: TranslationState):
    """ì˜ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ë…¸ë“œ"""
    prompt = f"""ë‹¤ìŒ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.

    ì˜ì–´: {state['original_text']}

    í•œêµ­ì–´:"""

    translation = llm.invoke(prompt).content
    return {"translated_text": translation}

# ì¡°ê±´ë¶€ í•¨ìˆ˜
def route_translation(state: TranslationState) -> Literal["translate_to_english", "translate_to_korean"]:
    """ì–¸ì–´ì— ë”°ë¼ ë²ˆì—­ ë°©í–¥ì„ ê²°ì •"""
    if "korean" in state['detected_language']:
        return "translate_to_english"
    else:
        return "translate_to_korean"

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(TranslationState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("detect_language", detect_language)
workflow.add_node("translate_to_english", translate_to_english)
workflow.add_node("translate_to_korean", translate_to_korean)

# ì—£ì§€ ì¶”ê°€
workflow.add_edge(START, "detect_language")
workflow.add_conditional_edges(
    "detect_language",
    route_translation,
    {
        "translate_to_english": "translate_to_english",
        "translate_to_korean": "translate_to_korean"
    }
)
workflow.add_edge("translate_to_english", END)
workflow.add_edge("translate_to_korean", END)

# ì»´íŒŒì¼
translation_graph = workflow.compile()

# í…ŒìŠ¤íŠ¸
test_cases = [
    "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì€ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
    "Artificial intelligence is rapidly advancing."
]

for text in test_cases:
    print(f"\nì›ë³¸: {text}")
    result = translation_graph.invoke({"original_text": text})
    print(f"ê°ì§€ëœ ì–¸ì–´: {result['detected_language']}")
    print(f"ë²ˆì—­: {result['translated_text']}")
```

### ë¬¸ì œ 2 ì†”ë£¨ì…˜: ë‹¤ë‹¨ê³„ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ

```python
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END

# State ì •ì˜
class QualityCheckState(TypedDict):
    document: str
    grammar_score: int
    clarity_score: int
    final_document: str
    revision_count: int

llm = ChatOpenAI(model="gpt-4.1-mini")

# ë…¸ë“œ í•¨ìˆ˜ë“¤
def check_grammar(state: QualityCheckState) -> Literal["improve_grammar", "check_clarity", "finalize"]:
    """ë¬¸ë²•ì„ ê²€ì‚¬í•˜ëŠ” ë…¸ë“œ"""
    # ìµœëŒ€ ìˆ˜ì • íšŸìˆ˜ ì²´í¬
    if state.get('revision_count', 0) >= 3:
        return "finalize"

    prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ ë¬¸ë²•ì„ 0-100ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”. ìˆ«ìë§Œ ì‘ë‹µí•˜ì„¸ìš”.

    ë¬¸ì„œ: {state['document']}

    ì ìˆ˜:"""

    score = int(llm.invoke(prompt).content.strip())

    if score < 70:
        return "improve_grammar"
    return "check_clarity"

def check_clarity(state: QualityCheckState) -> Literal["improve_clarity", "finalize"]:
    """ëª…í™•ì„±ì„ ê²€ì‚¬í•˜ëŠ” ë…¸ë“œ"""
    prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ ëª…í™•ì„±ì„ 0-100ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”. ìˆ«ìë§Œ ì‘ë‹µí•˜ì„¸ìš”.

    ë¬¸ì„œ: {state['document']}

    ì ìˆ˜:"""

    score = int(llm.invoke(prompt).content.strip())

    if score < 70 and state.get('revision_count', 0) < 3:
        return "improve_clarity"
    return "finalize"

def improve_grammar(state: QualityCheckState):
    """ë¬¸ë²•ì„ ê°œì„ í•˜ëŠ” ë…¸ë“œ"""
    prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ ë¬¸ë²•ì„ ê°œì„ í•˜ì„¸ìš”.

    ì›ë³¸ ë¬¸ì„œ: {state['document']}

    ê°œì„ ëœ ë¬¸ì„œ:"""

    improved = llm.invoke(prompt).content
    revision_count = state.get('revision_count', 0) + 1

    return {
        "document": improved,
        "revision_count": revision_count
    }

def improve_clarity(state: QualityCheckState):
    """ëª…í™•ì„±ì„ ê°œì„ í•˜ëŠ” ë…¸ë“œ"""
    prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ë” ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ê°œì„ í•˜ì„¸ìš”.

    ì›ë³¸ ë¬¸ì„œ: {state['document']}

    ê°œì„ ëœ ë¬¸ì„œ:"""

    improved = llm.invoke(prompt).content
    revision_count = state.get('revision_count', 0) + 1

    return {
        "document": improved,
        "revision_count": revision_count
    }

def finalize(state: QualityCheckState):
    """ìµœì¢… ë¬¸ì„œë¥¼ í™•ì •í•˜ëŠ” ë…¸ë“œ"""
    return {"final_document": state['document']}

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(QualityCheckState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("check_grammar", check_grammar)
workflow.add_node("check_clarity", check_clarity)
workflow.add_node("improve_grammar", improve_grammar)
workflow.add_node("improve_clarity", improve_clarity)
workflow.add_node("finalize", finalize)

# ì—£ì§€ ì¶”ê°€
workflow.add_edge(START, "check_grammar")

# ì¡°ê±´ë¶€ ì—£ì§€
workflow.add_conditional_edges(
    "check_grammar",
    check_grammar,
    {
        "improve_grammar": "improve_grammar",
        "check_clarity": "check_clarity",
        "finalize": "finalize"
    }
)

workflow.add_conditional_edges(
    "check_clarity",
    check_clarity,
    {
        "improve_clarity": "improve_clarity",
        "finalize": "finalize"
    }
)

# ê°œì„  í›„ ë‹¤ì‹œ ê²€ì‚¬ë¡œ
workflow.add_edge("improve_grammar", "check_grammar")
workflow.add_edge("improve_clarity", "check_clarity")
workflow.add_edge("finalize", END)

# ì»´íŒŒì¼
quality_graph = workflow.compile()

# í…ŒìŠ¤íŠ¸
test_document = """
AI ê¸°ìˆ ì´ ë°œì „í•˜ê³ ìˆìŒ. ì´ê²ƒì€ ë§ì€ ë¶„ì•¼ì— ì˜í–¥ë¼ì¹¨.
"""

result = quality_graph.invoke({
    "document": test_document,
    "revision_count": 0
})

print(f"ìµœì¢… ë¬¸ì„œ: {result['final_document']}")
print(f"ìˆ˜ì • íšŸìˆ˜: {result['revision_count']}")
```

### ë¬¸ì œ 3 ì†”ë£¨ì…˜: ìŠ¤ë§ˆíŠ¸ ê³ ê° ì§€ì› ë¼ìš°í„° (Command ì‚¬ìš©)

```python
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command

# State ì •ì˜
class SupportState(TypedDict):
    customer_query: str
    category: str
    priority: str
    assigned_department: str
    response: str

llm = ChatOpenAI(model="gpt-4.1-mini")

def categorize_query(state: SupportState) -> Command[Literal["technical_support", "billing_support", "general_support"]]:
    """ë¬¸ì˜ë¥¼ ë¶„ë¥˜í•˜ê³  ìš°ì„ ìˆœìœ„ë¥¼ íŒë‹¨í•˜ëŠ” ë…¸ë“œ"""
    prompt = f"""ë‹¤ìŒ ê³ ê° ë¬¸ì˜ë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

    ê³ ê° ë¬¸ì˜: {state['customer_query']}

    ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
    {{
        "category": "ê¸°ìˆ ì§€ì›|ê²°ì œ|ì¼ë°˜",
        "priority": "ê¸´ê¸‰|ë³´í†µ|ë‚®ìŒ"
    }}
    """

    analysis = llm.invoke(prompt).content

    # ê°„ë‹¨í•œ íŒŒì‹± (ì‹¤ì œë¡œëŠ” json.loads ì‚¬ìš©)
    if "ê¸°ìˆ ì§€ì›" in analysis:
        category = "ê¸°ìˆ ì§€ì›"
        next_node = "technical_support"
    elif "ê²°ì œ" in analysis:
        category = "ê²°ì œ"
        next_node = "billing_support"
    else:
        category = "ì¼ë°˜"
        next_node = "general_support"

    if "ê¸´ê¸‰" in analysis:
        priority = "ê¸´ê¸‰"
    elif "ë‚®ìŒ" in analysis:
        priority = "ë‚®ìŒ"
    else:
        priority = "ë³´í†µ"

    return Command(
        goto=next_node,
        update={
            "category": category,
            "priority": priority,
            "assigned_department": next_node
        }
    )

def technical_support(state: SupportState) -> Command[Literal[END]]:
    """ê¸°ìˆ  ì§€ì› ì‘ë‹µ ìƒì„±"""
    priority_context = {
        "ê¸´ê¸‰": "ì¦‰ì‹œ í•´ê²°í•˜ê² ìŠµë‹ˆë‹¤.",
        "ë³´í†µ": "ë¹ ë¥¸ ì‹œì¼ ë‚´ì— í•´ê²°í•˜ê² ìŠµë‹ˆë‹¤.",
        "ë‚®ìŒ": "ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤."
    }

    prompt = f"""ê¸°ìˆ  ì§€ì› ë‹´ë‹¹ìë¡œì„œ ë‹¤ìŒ ë¬¸ì˜ì— ì‘ë‹µí•˜ì„¸ìš”.
    ìš°ì„ ìˆœìœ„: {state['priority']}

    ê³ ê° ë¬¸ì˜: {state['customer_query']}

    ì‘ë‹µ (ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ):"""

    response = llm.invoke(prompt).content
    response = f"[ê¸°ìˆ ì§€ì›íŒ€ - {state['priority']}]\n{response}\n{priority_context.get(state['priority'], '')}"

    return Command(
        goto=END,
        update={"response": response}
    )

def billing_support(state: SupportState) -> Command[Literal[END]]:
    """ê²°ì œ ì§€ì› ì‘ë‹µ ìƒì„±"""
    prompt = f"""ê²°ì œ ì§€ì› ë‹´ë‹¹ìë¡œì„œ ë‹¤ìŒ ë¬¸ì˜ì— ì‘ë‹µí•˜ì„¸ìš”.
    ìš°ì„ ìˆœìœ„: {state['priority']}

    ê³ ê° ë¬¸ì˜: {state['customer_query']}

    ì‘ë‹µ (ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ):"""

    response = llm.invoke(prompt).content
    response = f"[ê²°ì œì§€ì›íŒ€ - {state['priority']}]\n{response}"

    return Command(
        goto=END,
        update={"response": response}
    )

def general_support(state: SupportState) -> Command[Literal[END]]:
    """ì¼ë°˜ ì§€ì› ì‘ë‹µ ìƒì„±"""
    prompt = f"""ê³ ê° ì§€ì› ë‹´ë‹¹ìë¡œì„œ ë‹¤ìŒ ë¬¸ì˜ì— ì‘ë‹µí•˜ì„¸ìš”.

    ê³ ê° ë¬¸ì˜: {state['customer_query']}

    ì‘ë‹µ (ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ê²Œ):"""

    response = llm.invoke(prompt).content
    response = f"[ê³ ê°ì§€ì›íŒ€ - {state['priority']}]\n{response}"

    return Command(
        goto=END,
        update={"response": response}
    )

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(SupportState)

workflow.add_node("categorize_query", categorize_query)
workflow.add_node("technical_support", technical_support)
workflow.add_node("billing_support", billing_support)
workflow.add_node("general_support", general_support)

workflow.add_edge(START, "categorize_query")

support_graph = workflow.compile()

# í…ŒìŠ¤íŠ¸
test_queries = [
    "ë¡œê·¸ì¸ì´ ì•ˆ ë©ë‹ˆë‹¤. ì§€ê¸ˆ ë‹¹ì¥ í•´ê²°í•´ì£¼ì„¸ìš”!",
    "ì´ë²ˆ ë‹¬ ê²°ì œ ë‚´ì—­ì„ í™•ì¸í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.",
    "ì„œë¹„ìŠ¤ ì‚¬ìš© ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”."
]

for query in test_queries:
    print(f"\n{'='*60}")
    print(f"ë¬¸ì˜: {query}")
    result = support_graph.invoke({"customer_query": query})
    print(f"\nì¹´í…Œê³ ë¦¬: {result['category']}")
    print(f"ìš°ì„ ìˆœìœ„: {result['priority']}")
    print(f"ë°°ì • ë¶€ì„œ: {result['assigned_department']}")
    print(f"\nì‘ë‹µ:\n{result['response']}")
```

### ë¬¸ì œ 4 ì†”ë£¨ì…˜: ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œ ê°œì„ 

```python
from typing import TypedDict, List, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# State ì •ì˜
class MultilingualRAGState(TypedDict):
    user_query: str
    detected_languages: List[str]
    primary_language: str
    search_results_ko: List[str]
    search_results_en: List[str]
    search_results_ja: List[str]
    confidence_score: float
    final_answer: str
    fallback_used: bool

# ë²¡í„° DB ì„¤ì • (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ê° ì–¸ì–´ë³„ DBë¥¼ ë¯¸ë¦¬ êµ¬ì¶•)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# ì‹¤ì œë¡œëŠ” ì´ë ‡ê²Œ ê° ì–¸ì–´ë³„ DBë¥¼ ë¡œë“œ
# db_korean = Chroma(embedding_function=embeddings, persist_directory="./chroma_ko")
# db_english = Chroma(embedding_function=embeddings, persist_directory="./chroma_en")
# db_japanese = Chroma(embedding_function=embeddings, persist_directory="./chroma_ja")

llm = ChatOpenAI(model="gpt-4.1-mini")

def analyze_query_languages(state: MultilingualRAGState) -> Command[Literal["search_primary", "search_all"]]:
    """ì¿¼ë¦¬ì˜ ì–¸ì–´ë¥¼ ë¶„ì„í•˜ê³  ì£¼ ì–¸ì–´ë¥¼ ê²°ì •"""
    prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ì–¸ì–´ë“¤ì„ ë¶„ì„í•˜ì„¸ìš”.

    í…ìŠ¤íŠ¸: {state['user_query']}

    ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
    ì£¼ ì–¸ì–´: korean|english|japanese
    í¬í•¨ëœ ì–¸ì–´: korean, english, japanese (í•´ë‹¹í•˜ëŠ” ê²ƒë§Œ)
    """

    analysis = llm.invoke(prompt).content

    # ê°„ë‹¨í•œ íŒŒì‹±
    if "korean" in analysis.lower():
        primary = "korean"
        detected = ["korean"]
    elif "japanese" in analysis.lower():
        primary = "japanese"
        detected = ["japanese"]
    else:
        primary = "english"
        detected = ["english"]

    # í˜¼í•© ì–¸ì–´ ê°ì§€
    if "," in analysis:
        detected = [lang.strip() for lang in analysis.split("í¬í•¨ëœ ì–¸ì–´:")[-1].split("\n")[0].split(",")]

    # ë‹¨ì¼ ì–¸ì–´ë©´ primary ê²€ìƒ‰, í˜¼í•©ì´ë©´ all ê²€ìƒ‰
    next_node = "search_primary" if len(detected) == 1 else "search_all"

    return Command(
        goto=next_node,
        update={
            "detected_languages": detected,
            "primary_language": primary
        }
    )

def search_primary(state: MultilingualRAGState) -> Command[Literal["evaluate_results", "search_fallback"]]:
    """ì£¼ ì–¸ì–´ DBì—ì„œ ê²€ìƒ‰"""
    primary_lang = state['primary_language']
    query = state['user_query']

    # ì‹¤ì œ ê²€ìƒ‰ (ì—¬ê¸°ì„œëŠ” mock)
    # if primary_lang == "korean":
    #     results = db_korean.similarity_search(query, k=3)
    # elif primary_lang == "japanese":
    #     results = db_japanese.similarity_search(query, k=3)
    # else:
    #     results = db_english.similarity_search(query, k=3)

    # Mock ê²°ê³¼
    results = [f"[{primary_lang.upper()}] Mock result {i+1} for query: {query}" for i in range(3)]

    # ê²°ê³¼ í’ˆì§ˆ í‰ê°€
    confidence = 0.8 if len(results) >= 2 else 0.3

    # ì—…ë°ì´íŠ¸í•  í•„ë“œ ê²°ì •
    update_dict = {"confidence_score": confidence}
    if primary_lang == "korean":
        update_dict["search_results_ko"] = results
    elif primary_lang == "japanese":
        update_dict["search_results_ja"] = results
    else:
        update_dict["search_results_en"] = results

    next_node = "evaluate_results" if confidence > 0.5 else "search_fallback"

    return Command(
        goto=next_node,
        update=update_dict
    )

def search_fallback(state: MultilingualRAGState) -> Command[Literal["evaluate_results"]]:
    """í´ë°±: ë‹¤ë¥¸ ì–¸ì–´ DBì—ì„œë„ ê²€ìƒ‰"""
    query = state['user_query']
    primary = state['primary_language']

    # ë²ˆì—­ ë° ê²€ìƒ‰
    other_langs = ["korean", "english", "japanese"]
    other_langs.remove(primary)

    update_dict = {"fallback_used": True}

    for lang in other_langs:
        # ì‹¤ì œë¡œëŠ” ë²ˆì—­ í›„ ê²€ìƒ‰
        translated_query = f"[TRANSLATED to {lang}] {query}"
        results = [f"[{lang.upper()}] Fallback result {i+1}" for i in range(2)]

        if lang == "korean":
            update_dict["search_results_ko"] = results
        elif lang == "japanese":
            update_dict["search_results_ja"] = results
        else:
            update_dict["search_results_en"] = results

    update_dict["confidence_score"] = 0.6

    return Command(
        goto="evaluate_results",
        update=update_dict
    )

def search_all(state: MultilingualRAGState) -> Command[Literal["evaluate_results"]]:
    """ëª¨ë“  ì–¸ì–´ DBì—ì„œ ê²€ìƒ‰ (í˜¼í•© ì–¸ì–´ ì¿¼ë¦¬)"""
    query = state['user_query']

    # ê° ì–¸ì–´ DBì—ì„œ ê²€ìƒ‰
    results_ko = [f"[KO] Mixed lang result {i+1}" for i in range(2)]
    results_en = [f"[EN] Mixed lang result {i+1}" for i in range(2)]
    results_ja = [f"[JA] Mixed lang result {i+1}" for i in range(2)]

    return Command(
        goto="evaluate_results",
        update={
            "search_results_ko": results_ko,
            "search_results_en": results_en,
            "search_results_ja": results_ja,
            "confidence_score": 0.7
        }
    )

def evaluate_results(state: MultilingualRAGState) -> Command[Literal["generate_answer"]]:
    """ê²€ìƒ‰ ê²°ê³¼ í‰ê°€ ë° ì¢…í•©"""
    # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
    all_results = []
    all_results.extend(state.get('search_results_ko', []))
    all_results.extend(state.get('search_results_en', []))
    all_results.extend(state.get('search_results_ja', []))

    # ì‹ ë¢°ë„ ì¬í‰ê°€
    total_results = len([r for r in all_results if r])
    confidence = min(0.9, state['confidence_score'] + (total_results * 0.05))

    return Command(
        goto="generate_answer",
        update={"confidence_score": confidence}
    )

def generate_answer(state: MultilingualRAGState) -> Command[Literal[END]]:
    """ìµœì¢… ë‹µë³€ ìƒì„±"""
    all_results = []
    all_results.extend(state.get('search_results_ko', []))
    all_results.extend(state.get('search_results_en', []))
    all_results.extend(state.get('search_results_ja', []))

    prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

    ì‚¬ìš©ì ì§ˆë¬¸: {state['user_query']}
    ì£¼ ì–¸ì–´: {state['primary_language']}

    ê²€ìƒ‰ ê²°ê³¼:
    {chr(10).join(all_results)}

    ë‹µë³€ ({state['primary_language']}ë¡œ ì‘ì„±):"""

    answer = llm.invoke(prompt).content

    if state.get('fallback_used', False):
        answer += "\n\n(ì°¸ê³ : ì—¬ëŸ¬ ì–¸ì–´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í–ˆìŠµë‹ˆë‹¤)"

    return Command(
        goto=END,
        update={"final_answer": answer}
    )

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(MultilingualRAGState)

workflow.add_node("analyze_query_languages", analyze_query_languages)
workflow.add_node("search_primary", search_primary)
workflow.add_node("search_fallback", search_fallback)
workflow.add_node("search_all", search_all)
workflow.add_node("evaluate_results", evaluate_results)
workflow.add_node("generate_answer", generate_answer)

workflow.add_edge(START, "analyze_query_languages")

multilingual_rag_graph = workflow.compile()

# í…ŒìŠ¤íŠ¸
test_queries = [
    "í…ŒìŠ¬ë¼ì˜ ì°½ì—…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
    "Who founded Tesla?",
    "Teslaì™€ Rivianì„ ë¹„êµí•´ì£¼ì„¸ìš”",  # í˜¼í•© ì–¸ì–´ (ì˜ì–´ ë‹¨ì–´ + í•œê¸€)
]

for query in test_queries:
    print(f"\n{'='*60}")
    print(f"ì§ˆë¬¸: {query}")
    result = multilingual_rag_graph.invoke({"user_query": query})
    print(f"ê°ì§€ëœ ì–¸ì–´: {result['detected_languages']}")
    print(f"ì£¼ ì–¸ì–´: {result['primary_language']}")
    print(f"ì‹ ë¢°ë„: {result['confidence_score']:.2f}")
    print(f"í´ë°± ì‚¬ìš©: {result.get('fallback_used', False)}")
    print(f"\në‹µë³€:\n{result['final_answer']}")
```

## ğŸš€ ì‹¤ë¬´ í™œìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ê³ ê¸‰ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

ì‹¤ë¬´ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë¬¸ì„œ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°ì…ë‹ˆë‹¤. PDF/Word ë¬¸ì„œë¥¼ ë°›ì•„ ìš”ì•½, ë¶„ë¥˜, í‚¤ì›Œë“œ ì¶”ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
from typing import TypedDict, List, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command

class DocumentProcessingState(TypedDict):
    document_path: str
    document_text: str
    document_type: str  # ê³„ì•½ì„œ, ë³´ê³ ì„œ, ì œì•ˆì„œ ë“±
    summary: str
    key_points: List[str]
    entities: List[dict]  # ì¸ë¬¼, ì¡°ì§, ë‚ ì§œ ë“±
    action_items: List[str]
    metadata: dict

llm = ChatOpenAI(model="gpt-4.1-mini")

def extract_text(state: DocumentProcessingState) -> Command[Literal["classify_document"]]:
    """ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    # ì‹¤ì œë¡œëŠ” PyPDF2, python-docx ë“± ì‚¬ìš©
    with open(state['document_path'], 'r', encoding='utf-8') as f:
        text = f.read()

    return Command(
        goto="classify_document",
        update={"document_text": text}
    )

def classify_document(state: DocumentProcessingState) -> Command[Literal["process_contract", "process_report", "process_proposal"]]:
    """ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜"""
    prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ ìœ í˜•ì„ ë¶„ë¥˜í•˜ì„¸ìš”: ê³„ì•½ì„œ, ë³´ê³ ì„œ, ì œì•ˆì„œ, ê¸°íƒ€

    ë¬¸ì„œ ë‚´ìš© (ì• 500ì):
    {state['document_text'][:500]}

    ìœ í˜•:"""

    doc_type = llm.invoke(prompt).content.strip()

    # ë¬¸ì„œ ìœ í˜•ë³„ ì²˜ë¦¬ ê²½ë¡œ ê²°ì •
    if "ê³„ì•½ì„œ" in doc_type:
        next_node = "process_contract"
    elif "ë³´ê³ ì„œ" in doc_type:
        next_node = "process_report"
    else:
        next_node = "process_proposal"

    return Command(
        goto=next_node,
        update={"document_type": doc_type}
    )

def process_contract(state: DocumentProcessingState) -> Command[Literal["finalize_processing"]]:
    """ê³„ì•½ì„œ íŠ¹í™” ì²˜ë¦¬"""
    text = state['document_text']

    # ê³„ì•½ ì¡°ê±´ ì¶”ì¶œ
    prompt = f"""ë‹¤ìŒ ê³„ì•½ì„œì—ì„œ ì¤‘ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
    1. ê³„ì•½ ë‹¹ì‚¬ì
    2. ê³„ì•½ ê¸°ê°„
    3. ì£¼ìš” ì¡°ê±´
    4. ê¸ˆì•¡/ìˆ˜ëŸ‰

    ê³„ì•½ì„œ:
    {text}

    JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:"""

    extraction = llm.invoke(prompt).content

    # ìš”ì•½ ìƒì„±
    summary_prompt = f"ë‹¤ìŒ ê³„ì•½ì„œë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:\n{text}"
    summary = llm.invoke(summary_prompt).content

    return Command(
        goto="finalize_processing",
        update={
            "summary": summary,
            "key_points": [extraction],
            "entities": [],  # ì‹¤ì œë¡œëŠ” NER ìˆ˜í–‰
            "metadata": {"contract_terms": extraction}
        }
    )

def process_report(state: DocumentProcessingState) -> Command[Literal["finalize_processing"]]:
    """ë³´ê³ ì„œ íŠ¹í™” ì²˜ë¦¬"""
    text = state['document_text']

    # í•µì‹¬ ë‚´ìš© ì¶”ì¶œ
    prompt = f"""ë‹¤ìŒ ë³´ê³ ì„œì—ì„œ:
    1. ì£¼ìš” ë°œê²¬ì‚¬í•­
    2. ë°ì´í„°/í†µê³„
    3. ê²°ë¡  ë° ì œì–¸

    ë³´ê³ ì„œ:
    {text}

    êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬:"""

    analysis = llm.invoke(prompt).content

    # ìš”ì•½
    summary = llm.invoke(f"ë‹¤ìŒ ë³´ê³ ì„œë¥¼ 5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½:\n{text}").content

    return Command(
        goto="finalize_processing",
        update={
            "summary": summary,
            "key_points": [analysis],
            "action_items": []  # ì‹¤ì œë¡œëŠ” ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ
        }
    )

def process_proposal(state: DocumentProcessingState) -> Command[Literal["finalize_processing"]]:
    """ì œì•ˆì„œ íŠ¹í™” ì²˜ë¦¬"""
    text = state['document_text']

    # ì œì•ˆ ë‚´ìš© ë¶„ì„
    prompt = f"""ë‹¤ìŒ ì œì•ˆì„œë¥¼ ë¶„ì„í•˜ì„¸ìš”:
    1. ì œì•ˆ ë°°ê²½
    2. ì œì•ˆ ë‚´ìš©
    3. ê¸°ëŒ€ íš¨ê³¼
    4. ì†Œìš” ì˜ˆì‚°/ì¼ì •

    ì œì•ˆì„œ:
    {text}

    êµ¬ì¡°í™”:"""

    analysis = llm.invoke(prompt).content
    summary = llm.invoke(f"ì œì•ˆì„œ ìš”ì•½:\n{text}").content

    return Command(
        goto="finalize_processing",
        update={
            "summary": summary,
            "key_points": [analysis]
        }
    )

def finalize_processing(state: DocumentProcessingState):
    """ì²˜ë¦¬ ì™„ë£Œ ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
    metadata = {
        "processed_at": "2025-10-31",
        "document_type": state['document_type'],
        "summary_length": len(state['summary']),
        "key_points_count": len(state.get('key_points', []))
    }

    return {"metadata": metadata}

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(DocumentProcessingState)

workflow.add_node("extract_text", extract_text)
workflow.add_node("classify_document", classify_document)
workflow.add_node("process_contract", process_contract)
workflow.add_node("process_report", process_report)
workflow.add_node("process_proposal", process_proposal)
workflow.add_node("finalize_processing", finalize_processing)

workflow.add_edge(START, "extract_text")
workflow.add_edge("finalize_processing", END)

document_pipeline = workflow.compile()

# ì‚¬ìš© ì˜ˆì‹œ
result = document_pipeline.invoke({
    "document_path": "/path/to/contract.txt"
})

print(f"ë¬¸ì„œ ìœ í˜•: {result['document_type']}")
print(f"ìš”ì•½: {result['summary']}")
print(f"í•µì‹¬ ì‚¬í•­: {result['key_points']}")
```

### ì˜ˆì‹œ 2: ì§€ëŠ¥í˜• ê³ ê° ì§€ì› ì‹œìŠ¤í…œ

ì‹¤ì œ ê³ ê° ì§€ì› ì„¼í„°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í‹°ì¼“ ì²˜ë¦¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

```python
from typing import TypedDict, List, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command
from datetime import datetime

class SupportTicketState(TypedDict):
    ticket_id: str
    customer_id: str
    issue_description: str
    category: str
    priority: str
    sentiment: str
    escalation_required: bool
    similar_tickets: List[str]
    suggested_solutions: List[str]
    agent_notes: str
    resolution: str
    satisfaction_score: float

llm = ChatOpenAI(model="gpt-4.1-mini")

def analyze_ticket(state: SupportTicketState) -> Command[Literal["search_knowledge_base", "escalate_immediately"]]:
    """í‹°ì¼“ ë¶„ì„: ì¹´í…Œê³ ë¦¬, ìš°ì„ ìˆœìœ„, ê°ì • ë¶„ì„"""
    issue = state['issue_description']

    analysis_prompt = f"""ê³ ê° ë¬¸ì˜ë¥¼ ë¶„ì„í•˜ì„¸ìš”:

    ë¬¸ì˜: {issue}

    ë‹¤ìŒ í•­ëª©ì„ ë¶„ì„:
    1. ì¹´í…Œê³ ë¦¬: ê¸°ìˆ /ê²°ì œ/ê³„ì •/ê¸°íƒ€
    2. ìš°ì„ ìˆœìœ„: ê¸´ê¸‰/ë†’ìŒ/ë³´í†µ/ë‚®ìŒ
    3. ê°ì •: ë§¤ìš°ë¶ˆë§Œ/ë¶ˆë§Œ/ì¤‘ë¦½/ë§Œì¡±
    4. ì¦‰ì‹œ ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì—¬ë¶€: yes/no

    JSON í˜•ì‹:"""

    analysis = llm.invoke(analysis_prompt).content

    # ê°„ë‹¨í•œ íŒŒì‹±
    category = "ê¸°ìˆ " if "ê¸°ìˆ " in analysis else "ì¼ë°˜"
    priority = "ê¸´ê¸‰" if "ê¸´ê¸‰" in analysis else "ë³´í†µ"
    sentiment = "ë¶ˆë§Œ" if "ë¶ˆë§Œ" in analysis else "ì¤‘ë¦½"
    escalation = "yes" in analysis.lower()

    next_node = "escalate_immediately" if escalation else "search_knowledge_base"

    return Command(
        goto=next_node,
        update={
            "category": category,
            "priority": priority,
            "sentiment": sentiment,
            "escalation_required": escalation
        }
    )

def search_knowledge_base(state: SupportTicketState) -> Command[Literal["generate_solution", "escalate_to_specialist"]]:
    """ìœ ì‚¬ ì‚¬ë¡€ ë° í•´ê²°ì±… ê²€ìƒ‰"""
    issue = state['issue_description']

    # ì‹¤ì œë¡œëŠ” ë²¡í„° DB ê²€ìƒ‰
    similar_tickets = [
        "í‹°ì¼“ #1234: ìœ ì‚¬í•œ ë¡œê·¸ì¸ ë¬¸ì œ - ìºì‹œ ì‚­ì œë¡œ í•´ê²°",
        "í‹°ì¼“ #5678: ë™ì¼ ì¦ìƒ - ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì • í•„ìš”"
    ]

    # í•´ê²°ì±… ìƒì„±
    solution_prompt = f"""ë‹¤ìŒ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ í•´ê²°ì±…ì„ ì œì•ˆí•˜ì„¸ìš”:

    í˜„ì¬ ë¬¸ì˜: {issue}

    ìœ ì‚¬ ì‚¬ë¡€:
    {chr(10).join(similar_tickets)}

    ì œì•ˆ í•´ê²°ì±… (3ê°œ):"""

    solutions = llm.invoke(solution_prompt).content
    suggested_solutions = solutions.split("\n")[:3]

    # ì‹ ë¢°ë„ í‰ê°€
    confidence = 0.8 if len(similar_tickets) >= 2 else 0.4

    next_node = "generate_solution" if confidence > 0.6 else "escalate_to_specialist"

    return Command(
        goto=next_node,
        update={
            "similar_tickets": similar_tickets,
            "suggested_solutions": suggested_solutions
        }
    )

def generate_solution(state: SupportTicketState) -> Command[Literal["finalize_ticket"]]:
    """ê³ ê° ì‘ë‹µ ìƒì„±"""
    issue = state['issue_description']
    solutions = state['suggested_solutions']

    response_prompt = f"""ì¹œì ˆí•œ ê³ ê° ì§€ì› ë‹´ë‹¹ìë¡œì„œ ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”:

    ê³ ê° ë¬¸ì˜: {issue}

    ì œì•ˆ í•´ê²°ì±…:
    {chr(10).join(solutions)}

    ê³ ê° ì‘ë‹µ (ë‹¨ê³„ë³„ ì•ˆë‚´ í¬í•¨):"""

    resolution = llm.invoke(response_prompt).content

    # ì—ì´ì „íŠ¸ ë…¸íŠ¸ ì‘ì„±
    notes = f"[{datetime.now()}] ìë™ ì†”ë£¨ì…˜ ì œê³µ. ìœ ì‚¬ ì‚¬ë¡€ {len(state['similar_tickets'])}ê±´ ì°¸ì¡°."

    return Command(
        goto="finalize_ticket",
        update={
            "resolution": resolution,
            "agent_notes": notes
        }
    )

def escalate_immediately(state: SupportTicketState) -> Command[Literal["finalize_ticket"]]:
    """ì¦‰ì‹œ ì—ìŠ¤ì»¬ë ˆì´ì…˜"""
    resolution = f"""[ê¸´ê¸‰ ì—ìŠ¤ì»¬ë ˆì´ì…˜]

    ì´ ë¬¸ì˜ëŠ” ì¦‰ì‹œ ë§¤ë‹ˆì €ì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜ë˜ì—ˆìŠµë‹ˆë‹¤.
    ê³ ê°: {state['customer_id']}
    ì´ìŠˆ: {state['issue_description']}
    ê°ì •: {state['sentiment']}

    ë‹´ë‹¹ ë§¤ë‹ˆì €ê°€ ê³§ ì—°ë½ë“œë¦´ ì˜ˆì •ì…ë‹ˆë‹¤."""

    notes = f"[{datetime.now()}] ê¸´ê¸‰ ì—ìŠ¤ì»¬ë ˆì´ì…˜ - ë§¤ë‹ˆì € ì•Œë¦¼ ë°œì†¡"

    return Command(
        goto="finalize_ticket",
        update={
            "resolution": resolution,
            "agent_notes": notes
        }
    )

def escalate_to_specialist(state: SupportTicketState) -> Command[Literal["finalize_ticket"]]:
    """ì „ë¬¸ê°€ì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜"""
    resolution = f"""[ì „ë¬¸ê°€ ì—ìŠ¤ì»¬ë ˆì´ì…˜]

    ì¹´í…Œê³ ë¦¬: {state['category']}
    ìš°ì„ ìˆœìœ„: {state['priority']}

    {state['category']} ì „ë¬¸ê°€ê°€ ë°°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
    ì°¸ê³  ìë£Œ: {state['similar_tickets']}

    1-2 ì˜ì—…ì¼ ë‚´ ìƒì„¸ ë‹µë³€ ì œê³µ ì˜ˆì •ì…ë‹ˆë‹¤."""

    notes = f"[{datetime.now()}] ì „ë¬¸ê°€ ì—ìŠ¤ì»¬ë ˆì´ì…˜ - {state['category']} íŒ€ì— ë°°ì •"

    return Command(
        goto="finalize_ticket",
        update={
            "resolution": resolution,
            "agent_notes": notes
        }
    )

def finalize_ticket(state: SupportTicketState):
    """í‹°ì¼“ ë§ˆë¬´ë¦¬"""
    # ë§Œì¡±ë„ ì˜ˆì¸¡ (ì‹¤ì œë¡œëŠ” ML ëª¨ë¸ ì‚¬ìš©)
    satisfaction = 0.9 if not state['escalation_required'] else 0.7

    return {"satisfaction_score": satisfaction}

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(SupportTicketState)

workflow.add_node("analyze_ticket", analyze_ticket)
workflow.add_node("search_knowledge_base", search_knowledge_base)
workflow.add_node("generate_solution", generate_solution)
workflow.add_node("escalate_immediately", escalate_immediately)
workflow.add_node("escalate_to_specialist", escalate_to_specialist)
workflow.add_node("finalize_ticket", finalize_ticket)

workflow.add_edge(START, "analyze_ticket")
workflow.add_edge("finalize_ticket", END)

support_system = workflow.compile()

# í…ŒìŠ¤íŠ¸
test_tickets = [
    {
        "ticket_id": "T001",
        "customer_id": "C12345",
        "issue_description": "ë¡œê·¸ì¸ì´ ê³„ì† ì‹¤íŒ¨í•©ë‹ˆë‹¤. ë¹„ë°€ë²ˆí˜¸ë¥¼ ì—¬ëŸ¬ ë²ˆ ì¬ì„¤ì •í–ˆëŠ”ë°ë„ ì•ˆ ë©ë‹ˆë‹¤."
    },
    {
        "ticket_id": "T002",
        "customer_id": "C67890",
        "issue_description": "ê²°ì œê°€ ë‘ ë²ˆ ì²­êµ¬ë˜ì—ˆìŠµë‹ˆë‹¤! ì¦‰ì‹œ í™˜ë¶ˆí•´ì£¼ì„¸ìš”!"
    }
]

for ticket in test_tickets:
    print(f"\n{'='*60}")
    print(f"í‹°ì¼“ ID: {ticket['ticket_id']}")
    result = support_system.invoke(ticket)
    print(f"ì¹´í…Œê³ ë¦¬: {result['category']}")
    print(f"ìš°ì„ ìˆœìœ„: {result['priority']}")
    print(f"ê°ì •: {result['sentiment']}")
    print(f"ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”: {result['escalation_required']}")
    print(f"\ní•´ê²°ì±…:\n{result['resolution']}")
    print(f"\nì—ì´ì „íŠ¸ ë…¸íŠ¸: {result['agent_notes']}")
    print(f"ì˜ˆìƒ ë§Œì¡±ë„: {result['satisfaction_score']}")
```

## ğŸ“– ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LangGraph ê³µì‹ ë¬¸ì„œ](https://langchain-ai.github.io/langgraph/)
- [LangGraph StateGraph API ë ˆí¼ëŸ°ìŠ¤](https://langchain-ai.github.io/langgraph/reference/graphs/)
- [LangGraph Command ê°ì²´ ê°€ì´ë“œ](https://langchain-ai.github.io/langgraph/how-tos/command/)
- [LangGraph Studio ì„¤ì • ê°€ì´ë“œ](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/)

### ì¶”ê°€ í•™ìŠµ ìë£Œ
- [LangGraph ì‹¤ì „ ì˜ˆì œ ëª¨ìŒ](https://github.com/langchain-ai/langgraph/tree/main/examples)
- [Multi-Agent ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/)
- [LangGraph vs LangChain ë¹„êµ](https://python.langchain.com/docs/langgraph)

### ê´€ë ¨ ê¸°ìˆ  ìŠ¤íƒ
- **TypedDict**: Python íƒ€ì… íŒíŒ… - [Python ê³µì‹ ë¬¸ì„œ](https://docs.python.org/3/library/typing.html#typing.TypedDict)
- **Literal Types**: íƒ€ì… ì œì•½ - [PEP 586](https://peps.python.org/pep-0586/)
- **Async/Await íŒ¨í„´**: ë¹„ë™ê¸° ì²˜ë¦¬ - [Python Async ê°€ì´ë“œ](https://docs.python.org/3/library/asyncio.html)

### ë””ë²„ê¹… ë„êµ¬
- **LangSmith**: LangGraph ì‹¤í–‰ ì¶”ì  ë° ë””ë²„ê¹… - [LangSmith ë¬¸ì„œ](https://docs.smith.langchain.com/)
- **Mermaid ë‹¤ì´ì–´ê·¸ë¨**: ê·¸ë˜í”„ ì‹œê°í™” - [Mermaid ë¬¸ë²•](https://mermaid.js.org/)

---

**ë‹¤ìŒ ë‹¨ê³„:**
- LangGraph Send ê°ì²´ë¥¼ í™œìš©í•œ ë³‘ë ¬ ì²˜ë¦¬ í•™ìŠµ
- Checkpointerë¥¼ ì‚¬ìš©í•œ ìƒíƒœ ì˜ì†í™”
- ë³µì¡í•œ Multi-Agent ì‹œìŠ¤í…œ êµ¬ì¶•
- Human-in-the-Loop íŒ¨í„´ êµ¬í˜„
