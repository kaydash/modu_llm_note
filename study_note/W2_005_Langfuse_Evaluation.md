# W2_005 Langfuse Evaluationì„ ì‚¬ìš©í•œ RAG ë‹µë³€ í‰ê°€

## í•™ìŠµ ëª©í‘œ
- Langfuse í”Œë«í¼ì„ í™œìš©í•œ ì²´ê³„ì ì¸ RAG ì‹œìŠ¤í…œ í‰ê°€ ë°©ë²• í•™ìŠµ
- ë°ì´í„°ì…‹ ê¸°ë°˜ ìë™í™” í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- ROUGE ì ìˆ˜ì™€ LLM-as-Judge í‰ê°€ ì§€í‘œ í†µí•© í™œìš©
- Langfuse ëŒ€ì‹œë³´ë“œë¥¼ í†µí•œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„

## í•µì‹¬ ê°œë…

### 1. Langfuseë€?
- **ì •ì˜**: LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ ê´€ì¸¡ì„±(Observability) í”Œë«í¼
- **ëª©ì **: RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§
- **íŠ¹ì§•**: ìë™í™”ëœ ì¶”ì , ë°ì´í„°ì…‹ ê´€ë¦¬, í‰ê°€ ì§€í‘œ ìˆ˜ì§‘, ì‹œê°ì  ëŒ€ì‹œë³´ë“œ

### 2. Langfuseì˜ ì£¼ìš” ì¥ì 
- ğŸ”„ **ìë™í™”ëœ ì¶”ì  ë° ë¡œê¹…**: CallbackHandlerë¥¼ í†µí•œ íˆ¬ëª…í•œ ì‹¤í–‰ ê¸°ë¡
- ğŸ“Š **ì‹œê°ì  ëŒ€ì‹œë³´ë“œ**: ì§ê´€ì ì¸ ì„±ëŠ¥ ë¶„ì„ ì¸í„°í˜ì´ìŠ¤
- ğŸ” **ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ**: ROUGE, LLM-as-Judge ë“± í†µí•© ì§€ì›
- ğŸš€ **í™•ì¥ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸**: ëŒ€ê·œëª¨ í‰ê°€ ìë™í™” ê°€ëŠ¥

### 3. í‰ê°€ í”„ë¡œì„¸ìŠ¤
1. **í™˜ê²½ ì„¤ì •**: Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì¸ì¦
2. **ë°ì´í„°ì…‹ ì—…ë¡œë“œ**: `create_dataset()` ë° `create_dataset_item()` ì‚¬ìš©
3. **RAG ì²´ì¸ ì‹¤í–‰**: CallbackHandlerë¡œ ìë™ ì¶”ì  ì„¤ì •
4. **í‰ê°€ ì‹¤í–‰**: ë°ì´í„°ì…‹ ê¸°ë°˜ ì²´ê³„ì  í‰ê°€
5. **ê²°ê³¼ ë¶„ì„**: ëŒ€ì‹œë³´ë“œë¥¼ í†µí•œ ì„±ëŠ¥ ë¶„ì„

## í™˜ê²½ ì„¤ì •

### 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
pip install langfuse langchain langchain-openai langchain-chroma
pip install korouge-score krag pandas openpyxl
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
# .env íŒŒì¼
OPENAI_API_KEY=your_openai_api_key
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
LANGFUSE_HOST=https://cloud.langfuse.com
LANGSMITH_TRACING=true
```

### 3. ê¸°ë³¸ ì„¤ì •
```python
from dotenv import load_dotenv
import os
import pandas as pd
import numpy as np
import json
from pprint import pprint
import warnings

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
warnings.filterwarnings("ignore")

# Langsmith ì¶”ì  í™•ì¸
print("langsmith ì¶”ì  ì—¬ë¶€:", os.getenv('LANGSMITH_TRACING'))
```

## 1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„

### í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
df_qa_test = pd.read_excel("data/testset.xlsx")
print(f"í…ŒìŠ¤íŠ¸ì…‹: {df_qa_test.shape[0]}ê°œ ë¬¸ì„œ")

# ë°ì´í„° êµ¬ì¡° í™•ì¸
df_qa_test.head(2)
```

**ë°ì´í„°ì…‹ êµ¬ì¡°:**
- `user_input`: ì‚¬ìš©ì ì§ˆë¬¸
- `reference`: ì •ë‹µ ì°¸ì¡°
- `reference_contexts`: ì°¸ì¡° ë¬¸ë§¥
- `synthesizer_name`: ë°ì´í„° ìƒì„± ë°©ì‹

## 2ë‹¨ê³„: ê²€ìƒ‰ ë„êµ¬ ì •ì˜

### 1. ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Chroma DB ë¡œë“œ
chroma_db = Chroma(
    collection_name="db_korean_cosine_metadata",
    embedding_function=embeddings,
    persist_directory="./chroma_db",
)

# ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„±
chroma_k = chroma_db.as_retriever(search_kwargs={'k': 4})

# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
query = "í…ŒìŠ¬ë¼ì˜ íšŒì¥ì€ ëˆ„êµ¬ì¸ê°€ìš”?"
retrieved_docs = chroma_k.invoke(query)

for doc in retrieved_docs:
    print(f"- {doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]")
```

### 2. BM25 ê²€ìƒ‰ê¸° ì¤€ë¹„
```python
from krag.tokenizers import KiwiTokenizer
from krag.retrievers import KiWiBM25RetrieverWithScore
from langchain.schema import Document

# ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜
def load_jsonlines(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        docs = [json.loads(line) for line in f]
    return docs

# í•œêµ­ì–´ ë¬¸ì„œ ë¡œë“œ
korean_docs = load_jsonlines('data/korean_docs_final.jsonl')
print(f"ë¡œë“œëœ ë¬¸ì„œ: {len(korean_docs)}ê°œ")

# Document ê°ì²´ë¡œ ë³€í™˜
documents = []
for data in korean_docs:
    if isinstance(data, str):
        doc_data = json.loads(data)
    else:
        doc_data = data

    documents.append(Document(
        page_content=doc_data['page_content'],
        metadata=doc_data['metadata']
    ))

print(f"ë³€í™˜ëœ ë¬¸ì„œ: {len(documents)}ê°œ")

# BM25 ê²€ìƒ‰ê¸° ì„¤ì •
kiwi_tokenizer = KiwiTokenizer(
    model_type='knlm',    # Kiwi ì–¸ì–´ ëª¨ë¸ íƒ€ì…
    typos='basic'         # ê¸°ë³¸ ì˜¤íƒ€êµì •
)

bm25_db = KiWiBM25RetrieverWithScore(
    documents=documents,
    kiwi_tokenizer=kiwi_tokenizer,
    k=4,
)

# BM25 ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
retrieved_docs = bm25_db.invoke(query)
for doc in retrieved_docs:
    print(f"BM25 ì ìˆ˜: {doc.metadata['bm25_score']:.2f}")
    print(f"{doc.page_content}")
```

### 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬ì„±
```python
from langchain.retrievers import EnsembleRetriever

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
hybrid_retriever = EnsembleRetriever(
    retrievers=[bm25_db, chroma_k],
    weights=[0.5, 0.5],  # BM25ì™€ ë²¡í„° ê²€ìƒ‰ì˜ ê°€ì¤‘ì¹˜
)

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
retrieved_docs = hybrid_retriever.invoke(query)
for doc in retrieved_docs:
    print(f"{doc.page_content}\n[ì¶œì²˜: {doc.metadata['source']}]")
```

## 3ë‹¨ê³„: RAG Chain ì •ì˜

### RAG ì²´ì¸ í•¨ìˆ˜ ìƒì„±
```python
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

def create_rag_chain(retriever, llm):
    """RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜"""

    template = """Answer the following question based on this context.
    If the context is not relevant to the question, just answer with 'ë‹µë³€ì— í•„ìš”í•œ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'

    [Context]
    {context}

    [Question]
    {question}

    [Answer]
    """

    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join([f"{doc.page_content}" for doc in docs])

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain

# RAG ì²´ì¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.5)
openai_rag_chain = create_rag_chain(hybrid_retriever, llm)

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
question = "í…ŒìŠ¬ë¼ì˜ íšŒì¥ì€ ëˆ„êµ¬ì¸ê°€ìš”?"
answer = openai_rag_chain.invoke(question)

print(f"ì¿¼ë¦¬: {question}")
print(f"ë‹µë³€: {answer}")
```

## 4ë‹¨ê³„: Langfuse í™˜ê²½ ì„¤ì •

### 1. Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
```python
from langfuse.langchain import CallbackHandler
from langfuse import get_client

# Langfuse ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
langfuse_handler = CallbackHandler()

# Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
langfuse_client = get_client()

# ì¸ì¦ í™•ì¸
print("Langfuse ì¸ì¦ ìƒíƒœ:", langfuse_client.auth_check())
```

## 5ë‹¨ê³„: í‰ê°€ìš© ë°ì´í„°ì…‹ ì—…ë¡œë“œ

### ë°ì´í„°ì…‹ ìƒì„± ë° ì•„ì´í…œ ì¶”ê°€
```python
# ë°ì´í„°ì…‹ ìƒì„±
dataset_name = "RAG_Evaluation_Dataset_Test"
dataset = langfuse_client.create_dataset(name=dataset_name)
print(f"ìƒì„±ëœ ë°ì´í„°ì…‹: {dataset.name}")

# í‰ê°€ìš© ë°ì´í„°ì…‹ ë³€í™˜
data = [
    {
        "user_input": row["user_input"],
        "reference": row["reference"],
        "reference_contexts": row["reference_contexts"],
    }
    for _, row in df_qa_test.iterrows()
]

print(f"í‰ê°€ìš© ë°ì´í„°ì…‹ ì•„ì´í…œ ìˆ˜: {len(data)}ê°œ")

# ë°ì´í„°ì…‹ ì•„ì´í…œ ì¶”ê°€
for item in data:
    langfuse_client.create_dataset_item(
        dataset_name=dataset_name,
        input=item.get("user_input", ""),
        expected_output=item.get("reference", ""),
        metadata={
            "reference_contexts": item.get("reference_contexts", ""),
        }
    )

# Langfuseì— ì €ì¥
langfuse_client.flush()

# ë°ì´í„°ì…‹ í™•ì¸
dataset = langfuse_client.get_dataset(name=dataset_name)
print(f"ìƒì„±ëœ ë°ì´í„°ì…‹: {dataset.name}")
print(f"ë°ì´í„°ì…‹ ì•„ì´í…œ ìˆ˜: {len(dataset.items)}ê°œ")
```

### ë°ì´í„°ì…‹ ì•„ì´í…œ ì¶œë ¥
```python
# ì²˜ìŒ 5ê°œ ì•„ì´í…œ í™•ì¸
for item in dataset.items[:5]:
    print(f"ì…ë ¥: {item.input}")
    print(f"ê¸°ëŒ€ ì¶œë ¥: {item.expected_output}")
    print(f"ë©”íƒ€ë°ì´í„°: {item.metadata}")
    print("-" * 200)
```

## 6ë‹¨ê³„: í‰ê°€ ì§€í‘œ ì„¤ì •

### ROUGE ìŠ¤ì½”ì–´ ë° ê°„ê²°ì„± í‰ê°€ì ì„¤ì •
```python
from langchain.evaluation import load_evaluator
from korouge_score import rouge_scorer
from krag.tokenizers import KiwiTokenizer

# Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš©í•˜ì—¬ í† í°í™”í•˜ëŠ” í´ë˜ìŠ¤ ì •ì˜
class CustomKiwiTokenizer(KiwiTokenizer):
    def tokenize(self, text):
        return [t.form for t in super().tokenize(text)]

# í† í¬ë‚˜ì´ì € ìƒì„±
kiwi_tokenizer = CustomKiwiTokenizer(model_type='knlm', typos='basic')

# ROUGE ìŠ¤ì½”ì–´ ê³„ì‚°ê¸°
scorer = rouge_scorer.RougeScorer(
    ["rouge1", "rouge2", "rougeL"],
    tokenizer=kiwi_tokenizer
)

# ê°„ê²°ì„± í‰ê°€ì ë¡œë“œ
conciseness_evaluator = load_evaluator(
    evaluator="labeled_criteria",
    criteria="conciseness",
    llm=llm
)
```

## 7ë‹¨ê³„: ë°ì´í„°ì…‹ ê¸°ë°˜ í‰ê°€ ì‹¤í–‰

### í‰ê°€ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜
```python
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

@dataclass
class EvaluationResult:
    """í‰ê°€ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    item_id: str
    input: Any
    output: str
    expected_output: str
    scores: Dict[str, float]
    details: Dict[str, Any]
    trace_id: Optional[str] = None
    error: Optional[str] = None
```

### í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜
```python
def run_dataset_evaluation(
    dataset_name: str,
    rag_chain,
    run_name: str
) -> List[EvaluationResult]:
    """ë°ì´í„°ì…‹ ì „ì²´ì— ëŒ€í•œ í‰ê°€ ì‹¤í–‰"""

    # ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°
    langfuse_client = get_client()
    dataset = langfuse_client.get_dataset(name=dataset_name)

    if not dataset:
        raise ValueError(f"ë°ì´í„°ì…‹ '{dataset_name}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    print(f"ğŸ“Š RAG í‰ê°€ ì‹œì‘: {dataset_name} ({len(dataset.items)}ê°œ í•­ëª©)")

    results = []
    successful = 0
    failed = 0

    for idx, item in enumerate(dataset.items, 1):
        try:
            print(f"\nğŸ”„ ì•„ì´í…œ {idx}/{len(dataset.items)} ì²˜ë¦¬ ì¤‘...")

            # Langfuse íŠ¸ë ˆì´ì‹± ì„¤ì •
            with item.run(run_name=run_name) as root_span:

                # RAG ì²´ì¸ ì‹¤í–‰
                output = rag_chain.invoke(
                    item.input,
                    config={"callbacks": [CallbackHandler()]}
                )

                # í‰ê°€ ìˆ˜í–‰
                scores, details = {}, {}

                # 1. ROUGE ì ìˆ˜ í‰ê°€
                try:
                    rouge_results = scorer.score(
                        str(item.expected_output),
                        str(output)
                    )
                    rouge_scores = {
                        "rouge1": rouge_results['rouge1'].fmeasure,
                        "rouge2": rouge_results['rouge2'].fmeasure,
                        "rougeL": rouge_results['rougeL'].fmeasure
                    }
                    scores["rouge"] = sum(rouge_scores.values()) / len(rouge_scores)
                    details["rouge"] = rouge_scores
                except Exception as e:
                    scores["rouge"] = 0.0
                    details["rouge"] = {"error": str(e)}

                # 2. ê°„ê²°ì„± í‰ê°€
                try:
                    conciseness_result = conciseness_evaluator.evaluate_strings(
                        input=str(item.input),
                        prediction=str(output),
                        reference=str(item.expected_output)
                    )
                    scores["conciseness"] = float(conciseness_result.get('score', 0))
                    details["conciseness"] = {
                        "reasoning": conciseness_result.get('reasoning', ''),
                        "score": conciseness_result.get('score', 0)
                    }
                except Exception as e:
                    scores["conciseness"] = 0.0
                    details["conciseness"] = {"error": str(e)}

                # ì „ì²´ ì ìˆ˜ ê³„ì‚° ë° ê¸°ë¡
                overall_score = sum(scores.values()) / len(scores)
                root_span.score(name="overall", value=overall_score)

                # ê° í‰ê°€ ì ìˆ˜ ê¸°ë¡
                for score_name, score_value in scores.items():
                    root_span.score(name=score_name, value=score_value)

                # ê²°ê³¼ ì €ì¥
                result = EvaluationResult(
                    item_id=item.id,
                    input=item.input,
                    output=str(output),
                    expected_output=str(item.expected_output) if item.expected_output else "",
                    scores=scores,
                    details=details,
                    trace_id=getattr(root_span, 'trace_id', None)
                )
                results.append(result)
                successful += 1

                print(f"   âœ… ì™„ë£Œ (ì¢…í•© ì ìˆ˜: {overall_score:.2f})")
                print(f"   ğŸ” ì„¸ë¶€ ì •ë³´: {details}")

        except Exception as e:
            failed += 1
            print(f"   âŒ ì‹¤íŒ¨: {str(e)}")

            # ì‹¤íŒ¨í•´ë„ ê²°ê³¼ì— ê¸°ë¡
            results.append(EvaluationResult(
                item_id=item.id,
                input=item.input,
                output="",
                expected_output=str(item.expected_output) if item.expected_output else "",
                scores={},
                details={},
                error=str(e)
            ))

    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“‹ í‰ê°€ ì™„ë£Œ: ì„±ê³µ {successful}ê°œ, ì‹¤íŒ¨ {failed}ê°œ")

    return results
```

### í‰ê°€ ì‹¤í–‰
```python
# í‰ê°€ ì‹¤í–‰
results = run_dataset_evaluation(
    dataset_name="RAG_Evaluation_Dataset_Test",
    rag_chain=openai_rag_chain,
    run_name="simple_evaluation_v1"
)

print(f"\ní‰ê°€ ì™„ë£Œ: {len(results)}ê°œ í•­ëª©")
```

## 8ë‹¨ê³„: ê²°ê³¼ ë¶„ì„

### í‰ê°€ ê²°ê³¼ í†µê³„
```python
# ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
successful_results = [r for r in results if not r.error]

# ROUGE ì ìˆ˜ ë¶„ì„
rouge_scores = [r.scores.get('rouge', 0) for r in successful_results]
conciseness_scores = [r.scores.get('conciseness', 0) for r in successful_results]

print("ğŸ“Š í‰ê°€ ê²°ê³¼ í†µê³„:")
print(f"   - ì „ì²´ í‰ê°€: {len(results)}ê°œ")
print(f"   - ì„±ê³µ: {len(successful_results)}ê°œ")
print(f"   - ì‹¤íŒ¨: {len(results) - len(successful_results)}ê°œ")
print(f"\n   - ROUGE í‰ê·  ì ìˆ˜: {np.mean(rouge_scores):.3f}")
print(f"   - ROUGE í‘œì¤€í¸ì°¨: {np.std(rouge_scores):.3f}")
print(f"\n   - ê°„ê²°ì„± í‰ê·  ì ìˆ˜: {np.mean(conciseness_scores):.3f}")
print(f"   - ê°„ê²°ì„± í‘œì¤€í¸ì°¨: {np.std(conciseness_scores):.3f}")
print(f"\n   - ì „ì²´ í‰ê·  ì ìˆ˜: {np.mean([np.mean([r, c]) for r, c in zip(rouge_scores, conciseness_scores)]):.3f}")
```

### ìƒìœ„/í•˜ìœ„ ì„±ëŠ¥ ë¶„ì„
```python
# ì¢…í•© ì ìˆ˜ ê³„ì‚°
for result in successful_results:
    if result.scores:
        result.overall_score = np.mean(list(result.scores.values()))

# ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
sorted_results = sorted(
    successful_results,
    key=lambda x: getattr(x, 'overall_score', 0),
    reverse=True
)

# ìƒìœ„ 3ê°œ
print("\nğŸ† ìƒìœ„ 3ê°œ ê²°ê³¼:")
for i, result in enumerate(sorted_results[:3], 1):
    print(f"\n{i}. ì§ˆë¬¸: {result.input}")
    print(f"   ë‹µë³€: {result.output}")
    print(f"   ì¢…í•© ì ìˆ˜: {result.overall_score:.3f}")

# í•˜ìœ„ 3ê°œ
print("\nâš ï¸ í•˜ìœ„ 3ê°œ ê²°ê³¼:")
for i, result in enumerate(sorted_results[-3:], 1):
    print(f"\n{i}. ì§ˆë¬¸: {result.input}")
    print(f"   ë‹µë³€: {result.output}")
    print(f"   ì¢…í•© ì ìˆ˜: {result.overall_score:.3f}")
```

## ì‹¤ìŠµ ê³¼ì œ

### ê¸°ë³¸ ì‹¤ìŠµ
1. **Langfuse ë°ì´í„°ì…‹ ìƒì„±**
   - ìì‹ ë§Œì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± (ìµœì†Œ 5ê°œ í•­ëª©)
   - RAG ì²´ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„±
   - Langfuse ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ í™•ì¸

2. **í‰ê°€ ì§€í‘œ ì¶”ê°€**
   - Helpfulness í‰ê°€ì ì¶”ê°€
   - Relevance í‰ê°€ì ì¶”ê°€
   - ë‹¤ì¤‘ í‰ê°€ ì§€í‘œë¡œ ì¢…í•© í‰ê°€

### ì‘ìš© ì‹¤ìŠµ
3. **ì‚¬ìš©ì ì •ì˜ í‰ê°€ ê¸°ì¤€**
   - í•œêµ­ì–´ íŠ¹í™” í‰ê°€ ê¸°ì¤€ ê°œë°œ
   - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ì êµ¬í˜„
   - ê¸°ì¡´ ì§€í‘œì™€ ì„±ëŠ¥ ë¹„êµ

4. **ë‹¤ì–‘í•œ ê²€ìƒ‰ê¸° ë¹„êµ**
   - Vector Search, BM25, Hybrid Search ê°ê° í‰ê°€
   - ê²€ìƒ‰ê¸°ë³„ ì„±ëŠ¥ ì°¨ì´ ë¶„ì„
   - ìµœì  ê²€ìƒ‰ê¸° ì¡°í•© íƒìƒ‰

### ì‹¬í™” ì‹¤ìŠµ
5. **ëª¨ë¸ ë¹„êµ í‰ê°€**
   - ì—¬ëŸ¬ LLM ëª¨ë¸(GPT-4, Gemini ë“±) ì„±ëŠ¥ ë¹„êµ
   - ë™ì¼ ë°ì´í„°ì…‹ìœ¼ë¡œ A/B í…ŒìŠ¤íŠ¸
   - ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ ë¶„ì„

6. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ**
   - ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì • ë° ëª¨ë‹ˆí„°ë§
   - ì„±ëŠ¥ ì €í•˜ ì‹œ ìë™ ì•Œë¦¼
   - ì§€ì†ì  ê°œì„  íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜ë“¤
1. **Langfuse ì¸ì¦ ì˜¤ë¥˜**
   ```python
   # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
   print("LANGFUSE_PUBLIC_KEY:", bool(os.getenv("LANGFUSE_PUBLIC_KEY")))
   print("LANGFUSE_SECRET_KEY:", bool(os.getenv("LANGFUSE_SECRET_KEY")))
   print("LANGFUSE_HOST:", os.getenv("LANGFUSE_HOST"))
   ```

2. **ë°ì´í„°ì…‹ ìƒì„± ì˜¤ë¥˜**
   ```python
   # ê¸°ì¡´ ë°ì´í„°ì…‹ í™•ì¸
   try:
       existing_dataset = langfuse_client.get_dataset(name=dataset_name)
       print(f"ë°ì´í„°ì…‹ '{dataset_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
   except:
       print(f"ë°ì´í„°ì…‹ '{dataset_name}'ì„ ìƒì„±í•©ë‹ˆë‹¤.")
   ```

3. **í‰ê°€ ì‹¤í–‰ ì˜¤ë¥˜**
   ```python
   # ê°œë³„ ì•„ì´í…œ í…ŒìŠ¤íŠ¸
   test_item = dataset.items[0]
   try:
       test_output = rag_chain.invoke(test_item.input)
       print("í…ŒìŠ¤íŠ¸ ì„±ê³µ:", test_output)
   except Exception as e:
       print("í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨:", e)
   ```

## ì°¸ê³  ìë£Œ
- [Langfuse ê³µì‹ ë¬¸ì„œ](https://langfuse.com/docs)
- [Langfuse LangChain í†µí•©](https://langfuse.com/docs/integrations/langchain)
- [ROUGE Score ê³„ì‚°](https://github.com/neural-dialogue-metrics/rouge)
- [í•œêµ­ì–´ ROUGE êµ¬í˜„](https://github.com/gucci-j/korouge-score)
- [LangChain í‰ê°€ ê°€ì´ë“œ](https://python.langchain.com/docs/guides/evaluation/)

ì´ í•™ìŠµ ê°€ì´ë“œë¥¼ í†µí•´ Langfuseë¥¼ í™œìš©í•œ ì²´ê³„ì ì¸ RAG í‰ê°€ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê³ , ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.