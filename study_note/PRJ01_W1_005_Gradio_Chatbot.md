# W1_005_Gradio_Chatbot.md - Gradio ì±—ë´‡ UI êµ¬í˜„

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- Gradio ChatInterfaceë¥¼ í™œìš©í•œ ëŒ€í™”í˜• UI êµ¬ì¶• ëŠ¥ë ¥ ìŠµë“
- LangChainê³¼ Gradio í†µí•©ì„ í†µí•œ AI ì±—ë´‡ ê°œë°œ
- ë©”ëª¨ë¦¬ ê¸°ëŠ¥ê³¼ ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ê¸°ë²• í•™ìŠµ
- ë©€í‹°ëª¨ë‹¬ ë° ê³ ê¸‰ UI ì»´í¬ë„ŒíŠ¸ í™œìš©ë²• ì´í•´

## ğŸ“š í•µì‹¬ ê°œë…

### Gradio ChatInterface
- **ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤**: ì±„íŒ… í˜•íƒœì˜ ì‚¬ìš©ì ì¹œí™”ì  UI ì œê³µ
- **ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©**: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µê³¼ ì¦‰ì‹œ í”¼ë“œë°± ì§€ì›
- **í™•ì¥ì„±**: ì¶”ê°€ ì…ë ¥ ì»´í¬ë„ŒíŠ¸ì™€ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ í†µí•© ê°€ëŠ¥
- **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ì œëª©, ì„¤ëª…, ì˜ˆì‹œ ì§ˆë¬¸ ë“± ë‹¤ì–‘í•œ ì„¤ì • ì˜µì…˜

### ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬
- **ë©”ì‹œì§€ í˜•ì‹**: OpenAI ìŠ¤íƒ€ì¼ì˜ role/content êµ¬ì¡°
- **ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´**: ì´ì „ ëŒ€í™” ë‚´ìš©ì„ í™œìš©í•œ ì—°ì†ì  ëŒ€í™”
- **ë©”ëª¨ë¦¬ í†µí•©**: LangChain ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œê³¼ì˜ ì›í™œí•œ ì—°ë™
- **ìƒíƒœ ê´€ë¦¬**: ì„¸ì…˜ë³„ ëŒ€í™” ìƒíƒœ ìœ ì§€ ë° ê´€ë¦¬

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install gradio langchain langchain-openai

# UV íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì‚¬ìš© ì‹œ
uv add gradio langchain langchain-openai langchain-google-genai

# ì¶”ê°€ ê¸°ëŠ¥ìš© íŒ¨í‚¤ì§€
pip install gradio-pdf  # PDF ë·°ì–´ ê¸°ëŠ¥
```

### Langfuse í†µí•© ì„¤ì •
```python
from langfuse.langchain import CallbackHandler
from dotenv import load_dotenv
from langchain_core.runnables.base import Runnable
import functools

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì „ì—­ Langfuse handler ìƒì„±
_langfuse_handler = CallbackHandler()

# ê¸°ì¡´ invoke ë©”ì„œë“œë¥¼ ë˜í•‘
_original_invoke = Runnable.invoke

@functools.wraps(_original_invoke)
def _invoke_with_langfuse(self, input, config=None, **kwargs):
    if config is None:
        config = {}
    if "callbacks" not in config:
        config["callbacks"] = []
    config["callbacks"].append(_langfuse_handler)
    return _original_invoke(self, input, config, **kwargs)

# Monkey patch ì ìš©
Runnable.invoke = _invoke_with_langfuse
```

## ğŸ’» ì½”ë“œ ì˜ˆì œ

### 1. ê¸°ë³¸ ChatInterface êµ¬ì¡°

#### Simple QA Chain
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ íŒŒì´ì¬(Python) ì½”ë“œ ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
    ("human", "{user_input}")
])

# LLM ëª¨ë¸ ì •ì˜
model = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=0.3
)

# ì²´ì¸ ìƒì„±
chain = prompt | model | StrOutputParser()

# ì²´ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
response = chain.invoke({
    "user_input": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?"
})

print(response)
```

#### ê¸°ë³¸ Gradio ì¸í„°í˜ì´ìŠ¤
```python
import gradio as gr

# ì±—ë´‡ í•¨ìˆ˜ ì •ì˜
def chat_function(message, history):
    print(f"ì…ë ¥ ë©”ì‹œì§€: {message}")
    print("-" * 40)
    print(f"ì±„íŒ… íˆìŠ¤í† ë¦¬:")
    for chat in history:
        print(f"ì‚¬ìš©ì: {chat['role']}, ë©”ì‹œì§€: {chat['content']}")
    return "ì‘ë‹µ ë©”ì‹œì§€"

# ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
demo = gr.ChatInterface(
    fn=chat_function,
    analytics_enabled=False,
    type="messages"  # OpenAI ìŠ¤íƒ€ì¼ ë©”ì‹œì§€ í˜•ì‹
)

# ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
demo.launch()
```

### 2. Echo ì±—ë´‡ ì˜ˆì œ

```python
def echo_bot(message, history):
    return f"ë‹¹ì‹ ì´ ì…ë ¥í•œ ë©”ì‹œì§€: {message}"

demo = gr.ChatInterface(
    fn=echo_bot,
    title="Echo ì±—ë´‡",
    description="ì…ë ¥í•œ ë©”ì‹œì§€ë¥¼ ê·¸ëŒ€ë¡œ ë˜ëŒë ¤ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.",
    analytics_enabled=False
)

demo.launch()
```

### 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
import time

def streaming_bot(message, history):
    response = f"ì²˜ë¦¬ ì¤‘ì¸ ë©”ì‹œì§€: {message}"
    for i in range(len(response)):
        time.sleep(0.1)  # 0.1ì´ˆ ëŒ€ê¸°
        yield response[:i+1]

demo = gr.ChatInterface(
    fn=streaming_bot,
    title="ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡",
    description="ì…ë ¥í•œ ë©”ì‹œì§€ë¥¼ í•œ ê¸€ìì”© ì²˜ë¦¬í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.",
    analytics_enabled=False
)

demo.launch()
```

### 4. ì¶”ê°€ ì…ë ¥ ì»´í¬ë„ŒíŠ¸

```python
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

def chat_function(message, history, model, temperature):
    if model == "gpt-4.1-mini":
        model = ChatOpenAI(model=model, temperature=temperature)
    elif model == "gemini-2.0-flash":
        model = ChatGoogleGenerativeAI(model=model, temperature=temperature)

    chain = prompt | model | StrOutputParser()

    response = chain.invoke({"user_input": message})
    return response

# ì¶”ê°€ ì…ë ¥ ì»´í¬ë„ŒíŠ¸ê°€ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤
with gr.Blocks() as demo:
    model_selector = gr.Dropdown(
        ["gpt-4.1-mini", "gemini-2.0-flash"],
        label="ëª¨ë¸ ì„ íƒ"
    )
    slider = gr.Slider(
        0.0, 1.0,
        label="Temperature",
        value=0.3,
        step=0.1,
        render=False
    )

    gr.ChatInterface(
        fn=chat_function,
        additional_inputs=[model_selector, slider],
        analytics_enabled=False
    )

demo.launch()
```

### 5. ì˜ˆì‹œ ì§ˆë¬¸ ì„¤ì •

```python
demo = gr.ChatInterface(
    fn=streaming_bot,
    title="ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡",
    description="ì…ë ¥í•œ ë©”ì‹œì§€ë¥¼ í•œ ê¸€ìì”© ì²˜ë¦¬í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.",
    analytics_enabled=False,
    examples=[
        "íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    ]
)

demo.launch()
```

### 6. ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥

```python
import base64
from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI

def convert_to_url(image_path):
    """ì´ë¯¸ì§€ë¥¼ URL í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    with open(image_path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
        return f"data:image/jpeg;base64,{encoded_string}"

def multimodal_bot(message, history):
    model = ChatGoogleGenerativeAI(model="gemini-2.0-flash")

    if isinstance(message, dict):
        text = message.get("text", "")
        files = message.get("files", [])

        if files:
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            image_urls = []
            for file_path in files:
                try:
                    image_url = convert_to_url(file_path)
                    image_urls.append({
                        "type": "image_url",
                        "image_url": image_url
                    })
                except Exception as e:
                    print(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue

            if image_urls:
                content = [
                    {"type": "text", "text": text if text else "ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."},
                    *image_urls
                ]

                try:
                    response = model.invoke([HumanMessage(content=content)])
                    return response.content
                except Exception as e:
                    return f"ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

        return text if text else "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

    return "í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

# ë©€í‹°ëª¨ë‹¬ ì¸í„°í˜ì´ìŠ¤
demo = gr.ChatInterface(
    fn=multimodal_bot,
    type="messages",
    multimodal=True,
    title="ë©€í‹°ëª¨ë‹¬ ì±—ë´‡",
    description="í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.",
    analytics_enabled=False,
    textbox=gr.MultimodalTextbox(
        placeholder="í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
        file_count="multiple",
        file_types=["image"]
    )
)

demo.launch()
```

### 7. PDF ë·°ì–´ í†µí•©

```python
from gradio_pdf import PDF

def answer_invoke(message, history):
    return message

with gr.Blocks(analytics_enabled=False) as demo:
    with gr.Row():
        api_key_input = gr.Textbox(
            label="Enter OpenAI API Key",
            type="password",
            placeholder="sk-..."
        )

    with gr.Row():
        with gr.Column(scale=2):
            pdf_file = PDF(
                label="Upload PDF File",
                height=600
            )
        with gr.Column(scale=1):
            chatbot = gr.ChatInterface(
                fn=answer_invoke,
                type="messages",
                title="PDF-based Chatbot",
                description="Upload a PDF file and ask questions about its contents."
            )

demo.launch()
```

### 8. ë©”ëª¨ë¦¬ í†µí•© ì±—ë´‡

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

# ë©”ì‹œì§€ í”Œë ˆì´ìŠ¤í™€ë”ê°€ ìˆëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ íŒŒì´ì¬(Python) ì½”ë“œ ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
    MessagesPlaceholder("chat_history"),
    ("system", "ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤."),
    ("human", "{user_input}")
])

# ì²´ì¸ ìƒì„±
chain = prompt | model | StrOutputParser()

def answer_invoke(message, history):
    # íˆìŠ¤í† ë¦¬ë¥¼ LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    history_messages = []
    for msg in history:
        if msg['role'] == "user":
            history_messages.append(HumanMessage(content=msg['content']))
        elif msg['role'] == "assistant":
            history_messages.append(AIMessage(content=msg['content']))

    response = chain.invoke({
        "chat_history": history_messages,
        "user_input": message
    })
    return response

# ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì´ ìˆëŠ” ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
demo = gr.ChatInterface(
    fn=answer_invoke,
    type="messages",
    title="íŒŒì´ì¬ ì½”ë“œ ì–´ì‹œìŠ¤í„´íŠ¸"
)

demo.launch()
```

## ğŸš€ ì‹¤ìŠµí•´ë³´ê¸°

### ì‹¤ìŠµ: ë§ì¶¤í˜• ì—¬í–‰ ì¼ì • ê³„íš ì–´ì‹œìŠ¤í„´íŠ¸

**ëª©í‘œ**: Gradio ChatInterfaceë¥¼ í™œìš©í•œ ëŒ€í™”í˜• ì—¬í–‰ ê³„íš ì–´ì‹œìŠ¤í„´íŠ¸ êµ¬í˜„

#### ìš”êµ¬ì‚¬í•­
1. **ê¸°ë³¸ ê¸°ëŠ¥**
   - OpenAI Chat Completion APIì™€ LangChain í™œìš©
   - LCELì„ ì‚¬ìš©í•œ ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ ì²´ì¸ êµ¬ì„±
   - ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ í™œìš©í•œ ì—°ì†ì  ëŒ€í™”

2. **ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ìµœì í™”**
   - temperature=0.7: ì ë‹¹í•œ ì°½ì˜ì„±ê³¼ ì¼ê´€ì„± ê· í˜•
   - top_p=0.9: ê³ í’ˆì§ˆ ì‘ë‹µ ìƒì„±
   - presence_penaltyì™€ frequency_penalty: ë°˜ë³µ ì¤„ì´ê³  ë‹¤ì–‘ì„± ì¦ëŒ€

3. **í”„ë¡¬í”„íŠ¸ ì„¤ê³„**
   - ì—¬í–‰ í”Œë˜ë„ˆ ì—­í•  ì •ì˜
   - êµ¬ì²´ì ì¸ ì •ë³´ í¬í•¨ ì§€ì‹œ
   - í•œêµ­ì–´ ì‘ë‹µ ëª…ì‹œ

#### êµ¬í˜„ ì½”ë“œ
```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
import gradio as gr

# ì—¬í–‰ ê³„íš ì „ë¬¸ í”„ë¡¬í”„íŠ¸
prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì—¬í–‰ ì¼ì • ê³„íš AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ì„ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”:
1. ì‚¬ìš©ìì˜ ì—¬í–‰ ëª©ì , ê¸°ê°„, ì˜ˆì‚°, ì„ í˜¸ë„ë¥¼ íŒŒì•…í•˜ì—¬ ë§ì¶¤í˜• ì¼ì •ì„ ì œì•ˆí•©ë‹ˆë‹¤
2. êµ¬ì²´ì ì¸ ì¥ì†Œ, ì‹œê°„, êµí†µí¸, ë¹„ìš© ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤
3. í˜„ì§€ ë¬¸í™”ì™€ íŠ¹ìƒ‰ì„ ë°˜ì˜í•œ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤
4. ì•ˆì „ ì •ë³´ì™€ ìœ ìš©í•œ íŒì„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤
5. ëª¨ë“  ì‘ë‹µì€ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤

ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì—°ì†ì„± ìˆëŠ” ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”."""),
    MessagesPlaceholder("chat_history"),
    ("system", "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬í–‰ ì „ë¬¸ê°€ë¡œì„œ ìƒì„¸í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."),
    ("human", "{user_input}")
])

# ìµœì í™”ëœ LLM ëª¨ë¸ ì„¤ì •
model = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=0.7,        # ì°½ì˜ì„±ê³¼ ì¼ê´€ì„± ê· í˜•
    top_p=0.9,             # ê³ í’ˆì§ˆ í† í° ì„ íƒ
    presence_penalty=0.3,   # ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì´‰ì§„
    frequency_penalty=0.3,  # ë°˜ë³µ í‘œí˜„ ë°©ì§€
    max_tokens=1500        # ì¶©ë¶„í•œ ì‘ë‹µ ê¸¸ì´
)

# LCEL ì²´ì¸ êµ¬ì„±
chain = prompt | model | StrOutputParser()

def travel_planner(message, history):
    """ì—¬í–‰ ê³„íš ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì¸ í•¨ìˆ˜"""
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    history_messages = []
    for msg in history:
        if msg['role'] == "user":
            history_messages.append(HumanMessage(content=msg['content']))
        elif msg['role'] == "assistant":
            history_messages.append(AIMessage(content=msg['content']))

    try:
        # ì²´ì¸ ì‹¤í–‰
        response = chain.invoke({
            "chat_history": history_messages,
            "user_input": message
        })
        return response
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# Gradio ChatInterface ìƒì„±
demo = gr.ChatInterface(
    fn=travel_planner,
    type="messages",
    title="ğŸŒ ë§ì¶¤í˜• ì—¬í–‰ ì¼ì • ê³„íš ì–´ì‹œìŠ¤í„´íŠ¸",
    description="ë‹¹ì‹ ë§Œì˜ íŠ¹ë³„í•œ ì—¬í–‰ì„ ê³„íší•´ë“œë¦½ë‹ˆë‹¤. ì—¬í–‰ì§€, ê¸°ê°„, ì˜ˆì‚°, ì„ í˜¸ë„ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”!",
    analytics_enabled=False,
    examples=[
        "ì œì£¼ë„ 2ë°• 3ì¼ ì—¬í–‰ ê³„íšì„ ì„¸ì›Œì£¼ì„¸ìš”",
        "ë¶€ì‚°ì—ì„œ ë¨¹ì„ê±°ë¦¬ ìœ„ì£¼ë¡œ 1ë°• 2ì¼ ì—¬í–‰í•˜ê³  ì‹¶ì–´ìš”",
        "ì„œìš¸ ê·¼êµì—ì„œ ê°€ì¡±ê³¼ í•¨ê»˜ ë‹¹ì¼ì¹˜ê¸° ì—¬í–‰ ì¶”ì²œí•´ì£¼ì„¸ìš”",
        "ìœ ëŸ½ ë°°ë‚­ì—¬í–‰ 3ì£¼ ì¼ì •ì„ ê³„íší•´ì£¼ì„¸ìš”"
    ]
)

# ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
demo.launch()
```

## ğŸ“‹ í•´ë‹µ

### ì™„ì „í•œ ì—¬í–‰ ê³„íš ì–´ì‹œìŠ¤í„´íŠ¸ êµ¬í˜„

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
import gradio as gr

# ì „ë¬¸ì ì¸ ì—¬í–‰ ê³„íš í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ê²½í—˜ ë§ì€ ì—¬í–‰ ì „ë¬¸ê°€ì´ì ë§ì¶¤í˜• ì—¬í–‰ ì¼ì • ê³„íš AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ğŸ¯ ì—­í• ê³¼ ëª©í‘œ:
- ì‚¬ìš©ìì˜ ë‹ˆì¦ˆì— ì™„ë²½íˆ ë§ëŠ” ê°œì¸í™”ëœ ì—¬í–‰ ê³„íš ìˆ˜ë¦½
- ì‹¤ìš©ì ì´ê³  ì‹¤í˜„ ê°€ëŠ¥í•œ ì¼ì • ì œì•ˆ
- í˜„ì§€ ë¬¸í™”ì™€ íŠ¹ìƒ‰ì„ ë°˜ì˜í•œ ì§„ì •ì„± ìˆëŠ” ì¶”ì²œ

ğŸ“‹ ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
1. **ì •ë³´ ìˆ˜ì§‘**: ì—¬í–‰ ëª©ì , ê¸°ê°„, ì˜ˆì‚°, ì¸ì›, ì„ í˜¸ í™œë™, ìˆ™ë°• ìŠ¤íƒ€ì¼ íŒŒì•…
2. **ë§ì¶¤ ì œì•ˆ**: ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì¸í™”ëœ ì¼ì • êµ¬ì„±
3. **êµ¬ì²´ì  ì •ë³´**: ì¥ì†Œëª…, ìš´ì˜ì‹œê°„, ì˜ˆìƒ ë¹„ìš©, êµí†µ ì •ë³´, ì†Œìš” ì‹œê°„ ëª…ì‹œ
4. **ì‹¤ìš©ì  íŒ**: ì˜ˆì•½ ë°©ë²•, í• ì¸ ì •ë³´, í˜„ì§€ ì—í‹°ì¼“, ì£¼ì˜ì‚¬í•­ í¬í•¨
5. **ìœ ì—°ì„±**: ëŒ€ì•ˆ ì˜µì…˜ê³¼ ë‚ ì”¨/ìƒí™©ë³„ ë°±ì—… í”Œëœ ì œì‹œ

ğŸ’¡ íŠ¹ë³„ ê³ ë ¤ì‚¬í•­:
- ê³„ì ˆê³¼ ë‚ ì”¨ ê³ ë ¤í•œ ì¶”ì²œ
- í˜„ì§€ ì¶•ì œì™€ ì´ë²¤íŠ¸ ì •ë³´
- êµí†µë¹„ì™€ ì…ì¥ë£Œ ë“± ë¹„ìš© íˆ¬ëª…ì„±
- ì ‘ê·¼ì„±ê³¼ ì•ˆì „ ì •ë³´
- í¬í†  ìŠ¤íŒŸê³¼ ì¸ìŠ¤íƒ€ê·¸ë¨ ëª…ì†Œ

ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬ ì—°ì†ì„± ìˆê³  ë°œì „ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”."""),
    MessagesPlaceholder("chat_history"),
    ("system", "ì‚¬ìš©ìì˜ ì—¬í–‰ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ìƒì„¸í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•©ë‹ˆë‹¤."),
    ("human", "{user_input}")
])

# ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì´ ê· í˜•ì¡íŒ ëª¨ë¸ ì„¤ì •
model = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=0.7,        # ì ë‹¹í•œ ì°½ì˜ì„± ìœ ì§€
    top_p=0.9,             # ë†’ì€ í’ˆì§ˆì˜ í† í° ì„ íƒ
    presence_penalty=0.3,   # ìƒˆë¡œìš´ ì£¼ì œì™€ ì•„ì´ë””ì–´ ë„ì…
    frequency_penalty=0.3,  # ë°˜ë³µì  í‘œí˜„ ì¤„ì´ê¸°
    max_tokens=2000        # ì¶©ë¶„í•œ ì‘ë‹µ ê¸¸ì´
)

# LCELì„ í™œìš©í•œ ì²´ì¸ êµ¬ì„±
planning_chain = prompt | model | StrOutputParser()

def travel_planning_assistant(message, history):
    """
    ì—¬í–‰ ê³„íš ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì¸ í•¨ìˆ˜

    Args:
        message (str): ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€
        history (list): ì±„íŒ… íˆìŠ¤í† ë¦¬ (OpenAI í˜•ì‹)

    Returns:
        str: AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µ
    """
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ LangChain ë©”ì‹œì§€ ê°ì²´ë¡œ ë³€í™˜
    history_messages = []
    for msg in history:
        if msg['role'] == "user":
            history_messages.append(HumanMessage(content=msg['content']))
        elif msg['role'] == "assistant":
            history_messages.append(AIMessage(content=msg['content']))

    try:
        # LCEL ì²´ì¸ì„ í†µí•œ ì‘ë‹µ ìƒì„±
        response = planning_chain.invoke({
            "chat_history": history_messages,
            "user_input": message
        })
        return response

    except Exception as e:
        error_message = f"""ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ğŸ”§ **ì˜¤ë¥˜ ì •ë³´**: {str(e)}

ğŸ“ **í•´ê²° ë°©ë²•**:
1. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”
2. ë©”ì‹œì§€ë¥¼ ë” ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•´ ë³´ì„¸ìš”
3. ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ìƒˆë¡œê³ ì¹¨ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”

ğŸ’¡ **ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´** êµ¬ì²´ì ì¸ ì—¬í–‰ ê³„íš ìš”ì²­ì„ ë‹¤ì‹œ í•´ì£¼ì„¸ìš”!"""
        return error_message

# Gradio ChatInterface êµ¬ì„±
demo = gr.ChatInterface(
    fn=travel_planning_assistant,
    type="messages",
    title="ğŸŒŸ ë§ì¶¤í˜• ì—¬í–‰ ì¼ì • ê³„íš ì–´ì‹œìŠ¤í„´íŠ¸",
    description="""
    ğŸ—ºï¸ **ë‚˜ë§Œì˜ íŠ¹ë³„í•œ ì—¬í–‰ì„ ê³„íší•´ë“œë¦½ë‹ˆë‹¤!**

    ì—¬í–‰ì§€, ê¸°ê°„, ì˜ˆì‚°, ì„ í˜¸ë„ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ì™„ë²½í•œ ë§ì¶¤í˜• ì¼ì •ì„ ì œì•ˆí•´ë“œë¦½ë‹ˆë‹¤.
    í˜„ì§€ ë¬¸í™”, ìˆ¨ì€ ëª…ì†Œ, ì‹¤ìš©ì ì¸ íŒê¹Œì§€ ëª¨ë“  ê²ƒì„ ê³ ë ¤í•œ ì „ë¬¸ì ì¸ ì—¬í–‰ ê³„íšì„ ê²½í—˜í•´ë³´ì„¸ìš”.
    """,
    analytics_enabled=False,
    examples=[
        "ì œì£¼ë„ 2ë°• 3ì¼ íë§ ì—¬í–‰ì„ ê³„íší•˜ê³  ì‹¶ì–´ìš”. ìì—° í’ê²½ê³¼ ë§›ì§‘ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
        "ë¶€ì‚°ì—ì„œ ì¹œêµ¬ë“¤ê³¼ 1ë°• 2ì¼ ë¨¹ë°© ì—¬í–‰! í˜„ì§€ ë§›ì§‘ê³¼ ì•¼ì‹œì¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì¼ì •ì„ ì§œì£¼ì„¸ìš”.",
        "ì„œìš¸ ê·¼êµì—ì„œ ì•„ì´ë“¤ê³¼ í•¨ê»˜í•˜ëŠ” ê°€ì¡± ë‹¹ì¼ì¹˜ê¸° ì—¬í–‰ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
        "ìœ ëŸ½ ë°°ë‚­ì—¬í–‰ 3ì£¼ ì¼ì •ì„ ê³„íš ì¤‘ì…ë‹ˆë‹¤. ë™ìœ ëŸ½ ìœ„ì£¼ë¡œ ì˜ˆì‚°ì€ 300ë§Œì› ì •ë„ì˜ˆìš”.",
        "ì¼ë³¸ ë„ì¿„ 4ë°• 5ì¼ ì—¬í–‰ì¸ë°, ì• ë‹ˆë©”ì´ì…˜ê³¼ íŒì»¬ì²˜ì— ê´€ì‹¬ì´ ë§ì•„ìš”.",
        "ê°•ë¦‰ì—ì„œ ë°”ë‹¤ê°€ ë³´ì´ëŠ” ì¹´í˜ì™€ í•´ë³€ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë¡œë§¨í‹±í•œ 1ë°• 2ì¼ ì—¬í–‰ì„ ì›í•´ìš”."
    ],
    theme=gr.themes.Soft(),
    css="""
    .gradio-container {
        max-width: 1200px !important;
        margin: auto !important;
    }
    """
)

# ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
if __name__ == "__main__":
    demo.launch()
```

## ğŸ” ê³ ê¸‰ ê¸°ëŠ¥ê³¼ ìµœì í™”

### 1. ì‘ë‹µ í’ˆì§ˆ í–¥ìƒ ê¸°ë²•

#### í† í° ì‚¬ìš©ëŸ‰ ìµœì í™”
```python
# í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
def monitor_token_usage(response_obj):
    if hasattr(response_obj, 'response_metadata'):
        usage = response_obj.response_metadata.get('token_usage', {})
        print(f"í† í° ì‚¬ìš©ëŸ‰ - ì…ë ¥: {usage.get('prompt_tokens', 0)}, "
              f"ì¶œë ¥: {usage.get('completion_tokens', 0)}, "
              f"ì´ê³„: {usage.get('total_tokens', 0)}")
```

#### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬í˜„
```python
def streaming_travel_planner(message, history):
    # ìŠ¤íŠ¸ë¦¬ë° ì²´ì¸ êµ¬ì„±
    streaming_chain = prompt | model.stream | StrOutputParser()

    history_messages = []
    for msg in history:
        if msg['role'] == "user":
            history_messages.append(HumanMessage(content=msg['content']))
        elif msg['role'] == "assistant":
            history_messages.append(AIMessage(content=msg['content']))

    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
    partial_response = ""
    for chunk in streaming_chain.stream({
        "chat_history": history_messages,
        "user_input": message
    }):
        partial_response += chunk
        yield partial_response
```

### 2. ì—ëŸ¬ ì²˜ë¦¬ì™€ ì‚¬ìš©ì„± ê°œì„ 

#### í¬ê´„ì  ì—ëŸ¬ ì²˜ë¦¬
```python
def robust_travel_planner(message, history):
    try:
        # ì…ë ¥ ê²€ì¦
        if not message.strip():
            return "ì—¬í–‰ ê³„íšì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ì•Œë ¤ì£¼ì„¸ìš”!"

        # ì²´ì¸ ì‹¤í–‰
        response = planning_chain.invoke({
            "chat_history": history_messages,
            "user_input": message
        })

        # ì‘ë‹µ ê²€ì¦
        if not response.strip():
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."

        return response

    except Exception as e:
        logger.error(f"Travel planner error: {e}")
        return "ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
```

### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

#### Langfuse í†µí•© ëª¨ë‹ˆí„°ë§
```python
# ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
def collect_metrics(start_time, response, user_message):
    end_time = time.time()
    processing_time = end_time - start_time

    metrics = {
        "processing_time": processing_time,
        "message_length": len(user_message),
        "response_length": len(response),
        "timestamp": datetime.now().isoformat()
    }

    # Langfuseë¡œ ë©”íŠ¸ë¦­ ì „ì†¡
    langfuse_handler.log_metrics(metrics)
```

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Gradio ChatInterface](https://gradio.app/docs/chatinterface) - ê³µì‹ ChatInterface ê°€ì´ë“œ
- [LangChain Memory](https://python.langchain.com/docs/concepts/memory/) - ë©”ëª¨ë¦¬ ê´€ë¦¬ ë¬¸ì„œ
- [OpenAI API](https://platform.openai.com/docs/api-reference/chat) - Chat Completions API

### í•™ìŠµ ìë£Œ
- [Gradio ì¿¡ë¶](https://github.com/gradio-app/gradio/tree/main/demo) - ë‹¤ì–‘í•œ ì˜ˆì œ ëª¨ìŒ
- [LangChain Gradio í†µí•©](https://python.langchain.com/docs/integrations/tools/gradio_tools/) - í†µí•© ê°€ì´ë“œ
- [ë©€í‹°ëª¨ë‹¬ ì±—ë´‡ êµ¬í˜„](https://huggingface.co/spaces) - HuggingFace ì˜ˆì œ

### ê°œë°œ ë„êµ¬
- [Gradio Hub](https://gradio.app/) - ì˜¨ë¼ì¸ ë°°í¬ í”Œë«í¼
- [HuggingFace Spaces](https://huggingface.co/spaces) - ë¬´ë£Œ í˜¸ìŠ¤íŒ…
- [Langfuse](https://langfuse.com/) - LLM ì˜µì €ë¹Œë¦¬í‹°

### ì¶”ê°€ í•™ìŠµ
- UI/UX ë””ìì¸ íŒ¨í„´ê³¼ ì‚¬ìš©ì ê²½í—˜ ìµœì í™”
- ì‹¤ì‹œê°„ ì±„íŒ…ê³¼ WebSocket í†µì‹ 
- ë©€í‹°ëª¨ë‹¬ AIì™€ ì´ë¯¸ì§€/ë¬¸ì„œ ì²˜ë¦¬
- í”„ë¡œë•ì…˜ ë°°í¬ì™€ ìŠ¤ì¼€ì¼ë§ ì „ëµ