# W2_006 RunnableConfigì™€ Fallback ì²˜ë¦¬

## í•™ìŠµ ëª©í‘œ
- LangChain RunnableConfigë¥¼ í™œìš©í•œ ëŸ°íƒ€ì„ ë™ì‘ ì œì–´ ë°©ë²• í•™ìŠµ
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì½œë°± í•¸ë“¤ëŸ¬ êµ¬í˜„
- Fallback ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•œ ì•ˆì •ì ì¸ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•
- ë™ì  ëª¨ë¸ ì„ íƒ ë° í”„ë¡¬í”„íŠ¸ ì „í™˜ ê¸°ë²• ìŠµë“

## í•µì‹¬ ê°œë…

### 1. RunnableConfigë€?
- **ì •ì˜**: LangChainì—ì„œ ëŸ°íƒ€ì„ì— Runnable(ì‹¤í–‰ ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸)ì˜ ë™ì‘ì„ ì„¸ë°€í•˜ê²Œ ì œì–´í•˜ê¸° ìœ„í•œ ì„¤ì • ê°ì²´
- **ì—­í• **: ì²´ì¸, íˆ´, ëª¨ë¸ ë“± ë‹¤ì–‘í•œ Runnableì— ì „ë‹¬ë˜ì–´ ì‹¤í–‰ ì‹œ ë™ì‘ì„ ì¡°ì •
- **íŠ¹ì§•**: ì‹¤í–‰ ì¤‘ì¸ Runnableê³¼ í•˜ìœ„ í˜¸ì¶œë“¤ì— ì„¤ì •ì„ ì „ë‹¬í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ì—­í• 

### 2. RunnableConfigì˜ ì£¼ìš” ì†ì„±

#### configurable
- **ìš©ë„**: ëŸ°íƒ€ì„ì— ì¡°ì • ê°€ëŠ¥í•œ ì†ì„± ê°’ ì „ë‹¬
- **ì˜ˆì‹œ**: ëª¨ë¸ì˜ ì˜¨ë„, ì„¸ì…˜ ID, í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë“±

#### callbacks
- **ìš©ë„**: ì‹¤í–‰ ê³¼ì •ì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ì½œë°± í•¸ë“¤ëŸ¬ ì§€ì •
- **ì˜ˆì‹œ**: ë¡œê¹…, ëª¨ë‹ˆí„°ë§, ì´ë²¤íŠ¸ ì¶”ì 

#### tags
- **ìš©ë„**: ì‹¤í–‰ì— íƒœê·¸ë¥¼ ë¶™ì—¬ ì¶”ì  ë° í•„í„°ë§
- **ì˜ˆì‹œ**: ì‹¤í—˜ ë²„ì „, ì‚¬ìš©ì ê·¸ë£¹, ìš”ì²­ íƒ€ì… ë“±

#### metadata
- **ìš©ë„**: ì‹¤í–‰ ê´€ë ¨ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì „ë‹¬
- **ì˜ˆì‹œ**: ìš”ì²­ ID, ì‚¬ìš©ì ì •ë³´, ì„¸ì…˜ ë°ì´í„° ë“±

### 3. Fallbackì´ë€?
- **ì •ì˜**: ì£¼ìš” ì‹¤í–‰ ê²½ë¡œê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ ìë™ìœ¼ë¡œ ëŒ€ì²´ ê²½ë¡œë¥¼ ì‹¤í–‰í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜
- **í•„ìš”ì„±**: API ì•ˆì •ì„± ë¬¸ì œ, ë¹„ìš© ìµœì í™”, ì„±ëŠ¥ ìµœì í™”, ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ëŒ€ì‘

## í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
pip install langchain langchain-openai langchain-google-genai
pip install python-dotenv
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
from dotenv import load_dotenv
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Langsmith ì¶”ì  í™•ì¸
print("Langsmith ì¶”ì :", os.getenv('LANGSMITH_TRACING'))
```

## 1ë‹¨ê³„: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì½œë°± í•¸ë“¤ëŸ¬ êµ¬í˜„

### PerformanceMonitoringCallback í´ë˜ìŠ¤
```python
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class PerformanceMonitoringCallback(BaseCallbackHandler):
    """LLM í˜¸ì¶œ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°± í•¸ë“¤ëŸ¬"""

    def __init__(self):
        self.start_time: Optional[float] = None       # LLM í˜¸ì¶œ ì‹œì‘ ì‹œê°„
        self.token_usage: Dict[str, Any] = {}         # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´
        self.call_count: int = 0                      # LLM í˜¸ì¶œ íšŸìˆ˜

    def on_llm_start(
        self,
        serialized: Dict[str, Any],
        prompts: List[str],
        **kwargs: Any
    ) -> None:
        """LLM í˜¸ì¶œì´ ì‹œì‘ë  ë•Œ í˜¸ì¶œ"""
        self.start_time = time.time()
        self.call_count += 1
        print(f"ğŸš€ LLM í˜¸ì¶œ #{self.call_count} ì‹œì‘ - {datetime.now().strftime('%H:%M:%S')}")

        # ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ì˜ ê¸¸ì´ í™•ì¸
        if prompts:
            print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompts[0])} ë¬¸ì")

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """LLM í˜¸ì¶œì´ ì™„ë£Œë  ë•Œ í˜¸ì¶œ"""
        if self.start_time:
            duration = time.time() - self.start_time
            print(f"âœ… LLM í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ")

            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
            if response.generations:
                generation = response.generations[0][0]
                usage = response.llm_output.get('token_usage', {})

                if usage:
                    print(f"ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰: {usage}")
                    self.token_usage = usage

                # ì‘ë‹µ ê¸¸ì´ ì²´í¬
                if hasattr(generation, 'text'):
                    response_text = generation.text
                    print(f"ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(response_text)} ë¬¸ì")

    def on_llm_error(self, error: Exception, **kwargs: Any) -> None:
        """LLM í˜¸ì¶œì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ë•Œ í˜¸ì¶œ"""
        print(f"âŒ LLM í˜¸ì¶œ ì˜¤ë¥˜: {str(error)}")

    def get_statistics(self) -> Dict[str, Any]:
        """í˜„ì¬ê¹Œì§€ì˜ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜"""
        return {
            "total_calls": self.call_count,
            "last_token_usage": self.token_usage
        }

# ì‚¬ìš© ì˜ˆì‹œ
performance_callback = PerformanceMonitoringCallback()
print("ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„± ì™„ë£Œ")
```

## 2ë‹¨ê³„: ì‹¤ì‹œê°„ ì•Œë¦¼ ì½œë°± í•¸ë“¤ëŸ¬ êµ¬í˜„

### AlertCallback í´ë˜ìŠ¤
```python
class AlertCallback(BaseCallbackHandler):
    """íŠ¹ì • ì¡°ê±´ì—ì„œ ì•Œë¦¼ì„ ë³´ë‚´ëŠ” ì½œë°± í•¸ë“¤ëŸ¬"""

    def __init__(
        self,
        cost_threshold: float = 1.0,            # ë¹„ìš© ì„ê³„ê°’ (ë‹¬ëŸ¬ ë‹¨ìœ„)
        response_time_threshold: float = 10.0,  # ì‘ë‹µ ì‹œê°„ ì„ê³„ê°’ (ì´ˆ ë‹¨ìœ„)
        token_threshold: int = 4000             # ê¸´ í”„ë¡¬í”„íŠ¸ í† í° ì„ê³„ê°’
    ):
        self.cost_threshold = cost_threshold
        self.response_time_threshold = response_time_threshold
        self.token_threshold = token_threshold
        self.start_time: Optional[float] = None
        self.cumulative_cost: float = 0.0

    def on_llm_start(
        self,
        serialized: Dict[str, Any],
        prompts: List[str],
        **kwargs: Any
    ) -> None:
        """LLM í˜¸ì¶œì´ ì‹œì‘ë  ë•Œ í˜¸ì¶œ"""
        self.start_time = time.time()

        # ê¸´ í”„ë¡¬í”„íŠ¸ ê²½ê³ 
        if prompts and len(prompts[0]) > self.token_threshold:
            self._send_alert(f"âš ï¸ ê¸´ í”„ë¡¬í”„íŠ¸ ê°ì§€: {len(prompts[0])} ë¬¸ì")

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """LLM í˜¸ì¶œì´ ì™„ë£Œë  ë•Œ í˜¸ì¶œ"""
        # ì‘ë‹µ ì‹œê°„ ì²´í¬
        if self.start_time:
            duration = time.time() - self.start_time
            if duration > self.response_time_threshold:
                self._send_alert(f"ğŸŒ ëŠë¦° ì‘ë‹µ: {duration:.2f}ì´ˆ")

        # ë¹„ìš© ì²´í¬
        if response.generations:
            usage = response.llm_output.get('token_usage', {})

            if usage:
                # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
                total_tokens = usage.get('total_tokens', 0)
                if total_tokens == 0:
                    total_tokens = usage.get('input_tokens', 0) + usage.get('output_tokens', 0)

                # ê°„ë‹¨í•œ ë¹„ìš© ê³„ì‚° (ì‹¤ì œë¡œëŠ” ëª¨ë¸ë³„ ê°€ê²© ì ìš© í•„ìš”)
                estimated_cost = (total_tokens / 1000) * 0.002
                self.cumulative_cost += estimated_cost

                if self.cumulative_cost > self.cost_threshold:
                    self._send_alert(f"ğŸ’¸ ë¹„ìš© ì„ê³„ê°’ ì´ˆê³¼: ${self.cumulative_cost:.4f}")

    def on_llm_error(self, error: Exception, **kwargs: Any) -> None:
        """LLM í˜¸ì¶œì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ë•Œ í˜¸ì¶œ"""
        self._send_alert(f"ğŸš¨ LLM ì˜¤ë¥˜ ë°œìƒ: {str(error)}")

    def _send_alert(self, message: str) -> None:
        """ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Slack, Discord, ì´ë©”ì¼ ë“±ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        alert_message = f"[ALERT] {timestamp} - {message}"
        print("ğŸ”” ì•Œë¦¼ ì „ì†¡:", alert_message)

        # ë¡œê¹… ì‹œìŠ¤í…œì— ê¸°ë¡
        logging.warning(alert_message)

        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì™¸ë¶€ ì„œë¹„ìŠ¤ì— ì•Œë¦¼ ì „ì†¡
        # self._send_slack_notification(message)
        # self._send_email_notification(message)

    def reset_cost_tracking(self) -> None:
        """ëˆ„ì  ë¹„ìš© ì¶”ì ì„ ë¦¬ì…‹"""
        self.cumulative_cost = 0.0

# ì•Œë¦¼ ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
alert_callback = AlertCallback(
    cost_threshold=0.01,           # $0.01 ì´ìƒ ì‹œ ì•Œë¦¼
    response_time_threshold=5.0,   # 5ì´ˆ ì´ìƒ ì‹œ ì•Œë¦¼
    token_threshold=3000           # 3000 ë¬¸ì ì´ìƒ ì‹œ ì•Œë¦¼
)
print("ì•Œë¦¼ ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„± ì™„ë£Œ")
```

## 3ë‹¨ê³„: RunnableConfig ê¸°ë³¸ ì‚¬ìš©

### configurable_fieldsë¥¼ í†µí•œ ë™ì  ì„¤ì •
```python
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField

# ë™ì ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥í•œ ëª¨ë¸ ìƒì„±
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7
).configurable_fields(
    temperature=ConfigurableField(
        id="llm_temperature",
        name="LLM Temperature",
        description="The temperature of the LLM"
    ),
    model=ConfigurableField(
        id="llm_model",
        name="LLM Model",
        description="The model to use"
    )
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = PromptTemplate.from_template(
    "ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”: {question}"
)

# ì²´ì¸ êµ¬ì„±
chain = prompt | llm | StrOutputParser()

# RunnableConfigë¥¼ ì‚¬ìš©í•œ ì‹¤í–‰
from langchain_core.runnables import RunnableConfig

# ê¸°ë³¸ ì‹¤í–‰
response1 = chain.invoke(
    {"question": "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"},
    config=RunnableConfig(
        callbacks=[performance_callback, alert_callback],
        tags=["basic_query", "ai_question"],
        metadata={"user_id": "user_001", "session_id": "session_123"}
    )
)

print("\nê¸°ë³¸ ì‹¤í–‰ ê²°ê³¼:")
print(response1)

# ì˜¨ë„ ì¡°ì •í•œ ì‹¤í–‰
response2 = chain.invoke(
    {"question": "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"},
    config=RunnableConfig(
        configurable={"llm_temperature": 0.2},  # ë” ì¼ê´€ëœ ë‹µë³€
        callbacks=[performance_callback],
        tags=["low_temperature"]
    )
)

print("\në‚®ì€ ì˜¨ë„ ì‹¤í–‰ ê²°ê³¼:")
print(response2)

# ëª¨ë¸ ë³€ê²½í•œ ì‹¤í–‰
response3 = chain.invoke(
    {"question": "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"},
    config=RunnableConfig(
        configurable={"llm_model": "gpt-4o"},  # ë” ê°•ë ¥í•œ ëª¨ë¸
        callbacks=[performance_callback],
        tags=["high_quality_model"]
    )
)

print("\nGPT-4 ì‹¤í–‰ ê²°ê³¼:")
print(response3)
```

## 4ë‹¨ê³„: ë™ì  ëª¨ë¸ ë° í”„ë¡¬í”„íŠ¸ ì„ íƒ

### 1. ë™ì  ëª¨ë¸ ì„ íƒ
```python
from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

# ì—¬ëŸ¬ ëª¨ë¸ì„ ëŒ€ì•ˆìœ¼ë¡œ ì„¤ì •
llm_dynamic = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7
).configurable_alternatives(
    ConfigurableField(id="llm_model"),
    default_key="openai",
    gemini=ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.7),
    gpt4=ChatOpenAI(model="gpt-4o", temperature=0.7),
)

chain_dynamic = prompt | llm_dynamic | StrOutputParser()

# OpenAI ëª¨ë¸ ì‚¬ìš© (ê¸°ë³¸ê°’)
result_openai = chain_dynamic.invoke(
    {"question": "íŒŒì´ì¬ì˜ ì¥ì ì€?"},
    config={"configurable": {"llm_model": "openai"}}
)

print("OpenAI ê²°ê³¼:", result_openai)

# Gemini ëª¨ë¸ ì‚¬ìš©
result_gemini = chain_dynamic.invoke(
    {"question": "íŒŒì´ì¬ì˜ ì¥ì ì€?"},
    config={"configurable": {"llm_model": "gemini"}}
)

print("\nGemini ê²°ê³¼:", result_gemini)

# GPT-4 ëª¨ë¸ ì‚¬ìš©
result_gpt4 = chain_dynamic.invoke(
    {"question": "íŒŒì´ì¬ì˜ ì¥ì ì€?"},
    config={"configurable": {"llm_model": "gpt4"}}
)

print("\nGPT-4 ê²°ê³¼:", result_gpt4)
```

### 2. ë™ì  í”„ë¡¬í”„íŠ¸ ì„ íƒ
```python
from langchain_core.prompts import PromptTemplate

# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
default_prompt = PromptTemplate.from_template(
    "ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”: {question}"
)

# ìƒì„¸ í”„ë¡¬í”„íŠ¸
detailed_prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì— ìƒì„¸í•˜ê³  êµ¬ì¡°ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€ í˜•ì‹:
1. ê°œìš”
2. í•µì‹¬ ë‚´ìš©
3. ì˜ˆì‹œ
4. ê²°ë¡ 
"""
)

# ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
concise_prompt = PromptTemplate.from_template(
    "í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€: {question}"
)

# ë™ì  í”„ë¡¬í”„íŠ¸ ì„¤ì •
dynamic_prompt = default_prompt.configurable_alternatives(
    ConfigurableField(id="prompt_style"),
    default_key="default",
    detailed=detailed_prompt,
    concise=concise_prompt,
)

chain_with_dynamic_prompt = dynamic_prompt | llm | StrOutputParser()

# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
print("=== ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ===")
result1 = chain_with_dynamic_prompt.invoke(
    {"question": "ë¨¸ì‹ ëŸ¬ë‹ì´ë€?"},
    config={"configurable": {"prompt_style": "default"}}
)
print(result1)

# ìƒì„¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
print("\n=== ìƒì„¸ í”„ë¡¬í”„íŠ¸ ===")
result2 = chain_with_dynamic_prompt.invoke(
    {"question": "ë¨¸ì‹ ëŸ¬ë‹ì´ë€?"},
    config={"configurable": {"prompt_style": "detailed"}}
)
print(result2)

# ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
print("\n=== ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ ===")
result3 = chain_with_dynamic_prompt.invoke(
    {"question": "ë¨¸ì‹ ëŸ¬ë‹ì´ë€?"},
    config={"configurable": {"prompt_style": "concise"}}
)
print(result3)
```

## 5ë‹¨ê³„: Fallback ì²˜ë¦¬ êµ¬í˜„

### Fallbackì´ í•„ìš”í•œ ì´ìœ 
1. **API ì•ˆì •ì„± ë¬¸ì œ**: ìš”ê¸ˆ ì œí•œ, ì„œë²„ ë‹¤ìš´íƒ€ì„, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜
2. **ë¹„ìš© ìµœì í™”**: ì €ë ´í•œ ëª¨ë¸ì„ ë¨¼ì € ì‹œë„, ì‹¤íŒ¨ ì‹œ ë¹„ì‹¼ ëª¨ë¸ ì‚¬ìš©
3. **ì„±ëŠ¥ ìµœì í™”**: ë¹ ë¥¸ ëª¨ë¸ ìš°ì„  ì‚¬ìš©, ë³µì¡í•œ ì‘ì—…ì—ë§Œ ê³ ì„±ëŠ¥ ëª¨ë¸
4. **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ**: ì§§ì€ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ ë¨¼ì € ì‹œë„

### Fallback ì²´ì¸ êµ¬ì„±
```python
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

# ì£¼ ëª¨ë¸ (ì €ë ´í•˜ê³  ë¹ ë¦„)
primary_llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    request_timeout=5  # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
)

# ë°±ì—… ëª¨ë¸ 1
backup_llm_1 = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.7
)

# ë°±ì—… ëª¨ë¸ 2 (ê°€ì¥ ê°•ë ¥í•˜ì§€ë§Œ ë¹„ìŒˆ)
backup_llm_2 = ChatOpenAI(
    model="gpt-4o",
    temperature=0.7
)

# Fallback ì²´ì¸ êµ¬ì„±
llm_with_fallback = primary_llm.with_fallbacks(
    [backup_llm_1, backup_llm_2]
)

# í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸
prompt = PromptTemplate.from_template("ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€: {question}")
fallback_chain = prompt | llm_with_fallback | StrOutputParser()

# Fallback í…ŒìŠ¤íŠ¸
try:
    result = fallback_chain.invoke(
        {"question": "ì–‘ì ì»´í“¨íŒ…ì˜ ë¯¸ë˜ëŠ”?"},
        config=RunnableConfig(
            callbacks=[performance_callback, alert_callback]
        )
    )
    print("ê²°ê³¼:", result)
except Exception as e:
    print(f"ëª¨ë“  Fallback ì‹¤íŒ¨: {e}")
```

### ì¡°ê±´ë¶€ Fallback
```python
from langchain_core.runnables import RunnableLambda

def check_response_quality(response):
    """ì‘ë‹µ í’ˆì§ˆì„ ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜"""
    # ì‘ë‹µì´ ë„ˆë¬´ ì§§ìœ¼ë©´ False ë°˜í™˜
    if len(response) < 50:
        raise ValueError("ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
    return response

# í’ˆì§ˆ ê²€ì‚¬ë¥¼ í¬í•¨í•œ ì²´ì¸
quality_checked_chain = (
    prompt
    | primary_llm
    | StrOutputParser()
    | RunnableLambda(check_response_quality)
).with_fallbacks([
    prompt | backup_llm_1 | StrOutputParser(),
    prompt | backup_llm_2 | StrOutputParser()
])

# ì‹¤í–‰
result = quality_checked_chain.invoke(
    {"question": "ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"}
)
print("í’ˆì§ˆ ê²€ì¦ëœ ê²°ê³¼:", result)
```

## 6ë‹¨ê³„: ì•Œë¦¼ ì¡°ê±´ í…ŒìŠ¤íŠ¸

### ë‹¤ì–‘í•œ ì•Œë¦¼ ì‹œë‚˜ë¦¬ì˜¤
```python
# 1. ê¸´ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
long_question = "ì¸ê³µì§€ëŠ¥" * 1000  # ê¸´ í”„ë¡¬í”„íŠ¸ ìƒì„±
try:
    chain.invoke(
        {"question": long_question},
        config=RunnableConfig(callbacks=[alert_callback])
    )
except Exception as e:
    print(f"ê¸´ í”„ë¡¬í”„íŠ¸ ì˜¤ë¥˜: {e}")

# 2. ë‹¤ìˆ˜ì˜ ìš”ì²­ìœ¼ë¡œ ë¹„ìš© ì„ê³„ê°’ í…ŒìŠ¤íŠ¸
alert_callback.reset_cost_tracking()
for i in range(10):
    chain.invoke(
        {"question": f"ì§ˆë¬¸ {i+1}: AIë€ ë¬´ì—‡ì¸ê°€?"},
        config=RunnableConfig(callbacks=[alert_callback])
    )
    print(f"ëˆ„ì  ë¹„ìš©: ${alert_callback.cumulative_cost:.4f}")

# 3. í†µê³„ í™•ì¸
stats = performance_callback.get_statistics()
print("\nìµœì¢… í†µê³„:")
print(f"ì´ í˜¸ì¶œ íšŸìˆ˜: {stats['total_calls']}")
print(f"ë§ˆì§€ë§‰ í† í° ì‚¬ìš©ëŸ‰: {stats['last_token_usage']}")
```

## ì‹¤ìŠµ ê³¼ì œ

### ê¸°ë³¸ ì‹¤ìŠµ
1. **ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬ ì‘ì„±**
   - íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì‘ë‹µì„ í•„í„°ë§í•˜ëŠ” ì½œë°±
   - ì‘ë‹µ ì‹œê°„ì´ ì¼ì • ì‹œê°„ ì´ìƒì´ë©´ ë¡œê·¸ ì €ì¥
   - ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ì‹œë„ ë¡œì§ í¬í•¨

2. **ë™ì  ì„¤ì • í™œìš©**
   - ì‚¬ìš©ì ë“±ê¸‰ì— ë”°ë¼ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
   - ì‹œê°„ëŒ€ì— ë”°ë¼ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ì ìš©
   - ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼ ì˜¨ë„ ìë™ ì¡°ì •

### ì‘ìš© ì‹¤ìŠµ
3. **ë‹¤ë‹¨ê³„ Fallback ì‹œìŠ¤í…œ**
   - 3ê°œ ì´ìƒì˜ ëª¨ë¸ë¡œ Fallback ì²´ì¸ êµ¬ì„±
   - ê° ëª¨ë¸ì˜ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ ë° ë¡œê¹…
   - ë¹„ìš©ê³¼ ì„±ëŠ¥ì„ ê³ ë ¤í•œ ìµœì  Fallback ì „ëµ ìˆ˜ë¦½

4. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**
   - ì½œë°± ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì‹œê°í™”
   - ì„±ëŠ¥ ì§€í‘œ ì¶”ì  (ì‘ë‹µ ì‹œê°„, í† í° ì‚¬ìš©ëŸ‰, ë¹„ìš©)
   - ì•Œë¦¼ ë°œìƒ ì´ë ¥ ê´€ë¦¬

### ì‹¬í™” ì‹¤ìŠµ
5. **ì ì‘í˜• ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ**
   - ì§ˆë¬¸ ìœ í˜• ìë™ ë¶„ë¥˜
   - ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ìµœì  ëª¨ë¸ ìë™ ì„ íƒ
   - ì„±ëŠ¥ í”¼ë“œë°±ì„ ë°˜ì˜í•œ ëª¨ë¸ ì„ íƒ ê°œì„ 

6. **í”„ë¡œë•ì…˜ ë ˆë²¨ ì—ëŸ¬ ì²˜ë¦¬**
   - ë‹¤ì–‘í•œ ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ ëŒ€ì‘
   - Circuit Breaker íŒ¨í„´ êµ¬í˜„
   - ê·¸ë ˆì´ìŠ¤í’€ ë””ê·¸ë ˆì´ë°ì´ì…˜(Graceful Degradation) êµ¬í˜„

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜ë“¤
1. **ì½œë°± í•¸ë“¤ëŸ¬ ì‘ë™ ì•ˆ í•¨**
   ```python
   # configì— callbacksë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í–ˆëŠ”ì§€ í™•ì¸
   config = RunnableConfig(callbacks=[your_callback])
   result = chain.invoke(input_data, config=config)
   ```

2. **Fallbackì´ ì‘ë™í•˜ì§€ ì•ŠìŒ**
   ```python
   # Fallbackì€ ì˜ˆì™¸ ë°œìƒ ì‹œì—ë§Œ ì‘ë™
   # íƒ€ì„ì•„ì›ƒ ì„¤ì • í™•ì¸
   llm = ChatOpenAI(request_timeout=5)
   ```

3. **ì„¤ì • ê°’ì´ ì ìš©ë˜ì§€ ì•ŠìŒ**
   ```python
   # configurable_fieldsì˜ idì™€ configì˜ í‚¤ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
   llm = ChatOpenAI().configurable_fields(
       temperature=ConfigurableField(id="temp")
   )
   # ì‚¬ìš© ì‹œ
   config = {"configurable": {"temp": 0.5}}
   ```

## ì°¸ê³  ìë£Œ
- [LangChain RunnableConfig ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/docs/concepts/runnables/)
- [LangChain Fallbacks ê°€ì´ë“œ](https://python.langchain.com/docs/how_to/fallbacks/)
- [LangSmith Alert ì„¤ì •](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
- [Callback Handlers ê°€ì´ë“œ](https://python.langchain.com/docs/modules/callbacks/)

ì´ í•™ìŠµ ê°€ì´ë“œë¥¼ í†µí•´ RunnableConfigì™€ Fallback ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•˜ì—¬ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.