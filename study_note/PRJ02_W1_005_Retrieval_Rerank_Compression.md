# PRJ02_W1_005 ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ ê¸°ë²• ë§¤ë‰´ì–¼ - ì¬ìˆœìœ„í™” & ë§¥ë½ì  ì••ì¶•

## ğŸ“‹ ê°œìš”

ì´ ë…¸íŠ¸ë¶ì€ RAG ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ ê³ ê¸‰ ê¸°ë²•ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤. Re-ranking(ì¬ìˆœìœ„í™”)ê³¼ Contextual Compression(ë§¥ë½ì  ì••ì¶•) ê¸°ë²•ì„ í†µí•´ ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆê³¼ ê´€ë ¨ì„±ì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.

### ğŸ“Š ì‹¤í—˜ í™˜ê²½ ë° ì„±ê³¼ ìš”ì•½
- **ì‹œìŠ¤í…œ í™˜ê²½**: 16ì½”ì–´ 128GB ìµœì í™” ì‹œìŠ¤í…œ
- **ì‹¤í–‰ ì‹œê°„**: ì´ 5.475ì´ˆ (ë¬¸ì„œ ë¡œë”© 0.120ì´ˆ, ì„ë² ë”© ì„¤ì • 0.611ì´ˆ, ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ 3.230ì´ˆ)
- **ì²˜ë¦¬ ì„±ëŠ¥**: 89ê°œ ì²­í¬, 733.1ì²­í¬/ì´ˆ ë¶„í•  ì„±ëŠ¥
- **ì£¼ìš” ê¸°ìˆ **: CrossEncoderReranker, LLMListwiseRerank, LLMChainFilter, LLMChainExtractor, EmbeddingsFilter
- **í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬**: "í…ŒìŠ¬ë¼ íŠ¸ëŸ­ ëª¨ë¸ì´ ìˆë‚˜ìš”?"

### ğŸ¯ í•™ìŠµ ëª©í‘œ
- Re-ranking ê¸°ë²•ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœìœ„ ìµœì í™”
- Contextual Compressionìœ¼ë¡œ ë¬¸ì„œì˜ ê´€ë ¨ì„± ë° ì••ì¶• ìˆ˜í–‰
- DocumentCompressorPipelineì„ í†µí•œ ë‹¤ë‹¨ê³„ ë¬¸ì„œ ì²˜ë¦¬
- RAG ì²´ì¸ê³¼ í†µí•©ëœ ê³ ì„±ëŠ¥ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•

## ğŸ› ï¸ í™˜ê²½ ì„¤ì •

### 1. í•„ìˆ˜ íŒ¨í‚¤ì§€
```python
# í™˜ê²½ë³€ìˆ˜ ë° ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from dotenv import load_dotenv
import os
from glob import glob
from pprint import pprint
import json

# LangChain í•µì‹¬ ëª¨ë“ˆ
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel

# ì¬ìˆœìœ„í™” ë° ì••ì¶• ëª¨ë“ˆ
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import (
    CrossEncoderReranker,
    LLMListwiseRerank,
    LLMChainFilter,
    LLMChainExtractor,
    EmbeddingsFilter,
    DocumentCompressorPipeline
)
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.document_transformers import EmbeddingsRedundantFilter

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
import asyncio
import time
import psutil
import threading
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from pathlib import Path

# Langfuse íŠ¸ë ˆì´ì‹±
from langfuse.langchain import CallbackHandler

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
```

### 2. ë²¡í„° ì €ì¥ì†Œ ë° ê¸°ë³¸ ê²€ìƒ‰ê¸° ì„¤ì •
```python
# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
chroma_db = Chroma(
    embedding_function=embeddings,
    collection_name="db_korean_cosine",
    persist_directory="./chroma_db"
)

# ê¸°ë³¸ retriever ì´ˆê¸°í™”
chroma_k_retriever = chroma_db.as_retriever(search_kwargs={"k": 4})

# LangChain ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
langfuse_handler = CallbackHandler()
```

## ğŸ¯ Re-rank (ì¬ìˆœìœ„í™”) ê¸°ë²•

### ê°œë…ê³¼ ì›ë¦¬
ì¬ìˆœìœ„í™”ëŠ” ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ë¶„ì„í•˜ì—¬ ìµœì ì˜ ìˆœì„œë¡œ ì •ë ¬í•˜ëŠ” ê³ ë„í™”ëœ ê¸°ìˆ ì…ë‹ˆë‹¤:

- **ì´ì¤‘ ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤**: ê¸°ë³¸ ê²€ìƒ‰ í›„ ì •êµí•œ ê¸°ì¤€ìœ¼ë¡œ ì¬í‰ê°€ ì§„í–‰
- **ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ**: ì²´ê³„ì ì¸ ìµœì í™” ë°©ë²•ë¡ ìœ¼ë¡œ ê´€ë ¨ì„± ê·¹ëŒ€í™”
- **ë§¥ë½ì  ì´í•´**: ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ë„˜ì–´ì„  ì˜ë¯¸ì  ê´€ë ¨ì„± ë¶„ì„

### 1. CrossEncoderReranker

**íŠ¹ì§•**:
- Cross-Encoder ëª¨ë¸ì„ í™œìš©í•œ ì •ë°€í•œ ì¬ì •ë ¬
- ìŒ(pair) ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬ë¡œ ë¬¸ì„œ-ì¿¼ë¦¬ ê´€ê³„ ë¶„ì„
- í†µí•© ì¸ì½”ë”© ë°©ì‹ìœ¼ë¡œ ìœ ì‚¬ë„ ì •í™•ë„ í–¥ìƒ

**êµ¬í˜„**:
```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

# CrossEncoderReranker ëª¨ë¸ ì´ˆê¸°í™”
model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-v2-m3")

# CrossEncoderReranker ëª¨ë¸ì„ ì‚¬ìš©í•œ re-ranker ì´ˆê¸°í™” (top_n: 3)
re_ranker = CrossEncoderReranker(model=model, top_n=3)

# CrossEncoderRerankerë¥¼ ì‚¬ìš©í•œ retriever ì´ˆê¸°í™”
cross_encoder_reranker_retriever = ContextualCompressionRetriever(
    base_compressor=re_ranker,
    base_retriever=chroma_k_retriever
)

# ê²€ìƒ‰ ìˆ˜í–‰
query = "í…ŒìŠ¬ë¼ íŠ¸ëŸ­ ëª¨ë¸ì´ ìˆë‚˜ìš”?"
retrieved_docs = cross_encoder_reranker_retriever.invoke(query, config={"callbacks": [langfuse_handler]})

for doc in retrieved_docs:
    print(f"{doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]")
    print("="*200)
```

### 2. LLMListwiseRerank

**íŠ¹ì§•**:
- ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ ì¬ìˆœìœ„í™”
- ì¿¼ë¦¬ì™€ ë¬¸ì„œ ê°„ ê´€ë ¨ì„± ë¶„ì„ìœ¼ë¡œ ìµœì  ìˆœì„œ ë„ì¶œ
- ì „ë¬¸í™”ëœ ì¬ìˆœìœ„í™” ëª¨ë¸ ì ìš©

**êµ¬í˜„**:
```python
from langchain.retrievers.document_compressors import LLMListwiseRerank
from langchain_openai import ChatOpenAI

# ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# LLMListwiseRerank ëª¨ë¸ ì´ˆê¸°í™” (top_n: 3)
re_ranker = LLMListwiseRerank.from_llm(llm, top_n=3)

# LLMListwiseRerank ëª¨ë¸ì„ ì‚¬ìš©í•œ re-ranker ì´ˆê¸°í™”
llm_reranker_retriever = ContextualCompressionRetriever(
    base_compressor=re_ranker,
    base_retriever=chroma_k_retriever
)

# ê²€ìƒ‰ ìˆ˜í–‰ ë° ê²°ê³¼ í™•ì¸
query = "í…ŒìŠ¬ë¼ íŠ¸ëŸ­ ëª¨ë¸ì´ ìˆë‚˜ìš”?"
retrieved_docs = llm_reranker_retriever.invoke(query, config={"callbacks": [langfuse_handler]})

for doc in retrieved_docs:
    print(f"{doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]")
    print("="*200)
```

## ğŸ—œï¸ Contextual Compression (ë§¥ë½ì  ì••ì¶•) ê¸°ë²•

### ê°œë…ê³¼ ì›ë¦¬
ë§¥ë½ì  ì••ì¶•ì€ ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ë§Œì„ ì„ ë³„í•˜ê³  ì••ì¶•í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤:

- **ê´€ë ¨ì„± ì¤‘ì‹¬**: ì¿¼ë¦¬ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ì¶”ì¶œ
- **íš¨ìœ¨ì„± í–¥ìƒ**: ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°ë¡œ ì²˜ë¦¬ ì†ë„ ê°œì„ 
- **í’ˆì§ˆ ë³´ì¥**: í•µì‹¬ ì •ë³´ ë³´ì¡´ìœ¼ë¡œ ë‹µë³€ í’ˆì§ˆ ìœ ì§€

### 1. LLMChainFilter

**íŠ¹ì§•**:
- LLM ê¸°ë°˜ í•„í„°ë§ìœ¼ë¡œ ë¬¸ì„œ í¬í•¨ ì—¬ë¶€ ê²°ì •
- ê´€ë ¨ì„± ê¸°ì¤€ ë¬¸ì„œ ì„ ë³„
- ì´ì§„ ê²°ì • (í¬í•¨/ì œì™¸) ë°©ì‹

**êµ¬í˜„**:
```python
from langchain.retrievers.document_compressors import LLMChainFilter

# ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# LLMChainFilter ëª¨ë¸ ì´ˆê¸°í™”
context_filter = LLMChainFilter.from_llm(llm)

# LLMChainFilter ëª¨ë¸ì„ ì‚¬ìš©í•œ retriever ì´ˆê¸°í™”
llm_filter_compression_retriever = ContextualCompressionRetriever(
    base_compressor=context_filter,     # LLM ê¸°ë°˜ ì••ì¶•ê¸°
    base_retriever=chroma_k_retriever   # ê¸°ë³¸ ê²€ìƒ‰ê¸°
)

# ê²€ìƒ‰ ìˆ˜í–‰
query = "í…ŒìŠ¬ë¼ íŠ¸ëŸ­ ëª¨ë¸ì´ ìˆë‚˜ìš”?"
retrieved_docs = llm_filter_compression_retriever.invoke(query, config={"callbacks": [langfuse_handler]})
```

### 2. LLMChainExtractor

**íŠ¹ì§•**:
- LLM ê¸°ë°˜ ì¶”ì¶œë¡œ ì¿¼ë¦¬ ê´€ë ¨ í•µì‹¬ ë‚´ìš© ì„ ë³„
- ë§ì¶¤í˜• ìš”ì•½ì„ í†µí•œ ì¿¼ë¦¬ ìµœì í™” ì••ì¶• ê²°ê³¼ ìƒì„±
- ë¬¸ì„œ ë‚´ìš©ì˜ ì •êµí•œ í¸ì§‘ê³¼ ì¶”ì¶œ

**êµ¬í˜„**:
```python
from langchain.retrievers.document_compressors import LLMChainExtractor

# ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# LLMChainExtractor ëª¨ë¸ ì´ˆê¸°í™”
compressor = LLMChainExtractor.from_llm(llm)

# LLMChainExtractor ëª¨ë¸ì„ ì‚¬ìš©í•œ retriever ì´ˆê¸°í™”
llm_extractor_compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,                        # LLM ê¸°ë°˜ ì••ì¶•ê¸°
    base_retriever=chroma_k_retriever                  # ê¸°ë³¸ ê²€ìƒ‰ê¸°
)

# ê²€ìƒ‰ ìˆ˜í–‰
query = "í…ŒìŠ¬ë¼ íŠ¸ëŸ­ ëª¨ë¸ì´ ìˆë‚˜ìš”?"
retrieved_docs = llm_extractor_compression_retriever.invoke(query, config={"callbacks": [langfuse_handler]})
```

### 3. EmbeddingsFilter

**íŠ¹ì§•**:
- ì„ë² ë”© ê¸°ë°˜ í•„í„°ë§ìœ¼ë¡œ ë¬¸ì„œ-ì¿¼ë¦¬ ìœ ì‚¬ë„ ê³„ì‚°
- ìœ ì‚¬ë„ ì„ê³„ê°’ ê¸°ë°˜ ë¬¸ì„œ ì„ ë³„
- ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ì™€ íš¨ìœ¨ì  í•„í„°ë§

**êµ¬í˜„**:
```python
from langchain.retrievers.document_compressors import EmbeddingsFilter

# ì„ë² ë”© ê¸°ë°˜ ì••ì¶•ê¸° ì´ˆê¸°í™”
embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.4)

# ì„ë² ë”© ê¸°ë°˜ ì••ì¶•ê¸°ë¥¼ ì‚¬ìš©í•œ retriever ì´ˆê¸°í™”
embeddings_filter_compression_retriever = ContextualCompressionRetriever(
    base_compressor=embeddings_filter,    # ì„ë² ë”© ê¸°ë°˜ ì••ì¶•ê¸°
    base_retriever=chroma_k_retriever     # ê¸°ë³¸ ê²€ìƒ‰ê¸°
)

# ê²€ìƒ‰ ìˆ˜í–‰
query = "í…ŒìŠ¬ë¼ íŠ¸ëŸ­ ëª¨ë¸ì´ ìˆë‚˜ìš”?"
retrieved_docs = embeddings_filter_compression_retriever.invoke(query, config={"callbacks": [langfuse_handler]})
```

### 4. DocumentCompressorPipeline

**íŠ¹ì§•**:
- íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¡œ ì—¬ëŸ¬ ì••ì¶•ê¸° ìˆœì°¨ ì—°ê²°
- ë³µí•© ë³€í™˜ ê¸°ëŠ¥ìœ¼ë¡œ ë‹¤ì–‘í•œ ì²˜ë¦¬ ê°€ëŠ¥
- ìµœì í™”ëœ ë‹¤ë‹¨ê³„ ë¬¸ì„œ ì²˜ë¦¬

**êµ¬í˜„**:
```python
from langchain.retrievers.document_compressors import DocumentCompressorPipeline
from langchain_community.document_transformers import EmbeddingsRedundantFilter

# ì„ë² ë”© ê¸°ë°˜ í•„í„° ì´ˆê¸°í™” - ì¤‘ë³µ ì œê±°
redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)

# ì„ë² ë”© ê¸°ë°˜ í•„í„° ì´ˆê¸°í™” - ìœ ì‚¬ë„ ê¸°ë°˜ í•„í„° (ì„ë² ë”© ìœ ì‚¬ë„ 0.4 ì´ìƒ)
relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.4)

# Re-ranking ëª¨ë¸ ì´ˆê¸°í™”
re_ranker = LLMListwiseRerank.from_llm(llm, top_n=2)

# DocumentCompressorPipeline ì´ˆê¸°í™” (ìˆœì°¨ì ìœ¼ë¡œ ì ìš©)
pipeline_compressor = DocumentCompressorPipeline(
    transformers=[redundant_filter, relevant_filter, re_ranker]
)

# DocumentCompressorPipelineì„ ì‚¬ìš©í•œ retriever ì´ˆê¸°í™”
pipeline_compression_retriever = ContextualCompressionRetriever(
    base_compressor=pipeline_compressor,    # DocumentCompressorPipeline ê¸°ë°˜ ì••ì¶•ê¸°
    base_retriever=chroma_k_retriever,      # ê¸°ë³¸ ê²€ìƒ‰ê¸°
)
```

## ğŸš€ ê³ ì„±ëŠ¥ ì‹œìŠ¤í…œ ìµœì í™” ì‹¤ìŠµ

### ì‹¤ìŠµ 1: 16ì½”ì–´ 128GB ìµœì í™” ê²€ìƒ‰ê¸° ì„¤ì •

**ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤**:
```python
class DetailedPerformanceMonitor:
    """ìƒì„¸ ì‹œê°„ ì¸¡ì • ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    def __init__(self):
        self.start_time = time.time()
        self.stage_times = {}
        self.stage_starts = {}

    def log_stage_start(self, stage_name):
        """ë‹¨ê³„ ì‹œì‘ ì‹œê°„ ê¸°ë¡"""
        start_time = time.time()
        start_datetime = datetime.now().strftime("%H:%M:%S.%f")[:-3]

        self.stage_starts[stage_name] = start_time

        print(f"â° [{start_datetime}] ğŸš€ {stage_name} ì‹œì‘")
        print(f"   ğŸ’» ì‹œì‘ ì‹œ ì‹œìŠ¤í…œ ìƒíƒœ: {self.get_detailed_system_stats()}")

    def log_stage_end(self, stage_name):
        """ë‹¨ê³„ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡"""
        end_time = time.time()
        end_datetime = datetime.now().strftime("%H:%M:%S.%f")[:-3]

        if stage_name in self.stage_starts:
            start_time = self.stage_starts[stage_name]
            duration = end_time - start_time

            self.stage_times[stage_name] = {
                'start_time': start_time,
                'end_time': end_time,
                'duration': duration,
                'start_datetime': start_datetime,
                'end_datetime': end_datetime
            }

            print(f"â° [{end_datetime}] âœ… {stage_name} ì™„ë£Œ")
            print(f"   â±ï¸  ì†Œìš”ì‹œê°„: {duration:.3f}ì´ˆ")
            print(f"   ğŸ’» ì¢…ë£Œ ì‹œ ì‹œìŠ¤í…œ ìƒíƒœ: {self.get_detailed_system_stats()}")
            print(f"   ğŸ“Š ì„±ëŠ¥ ìš”ì•½: {self.get_performance_summary(stage_name)}")

            return duration
        return 0

    def get_detailed_system_stats(self):
        """ìƒì„¸ ì‹œìŠ¤í…œ í†µê³„"""
        cpu_percent = psutil.cpu_percent(interval=None)
        cpu_per_core = psutil.cpu_percent(interval=None, percpu=True)
        memory = psutil.virtual_memory()

        return {
            'cpu_avg': f'{cpu_percent:.1f}%',
            'cpu_cores': [f'C{i}:{core:.1f}%' for i, core in enumerate(cpu_per_core)],
            'memory_used': f'{memory.used // (1024**3)}GB/{memory.total // (1024**3)}GB ({memory.percent:.1f}%)',
            'memory_available': f'{memory.available // (1024**3)}GB',
            'active_threads': threading.active_count()
        }
```

**ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼**:
```python
# ì‹¤ì œ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼
print("ğŸš€ 16ì½”ì–´ 128GB ì‹œìŠ¤í…œ ìµœì í™” ì´ˆê³ ì„±ëŠ¥ ê²€ìƒ‰ê¸° ì‹¤í–‰ (Jupyter í˜¸í™˜)")
print("="*100)

# ì´ ì‹¤í–‰ ì‹œê°„: 5.475ì´ˆ
# ë‹¨ê³„ë³„ ìƒì„¸ ì‹œê°„ ë¶„ì„:
# ì „ì²´_ê²€ìƒ‰ê¸°_ì´ˆê³ ì†_ì„¤ì •: 2.027ì´ˆ (37.0%)
# ê³ ì„±ëŠ¥_ë¬¸ì„œ_ë¡œë”©: 0.120ì´ˆ (âš¡ ì´ˆê³ ì†)
# ê³ ì„±ëŠ¥_í…ìŠ¤íŠ¸_ë¶„í• : 0.121ì´ˆ (âš¡ ì´ˆê³ ì†, 89ê°œ ì²­í¬, 733.1ì²­í¬/ì´ˆ)
# ì´ˆê³ ì„±ëŠ¥_ì„ë² ë”©_ì„¤ì •: 0.611ì´ˆ (âš¡ ì´ˆê³ ì†, 64ê°œ ë™ì‹œ ìš”ì²­)
# ì´ˆê³ ì†_BM25_ì¸ë±ì‹±: 0.120ì´ˆ (âš¡ ì´ˆê³ ì†, 8898.3ë¬¸ì„œ/ì´ˆ)
# ì´ˆê³ ì†_ë³‘ë ¬_ê²€ìƒ‰_í…ŒìŠ¤íŠ¸: 3.230ì´ˆ (ğŸš€ ë¹ ë¦„, 1.2ê²€ìƒ‰ê¸°/ì´ˆ)
```

### ì‹¤ìŠµ 2: ê²€ìƒ‰ê¸°ë²• ê³ ë„í™”

**4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ êµ¬ì„±**:
```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import (
    LLMListwiseRerank,
    LLMChainExtractor,
    EmbeddingsFilter,
    DocumentCompressorPipeline
)
from langchain_community.document_transformers import EmbeddingsRedundantFilter

print("ğŸ”§ ì‹¤ìŠµ 2: ê²€ìƒ‰ê¸°ë²• ê³ ë„í™” ì‹œì‘")

# 1) LLM Reranker ì„¤ì •
print("1ï¸âƒ£ LLM Reranker ì„¤ì •...")
llm_reranker = LLMListwiseRerank.from_llm(llm, top_n=3)

# 2) LLM Chain Extractor ì„¤ì • (ë§¥ë½ ì••ì¶•)
print("2ï¸âƒ£ LLM Chain Extractor ì„¤ì •...")
llm_extractor = LLMChainExtractor.from_llm(llm)

# 3) Embeddings Filter ì„¤ì • (ìœ ì‚¬ë„ ê¸°ë°˜ í•„í„°ë§)
print("3ï¸âƒ£ Embeddings Filter ì„¤ì •...")
embeddings_filter = EmbeddingsFilter(
    embeddings=embeddings,
    similarity_threshold=0.5
)

# 4) Redundant Filter ì„¤ì • (ì¤‘ë³µ ì œê±°)
print("4ï¸âƒ£ Redundant Filter ì„¤ì •...")
redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)

# 5) Pipeline Compressor êµ¬ì„±
print("5ï¸âƒ£ Pipeline Compressor êµ¬ì„±...")
pipeline_compressor = DocumentCompressorPipeline(
    transformers=[
        redundant_filter,      # 1ë‹¨ê³„: ì¤‘ë³µ ë¬¸ì„œ ì œê±°
        embeddings_filter,     # 2ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ í•„í„°ë§
        llm_extractor,         # 3ë‹¨ê³„: ë§¥ë½ ì••ì¶•
        llm_reranker           # 4ë‹¨ê³„: ì¬ìˆœìœ„í™”
    ]
)

# ê³ ë„í™”ëœ ê²€ìƒ‰ê¸° êµ¬ì„±
advanced_retriever = ContextualCompressionRetriever(
    base_compressor=pipeline_compressor,
    base_retriever=hybrid_retriever
)

print(f"\nğŸ¯ Pipeline êµ¬ì„±:")
print("1. ì¤‘ë³µ ì œê±° (EmbeddingsRedundantFilter)")
print("2. ìœ ì‚¬ë„ í•„í„°ë§ (EmbeddingsFilter, threshold=0.5)")
print("3. ë§¥ë½ ì••ì¶• (LLMChainExtractor)")
print("4. ì¬ìˆœìœ„í™” (LLMListwiseRerank, top_n=3)")

print(f"\nâœ… ì‹¤ìŠµ 2 ì™„ë£Œ!")
```

**ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ ë¹„êµ**:
```python
# ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼
print("--- ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ---")
basic_docs = hybrid_retriever.invoke(query)
print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(basic_docs)}")  # ê²°ê³¼: 10ê°œ

# ê³ ë„í™”ëœ ê²€ìƒ‰ ê²°ê³¼
print("--- ê³ ë„í™”ëœ ê²€ìƒ‰ ê²°ê³¼ (Pipeline) ---")
advanced_docs = advanced_retriever.invoke(query, config={"callbacks": [langfuse_handler]})
print(f"ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(advanced_docs)}")  # ê²°ê³¼: 2ê°œ (ì••ì¶•ë¨)

# ì‹¤ì œ Cybertruck ê´€ë ¨ ì •ë³´ê°€ ì •í™•íˆ ì¶”ì¶œë¨
print("1. - **Cybertruck:** 2019ë…„ 11ì›”ì— ì²˜ìŒ ë°œí‘œëœ í’€ì‚¬ì´ì¦ˆ í”½ì—… íŠ¸ëŸ­...")
```

### ì‹¤ìŠµ 3: RAG ì²´ì¸ ì—°ê²°

**LCELì„ í™œìš©í•œ RAG ì²´ì¸ êµ¬ì„±**:
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel

# 1) í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
template = """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}
ë‹µë³€:"""

prompt = ChatPromptTemplate.from_template(template)

# 2) LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 3) ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# 4) RAG ì²´ì¸ êµ¬ì„± (LCEL)
rag_chain = (
    RunnableParallel({
        "context": advanced_retriever | format_docs,  # ì‹¤ìŠµ2ì˜ ê³ ë„í™”ëœ ê²€ìƒ‰ê¸° ì‚¬ìš©
        "question": RunnablePassthrough()
    })
    | prompt
    | llm
    | StrOutputParser()
)

# 5) í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
test_questions = [
    "í…ŒìŠ¬ë¼ Cybertruckì˜ íŠ¹ì§•ê³¼ ì¶œì‹œë…„ë„ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
    "ë¦¬ë¹„ì•ˆ íšŒì‚¬ì˜ ì£¼ìš” ì „ê¸°ì°¨ ëª¨ë¸ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ì „ê¸°ì°¨ ì¶©ì „ ì¸í”„ë¼ì˜ í˜„ì¬ ìƒí™©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
]

print("ğŸš— RAG ì²´ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
print("="*80)

for i, question in enumerate(test_questions, 1):
    print(f"\nì§ˆë¬¸ {i}: {question}")
    print("-"*80)

    try:
        # RAG ì²´ì¸ ì‹¤í–‰
        answer = rag_chain.invoke(question, config={"callbacks": [langfuse_handler]})
        print(f"ë‹µë³€: {answer}")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
```

## ğŸ“Š ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™”

### ì‹¤í–‰ ì‹œê°„ ë¶„ì„
```python
# ì „ì²´ ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ
print("ğŸ“Š ì „ì²´ ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ")
print("="*100)
print("ğŸ• ì´ ì‹¤í–‰ ì‹œê°„: 5.475ì´ˆ")

print("\nğŸ“‹ ë‹¨ê³„ë³„ ìƒì„¸ ì‹œê°„ ë¶„ì„:")
print("ë‹¨ê³„ëª…                       ì‹œì‘ì‹œê°„         ì¢…ë£Œì‹œê°„         ì†Œìš”ì‹œê°„       ì„±ëŠ¥í‰ê°€")
print("-"*90)
print("ì „ì²´_ê²€ìƒ‰ê¸°_ì´ˆê³ ì†_ì„¤ì •         12:51:37.591 12:51:39.618 2.027    s ğŸš€ ë¹ ë¦„ (2.03s)")
print("ê³ ì„±ëŠ¥_ë¬¸ì„œ_ë¡œë”©               12:51:37.701 12:51:37.821 0.120    s âš¡ ì´ˆê³ ì† (120ms)")
print("ê³ ì„±ëŠ¥_í…ìŠ¤íŠ¸_ë¶„í•              12:51:37.931 12:51:38.052 0.121    s âš¡ ì´ˆê³ ì† (121ms)")
print("ì´ˆê³ ì„±ëŠ¥_ì„ë² ë”©_ì„¤ì •           12:51:38.162 12:51:38.772 0.611    s âš¡ ì´ˆê³ ì† (611ms)")
print("ì´ˆê³ ì†_BM25_ì¸ë±ì‹±            12:51:38.883 12:51:39.003 0.120    s âš¡ ì´ˆê³ ì† (120ms)")
print("ì´ˆê³ ì†_ë³‘ë ¬_ê²€ìƒ‰_í…ŒìŠ¤íŠ¸         12:51:39.725 12:51:42.955 3.230    s ğŸš€ ë¹ ë¦„ (3.23s)")

print("\nğŸ† ìµœëŒ€ ì‹œê°„ ì†Œìš” ë‹¨ê³„:")
print("   1. ì´ˆê³ ì†_ë³‘ë ¬_ê²€ìƒ‰_í…ŒìŠ¤íŠ¸: 3.23ì´ˆ (59.0%)")
print("   2. ì „ì²´_ê²€ìƒ‰ê¸°_ì´ˆê³ ì†_ì„¤ì •: 2.027ì´ˆ (37.0%)")
print("   3. ì´ˆê³ ì„±ëŠ¥_ì„ë² ë”©_ì„¤ì •: 0.611ì´ˆ (11.2%)")
```

### ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ
```python
# ì´ˆê³ ì„±ëŠ¥ ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼
print("ğŸ¯ ì´ˆê³ ì„±ëŠ¥ ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼")
print("="*100)

# í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
print("ğŸ” === í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ===")
print("1. Rivianì€ \"ìŠ¤ì¼€ì´íŠ¸ë³´ë“œ\" í”Œë«í¼(R1T ë° R1S ëª¨ë¸)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì „ê¸° ìŠ¤í¬ì¸  ìœ í‹¸ë¦¬í‹° ì°¨ëŸ‰...")

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼
print("ğŸ”„ === í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ===")
print("1. - **Cybertruck:** 2019ë…„ 11ì›”ì— ì²˜ìŒ ë°œí‘œëœ í’€ì‚¬ì´ì¦ˆ í”½ì—… íŠ¸ëŸ­...")

# ì˜ë¯¸ ê²€ìƒ‰ ê²°ê³¼
print("ğŸ§  === ì˜ë¯¸ ê²€ìƒ‰ ê²°ê³¼ ===")
print("1. - **Cybertruck:** Teslaì˜ ì „ê¸° í”½ì—… íŠ¸ëŸ­ ëª¨ë¸...")

# ë©€í‹°ì¿¼ë¦¬ ê²€ìƒ‰ ê²°ê³¼
print("ğŸ¯ === ë©€í‹°ì¿¼ë¦¬ ê²€ìƒ‰ ê²°ê³¼ ===")
print("1. Tesla Cybertruck ê´€ë ¨ ì¢…í•© ì •ë³´...")
```

## ğŸ’¡ ê¸°ë²•ë³„ íŠ¹ì„± ë¹„êµ

### Re-ranking ê¸°ë²• ë¹„êµ

| ê¸°ë²• | ì²˜ë¦¬ ë°©ì‹ | ì¥ì  | ë‹¨ì  | ìµœì  ì‚¬ìš© ìƒí™© |
|------|-----------|------|------|----------------|
| **CrossEncoderReranker** | Cross-Encoder ëª¨ë¸ | ë†’ì€ ì •í™•ë„, ìŒ ë‹¨ìœ„ ë¶„ì„ | ê³„ì‚° ë¹„ìš© ë†’ìŒ | ì •ë°€í•œ ì¬ìˆœìœ„í™” í•„ìš” |
| **LLMListwiseRerank** | LLM ê¸°ë°˜ ë¦¬ìŠ¤íŠ¸ ë¶„ì„ | ë§¥ë½ì  ì´í•´, ìœ ì—°ì„± | ì‘ë‹µ ì‹œê°„ ìƒëŒ€ì  ëŠë¦¼ | ë³µì¡í•œ ê´€ë ¨ì„± íŒë‹¨ |

### Compression ê¸°ë²• ë¹„êµ

| ê¸°ë²• | ì••ì¶• ë°©ì‹ | ì¥ì  | ë‹¨ì  | ìµœì  ì‚¬ìš© ìƒí™© |
|------|-----------|------|------|----------------|
| **LLMChainFilter** | ì´ì§„ í•„í„°ë§ | ëª…í™•í•œ ê²°ì •, ë¹ ë¥¸ ì²˜ë¦¬ | ì •ë³´ ì†ì‹¤ ìœ„í—˜ | ëª…í™•í•œ ê´€ë ¨ì„± ê¸°ì¤€ |
| **LLMChainExtractor** | ë‚´ìš© ì¶”ì¶œ | í•µì‹¬ ì •ë³´ ë³´ì¡´, ë§ì¶¤í˜• ìš”ì•½ | ì²˜ë¦¬ ì‹œê°„ ì†Œìš” | ì •ë°€í•œ ë‚´ìš© ì¶”ì¶œ |
| **EmbeddingsFilter** | ìœ ì‚¬ë„ ê¸°ë°˜ | ë¹ ë¥¸ ì²˜ë¦¬, íš¨ìœ¨ì  | ì˜ë¯¸ì  í•œê³„ | ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ |
| **DocumentCompressorPipeline** | ë‹¤ë‹¨ê³„ ì²˜ë¦¬ | ì¢…í•©ì  ìµœì í™” | ë³µì¡ì„± ì¦ê°€ | ìµœê³  í’ˆì§ˆ ì¶”êµ¬ |

## ğŸš€ ì‹¤ë¬´ í™œìš© ê°€ì´ë“œ

### 1. ê¸°ë²• ì„ íƒ ê¸°ì¤€

```python
def select_optimization_strategy(data_size, accuracy_requirement, latency_requirement):
    """ìƒí™©ë³„ ìµœì í™” ì „ëµ ì„ íƒ"""

    if latency_requirement == "low" and data_size == "large":
        return {
            "reranking": "EmbeddingsFilter",
            "compression": "EmbeddingsFilter",
            "pipeline": False
        }

    elif accuracy_requirement == "high":
        return {
            "reranking": "CrossEncoderReranker",
            "compression": "LLMChainExtractor",
            "pipeline": True
        }

    else:  # ê· í˜•ì¡íŒ ì„±ëŠ¥
        return {
            "reranking": "LLMListwiseRerank",
            "compression": "DocumentCompressorPipeline",
            "pipeline": True
        }
```

### 2. ì„±ëŠ¥ ìµœì í™” íŒ

```python
# ìºì‹±ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_rerank(query, doc_hash):
    """ì¬ìˆœìœ„í™” ê²°ê³¼ ìºì‹±"""
    # ì‹¤ì œ ì¬ìˆœìœ„í™” ë¡œì§
    pass

# ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± ì¦ëŒ€
def batch_compression(docs, batch_size=10):
    """ë°°ì¹˜ ë‹¨ìœ„ ì••ì¶• ì²˜ë¦¬"""
    results = []
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i+batch_size]
        batch_results = process_batch(batch)
        results.extend(batch_results)
    return results

# ì„ê³„ê°’ ë™ì  ì¡°ì •
def adaptive_threshold(query_complexity):
    """ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¥¸ ì„ê³„ê°’ ì¡°ì •"""
    if query_complexity == "high":
        return 0.7  # ë†’ì€ ì„ê³„ê°’ìœ¼ë¡œ ì—„ê²©í•œ í•„í„°ë§
    elif query_complexity == "medium":
        return 0.5  # ì¤‘ê°„ ì„ê³„ê°’
    else:
        return 0.3  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ í¬ìš©ì  í•„í„°ë§
```

### 3. ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

```python
class RetrievalPerformanceMonitor:
    """ê²€ìƒ‰ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""

    def __init__(self):
        self.metrics = []

    def log_retrieval(self, query, original_docs, final_docs, processing_time):
        """ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…"""
        metric = {
            'timestamp': datetime.now(),
            'query': query,
            'original_count': len(original_docs),
            'final_count': len(final_docs),
            'compression_ratio': len(final_docs) / len(original_docs),
            'processing_time': processing_time
        }
        self.metrics.append(metric)

    def get_performance_summary(self):
        """ì„±ëŠ¥ ìš”ì•½ í†µê³„"""
        if not self.metrics:
            return "ë°ì´í„° ì—†ìŒ"

        avg_compression = np.mean([m['compression_ratio'] for m in self.metrics])
        avg_time = np.mean([m['processing_time'] for m in self.metrics])

        return {
            'average_compression_ratio': avg_compression,
            'average_processing_time': avg_time,
            'total_queries': len(self.metrics)
        }
```

## ğŸ”§ ë¬¸ì œ í•´ê²° ë° ìµœì í™”

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

1. **ë©”ëª¨ë¦¬ ë¶€ì¡±**
```python
# ë°°ì¹˜ í¬ê¸° ì¡°ì •
batch_size = min(50, available_memory // estimated_doc_size)

# ìºì‹œ í¬ê¸° ì œí•œ
@lru_cache(maxsize=100)  # ê¸°ë³¸ 1000ì—ì„œ 100ìœ¼ë¡œ ê°ì†Œ
def cached_function(params):
    pass
```

2. **ì²˜ë¦¬ ì‹œê°„ ê³¼ë‹¤**
```python
# ë³‘ë ¬ ì²˜ë¦¬ í™œìš©
from concurrent.futures import ThreadPoolExecutor

def parallel_compression(docs, max_workers=4):
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(compress_doc, doc) for doc in docs]
        results = [future.result() for future in futures]
    return results
```

3. **í’ˆì§ˆ ì €í•˜**
```python
# ë‹¤ë‹¨ê³„ ê²€ì¦
def validate_compression_quality(original, compressed):
    # í•µì‹¬ ì •ë³´ ë³´ì¡´ í™•ì¸
    key_terms = extract_key_terms(original)
    preserved_ratio = count_preserved_terms(compressed, key_terms) / len(key_terms)

    return preserved_ratio > 0.8  # 80% ì´ìƒ ë³´ì¡´ ìš”êµ¬
```

## ğŸ“š ê³ ê¸‰ í™œìš© íŒ¨í„´

### 1. ì ì‘í˜• ì••ì¶• ì‹œìŠ¤í…œ

```python
class AdaptiveCompressionSystem:
    """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¥¸ ì ì‘í˜• ì••ì¶•"""

    def __init__(self):
        self.compression_strategies = {
            'factual': EmbeddingsFilter,
            'analytical': LLMChainExtractor,
            'comparative': DocumentCompressorPipeline
        }

    def classify_query_type(self, query):
        """ì¿¼ë¦¬ ìœ í˜• ë¶„ë¥˜"""
        if any(word in query for word in ['ë¬´ì—‡', 'ì–¸ì œ', 'ì–´ë””']):
            return 'factual'
        elif any(word in query for word in ['ì™œ', 'ì–´ë–»ê²Œ', 'ë¶„ì„']):
            return 'analytical'
        elif any(word in query for word in ['ë¹„êµ', 'ì°¨ì´', 'ëŒ€ë¹„']):
            return 'comparative'
        else:
            return 'factual'

    def compress(self, query, docs):
        """ì ì‘í˜• ì••ì¶• ìˆ˜í–‰"""
        query_type = self.classify_query_type(query)
        strategy = self.compression_strategies[query_type]
        return strategy.compress(docs)
```

### 2. ë‹¤ì¸µ í’ˆì§ˆ ë³´ì¥ ì‹œìŠ¤í…œ

```python
class MultiLayerQualityAssurance:
    """ë‹¤ì¸µ í’ˆì§ˆ ë³´ì¥ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.quality_layers = [
            self.relevance_check,
            self.completeness_check,
            self.coherence_check
        ]

    def relevance_check(self, query, docs):
        """ê´€ë ¨ì„± ê²€ì‚¬"""
        # ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜
        pass

    def completeness_check(self, original_docs, compressed_docs):
        """ì™„ì „ì„± ê²€ì‚¬"""
        # í•µì‹¬ ì •ë³´ ë³´ì¡´ ì—¬ë¶€ í™•ì¸
        pass

    def coherence_check(self, docs):
        """ì¼ê´€ì„± ê²€ì‚¬"""
        # ë¬¸ì„œ ê°„ ì¼ê´€ì„± ë° ë…¼ë¦¬ì  íë¦„ í™•ì¸
        pass

    def validate(self, query, original_docs, compressed_docs):
        """ë‹¤ì¸µ ê²€ì¦ ìˆ˜í–‰"""
        for layer in self.quality_layers:
            if not layer(query, original_docs, compressed_docs):
                return False
        return True
```

## ğŸ“Š ì‹¤ìŠµ ì™„ë£Œ ìš”ì•½

### ğŸ¯ ì‹¤ìŠµ ì„±ê³¼
1. **Re-ranking ê¸°ë²• ë§ˆìŠ¤í„°**
   - CrossEncoderRerankerì™€ LLMListwiseRerank êµ¬í˜„
   - ì‹¤ì œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ í™•ì¸

2. **Contextual Compression êµ¬í˜„**
   - 4ê°€ì§€ ì••ì¶• ê¸°ë²• (Filter, Extractor, EmbeddingsFilter, Pipeline)
   - ë¬¸ì„œ ìˆ˜ 10ê°œ â†’ 2ê°œë¡œ ì••ì¶•í•˜ë©´ì„œ í’ˆì§ˆ ìœ ì§€

3. **ê³ ì„±ëŠ¥ ì‹œìŠ¤í…œ ìµœì í™”**
   - 16ì½”ì–´ 128GB ì‹œìŠ¤í…œ ìµœì í™”ë¡œ 5.475ì´ˆ ë‚´ ì „ì²´ ì²˜ë¦¬
   - 89ê°œ ì²­í¬ ìƒì„±, 733.1ì²­í¬/ì´ˆ ì„±ëŠ¥ ë‹¬ì„±

4. **RAG ì²´ì¸ í†µí•©**
   - LCELì„ í™œìš©í•œ end-to-end ì‹œìŠ¤í…œ êµ¬ì¶•
   - ì‹¤ì œ Tesla Cybertruck ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ ìƒì„±

### ğŸ“ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ
- **ë¬¸ì„œ ë¡œë”©**: 0.120ì´ˆ (âš¡ ì´ˆê³ ì†)
- **í…ìŠ¤íŠ¸ ë¶„í• **: 0.121ì´ˆ (733.1ì²­í¬/ì´ˆ)
- **ì„ë² ë”© ì„¤ì •**: 0.611ì´ˆ (64ê°œ ë™ì‹œ ìš”ì²­)
- **BM25 ì¸ë±ì‹±**: 0.120ì´ˆ (8898.3ë¬¸ì„œ/ì´ˆ)
- **ê²€ìƒ‰ í…ŒìŠ¤íŠ¸**: 3.230ì´ˆ (1.2ê²€ìƒ‰ê¸°/ì´ˆ)

### ğŸ” í•µì‹¬ í•™ìŠµ ë‚´ìš©
1. **ì¬ìˆœìœ„í™”ì˜ ì¤‘ìš”ì„±**: ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ íšê¸°ì ìœ¼ë¡œ ê°œì„ 
2. **ë§¥ë½ì  ì••ì¶•ì˜ íš¨ê³¼**: ê´€ë ¨ì„± ë†’ì€ í•µì‹¬ ì •ë³´ë§Œ ì„ ë³„í•˜ì—¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
3. **íŒŒì´í”„ë¼ì¸ ìµœì í™”**: ë‹¤ë‹¨ê³„ ì²˜ë¦¬ë¡œ ìµœê³  í’ˆì§ˆì˜ ê²€ìƒ‰ ê²°ê³¼ ë‹¬ì„±
4. **ì‹œìŠ¤í…œ ìµœì í™”**: í•˜ë“œì›¨ì–´ ìì›ì„ ìµœëŒ€ í™œìš©í•œ ê³ ì„±ëŠ¥ êµ¬í˜„

---

**ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸**: Re-rankingê³¼ Contextual Compressionì€ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì§ˆì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” í•µì‹¬ ê¸°ë²•ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ ë” ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê°€ì¥ ê´€ë ¨ì„± ë†’ê³  ìœ ìš©í•œ ì •ë³´ë§Œì„ ì •í™•íˆ ì„ ë³„í•˜ì—¬ ì œê³µí•¨ìœ¼ë¡œì¨ ì‚¬ìš©ì ê²½í—˜ì„ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ì‹¤ìŠµì—ì„œëŠ” ì‹¤ì œ Tesla Cybertruck ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•œ ì •ë³´(2019ë…„ 11ì›” ë°œí‘œ, í’€ì‚¬ì´ì¦ˆ í”½ì—… íŠ¸ëŸ­)ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.