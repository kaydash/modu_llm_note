{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  주요 언어 모델 공급자 및 활용 \n",
    "\n",
    "- Gemini, Groq, Ollama, Huggingface\n",
    "\n",
    "### **학습 목표:** 주요 언어 모델 공급자들의 특징을 비교하고 적절한 활용 방안을 수립한다\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "import uuid\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) langfuase handler 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "# LangChain 콜백 핸들러 생성\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# 정상 작동 확인\n",
    "langfuse_handler.auth_check() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) 벡터스토어 로드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 저장소 로드 \n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"db_korean_cosine_metadata\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(5) 백터 검색기 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 retriever 초기화\n",
    "chroma_k_retriever = chroma_db.as_retriever(\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "query = \"테슬라의 자율주행 기술의 특징은 무엇인가요?\"\n",
    "retrieved_docs = chroma_k_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(5) RAG 체인 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 쿼리에 대한 검색 결과를 한꺼번에 Context로 전달해서 답변을 생성\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def create_rag_chain(retriever, llm):\n",
    "\n",
    "    template = \"\"\"Answer the following question based on this context. If the context is not relevant to the question, just answer with '답변에 필요한 근거를 찾지 못했습니다.'\n",
    "\n",
    "    [Context]\n",
    "    {context}\n",
    "\n",
    "    [Question]\n",
    "    {question}\n",
    "\n",
    "    [Answer]\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} \n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 체인 생성 및 테스트 (OpenAI의 gpt-4.1-mini 사용)\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "openai_rag_chain = create_rag_chain(chroma_k_retriever, llm)\n",
    "\n",
    "query = \"테슬라의 자율주행 기술의 특징은 무엇인가요?\"\n",
    "answer = openai_rag_chain.invoke(query)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 주요 **LLM 모델 공급자** \n",
    "\n",
    "- **LLM 공급자**는 대규모 언어 모델을 개발하고 서비스를 제공하는 기업들을 의미함\n",
    "- **기술 서비스** 제공 방식에 따라 다양한 기업들이 시장에 참여하고 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) **Google Gemini** API\n",
    "\n",
    "- **Google의 최신 AI 모델** Gemini가 API를 통해 개발자들에게 공개됨\n",
    "- API는 **세 가지 모델 버전**을 제공: Gemini Pro, Gemini Pro Vision, Ultra 모델\n",
    "- **텍스트, 이미지, 오디오** 등 다양한 형식의 입력을 처리할 수 있는 **멀티모달 기능** 지원\n",
    "- 무료 체험판과 함께 **사용량 기반 과금 체계** 도입\n",
    "\n",
    "- 참고: https://ai.google.dev/\n",
    "- 설치: `pip install langchain_google_genai` / ```poetry add langchain_google_genai```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **API 키 설정**\n",
    "- 환경 변수: **GOOGLE_API_KEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGoogleGenerativeAI - LLM 모델 \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 모델 로드 \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# RAG 체인 생성 및 테스트\n",
    "google_genai_rag_chain = create_rag_chain(chroma_k_retriever, llm)\n",
    "\n",
    "query = \"테슬라의 자율주행 기술의 특징은 무엇인가요?\"\n",
    "answer = google_genai_rag_chain.invoke(query)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[실습]**\n",
    "\n",
    "- Google Gemini에서 제공하는 다른 모델을 RAG 체인에 적용합니다. \n",
    "- 각 모델이 생성한 답변의 품질과 특성을 비교합니다. \n",
    "- 모델 확인: https://ai.google.dev/gemini-api/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) **Groq** API\n",
    "\n",
    "- **초고속 추론 속도**를 제공하는 새로운 LLM API 서비스 출시\n",
    "- **오픈소스 모델** 지원으로 Llama, Mixtral 등 다양한 기존 모델 활용 가능\n",
    "- **1초 미만의 응답 시간**을 목표로 하는 **LPU**(Language Processing Unit) 기술 도입\n",
    "- API 사용량에 따른 **합리적인 가격 정책** 제시\n",
    "\n",
    "- 참고: https://groq.com/\n",
    "- 설치: `pip install langchain_groq` / ```poetry add langchain_groq```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **API 키 설정**\n",
    "- 환경 변수: **GROQ_API_KEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGroq - LLM 모델 \n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# 모델 로드 \n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "# RAG 체인 생성 및 테스트\n",
    "groq_rag_chain = create_rag_chain(chroma_k_retriever, llm)\n",
    "\n",
    "query = \"테슬라의 자율주행 기술의 특징은 무엇인가요?\"\n",
    "answer = groq_rag_chain.invoke(query)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[실습]**\n",
    "\n",
    "- Google Gemini에서 제공하는 다른 모델을 RAG 체인에 적용합니다. \n",
    "- 각 모델이 생성한 답변의 품질과 특성을 비교합니다. \n",
    "- 모델 확인: https://ai.google.dev/gemini-api/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) **Ollama** API\n",
    "\n",
    "- **로컬 환경**에서 LLM을 쉽게 실행할 수 있는 **오픈소스 플랫폼**\n",
    "- **다양한 LLM 모델** 지원 (Llama, Mistral, CodeLlama 등)\n",
    "- **단순한 명령어**로 모델 다운로드 및 실행 가능\n",
    "- 개인용 컴퓨터에서 AI 모델을 쉽게 활용할 수 있는 효율적인 솔루션 제공 (**macOS, Linux, Windows** 운영체제 지원)\n",
    "\n",
    "- 웹사이트: https://ollama.com/\n",
    "- 깃허브: https://github.com/ollama/ollama\n",
    "- 설치: `pip install langchain_ollama` / ```poetry add langchain_ollama```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama - LLM 모델 \n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 모델 로드 \n",
    "llm = ChatOllama(\n",
    "    model = \"qwen2.5:latest\",   # qwen2.5:latest 모델 적용 (ollama pull qwen2.5:latest)\n",
    "    temperature = 0,\n",
    "    num_predict = 200,\n",
    ")\n",
    "\n",
    "# RAG 체인 생성 및 테스트\n",
    "ollama_rag_chain = create_rag_chain(chroma_k_retriever, llm)\n",
    "\n",
    "query = \"테슬라의 자율주행 기술의 특징은 무엇인가요?\"\n",
    "answer = ollama_rag_chain.invoke(query)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[실습]**\n",
    "\n",
    "- Google Gemini에서 제공하는 다른 모델을 RAG 체인에 적용합니다. \n",
    "- 각 모델이 생성한 답변의 품질과 특성을 비교합니다. \n",
    "- 모델 확인: https://ai.google.dev/gemini-api/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
