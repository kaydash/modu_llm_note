{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LangChain의 개념과 주요 컴포넌트 이해\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain이란 \n",
    "\n",
    "- **LangChain**은 LLM 기반 애플리케이션 개발을 위한 프레임워크\n",
    "\n",
    "- **Chain**은 작업을 순차적으로 실행하는 파이프라인 구조를 제공\n",
    "\n",
    "- **Agent**는 자율적 의사결정이 가능한 실행 단위\n",
    "\n",
    "\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" \n",
    "            alt=\"langchain_stack\" \n",
    "            width=\"600\" \n",
    "            style=\"border: 0;\">\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 컴포넌트 \n",
    "\n",
    "- **언어 처리 기능**은 LLM/ChatModel이 중심이 되며, Prompt와 Memory로 대화를 관리\n",
    "\n",
    "- **문서 처리와 검색**은 Document Loader, Text Splitter, Embedding, Vectorstore가 담당\n",
    "\n",
    "- **모듈성**이 핵심 특징으로, 독립적인 컴포넌트들을 조합해 RAG와 같은 복잡한 시스템을 구현 가능 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치\n",
    "# uv add ipykernel python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env 파일 설정\n",
    "# OPENAI_API_KEY=your_openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 모델 (Models)\n",
    "- LLM, ChatModel 등으로 구분\n",
    "- OpenAI, Anthropic, Google 등 다양한 모델을 지원\n",
    "- 텍스트 생성, 대화, 요약 등의 작업을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini 답변: 안녕하세요! 반갑습니다. 😊\n",
      "OpenAI 답변: 안녕하세요! 어떻게 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# LangChain 모델 임포트 (OpenAI 및 Google Gemini)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Gemini 모델을 사용하여 대화 생성\n",
    "gemini_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "# OpenAI 모델을 사용하여 대화 생성\n",
    "openai_model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# 각 모델에 메시지를 보내고 응답을 받기\n",
    "gemini_response = gemini_model.invoke(\"안녕하세요!\")\n",
    "openai_response = openai_model.invoke(\"안녕하세요!\")\n",
    "\n",
    "print(\"Gemini 답변:\", gemini_response.content)\n",
    "print(\"OpenAI 답변:\", openai_response.content)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 반갑습니다! 😊', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--bc305c0c-d752-4212-86f6-11791ad40543-0', usage_metadata={'input_tokens': 3, 'output_tokens': 36, 'total_tokens': 39, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 31}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 10, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_3f58d112f7', 'id': 'chatcmpl-C9VOYMC1yYR2IcRjjRGi1Vov3MvSN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c4442121-a2b9-4fd2-8e56-0fe582682b65-0', usage_metadata={'input_tokens': 10, 'output_tokens': 10, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 응답 객체(AIMessage): 메시지(content)와 메타데이터(response_metadata 등)를 포함\n",
    "\n",
    "openai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  안녕하세요! 반갑습니다! 😊\n"
     ]
    }
   ],
   "source": [
    "# 응답 객체의 메시지 내용 출력\n",
    "print(\"답변: \", gemini_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  안녕하세요! 어떻게 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "print(\"답변: \", openai_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메타데이터:  {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}\n"
     ]
    }
   ],
   "source": [
    "# 응답 객체의 메타데이터 출력\n",
    "print(\"메타데이터: \", gemini_response.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메타데이터:  {'token_usage': {'completion_tokens': 10, 'prompt_tokens': 10, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_3f58d112f7', 'id': 'chatcmpl-C9VOYMC1yYR2IcRjjRGi1Vov3MvSN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "# 응답 객체의 메타데이터 출력\n",
    "print(\"메타데이터: \", openai_response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] \n",
    "\n",
    "- Google Gemini 모델을 사용하여 텍스트 생성하고 응답 객체를 확인해보세요.\n",
    "\n",
    "- 참고: https://python.langchain.com/docs/integrations/chat/google_generative_ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  안녕하세요! 무엇을 도와드릴까요? 😊\n",
      "메타데이터:  {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}\n"
     ]
    }
   ],
   "source": [
    "# pip install -U langchain-google-genai 또는 uv add langchain-google-genai\n",
    "# .env 파일에 GOOGLE_API_KEY 추가\n",
    "# Google GenAI 모델을 사용하여 대화 생성\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 모델에 메시지를 보내고 응답을 받기\n",
    "response = gemini.invoke(\"안녕하세요!\")\n",
    "\n",
    "# 응답 객체(AIMessage): 메시지(content)와 메타데이터(response_metadata 등)를 포함\n",
    "print(\"답변: \", response.content)\n",
    "\n",
    "# 응답 객체의 메타데이터 출력\n",
    "print(\"메타데이터: \", response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 메시지 (Messages)\n",
    "- Chat Model에서 사용할 수 있는 통합된 메시지 형식을 제공\n",
    "- 각 모델 제공자의 특정 메시지 형식을 신경 쓰지 않고도 다양한 채팅 모델을 활용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. HumanMessage`\n",
    "- 사용자 역할에 해당 (user, human 등)\n",
    "- 사용자의 입력을 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  \"Glory\"는 한국어로 다음과 같이 번역될 수 있습니다:\n",
      "\n",
      "*   **영광 (榮光):** 가장 일반적이고 공식적인 번역입니다. 명예롭고 빛나는 상태를 나타냅니다.\n",
      "*   **명예 (名譽):** 칭찬과 존경을 받는 상태를 강조합니다.\n",
      "*   **위업 (偉業):** 훌륭하고 큰 업적을 강조합니다.\n",
      "*   **찬란함:** 빛나고 아름다운 상태를 강조합니다.\n",
      "*   **영화 (榮華):** 번성하고 화려한 상태를 나타냅니다.\n",
      "\n",
      "어떤 단어가 가장 적절한지는 문맥에 따라 달라집니다.\n",
      "\n",
      "예를 들어:\n",
      "\n",
      "*   \"To God be the glory\"는 \"하나님께 영광을\"으로 번역됩니다.\n",
      "*   \"The glory of Rome\"은 \"로마의 영광\" 또는 \"로마의 위업\"으로 번역될 수 있습니다.\n",
      "*   \"She basked in the glory of her victory\"는 \"그녀는 승리의 영광을 만끽했다\"로 번역됩니다.\n",
      "\n",
      "어떤 문맥에서 \"glory\"를 번역하고 싶으신지 알려주시면 더 정확한 번역을 제공해 드릴 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Gemini 모델 초기화 (환경변수에 GOOGLE_API_KEY 필요)\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "# 사용자 메시지 생성\n",
    "human_message = HumanMessage(content=\"Glory를 한국어로 번역해주세요.\")\n",
    "\n",
    "# 번역 요청 및 응답 받기\n",
    "response = model.invoke([human_message])  # 메시지 리스트로 전달\n",
    "\n",
    "# 답변 출력\n",
    "print(\"답변: \", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Glory\"는 한국어로 다음과 같이 번역될 수 있습니다:\\n\\n*   **영광 (榮光):** 가장 일반적이고 공식적인 번역입니다. 명예롭고 빛나는 상태를 나타냅니다.\\n*   **명예 (名譽):** 칭찬과 존경을 받는 상태를 강조합니다.\\n*   **위업 (偉業):** 훌륭하고 큰 업적을 강조합니다.\\n*   **찬란함:** 빛나고 아름다운 상태를 강조합니다.\\n*   **영화 (榮華):** 번성하고 화려한 상태를 나타냅니다.\\n\\n어떤 단어가 가장 적절한지는 문맥에 따라 달라집니다.\\n\\n예를 들어:\\n\\n*   \"To God be the glory\"는 \"하나님께 영광을\"으로 번역됩니다.\\n*   \"The glory of Rome\"은 \"로마의 영광\" 또는 \"로마의 위업\"으로 번역될 수 있습니다.\\n*   \"She basked in the glory of her victory\"는 \"그녀는 승리의 영광을 만끽했다\"로 번역됩니다.\\n\\n어떤 문맥에서 \"glory\"를 번역하고 싶으신지 알려주시면 더 정확한 번역을 제공해 드릴 수 있습니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--1dbe1999-84cc-47d9-bb88-033fe16f7dac-0', usage_metadata={'input_tokens': 11, 'output_tokens': 297, 'total_tokens': 308, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열을 입력하면, 자동으로 HumanMessage로 변환하여 요청\n",
    "model.invoke(\"Glory를 한국어로 번역해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. AIMessage`\n",
    "- AI 모델의 응답을 표현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Glory\"는 한국어로 다음과 같이 번역될 수 있습니다:\\n\\n*   **영광 (榮光):** 가장 일반적이고 공식적인 번역입니다. 명예롭고 빛나는 상태를 나타냅니다.\\n*   **명예 (名譽):** 칭찬과 존경을 받는 상태를 강조합니다.\\n*   **위업 (偉業):** 훌륭하고 큰 업적을 강조합니다.\\n*   **찬란함:** 빛나고 아름다운 상태를 강조합니다.\\n*   **영화 (榮華):** 번성하고 화려한 상태를 나타냅니다.\\n\\n어떤 단어가 가장 적절한지는 문맥에 따라 달라집니다.\\n\\n예를 들어:\\n\\n*   \"To God be the glory\"는 \"하나님께 영광을\"으로 번역됩니다.\\n*   \"The glory of Rome\"은 \"로마의 영광\" 또는 \"로마의 위업\"으로 번역될 수 있습니다.\\n*   \"She basked in the glory of her victory\"는 \"그녀는 승리의 영광을 만끽했다\"로 번역됩니다.\\n\\n어떤 문맥에서 \"glory\"를 번역하고 싶으신지 알려주시면 더 정확한 번역을 제공해 드릴 수 있습니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--9cab6818-1200-461b-ba9a-1694979bb363-0', usage_metadata={'input_tokens': 11, 'output_tokens': 297, 'total_tokens': 308, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AI 모델의 응답 객체를 출력 \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 응답 객체의 자료형 확인\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Glory\"는 한국어로 다음과 같이 번역될 수 있습니다:\\n\\n*   **영광 (榮光):** 가장 일반적이고 공식적인 번역입니다. 명예롭고 빛나는 상태를 나타냅니다.\\n*   **명예 (名譽):** 칭찬과 존경을 받는 상태를 강조합니다.\\n*   **위업 (偉業):** 훌륭하고 큰 업적을 강조합니다.\\n*   **찬란함:** 빛나고 아름다운 상태를 강조합니다.\\n*   **영화 (榮華):** 번성하고 화려한 상태를 나타냅니다.\\n\\n어떤 단어가 가장 적절한지는 문맥에 따라 달라집니다.\\n\\n예를 들어:\\n\\n*   \"To God be the glory\"는 \"하나님께 영광을\"으로 번역됩니다.\\n*   \"The glory of Rome\"은 \"로마의 영광\" 또는 \"로마의 위업\"으로 번역될 수 있습니다.\\n*   \"She basked in the glory of her victory\"는 \"그녀는 승리의 영광을 만끽했다\"로 번역됩니다.\\n\\n어떤 문맥에서 \"glory\"를 번역하고 싶으신지 알려주시면 더 정확한 번역을 제공해 드릴 수 있습니다.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 응답 텍스트 부분을 출력\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 11,\n",
       " 'output_tokens': 297,\n",
       " 'total_tokens': 308,\n",
       " 'input_token_details': {'cache_read': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 사용량 출력\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. SystemMessage`\n",
    "- 시스템 역할에 해당 (system, developer 등)\n",
    "- AI 모델의 동작과 제약사항을 정의하는데 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='당신은 영어를 한국어로 번역하는 AI 어시스턴트입니다.', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage \n",
    "\n",
    "# 시스템 메시지 생성\n",
    "system_msg = SystemMessage(content=\"당신은 영어를 한국어로 번역하는 AI 어시스턴트입니다.\")\n",
    "\n",
    "# 메시지 객체 확인\n",
    "system_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  영광\n"
     ]
    }
   ],
   "source": [
    "# 번역 요청 (HumanMessage)과 시스템 메시지(SystemMessage)를 함께 사용\n",
    "human_message = HumanMessage(content=\"Glory\")\n",
    "messages = [system_msg, human_message]\n",
    "\n",
    "# 모델에 메시지를 보내고 응답 받기\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# 답변 출력\n",
    "print(\"답변: \", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] \n",
    "\n",
    "- Google Gemini 모델을 사용하여, 챗 메시지 목록을 기반으로 텍스트 생성하고 응답 객체를 확인해보세요.\n",
    "\n",
    "- 참고: https://python.langchain.com/docs/integrations/chat/google_generative_ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  영광\n",
      "메타데이터:  {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 시스템 메시지 생성\n",
    "system_msg = SystemMessage(content=\"당신은 영어를 한국어로 번역하는 AI 어시스턴트입니다.\")\n",
    "\n",
    "# 사용자 메시지 생성\n",
    "human_message = HumanMessage(content=\"Glory\")\n",
    "\n",
    "# 메시지 리스트 생성\n",
    "messages = [system_msg, human_message]\n",
    "\n",
    "# 모델에 메시지를 보내고 응답 받기\n",
    "response = gemini.invoke(messages)\n",
    "\n",
    "# 응답 객체의 메시지 내용 출력\n",
    "print(\"답변: \", response.content)\n",
    "\n",
    "# 응답 객체의 메타데이터 출력\n",
    "print(\"메타데이터: \", response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 프롬프트 템플릿 (Prompt Template)\n",
    "- 프롬프트 템플릿을 통해 일관된 입력 형식을 제공\n",
    "    1. 사용자의 입력과 파라미터를 언어 모델이 이해할 수 있는 형태로 변환하는 도구\n",
    "    2. 언어 모델에게 전달할 지시문을 만드는 틀\n",
    "- 변수를 포함한 동적 프롬프트 생성이 가능\n",
    "    1. 모든 템플릿은 딕셔너리 형태의 입력을 받아서 처리\n",
    "    2. 출력은 PromptValue 형태로 반환되며, 이는 문자열이나 메시지 리스트로 변환 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. 문자열 프롬프트 템플릿 (String PromptTemplate)`\n",
    "- 가장 기본적인 형태\n",
    "- 단일 문자열을 형식화하는데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='고양이에 대한 이야기를 해줘')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 템플릿 생성 \n",
    "# \"{topic}에 대한 이야기를 해줘\"라는 템플릿을 사용하여\n",
    "# topic이라는 변수를 포함하는 프롬프트를 생성\n",
    "template = PromptTemplate.from_template(\"{topic}에 대한 이야기를 해줘\")\n",
    "\n",
    "# 템플릿 사용\n",
    "# \"고양이\"라는 주제를 사용하여 프롬프트 생성\n",
    "# invoke 메서드를 통해 템플릿에 값을 전달\n",
    "prompt = template.invoke({\"topic\": \"고양이\"})\n",
    "\n",
    "# 템플릿 출력\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. 채팅 프롬프트 템플릿 (ChatPromptTemplate)`\n",
    "- 여러 메시지를 포함하는 대화형 템플릿을 만들 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='당신은 도움이 되는 비서입니다', additional_kwargs={}, response_metadata={}), HumanMessage(content='인공지능에 대해 설명해주세요', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 채팅 템플릿 생성\n",
    "# 여기서는 시스템 메시지와 사용자 메시지를 포함하여 정의\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 도움이 되는 비서입니다\"),\n",
    "    (\"user\", \"{subject}에 대해 설명해주세요\")\n",
    "])\n",
    "\n",
    "# 템플릿 사용\n",
    "prompt = template.invoke({\"subject\": \"인공지능\"})\n",
    "\n",
    "# 출력\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. 메시지 플레이스홀더 (MessagesPlaceholder)`\n",
    "- 기존 메시지 목록을 템플릿의 특정 위치에 삽입할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='당신은 도움이 되는 비서입니다', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='안녕하세요! 제 이름은 스티브입니다.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='안녕하세요! 무엇을 도와드릴까요?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='제 이름을 기억하나요?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# 메시지 플레이스홀더가 있는 템플릿\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 도움이 되는 비서입니다\"),\n",
    "    MessagesPlaceholder(\"chat_history\")   # 채팅 기록을 플레이스홀더로 사용 (예: 이전 대화 내용) -> 이 위치에 메시지 목록을 추가할 수 있음\n",
    "])\n",
    "\n",
    "# 템플릿 사용\n",
    "prompt = template.invoke({\n",
    "    \"chat_history\": [\n",
    "        HumanMessage(content=\"안녕하세요! 제 이름은 스티브입니다.\"),\n",
    "        AIMessage(content=\"안녕하세요! 무엇을 도와드릴까요?\"),\n",
    "        HumanMessage(content=\"제 이름을 기억하나요?\")\n",
    "        ]\n",
    "})\n",
    "\n",
    "# 출력\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] \n",
    "\n",
    "- 프롬프트 템플릿을 사용하여, 입력받은 텍스트를 요약하는 템플릿을 작성하고, Google Gemini 모델을 사용하여 요약 결과를 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"뉴턴의 제 1,2,3 법칙\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  인공지능은 인간의 지능을 모방하여 학습, 추론, 문제 해결 등을 수행하는 기술로, 머신러닝, 딥러닝, 자연어 처리 등 다양한 분야에서 활용됩니다. 특히 생성형 인공지능은 텍스트, 이미지, 음악 등 다양한 콘텐츠를 생성하며 창의적인 작업에 기여합니다.\n",
      "메타데이터:  {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 요약 요청을 위한 템플릿 생성\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 텍스트 요약을 잘하는 AI 어시스턴트입니다.\"),\n",
    "    (\"user\", \"다음 텍스트를 1~2 문장으로 핵심 내용을 위주로 간결하게 요약해주세요: {text}\")\n",
    "])\n",
    "\n",
    "# 요약 체인 생성\n",
    "summarization_chain = template | gemini\n",
    "\n",
    "# 요약할 텍스트\n",
    "text = \"\"\"\n",
    "인공지능은 컴퓨터 시스템이 인간의 지능을 모방하여 학습, 추론, 문제 해결 등을 수행하는 기술입니다.\n",
    "인공지능은 머신러닝, 딥러닝, 자연어 처리 등 다양한 분야에서 활용되며, 자율주행차, 음성 인식, 이미지 분석 등 여러 응용 프로그램에 적용됩니다.\n",
    "생성형 인공지능은 텍스트, 이미지, 음악 등 다양한 콘텐츠를 생성하는 데 사용되며, 창의적인 작업에서도 큰 역할을 하고 있습니다.\n",
    "\"\"\"\n",
    "\n",
    "# 요약 요청\n",
    "response = summarization_chain.invoke({\"text\": text})\n",
    "\n",
    "# 응답 객체의 메시지 내용 출력\n",
    "print(\"답변: \", response.content)\n",
    "\n",
    "# 응답 객체의 메타데이터 출력\n",
    "print(\"메타데이터: \", response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 출력 파서 (Output Parser)\n",
    "1. **역할과 기능**\n",
    "    - 모델의 텍스트 출력을 구조화된 데이터로 변환\n",
    "    - 채팅 모델과 LLM의 출력을 정규화\n",
    "    - 다운스트림 작업을 위한 데이터 형식 변환\n",
    "\n",
    "2. **사용 시 고려사항**\n",
    "    - OpenAI function calling과 같은 기능이 있는 경우, 해당 기능을 우선 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) StrOutputParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울의 특징에 대해 알려드리겠습니다.\n",
      "\n",
      "1. **대한민국의 수도**  \n",
      "   서울은 대한민국의 수도이자 정치, 경제, 문화의 중심지입니다.\n",
      "\n",
      "2. **인구와 규모**  \n",
      "   약 1,000만 명 이상의 인구가 살고 있으며, 광역권을 포함하면 천만 명 이상으로, 세계적으로도 대규모 도시 중 하나입니다.\n",
      "\n",
      "3. **역사적 중요성**  \n",
      "   삼국시대부터 중요한 도시였으며, 조선 왕조의 수도로서 많은 역사적 유적과 문화재가 남아 있습니다. 경복궁, 창덕궁, 덕수궁 등 고궁들이 대표적입니다.\n",
      "\n",
      "4. **경제 중심지**  \n",
      "   대한민국 경제의 중심지로서 주요 기업 본사, 금융 기관, 국제 무역 등이 집중되어 있습니다.\n",
      "\n",
      "5. **교통 인프라**  \n",
      "   지하철, 버스, 공항(인천국제공항, 김포국제공항) 등이 잘 발달되어 있어 국내외 이동이 편리합니다.\n",
      "\n",
      "6. **문화와 관광**  \n",
      "   다양한 박물관, 미술관, 공연장, 쇼핑 구역(명동, 동대문, 강남 등), 한류 콘텐츠 관련 장소들이 많아 관광객이 많이 방문합니다.\n",
      "\n",
      "7. **첨단 기술과 스마트 시티**  \n",
      "   디지털 기술과 스마트 시티 기반 시설이 발전하여 혁신적인 도시로 자리잡고 있습니다.\n",
      "\n",
      "8. **자연과 도시의 조화**  \n",
      "   한강을 중심으로 여러 공원과 녹지가 조성되어 있어 시민들의 여가 공간으로 활용되고 있습니다.\n",
      "\n",
      "서울은 전통과 현대가 공존하는 다채로운 매력을 가진 도시입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 기본적인 문자열 파서 사용\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 프롬프트 템플릿 설정\n",
    "prompt = PromptTemplate.from_template(\"도시 {city}의 특징을 알려주세요\")\n",
    "\n",
    "# 모델 정의\n",
    "model = ChatOpenAI(model='gpt-4.1-mini')\n",
    "\n",
    "# 체인 구성\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 체인 실행\n",
    "result = chain.invoke({\"city\": \"서울\"})\n",
    "\n",
    "# 결과 출력\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 구조화된 출력 (with_structured_output 메소드)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Pydantic 클래스로 출력 구조를 정의\n",
    "class CityInfo(BaseModel):\n",
    "    name: str = Field(description=\"도시 이름\")\n",
    "    description: str = Field(description=\"도시의 특징\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(\"도시 {city}의 특징을 알려주세요.\")\n",
    "\n",
    "# 모델 생성 및 구조화된 출력 바인딩\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "structured_model = model.with_structured_output(CityInfo)\n",
    "\n",
    "# 프롬프트와 모델 체인 연결\n",
    "chain = prompt | structured_model\n",
    "\n",
    "# 체인 실행\n",
    "result = chain.invoke({\"city\": \"서울\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CityInfo'>\n",
      "--------------------\n",
      "name='서울' description='서울은 대한민국의 수도로, 정치, 경제, 문화의 중심지입니다. 한강을 중심으로 발전한 도시로서 현대적인 고층 빌딩과 전통적인 궁궐이 공존하며, 다양한 문화 행사와 맛집, 쇼핑 지역이 풍부합니다.'\n",
      "--------------------\n",
      "도시 이름: 서울\n",
      "특징: 서울은 대한민국의 수도로, 정치, 경제, 문화의 중심지입니다. 한강을 중심으로 발전한 도시로서 현대적인 고층 빌딩과 전통적인 궁궐이 공존하며, 다양한 문화 행사와 맛집, 쇼핑 지역이 풍부합니다.\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력 (CityInfo 객체)\n",
    "print(type(result))\n",
    "print(\"-\" * 20)\n",
    "print(result)\n",
    "print(\"-\" * 20)\n",
    "print(f\"도시 이름: {result.name}\")\n",
    "print(f\"특징: {result.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] \n",
    "\n",
    "- 구조화된 출력을 사용하여, 다음 뉴스 기사에서 언론사, 기사 제목, 기사 내용, 작성자, 작성일을 추출해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 뉴스 기사를 복사하여 사용하세요.\n",
    "\n",
    "news_article = \"\"\"\n",
    "\"복잡한 납세 등 AI가 돕는다\"…정부, 초거대 AI 공공서비스 개발\n",
    "입력2025.06.20. 오후 12:00  수정2025.06.20. 오후 1:39 \n",
    "\n",
    "[서울=뉴시스]윤현성 기자 = 정부가 공공분야에 도입할 초거대 인공지능(AI) 기반 국민 편의 서비스 본격 개발에 나선다. 세금 납부 시 생성형 AI 챗봇을 통해 어려운 세무 용어 등에 대한 설명·상담을 듣거나, 대량의 민원업무도 생성형 AI 기반 분석으로 빠르게 답변·응대하는 등의 형태다.\n",
    "\n",
    "과학기술정보통신부와 디지털플랫폼정부위원회는 2025년도 '초거대 AI 서비스 개발 지원' 사업을 본격 추진하기 위해 수행기업 공모를 실시한다고 20일 밝혔다. 이 사업은 공공분야에 초거대 AI를 도입·확산하고 이를 통해 행정 효율화, 대국민 서비스 혁신, 사회현안 해결이 가능한 서비스 개발을 목표로 추진된다.\n",
    "\n",
    "올해는 다양한 공공분야에서 초거대 AI 기술을 통해 국민이 체감할 수 있고 실질적인 변화를 가져올 수 있는 과제를 중점적으로 발굴했다. 중앙부처·지자체·공공기관을 대상으로 1~2월에 과제 공모를 추진했으며 총 5개 과제가 선정됐다.\n",
    "\n",
    "국민권익위원회의 '생성형 AI 기반 국민소통·민원분석 체계 구축'은 국민소통시스템에 생성형 AI 기반 민원분석 체계를 도입해 민원처리 행정 효율화와 답변품질을 향상시킨다. 이를 통해 대량의 민원업무를 신속·효율적으로 대응해 민원업무의 효율성을 증대하고 국민의 신뢰도 향상에도 기여할 것으로 기대된다.\n",
    "\n",
    "국세청의 '생성형 AI 기반 국세 상담 지원 서비스'는 납세자가 홈택스 이용 시 전자신고 관련한 문의사항을 즉시 해소할 수 있는 실시간 상담 서비스를 제공한다. 홈택스에 상담전용 AI챗봇을 도입해 전화 상담 시 발생하는 장시간 대기 문제를 해결하고, 어려운 세무 용어 등으로 인한 불편사항을 개선할 예정이다.\n",
    "\n",
    "산업통상자원부의 '해외인증 공공특화 AI 에이전트 서비스'는 모바일 플랫폼, 소셜네트워크서비스 등 사용자 친화적인 모바일 기반 해외인증 특화 AI 에이전트 서비스를 제공한다. 중소기업이 겪는 수출 관련 애로사항인 해외 인증과 관련된 정보와 질의 응답을 AI기반으로 제공하여 손쉽게 확인할 수 있을 것으로 기대된다.\n",
    "\n",
    "국민건강보험공단의 '에이전틱 AI기반 전국민 맞춤형 민원 상담 서비스'는 국민 생활과 편익에 직결되는 건강보험 민원 상담업무에 AI를 도입해 24시간 개인 맞춤형 민원 상담 서비스를 구현한다. 기존의 전화 상담 방식의 대기 시간 문제 등을 해소하고, 고객센터 집중 상담을 분산시켜 업무의 효율성을 향상시키는데 기여할 것으로 전망이다.\n",
    "\n",
    "한국지역정보개발원의 '지방재정 지능화 서비스'는 e호조+, 지방재정365 등 지방재정서비스에 생성형 AI를 접목시켜 대국민, 공무원 등 각자의 요구에 부합하는 융복합 재정정보서비스 환경을 제공하고자 한다. 이 서비스가 도입되면 지방정부의 사회현안 해결을 위한 정책 수립의 적시성 향상 및 전문성 확보, 지자체 정보 접근성 강화로 대국민의 사회 참여 기회를 확대할 것으로 예상된다.\n",
    "\n",
    "이번 사업은 19일 국민권익위원회와 국세청 과제의 민간 전문기업 조달 공고를 시작으로, 5개 과제별 서비스 개발지원 사업이 순차적으로 입찰공고될 예정이다.\n",
    "\n",
    "김경만 과기정통부 인공지능기반정책관은 \"선정된 과제에 대해 민·관 협력을 기반으로 행정 현장의 변화와 국민이 체감할 수 있는 성과가 창출·확산될 수 있도록 적극 지원할 계획\"이라며 \"개발된 서비스는 공공분야에서 행정업무의 효율성을 높이고 대국민 서비스 품질을 높이는 데 실질적으로 기여할 수 있도록 민간에서도 많은 관심을 가져주실 것을 부탁드린다\"고 말했다.\n",
    "\n",
    "이승현 디플정위 인공지능·플랫폼혁신국장은 \"이번 사업은 노동, 복지, 민원 등 다양한 공공 분야에 AI를 도입·활용하는데 중요한 역할을 하고 있다\"며 \"올해도 AI를 활용해 사회문제를 해결하고, 대국민 서비스를 혁신적으로 개선할 수 있는 서비스가 개발되길 기대한다\"고 전했다．\n",
    "\n",
    "윤현성 기자(hsyhs@newsis.com)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  ```json\n",
      "{\n",
      "    \"언론사\": \"뉴시스\",\n",
      "    \"기사 제목\": \"“복잡한 납세 등 AI가 돕는다”…정부, 초거대 AI 공공서비스 개발\",\n",
      "    \"작성자\": \"윤현성\",\n",
      "    \"작성일\": \"2025.06.20\",\n",
      "    \"요약\": \"정부가 초거대 AI 기반 공공 서비스 개발에 본격적으로 나선다.\",\n",
      "    \"분야\": \"IT\",\n",
      "    \"중요도\": \"0.75\",\n",
      "    \"confidence\": \"0.95\"\n",
      "}\n",
      "```\n",
      "메타데이터:  {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}\n"
     ]
    }
   ],
   "source": [
    "#!/app/miniconda3/envs/bank/bin/python\n",
    "# ...existing code...\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 환경변수 확인\n",
    "if not (os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")):\n",
    "    print(\"Warning: GOOGLE_API_KEY or GOOGLE_APPLICATION_CREDENTIALS not found in environment.\")\n",
    "\n",
    "# 뉴스 기사 변수 존재 여부 확인\n",
    "try:\n",
    "    news_article  # type: ignore\n",
    "except NameError:\n",
    "    news_article = \"여기에 분석할 뉴스 기사 전문 텍스트를 넣으세요.\"\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# 수정된 템플릿 - 중괄호를 이스케이프 처리 ({{ }})\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 텍스트 요약을 잘하는 AI 어시스턴트입니다.\"),\n",
    "    (\"user\", \"\"\"\n",
    "     다음 뉴스 기사에서 언론사, 기사 제목, 기사 내용 요약, 작성자, 작성일, 분야를 추출해주세요.     \n",
    "     추출시에 중요도도 평가하고, 중요도는 0.00~1.00 사이로 표현해주세요.\n",
    "     그리고 얼마나 확신하는지 confidence도 0.00~1.00 사이로 출력하세요.\n",
    "\n",
    "     \n",
    "     답변 형태는 다음의 엄격한 json format 준수하세요.\n",
    "     ```답변 형태\n",
    "        {{\n",
    "            \"언론사\": \"[언론사이름]\",\n",
    "            \"기사 제목\": \"[기사 제목]\",\n",
    "            \"작성자\": \"[작성자]\",\n",
    "            \"작성일\": \"[작성일]\",\n",
    "            \"요약\": \"[기사 내용 요약. 20자 ~30자 사이]\",\n",
    "            \"분야\": \"[경제/사회/정치/국제/문화/IT/과학 등]\",\n",
    "            \"중요도\": \"[0.00~1.00 사이 숫자]\",\n",
    "            \"confidence\": \"[0.00~1.00 사이 숫자]\"\n",
    "        }}\n",
    "     ```\n",
    "     \n",
    "     ```기사 내용\n",
    "     {news_article}\n",
    "     ```     \n",
    "     \"\"\")\n",
    "])\n",
    "\n",
    "# 요약 체인 생성\n",
    "summarization_chain = template | gemini\n",
    "\n",
    "try:\n",
    "    response = summarization_chain.invoke({\"news_article\": news_article})\n",
    "    # response가 AIMessage 객체일 경우 content 사용, 아닐 경우 그대로 출력\n",
    "    content = getattr(response, \"content\", response)\n",
    "    metadata = getattr(response, \"response_metadata\", None)\n",
    "    print(\"답변: \", content)\n",
    "    print(\"메타데이터: \", metadata)\n",
    "except Exception as e:\n",
    "    print(\"Error invoking summarization_chain:\", e)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "언론사: 뉴시스\n",
      "기사 제목: \"복잡한 납세 등 AI가 돕는다\"…정부, 초거대 AI 공공서비스 개발\n",
      "작성자: 윤현성\n",
      "작성일: 2025.06.20\n",
      "요약: 정부, 초거대 AI 기반 공공 서비스 개발 착수. 세금 납부, 민원 응대 등에 AI 활용.\n",
      "분야: IT\n",
      "중요도: 0.85\n",
      "확신도: 0.95\n",
      "\n",
      "전체 결과:\n",
      "{\n",
      "  \"언론사\": \"뉴시스\",\n",
      "  \"기사_제목\": \"\\\"복잡한 납세 등 AI가 돕는다\\\"…정부, 초거대 AI 공공서비스 개발\",\n",
      "  \"작성자\": \"윤현성\",\n",
      "  \"작성일\": \"2025.06.20\",\n",
      "  \"요약\": \"정부, 초거대 AI 기반 공공 서비스 개발 착수. 세금 납부, 민원 응대 등에 AI 활용.\",\n",
      "  \"분야\": \"IT\",\n",
      "  \"중요도\": 0.85,\n",
      "  \"confidence\": 0.95\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#!/app/miniconda3/envs/bank/bin/python\n",
    "# ...existing code...\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from pydantic import BaseModel, Field, confloat\n",
    "from typing import Optional\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 환경변수 확인\n",
    "if not (os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")):\n",
    "    print(\"Warning: GOOGLE_API_KEY or GOOGLE_APPLICATION_CREDENTIALS not found in environment.\")\n",
    "\n",
    "# Pydantic 모델 정의\n",
    "class NewsAnalysis(BaseModel):\n",
    "    언론사: str = Field(description=\"뉴스 기사의 언론사 이름\")\n",
    "    기사_제목: str = Field(description=\"뉴스 기사의 제목\", alias=\"기사 제목\")\n",
    "    작성자: str = Field(description=\"뉴스 기사의 작성자\")\n",
    "    작성일: str = Field(description=\"뉴스 기사의 작성일\")\n",
    "    요약: str = Field(description=\"뉴스 기사 내용의 간결한 요약 (20-30자)\")\n",
    "    분야: str = Field(description=\"뉴스 기사의 분야 (경제/사회/정치/국제/문화/IT/과학 등)\")\n",
    "    중요도: confloat(ge=0.0, le=1.0) = Field(description=\"뉴스 기사의 중요도 (0.00-1.00)\")\n",
    "    confidence: confloat(ge=0.0, le=1.0) = Field(description=\"추출의 확신도 (0.00-1.00)\")\n",
    "\n",
    "# 뉴스 기사 변수 존재 여부 확인\n",
    "try:\n",
    "    news_article  # type: ignore\n",
    "except NameError:\n",
    "    news_article = \"여기에 분석할 뉴스 기사 전문 텍스트를 넣으세요.\"\n",
    "\n",
    "# Gemini 모델 초기화\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# 구조화된 출력을 위한 모델 구성\n",
    "structured_gemini = gemini.with_structured_output(NewsAnalysis)\n",
    "\n",
    "# 프롬프트 템플릿 생성 (단순화된 버전)\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 텍스트 요약과 분석을 잘하는 AI 어시스턴트입니다.\"),\n",
    "    (\"user\", \"\"\"\n",
    "     다음 뉴스 기사에서 언론사, 기사 제목, 작성자, 작성일, 기사 내용 요약(20-30자), 분야를 추출해주세요.\n",
    "     추출한 정보의 중요도(0.00~1.00 사이)와 추출에 대한 확신도(0.00~1.00 사이)도 함께 평가해 주세요.\n",
    "     \n",
    "     ```기사 내용\n",
    "     {news_article}\n",
    "     ```\n",
    "     \"\"\")\n",
    "])\n",
    "\n",
    "# 요약 체인 생성\n",
    "summarization_chain = template | structured_gemini\n",
    "\n",
    "try:\n",
    "    # 구조화된 결과를 바로 받음 (NewsAnalysis 객체)\n",
    "    result = summarization_chain.invoke({\"news_article\": news_article})\n",
    "    \n",
    "    # 결과 출력 (구조화된 객체)\n",
    "    print(f\"언론사: {result.언론사}\")\n",
    "    print(f\"기사 제목: {result.기사_제목}\")\n",
    "    print(f\"작성자: {result.작성자}\")\n",
    "    print(f\"작성일: {result.작성일}\")\n",
    "    print(f\"요약: {result.요약}\")\n",
    "    print(f\"분야: {result.분야}\")\n",
    "    print(f\"중요도: {result.중요도:.2f}\")\n",
    "    print(f\"확신도: {result.confidence:.2f}\")\n",
    "    \n",
    "    # 전체 객체 출력\n",
    "    print(\"\\n전체 결과:\")\n",
    "    print(result.model_dump_json(indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error invoking summarization_chain:\", e)\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
