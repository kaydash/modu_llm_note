{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e2d5bb",
   "metadata": {},
   "source": [
    "#  LangChain의 RAG 콤포넌트 \n",
    "- 텍스트 분할기 (Text Splitter) \n",
    "\n",
    "### **학습 목표:**  문서 로더와 텍스트 분할기를 적절히 활용할 수 있다\n",
    "\n",
    "### **실습 자료**\n",
    "\n",
    "- data/transformer.pdf\n",
    "- data/ai.txt\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff7b2f",
   "metadata": {},
   "source": [
    "# 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a225effd",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acc660f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d083b6e",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091ee72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f742b3e",
   "metadata": {},
   "source": [
    "`(3) 문서 로드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8ff006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 문서 개수: 15\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 로더 초기화\n",
    "pdf_loader = PyPDFLoader('./data/transformer.pdf')\n",
    "\n",
    "# 동기 로딩\n",
    "pdf_docs = pdf_loader.load()\n",
    "print(f'PDF 문서 개수: {len(pdf_docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae49f4",
   "metadata": {},
   "source": [
    "# 텍스트 분할(Text Splitting) \n",
    "\n",
    "- 대규모 텍스트 문서를 처리할 때 매우 중요한 전처리 단계\n",
    "- 고려사항:\n",
    "    1. 문서의 구조와 형식\n",
    "    2. 원하는 청크 크기\n",
    "    3. 문맥 보존의 중요도\n",
    "    4. 처리 속도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b7e8db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 문서: page_content='1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/transformer.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "print(f'첫 번째 문서: {pdf_docs[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa4c953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 문서의 텍스트 길이: 4257\n"
     ]
    }
   ],
   "source": [
    "long_text = pdf_docs[1].page_content\n",
    "print(f'첫 번째 문서의 텍스트 길이: {len(long_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e01d4",
   "metadata": {},
   "source": [
    "### 1. **CharacterTextSplitter**\n",
    "- 가장 기본적인 분할 방식\n",
    "- 문자 수를 기준으로 텍스트를 분할\n",
    "- 단순하지만 문맥을 고려하지 않는다는 단점이 있음\n",
    "\n",
    "- 설치: pip install langchain_text_splitters 또는 poetry add langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e47ca7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 텍스트 개수: 1\n",
      "첫 번째 분할된 텍스트: 1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter \n",
    "\n",
    "# 텍스트 분할기 초기화 (기본 설정값 적용 )\n",
    "text_splitter = CharacterTextSplitter(\n",
    "\n",
    "    # CharacterTextSplitter의 기본 설정값\n",
    "    separator = \"\\n\\n\",         # 청크 구분자: 두 개의 개행문자\n",
    "    is_separator_regex = False,  # 구분자가 정규식인지 여부\n",
    "\n",
    "    # TextSplitter의 기본 설정값\n",
    "    chunk_size = 4000,          # 청크 길이\n",
    "    chunk_overlap = 200,        # 청크 중첩\n",
    "    length_function = len,      # 길이 함수 (문자열 길이)\n",
    "    keep_separator = False,     # 구분자 유지 여부\n",
    "    add_start_index = False,   # 시작 인덱스 추가 여부\n",
    "    strip_whitespace = True,   # 공백 제거 여부\n",
    ")\n",
    "\n",
    "# 텍스트 분할 - split_text() 메서드 사용\n",
    "texts = text_splitter.split_text(long_text)\n",
    "\n",
    "# 분할된 텍스트 개수 출력\n",
    "print(f'분할된 텍스트 개수: {len(texts)}')\n",
    "\n",
    "# 첫 번째 분할된 텍스트 출력\n",
    "print(f'첫 번째 분할된 텍스트: {texts[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e962d5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/transformer.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dcoument 객체 출력 (첫 번째 분할된 텍스트로 생성)\n",
    "pdf_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e77ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 텍스트 개수: 6\n",
      "청크 1의 텍스트 길이: 933\n",
      "청크 2의 텍스트 길이: 995\n",
      "청크 3의 텍스트 길이: 902\n",
      "청크 4의 텍스트 길이: 907\n",
      "청크 5의 텍스트 길이: 996\n",
      "청크 6의 텍스트 길이: 385\n",
      "첫 번째 청크의 텍스트: 1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 문장 구분자를 개행문자로 설정\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",        # 청크 구분자: 개행문자\n",
    "    chunk_size = 1000,       # 청크 길이\n",
    "    chunk_overlap = 200      # 청크 중첩\n",
    ")\n",
    "\n",
    "# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할\n",
    "chunks = text_splitter.split_documents([pdf_docs[1]])   # 첫 번째 문서만 분할\n",
    "\n",
    "# 분할된 텍스트 개수 출력\n",
    "print(f'분할된 텍스트 개수: {len(chunks)}')\n",
    "\n",
    "# 각 청크의 텍스트 길이 출력\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'청크 {i+1}의 텍스트 길이: {len(chunk.page_content)}')\n",
    "\n",
    "# 첫 번째 청크의 텍스트 출력\n",
    "print(f'첫 번째 청크의 텍스트: {chunks[0].page_content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b26edb",
   "metadata": {},
   "source": [
    "### 2. **RecursiveCharacterTextSplitter**\n",
    "\n",
    "- 재귀적으로 텍스트를 분할\n",
    "- 구분자를 순차적으로 적용하여 큰 청크에서 시작하여 점진적으로 더 작은 단위로 분할\n",
    "- 문맥을 더 잘 보존할 수 있음  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52da03ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 텍스트 청크 수: 52\n",
      "각 청크의 길이: [986, 910, 975, 452, 933, 995, 902, 907, 996, 385, 924, 954, 216, 923, 900, 950, 992, 913, 908, 870, 945, 975, 946, 997, 196, 980, 980, 946, 938, 999, 943, 920, 734, 958, 946, 945, 617, 983, 988, 994, 624, 944, 909, 940, 913, 983, 924, 924, 845, 812, 815, 818]\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "====================================================================================================\n",
      "\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine transla\n",
      "----------------------------------------------------------------------------------------------------\n",
      "the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "====================================================================================================\n",
      "\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Eq\n",
      "----------------------------------------------------------------------------------------------------\n",
      " experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "====================================================================================================\n",
      "\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results \n",
      "----------------------------------------------------------------------------------------------------\n",
      "rmed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "====================================================================================================\n",
      "\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence mod\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 재귀적 텍스트 분할기 초기화\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,             # 청크 크기  \n",
    "    chunk_overlap=200,           # 청크 중 중복되는 부분 크기\n",
    "    length_function=len,         # 글자 수를 기준으로 분할\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # 구분자 - 재귀적으로 순차적으로 적용 \n",
    ")\n",
    "\n",
    "# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할\n",
    "chunks = text_splitter.split_documents(pdf_docs)\n",
    "print(f\"생성된 텍스트 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "print()\n",
    "\n",
    "# 각 청크의 시작 부분과 끝 부분 확인 - 5개 청크만 출력\n",
    "for chunk in chunks[:5]:\n",
    "    print(chunk.page_content[:200])\n",
    "    print(\"-\" * 100)\n",
    "    print(chunk.page_content[-200:])\n",
    "    print(\"=\" * 100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d5dc7c",
   "metadata": {},
   "source": [
    "### 3. **정규표현식 사용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2d1fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 텍스트 청크 수: 52\n",
      "각 청크의 길이: [996, 945, 986, 211, 908, 819, 796, 934, 962, 241, 999, 901, 842, 963, 972, 75, 789, 960, 853, 949, 995, 900, 938, 985, 302, 925, 945, 990, 755, 999, 914, 979, 854, 980, 944, 995, 415, 885, 964, 988, 714, 944, 918, 992, 907, 983, 994, 852, 873, 812, 815, 818]\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "r and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely.\n",
      "====================================================================================================\n",
      "\n",
      "We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks \n",
      "----------------------------------------------------------------------------------------------------\n",
      "hat the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random.\n",
      "====================================================================================================\n",
      "\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Tra\n",
      "----------------------------------------------------------------------------------------------------\n",
      "nting tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "====================================================================================================\n",
      "\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "arXiv:1706.03762v7  [cs.CL] \n",
      "----------------------------------------------------------------------------------------------------\n",
      "rmed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "====================================================================================================\n",
      "\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence mod\n",
      "----------------------------------------------------------------------------------------------------\n",
      "sition t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples.\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 문장을 구분하여 분할 - 정규표현식 사용 (문장 구분자: 마침표, 느낌표, 물음표 다음에 공백이 오는 경우)\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=r'(?<=[.!?])\\s+',  # 각 Document 객체의 page_content 속성을 문장으로 분할\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    is_separator_regex=True,      # 구분자가 정규식인지 여부: True\n",
    "    keep_separator=True,          # 구분자 유지 여부: True\n",
    ")\n",
    "\n",
    "# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할\n",
    "chunks = text_splitter.split_documents(pdf_docs)  # 모든 문서를 분할\n",
    "print(f\"생성된 텍스트 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "print()\n",
    "\n",
    "# 각 청크의 시작 부분과 끝 부분 확인 - 5개 청크만 출력\n",
    "for chunk in chunks[:5]:\n",
    "    print(chunk.page_content[:200])\n",
    "    print(\"-\" * 100)\n",
    "    print(chunk.page_content[-200:])\n",
    "    print(\"=\" * 100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe562f",
   "metadata": {},
   "source": [
    "### 4. **토큰 수를 기반으로 분할**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9198b73",
   "metadata": {},
   "source": [
    "`(1) tiktoken`  \n",
    "- OpenAI에서 만든 BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdea518c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2859"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 문서 객체의 텍스트 길이\n",
    "len(pdf_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49b5fda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 청크 수: 3\n",
      "각 청크의 길이: [1145, 1374, 338]\n",
      "Provided proper attribution is provided, Google he\n",
      "--------------------------------------------------\n",
      "ng more parallelizable and requiring significantly\n",
      "==================================================\n",
      "\n",
      "less time to train. Our model achieves 28.4 BLEU o\n",
      "--------------------------------------------------\n",
      "countless long days designing various parts of and\n",
      "==================================================\n",
      "\n",
      "implementing tensor2tensor, replacing our earlier \n",
      "--------------------------------------------------\n",
      ", CA, USA.\n",
      "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# TikToken 인코더를 사용하여 재귀적 텍스트 분할기 초기화\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", \n",
    "    # model_name=\"gpt-4.1-mini\",\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할\n",
    "chunks = text_splitter.split_documents([pdf_docs[0]])  # 첫 번째 문서만 분할\n",
    "\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "\n",
    "# 각 청크의 시작 부분과 끝 부분 확인\n",
    "for chunk in chunks[:5]:\n",
    "    print(chunk.page_content[:50])\n",
    "    print(\"-\" * 50)\n",
    "    print(chunk.page_content[-50:])\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f10e8b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n",
      "[36919, 291, 6300, 63124, 374, 3984, 11, 5195, 22552, 25076]\n",
      "['Provid', 'ed', ' proper', ' attribution', ' is', ' provided', ',', ' Google', ' hereby', ' grants']\n",
      "==================================================\n",
      "\n",
      "291\n",
      "[1752, 892, 311, 5542, 13, 5751, 1646, 83691, 220, 1591]\n",
      "['less', ' time', ' to', ' train', '.', ' Our', ' model', ' achieves', ' ', '28']\n",
      "==================================================\n",
      "\n",
      "84\n",
      "[95574, 287, 16000, 17, 47211, 11, 25935, 1057, 6931, 2082]\n",
      "['implement', 'ing', ' tensor', '2', 'tensor', ',', ' replacing', ' our', ' earlier', ' code']\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# tokenizer = tiktoken.encoding_for_model(\"gpt-4.1-mini\")\n",
    "\n",
    "for chunk in chunks[:5]:\n",
    "\n",
    "    # 각 청크를 토큰화\n",
    "    tokens = tokenizer.encode(chunk.page_content)\n",
    "    # 각 청크의 단어 수 확인\n",
    "    print(len(tokens))\n",
    "    # 각 청크의 토큰화 결과 확인 (첫 10개 토큰만 출력)\n",
    "    print(tokens[:10])\n",
    "    # 토큰 ID를 실제 토큰(문자열)로 변환해서 출력\n",
    "    token_strings = [tokenizer.decode([token]) for token in tokens[:10]]\n",
    "    print(token_strings)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b01677",
   "metadata": {},
   "source": [
    "`(2) Hugging Face 토크나이저`  \n",
    "- Hugging Face tokenizer 모델의 토큰 수를 기준으로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6872bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaTokenizerFast(name_or_path='BAAI/bge-m3', vocab_size=250002, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1477bfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 107687, 5, 20451, 54272, 16367, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 인코딩 - 문장을 토큰(ID)으로 변환\n",
    "tokens = tokenizer.encode(\"안녕하세요. 반갑습니다.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c07cafbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁안녕하세요', '.', '▁반', '갑', '습니다', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# 토큰을 출력 (토큰 ID를 실제 토큰(문자열)로 변환)\n",
    "print(tokenizer.convert_ids_to_tokens(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdcf6a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 반갑습니다.\n"
     ]
    }
   ],
   "source": [
    "# 디코딩 - 토큰을 문자열로 변환\n",
    "print(tokenizer.decode(tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "311dec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 청크 수: 3\n",
      "각 청크의 길이: [1219, 1300, 338]\n",
      "\n",
      "298\n",
      "[0, 123089, 71, 27798, 99, 179236, 83, 62952, 4, 1815]\n",
      "['<s>', '▁Provide', 'd', '▁proper', '▁at', 'tribution', '▁is', '▁provided', ',', '▁Google']\n",
      "==================================================\n",
      "\n",
      "295\n",
      "[0, 47, 9, 191697, 153648, 66211, 4, 224588, 645, 70]\n",
      "['<s>', '▁to', '-', 'German', '▁translation', '▁task', ',', '▁improving', '▁over', '▁the']\n",
      "==================================================\n",
      "\n",
      "88\n",
      "[0, 29479, 214, 1492, 4970, 304, 510, 4970, 4, 456]\n",
      "['<s>', '▁implement', 'ing', '▁ten', 'sor', '2', 'ten', 'sor', ',', '▁re']\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Huggingface 토크나이저를 사용하여 재귀적 텍스트 분할기 초기화\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할\n",
    "chunks = text_splitter.split_documents([pdf_docs[0]]) # 첫 번째 문서만 분할\n",
    "\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "print()\n",
    "\n",
    "for chunk in chunks[:5]:\n",
    "\n",
    "    # 각 청크를 토큰화\n",
    "    tokens = tokenizer.encode(chunk.page_content)\n",
    "    # 각 청크의 단어 수 확인\n",
    "    print(len(tokens))\n",
    "    # 각 청크의 토큰화 결과 확인 (첫 10개 토큰만 출력)\n",
    "    print(tokens[:10])\n",
    "    # 토큰 ID를 실제 토큰(문자열)로 변환해서 출력\n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens[:10]) \n",
    "    print(token_strings)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc751ee",
   "metadata": {},
   "source": [
    "### 5. **Semantic Chunking**\n",
    "\n",
    "- **SemanticChunker**는 텍스트를 의미 단위로 **분할**하는 특수한 텍스트 분할도구 \n",
    "\n",
    "- 단순 길이 기반이 아닌 **의미 기반**으로 텍스트를 청크화하는데 효과적\n",
    "\n",
    "- **breakpoint_threshold_type**: Text Splitting의 다양한 임계값(Threshold) 설정 방식 (통계적 기법) \n",
    "\n",
    "    - **Gradient** 방식: 임베딩 벡터 간의 **기울기 변화**를 기준으로 텍스트를 분할\n",
    "    - **Percentile** 방식: 임베딩 거리의 **백분위수**를 기준으로 분할 지점을 결정\n",
    "    - **Standard Deviation** 방식: 임베딩 거리의 **표준편차**를 활용하여 유의미한 변화점을 찾아서 분할\n",
    "    - **Interquartile** 방식: 임베딩 거리의 **사분위수 범위**를 기준으로 이상치를 감지하여 분할\n",
    "\n",
    "- 설치: pip install langchain_experimental 또는 poetry add langchain_experimental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "043984b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker \n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# 임베딩 모델을 사용하여 SemanticChunker를 초기화 \n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"),         # OpenAI 임베딩 사용\n",
    "    breakpoint_threshold_type=\"gradient\",  # 임계값 타입 설정 (gradient, percentile, standard_deviation, interquartile)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e067dc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 청크 수: 2\n",
      "각 청크의 길이: [1741, 1117]\n",
      "\n",
      "429\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and\n",
      "==================================================\n",
      "\n",
      "238\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and \n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.split_documents([pdf_docs[0]])\n",
    "\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "print()\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "for chunk in chunks[:5]:\n",
    "\n",
    "    # 각 청크를 토큰화\n",
    "    tokens = tokenizer.encode(chunk.page_content)\n",
    "    # 각 청크의 단어 수 확인\n",
    "    print(len(tokens))\n",
    "    # 각 청크의 내용을 확인\n",
    "    print(chunk.page_content[:100])\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a09ca",
   "metadata": {},
   "source": [
    "# [실습 프로젝트]\n",
    "\n",
    "## 텍스트 분할 전략 비교 \n",
    "1. 문서 파일(data/ai.txt)을 읽어서 랭체인 Document 객체로 변환합니다. \n",
    "2. 다음 텍스트 분할기를 구현하고 결과를 비교하세요:\n",
    "   - RecursiveCharacterTextSplitter\n",
    "   - CharacterTextSplitter (정규식 사용)\n",
    "   - SemanticChunker\n",
    "3. 각 분할기의 장단점을 분석하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79dd7358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveCharacterTextSplitter: 청크 수=4, min=278, mean=404, max=484\n",
      "첫 2개 청크(요약):\n",
      "  [1] len=484 snippet=인공지능의 발전과 미래 전망  제1장 인공지능의 기초 인공지능(Artificial Intelligence, AI)은 인간의 학습능력과 추론능력, 지각능력, 자연언어의 이해능력 등을 컴퓨터 프로그램으로 실현한 기술이다. 현대의 인공지능은 머신러닝과 딥러닝을 통해 비약적인 발전을 이루고 있다. 특히 자연어 처리, 컴퓨터 비전, 음성 인식 등의 분야에서 혁신적인 성과를 보여주고 있다.  1.1 머신러닝의 개념 머신러닝은 컴퓨터가 데이터로부터 패턴을 학습하여 의사결정을 내리는 기술이다. 지도학습, 비지도학습, 강화학습 등 다양한 학습 방법...\n",
      "  [2] len=387 snippet=제2장 인공지능의 응용 2.1 자연어 처리 자연어 처리 기술은 인간의 언어를 컴퓨터가 이해하고 처리할 수 있게 하는 기술이다. 기계번역, 감성분석, 문서요약 등 다양한 분야에서 활용되고 있다. 최근에는 트랜스포머 모델의 등장으로 성능이 크게 향상되었다.  2.2 컴퓨터 비전 컴퓨터 비전은 디지털 이미지나 비디오를 분석하여 의미 있는 정보를 추출하는 기술이다. 얼굴 인식, 객체 탐지, 이미지 분류 등에 활용되며, 자율주행 자동차, 의료 영상 분석 등 다양한 산업 분야에서 중요한 역할을 한다.  2.3 음성 인식 음성 인식 기술은 인간...\n",
      "------------------------------------------------------------\n",
      "CharacterTextSplitter (regex): 청크 수=4, min=278, mean=411, max=495\n",
      "첫 2개 청크(요약):\n",
      "  [1] len=484 snippet=인공지능의 발전과 미래 전망  제1장 인공지능의 기초 인공지능(Artificial Intelligence, AI)은 인간의 학습능력과 추론능력, 지각능력, 자연언어의 이해능력 등을 컴퓨터 프로그램으로 실현한 기술이다. 현대의 인공지능은 머신러닝과 딥러닝을 통해 비약적인 발전을 이루고 있다. 특히 자연어 처리, 컴퓨터 비전, 음성 인식 등의 분야에서 혁신적인 성과를 보여주고 있다.  1.1 머신러닝의 개념 머신러닝은 컴퓨터가 데이터로부터 패턴을 학습하여 의사결정을 내리는 기술이다. 지도학습, 비지도학습, 강화학습 등 다양한 학습 방법...\n",
      "  [2] len=387 snippet=제2장 인공지능의 응용 2.1 자연어 처리 자연어 처리 기술은 인간의 언어를 컴퓨터가 이해하고 처리할 수 있게 하는 기술이다. 기계번역, 감성분석, 문서요약 등 다양한 분야에서 활용되고 있다. 최근에는 트랜스포머 모델의 등장으로 성능이 크게 향상되었다.  2.2 컴퓨터 비전 컴퓨터 비전은 디지털 이미지나 비디오를 분석하여 의미 있는 정보를 추출하는 기술이다. 얼굴 인식, 객체 탐지, 이미지 분류 등에 활용되며, 자율주행 자동차, 의료 영상 분석 등 다양한 산업 분야에서 중요한 역할을 한다.  2.3 음성 인식 음성 인식 기술은 인간...\n",
      "------------------------------------------------------------\n",
      "SemanticChunker: 청크 수=3, min=75, mean=537, max=1344\n",
      "첫 2개 청크(요약):\n",
      "  [1] len=1344 snippet=인공지능의 발전과 미래 전망  제1장 인공지능의 기초 인공지능(Artificial Intelligence, AI)은 인간의 학습능력과 추론능력, 지각능력, 자연언어의 이해능력 등을 컴퓨터 프로그램으로 실현한 기술이다. 현대의 인공지능은 머신러닝과 딥러닝을 통해 비약적인 발전을 이루고 있다. 특히 자연어 처리, 컴퓨터 비전, 음성 인식 등의 분야에서 혁신적인 성과를 보여주고 있다. 1.1 머신러닝의 개념 머신러닝은 컴퓨터가 데이터로부터 패턴을 학습하여 의사결정을 내리는 기술이다. 지도학습, 비지도학습, 강화학습 등 다양한 학습 방법이...\n",
      "  [2] len=194 snippet=Smith, J. (2023). The Future of Artificial Intelligence. AI Journal, 15(2), 123-145. 2. Johnson, M. & Lee, K. (2022). Ethics in AI Development. Tech Ethics Review, 8(4), 78-92. 3. Kim, S. et al....\n",
      "------------------------------------------------------------\n",
      "\n",
      "간단 비교 분석:\n",
      "- RecursiveCharacterTextSplitter: 문맥 보존이 잘 되며 구분자 우선순위로 자연스러운 분할 가능. 하지만 긴 문서에서 일부 경계가 부정확할 수 있음.\n",
      "- CharacterTextSplitter (regex): 문장 기준 분할로 의미 단위가 비교적 명확. 단, 복잡한 문장부호나 약어에 취약할 수 있음.\n",
      "- SemanticChunker: 의미 기반 분할로 의미적 경계가 우수함(임베딩 의존). 단점은 임베딩 계산 비용 및 외부 모델/API 필요성.\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "# python: /app/miniconda3/envs/bank/bin/python\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "import os\n",
    "import statistics\n",
    "\n",
    "# 파일 읽기 -> Document 객체\n",
    "txt_path = \"./data/ai.txt\"\n",
    "with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "doc = Document(page_content=text, metadata={\"source\": txt_path})\n",
    "\n",
    "# 1) RecursiveCharacterTextSplitter\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "rc_chunks = rc_splitter.split_documents([doc])\n",
    "\n",
    "# 2) CharacterTextSplitter (정규표현식 사용: 문장 단위)\n",
    "re_splitter = CharacterTextSplitter(\n",
    "    separator=r'(?<=[.!?])\\s+',\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    is_separator_regex=True,\n",
    "    keep_separator=True,\n",
    ")\n",
    "re_chunks = re_splitter.split_documents([doc])\n",
    "\n",
    "# 3) SemanticChunker (가능하면 실행, 없으면 안내)\n",
    "semantic_chunks = None\n",
    "try:\n",
    "    from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "    # 가급적 OpenAI Embeddings 사용 가능하면 사용, 없으면 로컬 sentence-transformers 래퍼 사용\n",
    "    try:\n",
    "        from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    except Exception:\n",
    "        # 간단한 HuggingFace sentence-transformers 래퍼 (로컬 모델 필요)\n",
    "        class HFEmbeddingsWrapper:\n",
    "            def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "                from sentence_transformers import SentenceTransformer\n",
    "                self.model = SentenceTransformer(model_name)\n",
    "            def embed_documents(self, texts):\n",
    "                return [list(v) for v in self.model.encode(texts, show_progress_bar=False)]\n",
    "\n",
    "        embeddings = HFEmbeddingsWrapper()\n",
    "\n",
    "    semantic_splitter = SemanticChunker(\n",
    "        embeddings=embeddings,\n",
    "        breakpoint_threshold_type=\"percentile\",\n",
    "    )\n",
    "    semantic_chunks = semantic_splitter.split_documents([doc])\n",
    "except Exception as e:\n",
    "    print(\"SemanticChunker 사용 불가:\", str(e))\n",
    "    semantic_chunks = []\n",
    "\n",
    "# 유틸: 간단 통계 및 샘플 출력 (토큰 카운트는 tiktoken 사용 가능 시 적용)\n",
    "def summarize(chunks, name):\n",
    "    if not chunks:\n",
    "        print(f\"{name}: 생성된 청크 없음\")\n",
    "        return\n",
    "    lengths = [len(c.page_content) for c in chunks]\n",
    "    print(f\"{name}: 청크 수={len(chunks)}, min={min(lengths)}, mean={int(statistics.mean(lengths))}, max={max(lengths)}\")\n",
    "    print(f\"첫 2개 청크(요약):\")\n",
    "    for i, c in enumerate(chunks[:2], 1):\n",
    "        snippet = c.page_content.replace(\"\\n\", \" \")[:300]\n",
    "        print(f\"  [{i}] len={len(c.page_content)} snippet={snippet}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# 결과 요약\n",
    "summarize(rc_chunks, \"RecursiveCharacterTextSplitter\")\n",
    "summarize(re_chunks, \"CharacterTextSplitter (regex)\")\n",
    "summarize(semantic_chunks, \"SemanticChunker\")\n",
    "\n",
    "# 간단 분석 (장단점)\n",
    "print(\"\\n간단 비교 분석:\")\n",
    "print(\"- RecursiveCharacterTextSplitter: 문맥 보존이 잘 되며 구분자 우선순위로 자연스러운 분할 가능. 하지만 긴 문서에서 일부 경계가 부정확할 수 있음.\")\n",
    "print(\"- CharacterTextSplitter (regex): 문장 기준 분할로 의미 단위가 비교적 명확. 단, 복잡한 문장부호나 약어에 취약할 수 있음.\")\n",
    "print(\"- SemanticChunker: 의미 기반 분할로 의미적 경계가 우수함(임베딩 의존). 단점은 임베딩 계산 비용 및 외부 모델/API 필요성.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
