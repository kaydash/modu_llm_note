{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2217fa83",
      "metadata": {},
      "source": [
        "## **RunnableConfig** 설정\n",
        "- LangChain에서 런타임에 Runnable(실행 가능한 컴포넌트)의 동작을 세밀하게 제어하기 위한 설정 객체\n",
        "\n",
        "**주요 특징**:\n",
        "- 체인, 툴, 모델 등 다양한 Runnable에 전달되어 실행 시 동작을 조정\n",
        "- 실행 중인 Runnable과 하위 호출들에 설정을 전달하는 컨텍스트 역할\n",
        "- LangChain의 모든 Runnable은 이 설정을 받아 실행을 최적화하거나 동작 변경 가능\n",
        "\n",
        "**주요 속성**:\n",
        "\n",
        "1. **configurable**\n",
        "   - **용도**: 런타임에 조정 가능한 속성 값 전달\n",
        "   - **예시**: 모델의 온도, 세션 ID, 프롬프트 템플릿 등\n",
        "\n",
        "2. **callbacks**\n",
        "   - **용도**: 실행 과정에서 이벤트를 처리할 콜백 핸들러 지정\n",
        "   - **예시**: 로깅, 모니터링, 이벤트 추적\n",
        "\n",
        "3. **tags**\n",
        "   - **용도**: 실행에 태그를 붙여 추적 및 필터링\n",
        "   - **예시**: 실험 버전, 사용자 그룹, 요청 타입 등\n",
        "\n",
        "4. **metadata**\n",
        "   - **용도**: 실행 관련 추가 메타데이터 전달\n",
        "   - **예시**: 요청 ID, 사용자 정보, 세션 데이터 등\n",
        "\n",
        "- [LangChain RunnableConfig 공식 문서](https://python.langchain.com/docs/concepts/runnables/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "15ac3ddb",
      "metadata": {
        "id": "15ac3ddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "true\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Langsmith tracing 여부를 확인 (true: langsmith 추적 활성화, false: langsmith 추적 비활성화)\n",
        "import os\n",
        "print(os.getenv('LANGSMITH_TRACING'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d26956",
      "metadata": {},
      "source": [
        "### 1. 성능 모니터링 콜백 핸들러 구현\n",
        "\n",
        "- LLM 호출의 성능을 실시간으로 모니터링하는 콜백 핸들러 구현\n",
        "- `BaseCallbackHandler`를 상속하여 구현\n",
        "- `on_llm_start`, `on_llm_end`, `on_llm_error` 메서드를 오버라이드\n",
        "- 실행 시간, 토큰 사용량, 호출 횟수 등의 성능 지표 수집\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f98e0662",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional\n",
        "from langchain_core.callbacks import BaseCallbackHandler\n",
        "from langchain_core.outputs import LLMResult\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "class PerformanceMonitoringCallback(BaseCallbackHandler):\n",
        "    \"\"\"LLM 호출 성능을 모니터링하는 콜백 핸들러\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.start_time: Optional[float] = None       # LLM 호출 시작 시간\n",
        "        self.token_usage: Dict[str, Any] = {}         # 토큰 사용량 정보\n",
        "        self.call_count: int = 0                      # LLM 호출 횟수\n",
        "        \n",
        "    def on_llm_start(\n",
        "        self, \n",
        "        serialized: Dict[str, Any], \n",
        "        prompts: List[str], \n",
        "        **kwargs: Any\n",
        "    ) -> None:\n",
        "        \"\"\"LLM 호출이 시작될 때 호출\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.call_count += 1\n",
        "        print(f\"🚀 LLM 호출 #{self.call_count} 시작 - {datetime.now().strftime('%H:%M:%S')}\")\n",
        "        \n",
        "        # 첫 번째 프롬프트의 길이 확인\n",
        "        if prompts:\n",
        "            print(f\"📝 프롬프트 길이: {len(prompts[0])} 문자\")\n",
        "        \n",
        "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
        "        \"\"\"LLM 호출이 완료될 때 호출\"\"\"\n",
        "        if self.start_time:\n",
        "            duration = time.time() - self.start_time\n",
        "            print(f\"✅ LLM 호출 완료 - 소요시간: {duration:.2f}초\")\n",
        "            \n",
        "            # 토큰 사용량 추적\n",
        "            if response.generations:\n",
        "                generation = response.generations[0][0]\n",
        "            \n",
        "                usage = response.llm_output.get('token_usage', {})\n",
        "                if usage:\n",
        "                    print(f\"🔢 토큰 사용량: {usage}\")\n",
        "                    self.token_usage = usage\n",
        "                        \n",
        "                # 응답 길이 체크\n",
        "                if hasattr(generation, 'text'):\n",
        "                    response_text = generation.text\n",
        "                    print(f\"📊 응답 길이: {len(response_text)} 문자\")\n",
        "        \n",
        "    def on_llm_error(self, error: Exception, **kwargs: Any) -> None:\n",
        "        \"\"\"LLM 호출에서 오류가 발생할 때 호출\"\"\"\n",
        "        print(f\"❌ LLM 호출 오류: {str(error)}\")\n",
        "        \n",
        "    def get_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"현재까지의 통계 정보를 반환\"\"\"\n",
        "        return {\n",
        "            \"total_calls\": self.call_count,\n",
        "            \"last_token_usage\": self.token_usage\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70bcc70d",
      "metadata": {},
      "source": [
        "### 2. 실시간 알림 콜백 핸들러 구현\n",
        "\n",
        "- 특정 조건(비용 임계값, 응답 시간 등)에서 알림을 보내는 콜백 핸들러를 구현 (임계값 기반 알림 시스템 구현)\n",
        "- 비용, 응답 시간, 프롬프트 길이 등 다양한 조건 모니터링\n",
        "- 실제 프로덕션 환경에서는 외부 알림 서비스 연동 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4002a421",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AlertCallback(BaseCallbackHandler):\n",
        "    \"\"\"특정 조건에서 알림을 보내는 콜백 핸들러\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        cost_threshold: float = 1.0,            # 비용 임계값 (달러 단위)\n",
        "        response_time_threshold: float = 10.0,  # 응답 시간 임계값 (초 단위)\n",
        "        token_threshold: int = 4000             # 긴 프롬프트 토큰 임계값\n",
        "    ):\n",
        "        self.cost_threshold = cost_threshold\n",
        "        self.response_time_threshold = response_time_threshold\n",
        "        self.token_threshold = token_threshold\n",
        "        self.start_time: Optional[float] = None    # LLM 호출 시작 시간\n",
        "        self.cumulative_cost: float = 0.0          # 누적 비용 추적\n",
        "        \n",
        "    def on_llm_start(\n",
        "        self, \n",
        "        serialized: Dict[str, Any], \n",
        "        prompts: List[str], \n",
        "        **kwargs: Any\n",
        "    ) -> None:\n",
        "        \"\"\"LLM 호출이 시작될 때 호출\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        \n",
        "        # 긴 프롬프트 경고\n",
        "        if prompts and len(prompts[0]) > self.token_threshold:\n",
        "            self._send_alert(f\"⚠️ 긴 프롬프트 감지: {len(prompts[0])} 문자\")\n",
        "    \n",
        "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
        "        \"\"\"LLM 호출이 완료될 때 호출\"\"\"\n",
        "        # 응답 시간 체크\n",
        "        if self.start_time:\n",
        "            duration = time.time() - self.start_time\n",
        "            if duration > self.response_time_threshold:  # 임계값 초과 시 알림\n",
        "                self._send_alert(f\"🐌 느린 응답: {duration:.2f}초\")\n",
        "        \n",
        "        # 비용 체크\n",
        "        if response.generations:\n",
        "            generation = response.generations[0][0]\n",
        "            usage = response.llm_output.get('token_usage', {})\n",
        "            \n",
        "            if usage:\n",
        "                # 간단한 비용 계산 (실제로는 모델별 가격 적용 필요)\n",
        "                total_tokens = usage.get('total_tokens', 0)\n",
        "                if total_tokens == 0:\n",
        "                    # input_tokens와 output_tokens로 계산\n",
        "                    total_tokens = usage.get('input_tokens', 0) + usage.get('output_tokens', 0)\n",
        "                \n",
        "                estimated_cost = (total_tokens / 1000) * 0.002\n",
        "                self.cumulative_cost += estimated_cost\n",
        "                \n",
        "                if self.cumulative_cost > self.cost_threshold:\n",
        "                    self._send_alert(f\"💸 비용 임계값 초과: ${self.cumulative_cost:.4f}\")\n",
        "    \n",
        "    def on_llm_error(self, error: Exception, **kwargs: Any) -> None:\n",
        "        \"\"\"LLM 호출에서 오류가 발생할 때 호출\"\"\"\n",
        "        self._send_alert(f\"🚨 LLM 오류 발생: {str(error)}\")\n",
        "    \n",
        "    def _send_alert(self, message: str) -> None:\n",
        "        \"\"\"실제 환경에서는 Slack, Discord, 이메일 등으로 알림을 보냄\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        alert_message = f\"[ALERT] {timestamp} - {message}\"\n",
        "        print(\"🔔 알림 전송:\", alert_message)\n",
        "        \n",
        "        # 로깅 시스템에 기록\n",
        "        logging.warning(alert_message)\n",
        "        \n",
        "        # 실제 구현에서는 아래와 같은 방식으로 외부 서비스에 알림 전송\n",
        "        # self._send_slack_notification(message)\n",
        "        # self._send_email_notification(message)\n",
        "    \n",
        "    def reset_cost_tracking(self) -> None:\n",
        "        \"\"\"누적 비용 추적을 리셋\"\"\"\n",
        "        self.cumulative_cost = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f72499f",
      "metadata": {},
      "source": [
        "### 3. RunnableConfig 기본 사용\n",
        "\n",
        "- RunnableConfig를 사용하여 체인 실행을 제어하고 모니터링\n",
        "- `configurable_fields`를 통한 런타임 설정 가능한 모델 생성\n",
        "- RunnableConfig의 4가지 주요 속성 활용\n",
        "- 콜백 핸들러를 통한 실시간 성능 모니터링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a3180b2a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "h:\\miniconda3\\envs\\modu2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 LLM 호출 #1 시작 - 17:34:47\n",
            "📝 프롬프트 길이: 59 문자\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:48,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LLM 호출 완료 - 소요시간: 0.62초\n",
            "🔢 토큰 사용량: {'completion_tokens': 8, 'prompt_tokens': 38, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}\n",
            "📊 응답 길이: 32 문자\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "# 설정 가능한 모델 생성 - temperature를 런타임에 변경 가능하게 설정\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\",  \n",
        "    temperature=0.3,      # 기본값\n",
        "    top_p=0.95\n",
        ").configurable_fields(    # temperature 필드를 런타임에 변경 가능하게 설정\n",
        "    temperature=ConfigurableField(\n",
        "        id=\"temperature\",\n",
        "        name=\"Model Temperature\",\n",
        "        description=\"모델의 창의성을 조절하는 온도 매개변수\"\n",
        "    )\n",
        ")\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"'{text}'를 영어로 번역해주세요. 번역된 문장만을 출력해주세요.\")\n",
        "output_parser = StrOutputParser()\n",
        "translation_chain = prompt | model | output_parser\n",
        "\n",
        "# 콜백 핸들러 생성\n",
        "performance_handler = PerformanceMonitoringCallback()\n",
        "\n",
        "# 체인 실행\n",
        "result = translation_chain.invoke(\n",
        "    {\n",
        "        \"text\": \"안녕하세요, 오늘 날씨는 어떠신가요?\"\n",
        "    },\n",
        "    config={\n",
        "        \"configurable\": {\"temperature\": 0.7, \"session_id\": \"user123\"},   # 런타임에 temperature를 변경 가능\n",
        "        \"callbacks\": [performance_handler],\n",
        "        \"tags\": [\"experiment_v1\", \"production\"],\n",
        "        \"metadata\": {\"request_id\": \"req_001\", \"user_id\": \"user123\"},\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d4c00c73",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "번역 결과: Hello, how is the weather today?\n"
          ]
        }
      ],
      "source": [
        "print(f\"번역 결과: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34a3f5fd",
      "metadata": {},
      "source": [
        "### 4. 알림 조건 테스트\n",
        "\n",
        "- AlertCallback의 다양한 알림 조건을 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "89636754",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:48,192 - root - WARNING - [ALERT] 2025-09-27 17:34:48 - ⚠️ 긴 프롬프트 감지: 51 문자\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 LLM 호출 #2 시작 - 17:34:48\n",
            "📝 프롬프트 길이: 51 문자\n",
            "🔔 알림 전송: [ALERT] 2025-09-27 17:34:48 - ⚠️ 긴 프롬프트 감지: 51 문자\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:48,703 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-09-27 17:34:48,712 - root - WARNING - [ALERT] 2025-09-27 17:34:48 - 🐌 느린 응답: 0.52초\n",
            "2025-09-27 17:34:48,713 - root - WARNING - [ALERT] 2025-09-27 17:34:48 - 💸 비용 임계값 초과: $0.0001\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LLM 호출 완료 - 소요시간: 0.52초\n",
            "🔢 토큰 사용량: {'completion_tokens': 3, 'prompt_tokens': 28, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}\n",
            "📊 응답 길이: 12 문자\n",
            "🔔 알림 전송: [ALERT] 2025-09-27 17:34:48 - 🐌 느린 응답: 0.52초\n",
            "🔔 알림 전송: [ALERT] 2025-09-27 17:34:48 - 💸 비용 임계값 초과: $0.0001\n"
          ]
        }
      ],
      "source": [
        "# 조건을 완화해서 테스트\n",
        "alert_handler_test = AlertCallback(\n",
        "    cost_threshold=0.0,  # $0.0 임계값 -> 모든 호출에 대해 알림\n",
        "    response_time_threshold=0.1,  # 0.1초 임계값 -> 대부분의 호출에서 알림\n",
        "    token_threshold=10  # 10 문자 임계값 -> 짧은 프롬프트도 알림\n",
        ")\n",
        "\n",
        "# 테스트 실행\n",
        "test_result = translation_chain.invoke(\n",
        "    {\n",
        "        \"text\": \"Hello World!\"  # 짧은 텍스트로 테스트\n",
        "    },\n",
        "    config={\n",
        "        \"configurable\": {\"temperature\": 0.1},\n",
        "        \"callbacks\": [performance_handler, alert_handler_test],  # 성능 모니터링과 알림 콜백 핸들러 모두 사용\n",
        "        \"tags\": [\"test\", \"alert_validation\"],\n",
        "        \"metadata\": {\"request_id\": \"test_001\", \"test_type\": \"alert_threshold\"},\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "505d57ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "테스트 결과: Hello World!\n"
          ]
        }
      ],
      "source": [
        "print(f\"테스트 결과: {test_result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e58c4894",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 호출 횟수: 2\n",
            "마지막 토큰 사용량: {'completion_tokens': 3, 'prompt_tokens': 28, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}\n"
          ]
        }
      ],
      "source": [
        "# 통계 정보 확인\n",
        "stats = performance_handler.get_statistics()\n",
        "print(f\"총 호출 횟수: {stats['total_calls']}\")\n",
        "print(f\"마지막 토큰 사용량: {stats['last_token_usage']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf8d1ff0",
      "metadata": {},
      "source": [
        "**[참고]** [LangSmith Alert 설정](https://docs.smith.langchain.com/observability/how_to_guides/alerts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87732731",
      "metadata": {},
      "source": [
        "### 5. 기타 활용법"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1af3dac",
      "metadata": {},
      "source": [
        "`(1) 동적 모델 선택`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8e907d8d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "현재 모델 이름: gpt-4.1-mini\n"
          ]
        }
      ],
      "source": [
        "# model 변수의 모델 이름 출력\n",
        "print(\"현재 모델 이름:\", model.model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0ee2b116",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:50,485 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "번역 결과: Hello, how is the weather today?\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# 설정 가능한 모델 생성 (여러 모델을 선택할 수 있도록 설정)\n",
        "model_alternatives = model.configurable_alternatives(\n",
        "    ConfigurableField(id=\"model_name\"),\n",
        "    default_key=\"gpt4_mini\",  # model 변수의 기본 모델을 식별하는 키\n",
        "    openai_gpt4=ChatOpenAI(model=\"gpt-4.1\"),  # OpenAI GPT-4 모델\n",
        "    google_gemini=ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\"),  # Google Gemini 모델\n",
        ")\n",
        "\n",
        "# chain 설정 \n",
        "translation_chain = prompt | model_alternatives | output_parser\n",
        "\n",
        "# 기본 모델로 번역 실행 (default_key: gpt4_mini)\n",
        "result = translation_chain.invoke(\n",
        "    {\"text\": \"안녕하세요, 오늘 날씨는 어떠신가요?\"}\n",
        ")\n",
        "\n",
        "print(\"번역 결과:\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a0d7be5f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "번역 결과: Hello, how is the weather today?\n"
          ]
        }
      ],
      "source": [
        "# 런타임에 모델 변경 (google_gemini 키 사용 -> gemini-2.0-flash 모델 사용)\n",
        "result = translation_chain.invoke(\n",
        "    {\"text\": \"안녕하세요, 오늘 날씨는 어떠신가요?\"},\n",
        "    config={\"configurable\": {\"model_name\": \"google_gemini\"}} \n",
        ")\n",
        "\n",
        "print(\"번역 결과:\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "91402339",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:51,943 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, how is the weather today?\n"
          ]
        }
      ],
      "source": [
        "# 런타임에 모델 변경  (openai_gpt4 키 사용 -> gpt-4.1 모델 사용)\n",
        "result = translation_chain.invoke(\n",
        "    {\"text\": \"안녕하세요, 오늘 날씨는 어떠신가요?\"},\n",
        "    config={\"configurable\": {\"model_name\": \"openai_gpt4\"}}\n",
        ")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c80c203",
      "metadata": {},
      "source": [
        "`(2) 동적 프롬프트 선택`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e59f6a60",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 여러 모델을 alternatives로 설정\n",
        "model = ChatOpenAI(model=\"gpt-4.1-mini\").configurable_alternatives(\n",
        "    ConfigurableField(id=\"model_provider\"),\n",
        "    default_key=\"gpt4_mini\",\n",
        "    openai_gpt4=ChatOpenAI(model=\"gpt-4.1\"),\n",
        "    google_gemini=ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        ").configurable_fields(\n",
        "    temperature=ConfigurableField(\n",
        "        id=\"temperature\",\n",
        "        name=\"Temperature\",\n",
        "        description=\"Model temperature for response randomness\"\n",
        "    ),\n",
        "    max_tokens=ConfigurableField(\n",
        "        id=\"max_tokens\",\n",
        "        name=\"Max Tokens\", \n",
        "        description=\"Maximum tokens to generate\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# 기본 프롬프트\n",
        "default_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 도움이 되는 AI 어시스턴트입니다. 모든 질문에 최선을 다해 답변하세요.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# 창의적 프롬프트\n",
        "creative_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 창의적인 AI 어시스턴트입니다. 상상력을 발휘하여 답변하세요.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# 분석적 프롬프트\n",
        "analytical_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 분석적인 AI 어시스턴트입니다. 논리적으로 사고하여 답변하세요.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# 프롬프트를 configurable alternatives로 설정\n",
        "prompt = default_prompt.configurable_alternatives(\n",
        "    ConfigurableField(id=\"prompt_type\"),\n",
        "    default_key=\"default\",\n",
        "    creative=creative_prompt,\n",
        "    analytical=analytical_prompt\n",
        ")\n",
        "\n",
        "# chain 설정\n",
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "db79aa51",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:58,367 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기본 프롬프트 결과: 인공지능(人工知能, Artificial Intelligence, AI)은 인간의 지능을 기계나 컴퓨터 시스템에 구현하는 기술과 학문 분야를 말합니다. AI는 데이터를 분석하고 학습하며, 문제를 해결하거나 의사결정을 지원하는 능력을 갖추도록 설계됩니다.\n",
            "\n",
            "### 인공지능의 주요 개념\n",
            "1. **기계 학습(Machine Learning)**  \n",
            "   컴퓨터가 명시적인 프로그래밍 없이 데이터를 통해 스스로 학습하고 성능을 향상시키는 기술입니다. 대표적인 예로는 지도학습, 비지도학습, 강화학습 등이 있습니다.\n",
            "\n",
            "2. **딥러닝(Deep Learning)**  \n",
            "   인공신경망을 기반으로 한 기계학습의 한 분야로, 다층 신경망을 통해 복잡한 데이터의 패턴을 학습합니다. 이미지 인식, 음성 인식, 자연어 처리 등에 널리 사용됩니다.\n",
            "\n",
            "3. **자연어 처리(Natural Language Processing, NLP)**  \n",
            "   컴퓨터가 인간의 언어를 이해하고 생성하는 기술입니다. 번역, 챗봇, 음성 인식 등이 포함됩니다.\n",
            "\n",
            "4. **컴퓨터 비전(Computer Vision)**  \n",
            "   이미지나 동영상에서 유용한 정보를 추출하는 기술로, 얼굴 인식, 객체 탐지 등에 활용됩니다.\n",
            "\n",
            "### 인공지능의 활용 분야\n",
            "- **의료:** 질병 진단, 신약 개발, 의료 영상 분석\n",
            "- **자동차:** 자율주행차, 교통 관리\n",
            "- **금융:** 신용 평가, 투자 분석, 사기 탐지\n",
            "- **고객 서비스:** 챗봇, 음성 비서\n",
            "- **제조:** 스마트 팩토리, 품질 검사\n",
            "\n",
            "### 인공지능의 장점과 한계\n",
            "- **장점:** 대량 데이터 처리, 인간이 하기 어려운 복잡한 문제 해결, 업무 자동화\n",
            "- **한계:** 데이터 편향 문제, 윤리적 이슈, 완전한 인간 지능 대체 불가\n",
            "\n",
            "필요하시면 특정 분야나 기술에 대해 더 자세히 설명해 드릴 수 있습니다.\n"
          ]
        }
      ],
      "source": [
        "# 기본 프롬프트로 실행\n",
        "basic_config = {\n",
        "    \"configurable\": {\n",
        "        \"model_provider\": \"gpt4_mini\",\n",
        "        \"temperature\": 0.5,\n",
        "        \"max_tokens\": 500,\n",
        "        \"prompt_type\": \"default\"\n",
        "    }\n",
        "}\n",
        "\n",
        "result1 = chain.invoke(\n",
        "    {\"user_input\": \"인공지능에 대해 알려주세요\"}, \n",
        "    config=basic_config\n",
        ")\n",
        "\n",
        "print(f\"기본 프롬프트 결과: {result1.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1a7c0243",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "창의적 프롬프트 결과: 자, 인공지능에 대한 이야기는 놀라운 모험과 같습니다! 마치 우리가 상상력으로 가득 찬 로봇 친구를 만드는 것과 같아요.\n",
            "\n",
            "인공지능(AI)은 컴퓨터가 마치 사람처럼 생각하고 배울 수 있도록 만드는 기술입니다. AI는 거대한 두뇌와 같아서, 많은 정보를 빠르게 처리하고 새로운 것을 배울 수 있습니다.\n",
            "\n",
            "AI는 우리 생활 곳곳에 숨어 있어요. 예를 들어, 스마트폰에서 \"안녕, AI\"라고 말하면 AI가 우리의 말을 듣고 대답해 줍니다. 또, 영화를 추천해 주는 AI는 우리가 좋아할 만한 영화를 척척 골라주죠. 운전하는 로봇 자동차도 있고, 그림을 그리는 AI 화가도 있답니다!\n",
            "\n",
            "AI는 배우는 것을 아주 좋아합니다. 마치 어린 아이가 세상을 탐험하며 배우듯이, AI는 수많은 데이터를 분석하고 패턴을 찾아냅니다. 그리고 그 패턴을 바탕으로 미래를 예측하거나 새로운 것을 만들어내기도 합니다.\n",
            "\n",
            "하지만 AI는 아직 완벽하지 않아요. 때로는 엉뚱한 대답을 하거나 실수를 하기도 합니다. 그래서 우리는 AI를 계속 가르치고 훈련시켜야 합니다.\n",
            "\n",
            "AI의 미래는 정말 흥미진진합니다. AI는 우리의 삶을 더욱 편리하고 풍요롭게 만들어 줄 수 있습니다. 하지만 동시에 AI가 우리에게 어떤 영향을 미칠지 고민하고, AI를 올바르게 사용하는 방법을 배워야 합니다.\n",
            "\n",
            "자, 이제 AI의 세계로 함께 떠나볼까요? 어떤 놀라운 일들이 우리를 기다리고 있을지 기대되지 않나요!\n"
          ]
        }
      ],
      "source": [
        "# 창의적 프롬프트로 실행\n",
        "creative_config = {\n",
        "    \"configurable\": {\n",
        "        \"model_provider\": \"google_gemini\",\n",
        "        \"temperature\": 0.9,\n",
        "        \"max_tokens\": 700,\n",
        "        \"prompt_type\": \"creative\"\n",
        "    }\n",
        "}\n",
        "\n",
        "result2 = chain.invoke(\n",
        "    {\"user_input\": \"인공지능에 대해 알려주세요\"}, \n",
        "    config=creative_config\n",
        ")\n",
        "print(f\"창의적 프롬프트 결과: {result2.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "130a549b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## **LangChain Fallback 처리**\n",
        "- **Fallback**은 비상 상황에서 사용할 수 있는 대안적 계획을 의미\n",
        "- LangChain에서는 주요 실행 경로가 실패했을 때 자동으로 대체 경로를 실행하는 메커니즘을 제공\n",
        "\n",
        "**필요성**:\n",
        "\n",
        "1. **API 안정성 문제**\n",
        "    - 요금 제한(Rate Limiting)\n",
        "    - 서버 다운타임\n",
        "    - 네트워크 오류\n",
        "    - API 키 할당량 초과\n",
        "\n",
        "2. **비용 최적화**\n",
        "    - 저렴한 모델을 먼저 시도\n",
        "    - 실패 시에만 비싼 모델 사용\n",
        "\n",
        "3. **성능 최적화**\n",
        "   - 빠른 모델을 우선 사용\n",
        "    - 복잡한 작업에만 고성능 모델 사용\n",
        "\n",
        "4. **컨텍스트 길이 제한**\n",
        "   - 짧은 컨텍스트 모델을 먼저 시도\n",
        "   - 토큰 초과 시 긴 컨텍스트 모델 사용\n",
        "\n",
        "- [LangChain Fallbacks 가이드](https://python.langchain.com/docs/how_to/fallbacks/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2b220a7",
      "metadata": {},
      "source": [
        "### 1. API 오류에 대한 Fallback 처리\n",
        "\n",
        "- OpenAI API 오류 발생 시 Google Gemini로 자동 전환하는 시스템 구현\n",
        "- `max_retries=0` 설정으로 즉시 fallback 전환\n",
        "- 여러 단계의 fallback 체인 구성 가능"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7360d40",
      "metadata": {},
      "source": [
        "`(1) 기본 Fallback 구현`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9c4a59be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 기본 모델과 fallback 모델 설정\n",
        "# max_retries=0으로 설정하여 즉시 fallback으로 전환\n",
        "primary_model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\", \n",
        "    max_retries=0,  # 재시도 없이 바로 fallback으로 전환\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "fallback_model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\", \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Fallback 체인 생성\n",
        "llm_with_fallback = primary_model.with_fallbacks([fallback_model])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c951fbfa",
      "metadata": {},
      "source": [
        "`(2) 테스트용 오류 시뮬레이션`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a9674893",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Primary 모델만 사용 (오류 발생) ===\n",
            "오류 발생: Rate limit exceeded\n",
            "\n",
            "=== Fallback이 적용된 모델 사용 ===\n",
            "결과: 달과 지구 사이의 거리는 일정하지 않고 타원 궤도를 돌기 때문에 변합니다. \n",
            "\n",
            "*   **평균 거리:** 약 384,400km (238,900마일)\n",
            "\n",
            "*   **최근접점 (근지점):** 약 363,104km (225,623마일)\n",
            "\n",
            "*   **최원점 (원지점):** 약 405,696km (252,088마일)...\n",
            "✅ Fallback 모델로 자동 전환 성공!\n"
          ]
        }
      ],
      "source": [
        "from unittest.mock import patch\n",
        "import httpx\n",
        "from openai import RateLimitError\n",
        "\n",
        "# 테스트용 오류 객체 생성\n",
        "def create_mock_error():\n",
        "    request = httpx.Request(\"GET\", \"/\")\n",
        "    response = httpx.Response(429, request=request)  # 429 = 너무 많은 요청\n",
        "    return RateLimitError(\"Rate limit exceeded\", response=response, body=\"\")\n",
        "\n",
        "# 오류 시뮬레이션 테스트\n",
        "error = create_mock_error()\n",
        "\n",
        "print(\"=== Primary 모델만 사용 (오류 발생) ===\")\n",
        "# `patch`를 사용하여 OpenAI API 호출을 Mocking (side_effect를 error로 설정)\n",
        "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
        "    try:\n",
        "        result = primary_model.invoke(\"달과 지구 사이의 거리는?\")\n",
        "        print(f\"결과: {result.content}\")\n",
        "    except RateLimitError as e:\n",
        "        print(f\"오류 발생: {e}\")\n",
        "\n",
        "print(\"\\n=== Fallback이 적용된 모델 사용 ===\")\n",
        "# `patch`를 사용하여 OpenAI API 호출을 Mocking (side_effect를 error로 설정)\n",
        "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
        "    try:\n",
        "        result = llm_with_fallback.invoke(\"달과 지구 사이의 거리는?\")\n",
        "        print(f\"결과: {result.content[:200]}...\")\n",
        "        print(\"✅ Fallback 모델로 자동 전환 성공!\")\n",
        "    except Exception as e:\n",
        "        print(f\"오류 발생: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a092c8c6",
      "metadata": {},
      "source": [
        "`(3) Fallback 체인 예시`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "92bfddeb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 정상 작동 테스트 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:35:07,821 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "응답: 물론입니다! LangChain의 주요 장점 세 가지를 말씀드리자면:\n",
            "\n",
            "1. **유연한 체인 구성**  \n",
            "   LangChain은 다양한 작업을 연결하여 복잡한 워크플로우를 쉽게 구성할 수 있게 해줍니다. 예를 들어, 텍스트 생성, 검색, 데이터 추출 등의 여러 단계를 하나의 체인으로 만들어 자동화할 수 있습니다.\n",
            "\n",
            "2. **다양한 데이터 소스 통합 지원**  \n",
            "   여러 데이터베이스, API, 문서, 검색 엔진 등 다양한 외부 소스와 쉽게 연동할 수 있어, 풍부한 정보를 바탕으로 한 지능형 애플리케이션 개발이 용이합니다.\n",
            "\n",
            "3. **언어 모델과의 원활한 통합**  \n",
            "   OpenAI, Hugging Face 등 다양한 대형 언어 모델(LLM)과 손쉽게 통합할 수 있어서 최신 AI 기술을 빠르게 적용하고 실험할 수 있습니다.\n",
            "\n",
            "필요하시면 더 자세한 설명도 드릴 수 있습니다!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 체인 생성 함수\n",
        "def create_fallback_chat_chain():\n",
        "    \"\"\"Fallback 채팅 체인 생성\"\"\"\n",
        "\n",
        "    # 프롬프트 템플릿\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"당신은 친절하고 도움이 되는 AI 어시스턴트입니다. 항상 예의바르게 답변해주세요.\"),\n",
        "        (\"human\", \"{user_input}\")\n",
        "    ])\n",
        "    \n",
        "    # 여러 단계의 fallback 설정\n",
        "    primary = ChatOpenAI(model=\"gpt-4.1-mini\", max_retries=0, temperature=0.7)\n",
        "    fallback1 = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", max_retries=0, temperature=0.7)\n",
        "    fallback2 = ChatOllama(model=\"qwen3:4b\", temperature=0.7)\n",
        "\n",
        "    # 3단계 fallback 체인\n",
        "    robust_llm = primary.with_fallbacks([fallback1, fallback2])\n",
        "    \n",
        "    # 완전한 체인 구성\n",
        "    chain = prompt | robust_llm | StrOutputParser()\n",
        "    \n",
        "    return chain\n",
        "\n",
        "# 견고한 체인 테스트\n",
        "robust_chain = create_fallback_chat_chain()\n",
        "\n",
        "# 정상 작동 테스트\n",
        "print(\"=== 정상 작동 테스트 ===\")\n",
        "try:\n",
        "    response = robust_chain.invoke({\"user_input\": \"LangChain의 장점을 3가지만 설명해주세요.\"})\n",
        "    print(f\"응답: {response}\")\n",
        "except Exception as e:\n",
        "    print(f\"오류: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ce80559",
      "metadata": {},
      "source": [
        "### 2. 모델별 최적화 적용\n",
        "\n",
        "- 각기 다른 모델에 최적화된 프롬프트를 사용하는 fallback 시스템을 구현\n",
        "- 각 모델의 특성에 맞는 프롬프트 최적화\n",
        "- 성능-비용 균형을 고려한 다단계 fallback\n",
        "- 프로덕션 환경에서의 안정성 확보"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "43393aad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 질문 1: 인공지능의 미래 전망은 어떻게 될까요? ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:35:19,814 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "답변: 질문 분석  \n",
            "“인공지능의 미래 전망”은 매우 포괄적인 주제입니다. 여기에는 기술 발전, 사회적 영향, 경제적 변화, 윤리적 쟁점, 산업별 적용 등 다양한 측면이 포함됩니다. 따라서 이 질문에 대한 답변은 여러 각도에서 접근해야 하며, 단순히 기술적 진보만이 아니라 사회 전반에 미치는 영향을 함께 고려해야 합니다.\n",
            "\n",
            "다각도 접근  \n",
            "1. 기술적 발전  \n",
            "- 딥러닝, 강화학습, 생성형 AI(예: ChatGPT, Midjourney 등) 등 인공지능의 핵심 기술은 빠르게 발전하고 있습니다.  \n",
            "- 앞으로는 멀티모달 AI(텍스트, 이미지...\n",
            "\n",
            "=== 질문 2: 기업에서 AI를 도입할 때 고려해야 할 요소들은? ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:35:30,361 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "답변: 질문 분석  \n",
            "“기업에서 AI를 도입할 때 고려해야 할 요소들은?”이라는 질문은 단순히 기술적 도입만이 아니라, 기업의 전략, 조직, 비용, 윤리 등 다양한 측면을 포괄합니다. AI 도입은 기업의 경쟁력 강화, 업무 효율화, 새로운 비즈니스 모델 창출 등 긍정적 효과가 있지만, 실패 시 비용 손실, 데이터 유출, 조직 저항 등 부정적 결과도 초래할 수 있습니다. 따라서 다각도에서 접근이 필요합니다.\n",
            "\n",
            "다각도 접근  \n",
            "1. 전략적 측면  \n",
            "   - 비즈니스 목표와의 정합성: AI 도입이 기업의 장기적 목표와 어떻게 연결되는지 분석해야...\n",
            "\n",
            "=== 질문 3: 머신러닝과 딥러닝의 차이점은? ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:35:42,939 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "답변: 질문 분석  \n",
            "\"머신러닝과 딥러닝의 차이점\"은 인공지능(AI) 분야에서 매우 자주 등장하는 질문입니다. 두 용어 모두 데이터 기반 학습을 통해 문제를 해결한다는 공통점이 있지만, 그 원리와 적용 방식, 성능, 필요 조건 등에서 중요한 차이점이 존재합니다.\n",
            "\n",
            "다각도 접근  \n",
            "1. 개념적 차이  \n",
            "- 머신러닝(Machine Learning):  \n",
            "  머신러닝은 데이터로부터 패턴을 학습하여 예측이나 분류 등의 작업을 수행하는 알고리즘의 집합입니다. 대표적으로 의사결정트리, 서포트 벡터 머신(SVM), 랜덤 포레스트, K-최근접 이웃(KN...\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def create_production_qa_system():\n",
        "    \"\"\"모델별 최적화 Q&A 시스템\"\"\"\n",
        "    \n",
        "    # 고성능 모델용 프롬프트 (gpt-4.1 적용)\n",
        "    premium_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"당신은 전문적인 분석가입니다. \n",
        "        다음 지침을 따라주세요:\n",
        "        1. 질문을 깊이 분석하세요\n",
        "        2. 다각도에서 접근하세요  \n",
        "        3. 구체적인 예시를 포함하세요\n",
        "        4. 결론을 명확히 제시하세요\"\"\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ])\n",
        "    \n",
        "    # 기본 모델용 프롬프트 (gpt-4.1-mini 적용)\n",
        "    standard_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"간결하고 정확한 답변을 제공해주세요.\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ])\n",
        "    \n",
        "    # 경량 모델용 프롬프트 (qwen3:4b 적용)\n",
        "    budget_prompt = PromptTemplate.from_template(\n",
        "        \"질문: {question}\\n\\n핵심 답변:\"\n",
        "    )\n",
        "    \n",
        "    # 모델 체인 구성\n",
        "    premium_chain = premium_prompt | ChatOpenAI(model=\"gpt-4.1\", temperature=0.3, max_retries=0)\n",
        "    standard_chain = standard_prompt | ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.3, max_retries=0)  \n",
        "    budget_chain = budget_prompt | ChatOllama(model=\"qwen3:4b\", temperature=0.3)\n",
        "    \n",
        "    # 3단계 fallback 시스템\n",
        "    qa_system = premium_chain.with_fallbacks([\n",
        "        standard_chain, \n",
        "        budget_chain\n",
        "    ]) | StrOutputParser()\n",
        "    \n",
        "    return qa_system\n",
        "\n",
        "# 모델별 최적화 Q&A 시스템 테스트\n",
        "production_qa = create_production_qa_system()\n",
        "\n",
        "test_questions = [\n",
        "    \"인공지능의 미래 전망은 어떻게 될까요?\",\n",
        "    \"기업에서 AI를 도입할 때 고려해야 할 요소들은?\",\n",
        "    \"머신러닝과 딥러닝의 차이점은?\"\n",
        "]\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n=== 질문 {i}: {question} ===\")\n",
        "    try:\n",
        "        answer = production_qa.invoke({\"question\": question})\n",
        "        print(f\"답변: {answer[:300]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"오류: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d28ed36c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "modu2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
