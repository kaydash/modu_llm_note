{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2217fa83",
      "metadata": {},
      "source": [
        "## **RunnableConfig** ì„¤ì •\n",
        "- LangChainì—ì„œ ëŸ°íƒ€ì„ì— Runnable(ì‹¤í–‰ ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸)ì˜ ë™ì‘ì„ ì„¸ë°€í•˜ê²Œ ì œì–´í•˜ê¸° ìœ„í•œ ì„¤ì • ê°ì²´\n",
        "\n",
        "**ì£¼ìš” íŠ¹ì§•**:\n",
        "- ì²´ì¸, íˆ´, ëª¨ë¸ ë“± ë‹¤ì–‘í•œ Runnableì— ì „ë‹¬ë˜ì–´ ì‹¤í–‰ ì‹œ ë™ì‘ì„ ì¡°ì •\n",
        "- ì‹¤í–‰ ì¤‘ì¸ Runnableê³¼ í•˜ìœ„ í˜¸ì¶œë“¤ì— ì„¤ì •ì„ ì „ë‹¬í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ì—­í• \n",
        "- LangChainì˜ ëª¨ë“  Runnableì€ ì´ ì„¤ì •ì„ ë°›ì•„ ì‹¤í–‰ì„ ìµœì í™”í•˜ê±°ë‚˜ ë™ì‘ ë³€ê²½ ê°€ëŠ¥\n",
        "\n",
        "**ì£¼ìš” ì†ì„±**:\n",
        "\n",
        "1. **configurable**\n",
        "   - **ìš©ë„**: ëŸ°íƒ€ì„ì— ì¡°ì • ê°€ëŠ¥í•œ ì†ì„± ê°’ ì „ë‹¬\n",
        "   - **ì˜ˆì‹œ**: ëª¨ë¸ì˜ ì˜¨ë„, ì„¸ì…˜ ID, í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë“±\n",
        "\n",
        "2. **callbacks**\n",
        "   - **ìš©ë„**: ì‹¤í–‰ ê³¼ì •ì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ì½œë°± í•¸ë“¤ëŸ¬ ì§€ì •\n",
        "   - **ì˜ˆì‹œ**: ë¡œê¹…, ëª¨ë‹ˆí„°ë§, ì´ë²¤íŠ¸ ì¶”ì \n",
        "\n",
        "3. **tags**\n",
        "   - **ìš©ë„**: ì‹¤í–‰ì— íƒœê·¸ë¥¼ ë¶™ì—¬ ì¶”ì  ë° í•„í„°ë§\n",
        "   - **ì˜ˆì‹œ**: ì‹¤í—˜ ë²„ì „, ì‚¬ìš©ì ê·¸ë£¹, ìš”ì²­ íƒ€ì… ë“±\n",
        "\n",
        "4. **metadata**\n",
        "   - **ìš©ë„**: ì‹¤í–‰ ê´€ë ¨ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì „ë‹¬\n",
        "   - **ì˜ˆì‹œ**: ìš”ì²­ ID, ì‚¬ìš©ì ì •ë³´, ì„¸ì…˜ ë°ì´í„° ë“±\n",
        "\n",
        "- [LangChain RunnableConfig ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/docs/concepts/runnables/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "15ac3ddb",
      "metadata": {
        "id": "15ac3ddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "true\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Langsmith tracing ì—¬ë¶€ë¥¼ í™•ì¸ (true: langsmith ì¶”ì  í™œì„±í™”, false: langsmith ì¶”ì  ë¹„í™œì„±í™”)\n",
        "import os\n",
        "print(os.getenv('LANGSMITH_TRACING'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d26956",
      "metadata": {},
      "source": [
        "### 1. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì½œë°± í•¸ë“¤ëŸ¬ êµ¬í˜„\n",
        "\n",
        "- LLM í˜¸ì¶œì˜ ì„±ëŠ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°± í•¸ë“¤ëŸ¬ êµ¬í˜„\n",
        "- `BaseCallbackHandler`ë¥¼ ìƒì†í•˜ì—¬ êµ¬í˜„\n",
        "- `on_llm_start`, `on_llm_end`, `on_llm_error` ë©”ì„œë“œë¥¼ ì˜¤ë²„ë¼ì´ë“œ\n",
        "- ì‹¤í–‰ ì‹œê°„, í† í° ì‚¬ìš©ëŸ‰, í˜¸ì¶œ íšŸìˆ˜ ë“±ì˜ ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f98e0662",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional\n",
        "from langchain_core.callbacks import BaseCallbackHandler\n",
        "from langchain_core.outputs import LLMResult\n",
        "\n",
        "# ë¡œê¹… ì„¤ì •\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "class PerformanceMonitoringCallback(BaseCallbackHandler):\n",
        "    \"\"\"LLM í˜¸ì¶œ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°± í•¸ë“¤ëŸ¬\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.start_time: Optional[float] = None       # LLM í˜¸ì¶œ ì‹œì‘ ì‹œê°„\n",
        "        self.token_usage: Dict[str, Any] = {}         # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´\n",
        "        self.call_count: int = 0                      # LLM í˜¸ì¶œ íšŸìˆ˜\n",
        "        \n",
        "    def on_llm_start(\n",
        "        self, \n",
        "        serialized: Dict[str, Any], \n",
        "        prompts: List[str], \n",
        "        **kwargs: Any\n",
        "    ) -> None:\n",
        "        \"\"\"LLM í˜¸ì¶œì´ ì‹œì‘ë  ë•Œ í˜¸ì¶œ\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.call_count += 1\n",
        "        print(f\"ğŸš€ LLM í˜¸ì¶œ #{self.call_count} ì‹œì‘ - {datetime.now().strftime('%H:%M:%S')}\")\n",
        "        \n",
        "        # ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ì˜ ê¸¸ì´ í™•ì¸\n",
        "        if prompts:\n",
        "            print(f\"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompts[0])} ë¬¸ì\")\n",
        "        \n",
        "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
        "        \"\"\"LLM í˜¸ì¶œì´ ì™„ë£Œë  ë•Œ í˜¸ì¶œ\"\"\"\n",
        "        if self.start_time:\n",
        "            duration = time.time() - self.start_time\n",
        "            print(f\"âœ… LLM í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ\")\n",
        "            \n",
        "            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì \n",
        "            if response.generations:\n",
        "                generation = response.generations[0][0]\n",
        "            \n",
        "                usage = response.llm_output.get('token_usage', {})\n",
        "                if usage:\n",
        "                    print(f\"ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰: {usage}\")\n",
        "                    self.token_usage = usage\n",
        "                        \n",
        "                # ì‘ë‹µ ê¸¸ì´ ì²´í¬\n",
        "                if hasattr(generation, 'text'):\n",
        "                    response_text = generation.text\n",
        "                    print(f\"ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(response_text)} ë¬¸ì\")\n",
        "        \n",
        "    def on_llm_error(self, error: Exception, **kwargs: Any) -> None:\n",
        "        \"\"\"LLM í˜¸ì¶œì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ë•Œ í˜¸ì¶œ\"\"\"\n",
        "        print(f\"âŒ LLM í˜¸ì¶œ ì˜¤ë¥˜: {str(error)}\")\n",
        "        \n",
        "    def get_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"í˜„ì¬ê¹Œì§€ì˜ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜\"\"\"\n",
        "        return {\n",
        "            \"total_calls\": self.call_count,\n",
        "            \"last_token_usage\": self.token_usage\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70bcc70d",
      "metadata": {},
      "source": [
        "### 2. ì‹¤ì‹œê°„ ì•Œë¦¼ ì½œë°± í•¸ë“¤ëŸ¬ êµ¬í˜„\n",
        "\n",
        "- íŠ¹ì • ì¡°ê±´(ë¹„ìš© ì„ê³„ê°’, ì‘ë‹µ ì‹œê°„ ë“±)ì—ì„œ ì•Œë¦¼ì„ ë³´ë‚´ëŠ” ì½œë°± í•¸ë“¤ëŸ¬ë¥¼ êµ¬í˜„ (ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬í˜„)\n",
        "- ë¹„ìš©, ì‘ë‹µ ì‹œê°„, í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ë“± ë‹¤ì–‘í•œ ì¡°ê±´ ëª¨ë‹ˆí„°ë§\n",
        "- ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ì™¸ë¶€ ì•Œë¦¼ ì„œë¹„ìŠ¤ ì—°ë™ ê°€ëŠ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4002a421",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AlertCallback(BaseCallbackHandler):\n",
        "    \"\"\"íŠ¹ì • ì¡°ê±´ì—ì„œ ì•Œë¦¼ì„ ë³´ë‚´ëŠ” ì½œë°± í•¸ë“¤ëŸ¬\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        cost_threshold: float = 1.0,            # ë¹„ìš© ì„ê³„ê°’ (ë‹¬ëŸ¬ ë‹¨ìœ„)\n",
        "        response_time_threshold: float = 10.0,  # ì‘ë‹µ ì‹œê°„ ì„ê³„ê°’ (ì´ˆ ë‹¨ìœ„)\n",
        "        token_threshold: int = 4000             # ê¸´ í”„ë¡¬í”„íŠ¸ í† í° ì„ê³„ê°’\n",
        "    ):\n",
        "        self.cost_threshold = cost_threshold\n",
        "        self.response_time_threshold = response_time_threshold\n",
        "        self.token_threshold = token_threshold\n",
        "        self.start_time: Optional[float] = None    # LLM í˜¸ì¶œ ì‹œì‘ ì‹œê°„\n",
        "        self.cumulative_cost: float = 0.0          # ëˆ„ì  ë¹„ìš© ì¶”ì \n",
        "        \n",
        "    def on_llm_start(\n",
        "        self, \n",
        "        serialized: Dict[str, Any], \n",
        "        prompts: List[str], \n",
        "        **kwargs: Any\n",
        "    ) -> None:\n",
        "        \"\"\"LLM í˜¸ì¶œì´ ì‹œì‘ë  ë•Œ í˜¸ì¶œ\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        \n",
        "        # ê¸´ í”„ë¡¬í”„íŠ¸ ê²½ê³ \n",
        "        if prompts and len(prompts[0]) > self.token_threshold:\n",
        "            self._send_alert(f\"âš ï¸ ê¸´ í”„ë¡¬í”„íŠ¸ ê°ì§€: {len(prompts[0])} ë¬¸ì\")\n",
        "    \n",
        "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
        "        \"\"\"LLM í˜¸ì¶œì´ ì™„ë£Œë  ë•Œ í˜¸ì¶œ\"\"\"\n",
        "        # ì‘ë‹µ ì‹œê°„ ì²´í¬\n",
        "        if self.start_time:\n",
        "            duration = time.time() - self.start_time\n",
        "            if duration > self.response_time_threshold:  # ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼\n",
        "                self._send_alert(f\"ğŸŒ ëŠë¦° ì‘ë‹µ: {duration:.2f}ì´ˆ\")\n",
        "        \n",
        "        # ë¹„ìš© ì²´í¬\n",
        "        if response.generations:\n",
        "            generation = response.generations[0][0]\n",
        "            usage = response.llm_output.get('token_usage', {})\n",
        "            \n",
        "            if usage:\n",
        "                # ê°„ë‹¨í•œ ë¹„ìš© ê³„ì‚° (ì‹¤ì œë¡œëŠ” ëª¨ë¸ë³„ ê°€ê²© ì ìš© í•„ìš”)\n",
        "                total_tokens = usage.get('total_tokens', 0)\n",
        "                if total_tokens == 0:\n",
        "                    # input_tokensì™€ output_tokensë¡œ ê³„ì‚°\n",
        "                    total_tokens = usage.get('input_tokens', 0) + usage.get('output_tokens', 0)\n",
        "                \n",
        "                estimated_cost = (total_tokens / 1000) * 0.002\n",
        "                self.cumulative_cost += estimated_cost\n",
        "                \n",
        "                if self.cumulative_cost > self.cost_threshold:\n",
        "                    self._send_alert(f\"ğŸ’¸ ë¹„ìš© ì„ê³„ê°’ ì´ˆê³¼: ${self.cumulative_cost:.4f}\")\n",
        "    \n",
        "    def on_llm_error(self, error: Exception, **kwargs: Any) -> None:\n",
        "        \"\"\"LLM í˜¸ì¶œì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ë•Œ í˜¸ì¶œ\"\"\"\n",
        "        self._send_alert(f\"ğŸš¨ LLM ì˜¤ë¥˜ ë°œìƒ: {str(error)}\")\n",
        "    \n",
        "    def _send_alert(self, message: str) -> None:\n",
        "        \"\"\"ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Slack, Discord, ì´ë©”ì¼ ë“±ìœ¼ë¡œ ì•Œë¦¼ì„ ë³´ëƒ„\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        alert_message = f\"[ALERT] {timestamp} - {message}\"\n",
        "        print(\"ğŸ”” ì•Œë¦¼ ì „ì†¡:\", alert_message)\n",
        "        \n",
        "        # ë¡œê¹… ì‹œìŠ¤í…œì— ê¸°ë¡\n",
        "        logging.warning(alert_message)\n",
        "        \n",
        "        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì™¸ë¶€ ì„œë¹„ìŠ¤ì— ì•Œë¦¼ ì „ì†¡\n",
        "        # self._send_slack_notification(message)\n",
        "        # self._send_email_notification(message)\n",
        "    \n",
        "    def reset_cost_tracking(self) -> None:\n",
        "        \"\"\"ëˆ„ì  ë¹„ìš© ì¶”ì ì„ ë¦¬ì…‹\"\"\"\n",
        "        self.cumulative_cost = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f72499f",
      "metadata": {},
      "source": [
        "### 3. RunnableConfig ê¸°ë³¸ ì‚¬ìš©\n",
        "\n",
        "- RunnableConfigë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ ì‹¤í–‰ì„ ì œì–´í•˜ê³  ëª¨ë‹ˆí„°ë§\n",
        "- `configurable_fields`ë¥¼ í†µí•œ ëŸ°íƒ€ì„ ì„¤ì • ê°€ëŠ¥í•œ ëª¨ë¸ ìƒì„±\n",
        "- RunnableConfigì˜ 4ê°€ì§€ ì£¼ìš” ì†ì„± í™œìš©\n",
        "- ì½œë°± í•¸ë“¤ëŸ¬ë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a3180b2a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "h:\\miniconda3\\envs\\modu2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ LLM í˜¸ì¶œ #1 ì‹œì‘ - 17:34:47\n",
            "ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: 59 ë¬¸ì\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:48,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LLM í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: 0.62ì´ˆ\n",
            "ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰: {'completion_tokens': 8, 'prompt_tokens': 38, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}\n",
            "ğŸ“Š ì‘ë‹µ ê¸¸ì´: 32 ë¬¸ì\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "# ì„¤ì • ê°€ëŠ¥í•œ ëª¨ë¸ ìƒì„± - temperatureë¥¼ ëŸ°íƒ€ì„ì— ë³€ê²½ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\",  \n",
        "    temperature=0.3,      # ê¸°ë³¸ê°’\n",
        "    top_p=0.95\n",
        ").configurable_fields(    # temperature í•„ë“œë¥¼ ëŸ°íƒ€ì„ì— ë³€ê²½ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •\n",
        "    temperature=ConfigurableField(\n",
        "        id=\"temperature\",\n",
        "        name=\"Model Temperature\",\n",
        "        description=\"ëª¨ë¸ì˜ ì°½ì˜ì„±ì„ ì¡°ì ˆí•˜ëŠ” ì˜¨ë„ ë§¤ê°œë³€ìˆ˜\"\n",
        "    )\n",
        ")\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"'{text}'ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ëœ ë¬¸ì¥ë§Œì„ ì¶œë ¥í•´ì£¼ì„¸ìš”.\")\n",
        "output_parser = StrOutputParser()\n",
        "translation_chain = prompt | model | output_parser\n",
        "\n",
        "# ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±\n",
        "performance_handler = PerformanceMonitoringCallback()\n",
        "\n",
        "# ì²´ì¸ ì‹¤í–‰\n",
        "result = translation_chain.invoke(\n",
        "    {\n",
        "        \"text\": \"ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë– ì‹ ê°€ìš”?\"\n",
        "    },\n",
        "    config={\n",
        "        \"configurable\": {\"temperature\": 0.7, \"session_id\": \"user123\"},   # ëŸ°íƒ€ì„ì— temperatureë¥¼ ë³€ê²½ ê°€ëŠ¥\n",
        "        \"callbacks\": [performance_handler],\n",
        "        \"tags\": [\"experiment_v1\", \"production\"],\n",
        "        \"metadata\": {\"request_id\": \"req_001\", \"user_id\": \"user123\"},\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d4c00c73",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë²ˆì—­ ê²°ê³¼: Hello, how is the weather today?\n"
          ]
        }
      ],
      "source": [
        "print(f\"ë²ˆì—­ ê²°ê³¼: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34a3f5fd",
      "metadata": {},
      "source": [
        "### 4. ì•Œë¦¼ ì¡°ê±´ í…ŒìŠ¤íŠ¸\n",
        "\n",
        "- AlertCallbackì˜ ë‹¤ì–‘í•œ ì•Œë¦¼ ì¡°ê±´ì„ í…ŒìŠ¤íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "89636754",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:48,192 - root - WARNING - [ALERT] 2025-09-27 17:34:48 - âš ï¸ ê¸´ í”„ë¡¬í”„íŠ¸ ê°ì§€: 51 ë¬¸ì\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ LLM í˜¸ì¶œ #2 ì‹œì‘ - 17:34:48\n",
            "ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: 51 ë¬¸ì\n",
            "ğŸ”” ì•Œë¦¼ ì „ì†¡: [ALERT] 2025-09-27 17:34:48 - âš ï¸ ê¸´ í”„ë¡¬í”„íŠ¸ ê°ì§€: 51 ë¬¸ì\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:48,703 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-09-27 17:34:48,712 - root - WARNING - [ALERT] 2025-09-27 17:34:48 - ğŸŒ ëŠë¦° ì‘ë‹µ: 0.52ì´ˆ\n",
            "2025-09-27 17:34:48,713 - root - WARNING - [ALERT] 2025-09-27 17:34:48 - ğŸ’¸ ë¹„ìš© ì„ê³„ê°’ ì´ˆê³¼: $0.0001\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LLM í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: 0.52ì´ˆ\n",
            "ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰: {'completion_tokens': 3, 'prompt_tokens': 28, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}\n",
            "ğŸ“Š ì‘ë‹µ ê¸¸ì´: 12 ë¬¸ì\n",
            "ğŸ”” ì•Œë¦¼ ì „ì†¡: [ALERT] 2025-09-27 17:34:48 - ğŸŒ ëŠë¦° ì‘ë‹µ: 0.52ì´ˆ\n",
            "ğŸ”” ì•Œë¦¼ ì „ì†¡: [ALERT] 2025-09-27 17:34:48 - ğŸ’¸ ë¹„ìš© ì„ê³„ê°’ ì´ˆê³¼: $0.0001\n"
          ]
        }
      ],
      "source": [
        "# ì¡°ê±´ì„ ì™„í™”í•´ì„œ í…ŒìŠ¤íŠ¸\n",
        "alert_handler_test = AlertCallback(\n",
        "    cost_threshold=0.0,  # $0.0 ì„ê³„ê°’ -> ëª¨ë“  í˜¸ì¶œì— ëŒ€í•´ ì•Œë¦¼\n",
        "    response_time_threshold=0.1,  # 0.1ì´ˆ ì„ê³„ê°’ -> ëŒ€ë¶€ë¶„ì˜ í˜¸ì¶œì—ì„œ ì•Œë¦¼\n",
        "    token_threshold=10  # 10 ë¬¸ì ì„ê³„ê°’ -> ì§§ì€ í”„ë¡¬í”„íŠ¸ë„ ì•Œë¦¼\n",
        ")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
        "test_result = translation_chain.invoke(\n",
        "    {\n",
        "        \"text\": \"Hello World!\"  # ì§§ì€ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸\n",
        "    },\n",
        "    config={\n",
        "        \"configurable\": {\"temperature\": 0.1},\n",
        "        \"callbacks\": [performance_handler, alert_handler_test],  # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ ì½œë°± í•¸ë“¤ëŸ¬ ëª¨ë‘ ì‚¬ìš©\n",
        "        \"tags\": [\"test\", \"alert_validation\"],\n",
        "        \"metadata\": {\"request_id\": \"test_001\", \"test_type\": \"alert_threshold\"},\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "505d57ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í…ŒìŠ¤íŠ¸ ê²°ê³¼: Hello World!\n"
          ]
        }
      ],
      "source": [
        "print(f\"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {test_result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e58c4894",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì´ í˜¸ì¶œ íšŸìˆ˜: 2\n",
            "ë§ˆì§€ë§‰ í† í° ì‚¬ìš©ëŸ‰: {'completion_tokens': 3, 'prompt_tokens': 28, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}\n"
          ]
        }
      ],
      "source": [
        "# í†µê³„ ì •ë³´ í™•ì¸\n",
        "stats = performance_handler.get_statistics()\n",
        "print(f\"ì´ í˜¸ì¶œ íšŸìˆ˜: {stats['total_calls']}\")\n",
        "print(f\"ë§ˆì§€ë§‰ í† í° ì‚¬ìš©ëŸ‰: {stats['last_token_usage']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf8d1ff0",
      "metadata": {},
      "source": [
        "**[ì°¸ê³ ]** [LangSmith Alert ì„¤ì •](https://docs.smith.langchain.com/observability/how_to_guides/alerts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87732731",
      "metadata": {},
      "source": [
        "### 5. ê¸°íƒ€ í™œìš©ë²•"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1af3dac",
      "metadata": {},
      "source": [
        "`(1) ë™ì  ëª¨ë¸ ì„ íƒ`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8e907d8d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í˜„ì¬ ëª¨ë¸ ì´ë¦„: gpt-4.1-mini\n"
          ]
        }
      ],
      "source": [
        "# model ë³€ìˆ˜ì˜ ëª¨ë¸ ì´ë¦„ ì¶œë ¥\n",
        "print(\"í˜„ì¬ ëª¨ë¸ ì´ë¦„:\", model.model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0ee2b116",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:50,485 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë²ˆì—­ ê²°ê³¼: Hello, how is the weather today?\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# ì„¤ì • ê°€ëŠ¥í•œ ëª¨ë¸ ìƒì„± (ì—¬ëŸ¬ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •)\n",
        "model_alternatives = model.configurable_alternatives(\n",
        "    ConfigurableField(id=\"model_name\"),\n",
        "    default_key=\"gpt4_mini\",  # model ë³€ìˆ˜ì˜ ê¸°ë³¸ ëª¨ë¸ì„ ì‹ë³„í•˜ëŠ” í‚¤\n",
        "    openai_gpt4=ChatOpenAI(model=\"gpt-4.1\"),  # OpenAI GPT-4 ëª¨ë¸\n",
        "    google_gemini=ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\"),  # Google Gemini ëª¨ë¸\n",
        ")\n",
        "\n",
        "# chain ì„¤ì • \n",
        "translation_chain = prompt | model_alternatives | output_parser\n",
        "\n",
        "# ê¸°ë³¸ ëª¨ë¸ë¡œ ë²ˆì—­ ì‹¤í–‰ (default_key: gpt4_mini)\n",
        "result = translation_chain.invoke(\n",
        "    {\"text\": \"ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë– ì‹ ê°€ìš”?\"}\n",
        ")\n",
        "\n",
        "print(\"ë²ˆì—­ ê²°ê³¼:\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a0d7be5f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë²ˆì—­ ê²°ê³¼: Hello, how is the weather today?\n"
          ]
        }
      ],
      "source": [
        "# ëŸ°íƒ€ì„ì— ëª¨ë¸ ë³€ê²½ (google_gemini í‚¤ ì‚¬ìš© -> gemini-2.0-flash ëª¨ë¸ ì‚¬ìš©)\n",
        "result = translation_chain.invoke(\n",
        "    {\"text\": \"ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë– ì‹ ê°€ìš”?\"},\n",
        "    config={\"configurable\": {\"model_name\": \"google_gemini\"}} \n",
        ")\n",
        "\n",
        "print(\"ë²ˆì—­ ê²°ê³¼:\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "91402339",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:51,943 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, how is the weather today?\n"
          ]
        }
      ],
      "source": [
        "# ëŸ°íƒ€ì„ì— ëª¨ë¸ ë³€ê²½  (openai_gpt4 í‚¤ ì‚¬ìš© -> gpt-4.1 ëª¨ë¸ ì‚¬ìš©)\n",
        "result = translation_chain.invoke(\n",
        "    {\"text\": \"ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë– ì‹ ê°€ìš”?\"},\n",
        "    config={\"configurable\": {\"model_name\": \"openai_gpt4\"}}\n",
        ")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c80c203",
      "metadata": {},
      "source": [
        "`(2) ë™ì  í”„ë¡¬í”„íŠ¸ ì„ íƒ`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e59f6a60",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# ì—¬ëŸ¬ ëª¨ë¸ì„ alternativesë¡œ ì„¤ì •\n",
        "model = ChatOpenAI(model=\"gpt-4.1-mini\").configurable_alternatives(\n",
        "    ConfigurableField(id=\"model_provider\"),\n",
        "    default_key=\"gpt4_mini\",\n",
        "    openai_gpt4=ChatOpenAI(model=\"gpt-4.1\"),\n",
        "    google_gemini=ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        ").configurable_fields(\n",
        "    temperature=ConfigurableField(\n",
        "        id=\"temperature\",\n",
        "        name=\"Temperature\",\n",
        "        description=\"Model temperature for response randomness\"\n",
        "    ),\n",
        "    max_tokens=ConfigurableField(\n",
        "        id=\"max_tokens\",\n",
        "        name=\"Max Tokens\", \n",
        "        description=\"Maximum tokens to generate\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸\n",
        "default_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ëª¨ë“  ì§ˆë¬¸ì— ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•˜ì„¸ìš”.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# ì°½ì˜ì  í”„ë¡¬í”„íŠ¸\n",
        "creative_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"ë‹¹ì‹ ì€ ì°½ì˜ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ìƒìƒë ¥ì„ ë°œíœ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# ë¶„ì„ì  í”„ë¡¬í”„íŠ¸\n",
        "analytical_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"ë‹¹ì‹ ì€ ë¶„ì„ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë…¼ë¦¬ì ìœ¼ë¡œ ì‚¬ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ë¥¼ configurable alternativesë¡œ ì„¤ì •\n",
        "prompt = default_prompt.configurable_alternatives(\n",
        "    ConfigurableField(id=\"prompt_type\"),\n",
        "    default_key=\"default\",\n",
        "    creative=creative_prompt,\n",
        "    analytical=analytical_prompt\n",
        ")\n",
        "\n",
        "# chain ì„¤ì •\n",
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "db79aa51",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:34:58,367 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ê²°ê³¼: ì¸ê³µì§€ëŠ¥(äººå·¥çŸ¥èƒ½, Artificial Intelligence, AI)ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ê¸°ê³„ë‚˜ ì»´í“¨í„° ì‹œìŠ¤í…œì— êµ¬í˜„í•˜ëŠ” ê¸°ìˆ ê³¼ í•™ë¬¸ ë¶„ì•¼ë¥¼ ë§í•©ë‹ˆë‹¤. AIëŠ” ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  í•™ìŠµí•˜ë©°, ë¬¸ì œë¥¼ í•´ê²°í•˜ê±°ë‚˜ ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•˜ëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ë„ë¡ ì„¤ê³„ë©ë‹ˆë‹¤.\n",
            "\n",
            "### ì¸ê³µì§€ëŠ¥ì˜ ì£¼ìš” ê°œë…\n",
            "1. **ê¸°ê³„ í•™ìŠµ(Machine Learning)**  \n",
            "   ì»´í“¨í„°ê°€ ëª…ì‹œì ì¸ í”„ë¡œê·¸ë˜ë° ì—†ì´ ë°ì´í„°ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ëŒ€í‘œì ì¸ ì˜ˆë¡œëŠ” ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµ ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "2. **ë”¥ëŸ¬ë‹(Deep Learning)**  \n",
            "   ì¸ê³µì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê¸°ê³„í•™ìŠµì˜ í•œ ë¶„ì•¼ë¡œ, ë‹¤ì¸µ ì‹ ê²½ë§ì„ í†µí•´ ë³µì¡í•œ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ì¸ì‹, ìŒì„± ì¸ì‹, ìì—°ì–´ ì²˜ë¦¬ ë“±ì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
            "\n",
            "3. **ìì—°ì–´ ì²˜ë¦¬(Natural Language Processing, NLP)**  \n",
            "   ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ë²ˆì—­, ì±—ë´‡, ìŒì„± ì¸ì‹ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\n",
            "\n",
            "4. **ì»´í“¨í„° ë¹„ì „(Computer Vision)**  \n",
            "   ì´ë¯¸ì§€ë‚˜ ë™ì˜ìƒì—ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¸°ìˆ ë¡œ, ì–¼êµ´ ì¸ì‹, ê°ì²´ íƒì§€ ë“±ì— í™œìš©ë©ë‹ˆë‹¤.\n",
            "\n",
            "### ì¸ê³µì§€ëŠ¥ì˜ í™œìš© ë¶„ì•¼\n",
            "- **ì˜ë£Œ:** ì§ˆë³‘ ì§„ë‹¨, ì‹ ì•½ ê°œë°œ, ì˜ë£Œ ì˜ìƒ ë¶„ì„\n",
            "- **ìë™ì°¨:** ììœ¨ì£¼í–‰ì°¨, êµí†µ ê´€ë¦¬\n",
            "- **ê¸ˆìœµ:** ì‹ ìš© í‰ê°€, íˆ¬ì ë¶„ì„, ì‚¬ê¸° íƒì§€\n",
            "- **ê³ ê° ì„œë¹„ìŠ¤:** ì±—ë´‡, ìŒì„± ë¹„ì„œ\n",
            "- **ì œì¡°:** ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬, í’ˆì§ˆ ê²€ì‚¬\n",
            "\n",
            "### ì¸ê³µì§€ëŠ¥ì˜ ì¥ì ê³¼ í•œê³„\n",
            "- **ì¥ì :** ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬, ì¸ê°„ì´ í•˜ê¸° ì–´ë ¤ìš´ ë³µì¡í•œ ë¬¸ì œ í•´ê²°, ì—…ë¬´ ìë™í™”\n",
            "- **í•œê³„:** ë°ì´í„° í¸í–¥ ë¬¸ì œ, ìœ¤ë¦¬ì  ì´ìŠˆ, ì™„ì „í•œ ì¸ê°„ ì§€ëŠ¥ ëŒ€ì²´ ë¶ˆê°€\n",
            "\n",
            "í•„ìš”í•˜ì‹œë©´ íŠ¹ì • ë¶„ì•¼ë‚˜ ê¸°ìˆ ì— ëŒ€í•´ ë” ìì„¸íˆ ì„¤ëª…í•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ì‹¤í–‰\n",
        "basic_config = {\n",
        "    \"configurable\": {\n",
        "        \"model_provider\": \"gpt4_mini\",\n",
        "        \"temperature\": 0.5,\n",
        "        \"max_tokens\": 500,\n",
        "        \"prompt_type\": \"default\"\n",
        "    }\n",
        "}\n",
        "\n",
        "result1 = chain.invoke(\n",
        "    {\"user_input\": \"ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”\"}, \n",
        "    config=basic_config\n",
        ")\n",
        "\n",
        "print(f\"ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ê²°ê³¼: {result1.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1a7c0243",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì°½ì˜ì  í”„ë¡¬í”„íŠ¸ ê²°ê³¼: ì, ì¸ê³µì§€ëŠ¥ì— ëŒ€í•œ ì´ì•¼ê¸°ëŠ” ë†€ë¼ìš´ ëª¨í—˜ê³¼ ê°™ìŠµë‹ˆë‹¤! ë§ˆì¹˜ ìš°ë¦¬ê°€ ìƒìƒë ¥ìœ¼ë¡œ ê°€ë“ ì°¬ ë¡œë´‡ ì¹œêµ¬ë¥¼ ë§Œë“œëŠ” ê²ƒê³¼ ê°™ì•„ìš”.\n",
            "\n",
            "ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ê°€ ë§ˆì¹˜ ì‚¬ëŒì²˜ëŸ¼ ìƒê°í•˜ê³  ë°°ìš¸ ìˆ˜ ìˆë„ë¡ ë§Œë“œëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. AIëŠ” ê±°ëŒ€í•œ ë‘ë‡Œì™€ ê°™ì•„ì„œ, ë§ì€ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ê³  ìƒˆë¡œìš´ ê²ƒì„ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "AIëŠ” ìš°ë¦¬ ìƒí™œ ê³³ê³³ì— ìˆ¨ì–´ ìˆì–´ìš”. ì˜ˆë¥¼ ë“¤ì–´, ìŠ¤ë§ˆíŠ¸í°ì—ì„œ \"ì•ˆë…•, AI\"ë¼ê³  ë§í•˜ë©´ AIê°€ ìš°ë¦¬ì˜ ë§ì„ ë“£ê³  ëŒ€ë‹µí•´ ì¤ë‹ˆë‹¤. ë˜, ì˜í™”ë¥¼ ì¶”ì²œí•´ ì£¼ëŠ” AIëŠ” ìš°ë¦¬ê°€ ì¢‹ì•„í•  ë§Œí•œ ì˜í™”ë¥¼ ì²™ì²™ ê³¨ë¼ì£¼ì£ . ìš´ì „í•˜ëŠ” ë¡œë´‡ ìë™ì°¨ë„ ìˆê³ , ê·¸ë¦¼ì„ ê·¸ë¦¬ëŠ” AI í™”ê°€ë„ ìˆë‹µë‹ˆë‹¤!\n",
            "\n",
            "AIëŠ” ë°°ìš°ëŠ” ê²ƒì„ ì•„ì£¼ ì¢‹ì•„í•©ë‹ˆë‹¤. ë§ˆì¹˜ ì–´ë¦° ì•„ì´ê°€ ì„¸ìƒì„ íƒí—˜í•˜ë©° ë°°ìš°ë“¯ì´, AIëŠ” ìˆ˜ë§ì€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  íŒ¨í„´ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ê·¸ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ê²ƒì„ ë§Œë“¤ì–´ë‚´ê¸°ë„ í•©ë‹ˆë‹¤.\n",
            "\n",
            "í•˜ì§€ë§Œ AIëŠ” ì•„ì§ ì™„ë²½í•˜ì§€ ì•Šì•„ìš”. ë•Œë¡œëŠ” ì—‰ëš±í•œ ëŒ€ë‹µì„ í•˜ê±°ë‚˜ ì‹¤ìˆ˜ë¥¼ í•˜ê¸°ë„ í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” AIë¥¼ ê³„ì† ê°€ë¥´ì¹˜ê³  í›ˆë ¨ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.\n",
            "\n",
            "AIì˜ ë¯¸ë˜ëŠ” ì •ë§ í¥ë¯¸ì§„ì§„í•©ë‹ˆë‹¤. AIëŠ” ìš°ë¦¬ì˜ ì‚¶ì„ ë”ìš± í¸ë¦¬í•˜ê³  í’ìš”ë¡­ê²Œ ë§Œë“¤ì–´ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë™ì‹œì— AIê°€ ìš°ë¦¬ì—ê²Œ ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹ ì§€ ê³ ë¯¼í•˜ê³ , AIë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œì•¼ í•©ë‹ˆë‹¤.\n",
            "\n",
            "ì, ì´ì œ AIì˜ ì„¸ê³„ë¡œ í•¨ê»˜ ë– ë‚˜ë³¼ê¹Œìš”? ì–´ë–¤ ë†€ë¼ìš´ ì¼ë“¤ì´ ìš°ë¦¬ë¥¼ ê¸°ë‹¤ë¦¬ê³  ìˆì„ì§€ ê¸°ëŒ€ë˜ì§€ ì•Šë‚˜ìš”!\n"
          ]
        }
      ],
      "source": [
        "# ì°½ì˜ì  í”„ë¡¬í”„íŠ¸ë¡œ ì‹¤í–‰\n",
        "creative_config = {\n",
        "    \"configurable\": {\n",
        "        \"model_provider\": \"google_gemini\",\n",
        "        \"temperature\": 0.9,\n",
        "        \"max_tokens\": 700,\n",
        "        \"prompt_type\": \"creative\"\n",
        "    }\n",
        "}\n",
        "\n",
        "result2 = chain.invoke(\n",
        "    {\"user_input\": \"ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”\"}, \n",
        "    config=creative_config\n",
        ")\n",
        "print(f\"ì°½ì˜ì  í”„ë¡¬í”„íŠ¸ ê²°ê³¼: {result2.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "130a549b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## **LangChain Fallback ì²˜ë¦¬**\n",
        "- **Fallback**ì€ ë¹„ìƒ ìƒí™©ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëŒ€ì•ˆì  ê³„íšì„ ì˜ë¯¸\n",
        "- LangChainì—ì„œëŠ” ì£¼ìš” ì‹¤í–‰ ê²½ë¡œê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ ìë™ìœ¼ë¡œ ëŒ€ì²´ ê²½ë¡œë¥¼ ì‹¤í–‰í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ì œê³µ\n",
        "\n",
        "**í•„ìš”ì„±**:\n",
        "\n",
        "1. **API ì•ˆì •ì„± ë¬¸ì œ**\n",
        "    - ìš”ê¸ˆ ì œí•œ(Rate Limiting)\n",
        "    - ì„œë²„ ë‹¤ìš´íƒ€ì„\n",
        "    - ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜\n",
        "    - API í‚¤ í• ë‹¹ëŸ‰ ì´ˆê³¼\n",
        "\n",
        "2. **ë¹„ìš© ìµœì í™”**\n",
        "    - ì €ë ´í•œ ëª¨ë¸ì„ ë¨¼ì € ì‹œë„\n",
        "    - ì‹¤íŒ¨ ì‹œì—ë§Œ ë¹„ì‹¼ ëª¨ë¸ ì‚¬ìš©\n",
        "\n",
        "3. **ì„±ëŠ¥ ìµœì í™”**\n",
        "   - ë¹ ë¥¸ ëª¨ë¸ì„ ìš°ì„  ì‚¬ìš©\n",
        "    - ë³µì¡í•œ ì‘ì—…ì—ë§Œ ê³ ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©\n",
        "\n",
        "4. **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ**\n",
        "   - ì§§ì€ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ì„ ë¨¼ì € ì‹œë„\n",
        "   - í† í° ì´ˆê³¼ ì‹œ ê¸´ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš©\n",
        "\n",
        "- [LangChain Fallbacks ê°€ì´ë“œ](https://python.langchain.com/docs/how_to/fallbacks/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2b220a7",
      "metadata": {},
      "source": [
        "### 1. API ì˜¤ë¥˜ì— ëŒ€í•œ Fallback ì²˜ë¦¬\n",
        "\n",
        "- OpenAI API ì˜¤ë¥˜ ë°œìƒ ì‹œ Google Geminië¡œ ìë™ ì „í™˜í•˜ëŠ” ì‹œìŠ¤í…œ êµ¬í˜„\n",
        "- `max_retries=0` ì„¤ì •ìœ¼ë¡œ ì¦‰ì‹œ fallback ì „í™˜\n",
        "- ì—¬ëŸ¬ ë‹¨ê³„ì˜ fallback ì²´ì¸ êµ¬ì„± ê°€ëŠ¥"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7360d40",
      "metadata": {},
      "source": [
        "`(1) ê¸°ë³¸ Fallback êµ¬í˜„`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9c4a59be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê¸°ë³¸ ëª¨ë¸ê³¼ fallback ëª¨ë¸ ì„¤ì •\n",
        "# max_retries=0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¦‰ì‹œ fallbackìœ¼ë¡œ ì „í™˜\n",
        "primary_model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\", \n",
        "    max_retries=0,  # ì¬ì‹œë„ ì—†ì´ ë°”ë¡œ fallbackìœ¼ë¡œ ì „í™˜\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "fallback_model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\", \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Fallback ì²´ì¸ ìƒì„±\n",
        "llm_with_fallback = primary_model.with_fallbacks([fallback_model])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c951fbfa",
      "metadata": {},
      "source": [
        "`(2) í…ŒìŠ¤íŠ¸ìš© ì˜¤ë¥˜ ì‹œë®¬ë ˆì´ì…˜`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a9674893",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Primary ëª¨ë¸ë§Œ ì‚¬ìš© (ì˜¤ë¥˜ ë°œìƒ) ===\n",
            "ì˜¤ë¥˜ ë°œìƒ: Rate limit exceeded\n",
            "\n",
            "=== Fallbackì´ ì ìš©ëœ ëª¨ë¸ ì‚¬ìš© ===\n",
            "ê²°ê³¼: ë‹¬ê³¼ ì§€êµ¬ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ì¼ì •í•˜ì§€ ì•Šê³  íƒ€ì› ê¶¤ë„ë¥¼ ëŒê¸° ë•Œë¬¸ì— ë³€í•©ë‹ˆë‹¤. \n",
            "\n",
            "*   **í‰ê·  ê±°ë¦¬:** ì•½ 384,400km (238,900ë§ˆì¼)\n",
            "\n",
            "*   **ìµœê·¼ì ‘ì  (ê·¼ì§€ì ):** ì•½ 363,104km (225,623ë§ˆì¼)\n",
            "\n",
            "*   **ìµœì›ì  (ì›ì§€ì ):** ì•½ 405,696km (252,088ë§ˆì¼)...\n",
            "âœ… Fallback ëª¨ë¸ë¡œ ìë™ ì „í™˜ ì„±ê³µ!\n"
          ]
        }
      ],
      "source": [
        "from unittest.mock import patch\n",
        "import httpx\n",
        "from openai import RateLimitError\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ìš© ì˜¤ë¥˜ ê°ì²´ ìƒì„±\n",
        "def create_mock_error():\n",
        "    request = httpx.Request(\"GET\", \"/\")\n",
        "    response = httpx.Response(429, request=request)  # 429 = ë„ˆë¬´ ë§ì€ ìš”ì²­\n",
        "    return RateLimitError(\"Rate limit exceeded\", response=response, body=\"\")\n",
        "\n",
        "# ì˜¤ë¥˜ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸\n",
        "error = create_mock_error()\n",
        "\n",
        "print(\"=== Primary ëª¨ë¸ë§Œ ì‚¬ìš© (ì˜¤ë¥˜ ë°œìƒ) ===\")\n",
        "# `patch`ë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI API í˜¸ì¶œì„ Mocking (side_effectë¥¼ errorë¡œ ì„¤ì •)\n",
        "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
        "    try:\n",
        "        result = primary_model.invoke(\"ë‹¬ê³¼ ì§€êµ¬ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ”?\")\n",
        "        print(f\"ê²°ê³¼: {result.content}\")\n",
        "    except RateLimitError as e:\n",
        "        print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\n",
        "print(\"\\n=== Fallbackì´ ì ìš©ëœ ëª¨ë¸ ì‚¬ìš© ===\")\n",
        "# `patch`ë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI API í˜¸ì¶œì„ Mocking (side_effectë¥¼ errorë¡œ ì„¤ì •)\n",
        "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
        "    try:\n",
        "        result = llm_with_fallback.invoke(\"ë‹¬ê³¼ ì§€êµ¬ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ”?\")\n",
        "        print(f\"ê²°ê³¼: {result.content[:200]}...\")\n",
        "        print(\"âœ… Fallback ëª¨ë¸ë¡œ ìë™ ì „í™˜ ì„±ê³µ!\")\n",
        "    except Exception as e:\n",
        "        print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a092c8c6",
      "metadata": {},
      "source": [
        "`(3) Fallback ì²´ì¸ ì˜ˆì‹œ`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "92bfddeb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ì •ìƒ ì‘ë™ í…ŒìŠ¤íŠ¸ ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:35:07,821 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì‘ë‹µ: ë¬¼ë¡ ì…ë‹ˆë‹¤! LangChainì˜ ì£¼ìš” ì¥ì  ì„¸ ê°€ì§€ë¥¼ ë§ì”€ë“œë¦¬ìë©´:\n",
            "\n",
            "1. **ìœ ì—°í•œ ì²´ì¸ êµ¬ì„±**  \n",
            "   LangChainì€ ë‹¤ì–‘í•œ ì‘ì—…ì„ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‰½ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í…ìŠ¤íŠ¸ ìƒì„±, ê²€ìƒ‰, ë°ì´í„° ì¶”ì¶œ ë“±ì˜ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ í•˜ë‚˜ì˜ ì²´ì¸ìœ¼ë¡œ ë§Œë“¤ì–´ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "2. **ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ í†µí•© ì§€ì›**  \n",
            "   ì—¬ëŸ¬ ë°ì´í„°ë² ì´ìŠ¤, API, ë¬¸ì„œ, ê²€ìƒ‰ ì—”ì§„ ë“± ë‹¤ì–‘í•œ ì™¸ë¶€ ì†ŒìŠ¤ì™€ ì‰½ê²Œ ì—°ë™í•  ìˆ˜ ìˆì–´, í’ë¶€í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì§€ëŠ¥í˜• ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì´ ìš©ì´í•©ë‹ˆë‹¤.\n",
            "\n",
            "3. **ì–¸ì–´ ëª¨ë¸ê³¼ì˜ ì›í™œí•œ í†µí•©**  \n",
            "   OpenAI, Hugging Face ë“± ë‹¤ì–‘í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ê³¼ ì†ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆì–´ì„œ ìµœì‹  AI ê¸°ìˆ ì„ ë¹ ë¥´ê²Œ ì ìš©í•˜ê³  ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "í•„ìš”í•˜ì‹œë©´ ë” ìì„¸í•œ ì„¤ëª…ë„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# ì²´ì¸ ìƒì„± í•¨ìˆ˜\n",
        "def create_fallback_chat_chain():\n",
        "    \"\"\"Fallback ì±„íŒ… ì²´ì¸ ìƒì„±\"\"\"\n",
        "\n",
        "    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ ì˜ˆì˜ë°”ë¥´ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\"),\n",
        "        (\"human\", \"{user_input}\")\n",
        "    ])\n",
        "    \n",
        "    # ì—¬ëŸ¬ ë‹¨ê³„ì˜ fallback ì„¤ì •\n",
        "    primary = ChatOpenAI(model=\"gpt-4.1-mini\", max_retries=0, temperature=0.7)\n",
        "    fallback1 = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", max_retries=0, temperature=0.7)\n",
        "    fallback2 = ChatOllama(model=\"qwen3:4b\", temperature=0.7)\n",
        "\n",
        "    # 3ë‹¨ê³„ fallback ì²´ì¸\n",
        "    robust_llm = primary.with_fallbacks([fallback1, fallback2])\n",
        "    \n",
        "    # ì™„ì „í•œ ì²´ì¸ êµ¬ì„±\n",
        "    chain = prompt | robust_llm | StrOutputParser()\n",
        "    \n",
        "    return chain\n",
        "\n",
        "# ê²¬ê³ í•œ ì²´ì¸ í…ŒìŠ¤íŠ¸\n",
        "robust_chain = create_fallback_chat_chain()\n",
        "\n",
        "# ì •ìƒ ì‘ë™ í…ŒìŠ¤íŠ¸\n",
        "print(\"=== ì •ìƒ ì‘ë™ í…ŒìŠ¤íŠ¸ ===\")\n",
        "try:\n",
        "    response = robust_chain.invoke({\"user_input\": \"LangChainì˜ ì¥ì ì„ 3ê°€ì§€ë§Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"})\n",
        "    print(f\"ì‘ë‹µ: {response}\")\n",
        "except Exception as e:\n",
        "    print(f\"ì˜¤ë¥˜: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ce80559",
      "metadata": {},
      "source": [
        "### 2. ëª¨ë¸ë³„ ìµœì í™” ì ìš©\n",
        "\n",
        "- ê°ê¸° ë‹¤ë¥¸ ëª¨ë¸ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” fallback ì‹œìŠ¤í…œì„ êµ¬í˜„\n",
        "- ê° ëª¨ë¸ì˜ íŠ¹ì„±ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìµœì í™”\n",
        "- ì„±ëŠ¥-ë¹„ìš© ê· í˜•ì„ ê³ ë ¤í•œ ë‹¤ë‹¨ê³„ fallback\n",
        "- í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ì•ˆì •ì„± í™•ë³´"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "43393aad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== ì§ˆë¬¸ 1: ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ ì „ë§ì€ ì–´ë–»ê²Œ ë ê¹Œìš”? ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:35:19,814 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë‹µë³€: ì§ˆë¬¸ ë¶„ì„  \n",
            "â€œì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ ì „ë§â€ì€ ë§¤ìš° í¬ê´„ì ì¸ ì£¼ì œì…ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ê¸°ìˆ  ë°œì „, ì‚¬íšŒì  ì˜í–¥, ê²½ì œì  ë³€í™”, ìœ¤ë¦¬ì  ìŸì , ì‚°ì—…ë³„ ì ìš© ë“± ë‹¤ì–‘í•œ ì¸¡ë©´ì´ í¬í•¨ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ì—¬ëŸ¬ ê°ë„ì—ì„œ ì ‘ê·¼í•´ì•¼ í•˜ë©°, ë‹¨ìˆœíˆ ê¸°ìˆ ì  ì§„ë³´ë§Œì´ ì•„ë‹ˆë¼ ì‚¬íšŒ ì „ë°˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í•¨ê»˜ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.\n",
            "\n",
            "ë‹¤ê°ë„ ì ‘ê·¼  \n",
            "1. ê¸°ìˆ ì  ë°œì „  \n",
            "- ë”¥ëŸ¬ë‹, ê°•í™”í•™ìŠµ, ìƒì„±í˜• AI(ì˜ˆ: ChatGPT, Midjourney ë“±) ë“± ì¸ê³µì§€ëŠ¥ì˜ í•µì‹¬ ê¸°ìˆ ì€ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.  \n",
            "- ì•ìœ¼ë¡œëŠ” ë©€í‹°ëª¨ë‹¬ AI(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€...\n",
            "\n",
            "=== ì§ˆë¬¸ 2: ê¸°ì—…ì—ì„œ AIë¥¼ ë„ì…í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ìš”ì†Œë“¤ì€? ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:35:30,361 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë‹µë³€: ì§ˆë¬¸ ë¶„ì„  \n",
            "â€œê¸°ì—…ì—ì„œ AIë¥¼ ë„ì…í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ìš”ì†Œë“¤ì€?â€ì´ë¼ëŠ” ì§ˆë¬¸ì€ ë‹¨ìˆœíˆ ê¸°ìˆ ì  ë„ì…ë§Œì´ ì•„ë‹ˆë¼, ê¸°ì—…ì˜ ì „ëµ, ì¡°ì§, ë¹„ìš©, ìœ¤ë¦¬ ë“± ë‹¤ì–‘í•œ ì¸¡ë©´ì„ í¬ê´„í•©ë‹ˆë‹¤. AI ë„ì…ì€ ê¸°ì—…ì˜ ê²½ìŸë ¥ ê°•í™”, ì—…ë¬´ íš¨ìœ¨í™”, ìƒˆë¡œìš´ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ì°½ì¶œ ë“± ê¸ì •ì  íš¨ê³¼ê°€ ìˆì§€ë§Œ, ì‹¤íŒ¨ ì‹œ ë¹„ìš© ì†ì‹¤, ë°ì´í„° ìœ ì¶œ, ì¡°ì§ ì €í•­ ë“± ë¶€ì •ì  ê²°ê³¼ë„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ê°ë„ì—ì„œ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
            "\n",
            "ë‹¤ê°ë„ ì ‘ê·¼  \n",
            "1. ì „ëµì  ì¸¡ë©´  \n",
            "   - ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œì™€ì˜ ì •í•©ì„±: AI ë„ì…ì´ ê¸°ì—…ì˜ ì¥ê¸°ì  ëª©í‘œì™€ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€ ë¶„ì„í•´ì•¼...\n",
            "\n",
            "=== ì§ˆë¬¸ 3: ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€? ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 17:35:42,939 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë‹µë³€: ì§ˆë¬¸ ë¶„ì„  \n",
            "\"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì \"ì€ ì¸ê³µì§€ëŠ¥(AI) ë¶„ì•¼ì—ì„œ ë§¤ìš° ìì£¼ ë“±ì¥í•˜ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤. ë‘ ìš©ì–´ ëª¨ë‘ ë°ì´í„° ê¸°ë°˜ í•™ìŠµì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤ëŠ” ê³µí†µì ì´ ìˆì§€ë§Œ, ê·¸ ì›ë¦¬ì™€ ì ìš© ë°©ì‹, ì„±ëŠ¥, í•„ìš” ì¡°ê±´ ë“±ì—ì„œ ì¤‘ìš”í•œ ì°¨ì´ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
            "\n",
            "ë‹¤ê°ë„ ì ‘ê·¼  \n",
            "1. ê°œë…ì  ì°¨ì´  \n",
            "- ë¨¸ì‹ ëŸ¬ë‹(Machine Learning):  \n",
            "  ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ ë¶„ë¥˜ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ ì§‘í•©ì…ë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ ì˜ì‚¬ê²°ì •íŠ¸ë¦¬, ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (SVM), ëœë¤ í¬ë ˆìŠ¤íŠ¸, K-ìµœê·¼ì ‘ ì´ì›ƒ(KN...\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def create_production_qa_system():\n",
        "    \"\"\"ëª¨ë¸ë³„ ìµœì í™” Q&A ì‹œìŠ¤í…œ\"\"\"\n",
        "    \n",
        "    # ê³ ì„±ëŠ¥ ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ (gpt-4.1 ì ìš©)\n",
        "    premium_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. \n",
        "        ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:\n",
        "        1. ì§ˆë¬¸ì„ ê¹Šì´ ë¶„ì„í•˜ì„¸ìš”\n",
        "        2. ë‹¤ê°ë„ì—ì„œ ì ‘ê·¼í•˜ì„¸ìš”  \n",
        "        3. êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”\n",
        "        4. ê²°ë¡ ì„ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”\"\"\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ])\n",
        "    \n",
        "    # ê¸°ë³¸ ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ (gpt-4.1-mini ì ìš©)\n",
        "    standard_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"ê°„ê²°í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ])\n",
        "    \n",
        "    # ê²½ëŸ‰ ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ (qwen3:4b ì ìš©)\n",
        "    budget_prompt = PromptTemplate.from_template(\n",
        "        \"ì§ˆë¬¸: {question}\\n\\ní•µì‹¬ ë‹µë³€:\"\n",
        "    )\n",
        "    \n",
        "    # ëª¨ë¸ ì²´ì¸ êµ¬ì„±\n",
        "    premium_chain = premium_prompt | ChatOpenAI(model=\"gpt-4.1\", temperature=0.3, max_retries=0)\n",
        "    standard_chain = standard_prompt | ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.3, max_retries=0)  \n",
        "    budget_chain = budget_prompt | ChatOllama(model=\"qwen3:4b\", temperature=0.3)\n",
        "    \n",
        "    # 3ë‹¨ê³„ fallback ì‹œìŠ¤í…œ\n",
        "    qa_system = premium_chain.with_fallbacks([\n",
        "        standard_chain, \n",
        "        budget_chain\n",
        "    ]) | StrOutputParser()\n",
        "    \n",
        "    return qa_system\n",
        "\n",
        "# ëª¨ë¸ë³„ ìµœì í™” Q&A ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
        "production_qa = create_production_qa_system()\n",
        "\n",
        "test_questions = [\n",
        "    \"ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ ì „ë§ì€ ì–´ë–»ê²Œ ë ê¹Œìš”?\",\n",
        "    \"ê¸°ì—…ì—ì„œ AIë¥¼ ë„ì…í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ìš”ì†Œë“¤ì€?\",\n",
        "    \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€?\"\n",
        "]\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n=== ì§ˆë¬¸ {i}: {question} ===\")\n",
        "    try:\n",
        "        answer = production_qa.invoke({\"question\": question})\n",
        "        print(f\"ë‹µë³€: {answer[:300]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"ì˜¤ë¥˜: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d28ed36c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "modu2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
