{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCslgB5VhS5k"
   },
   "source": [
    "#  LLM ì„±ëŠ¥í‰ê°€ ê°œìš”\n",
    "\n",
    "### **í•™ìŠµ ëª©í‘œ:** A/B í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ì—¬ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ í‰ê°€ë¥¼ ì ìš©í•œë‹¤\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQ9XIA-phS5n"
   },
   "source": [
    "## í™˜ê²½ ì„¤ì • ë° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTzn2Z9YhS5n"
   },
   "source": [
    "`(1) Env í™˜ê²½ë³€ìˆ˜`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3qZFTk5OhS5n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRS3jpKuhS5o"
   },
   "source": [
    "`(2) ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wQji9mZ8hS5p"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2x5sIVnhS5q"
   },
   "source": [
    "`(3) langfuase handler ì„¤ì •`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LpaldYSohS5r"
   },
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# LangChain ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±\n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGELe08khS5r"
   },
   "source": [
    "`(4) Test Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_MoYJOLthS5s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ì…‹: 49ê°œ ë¬¸ì„œ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla, Inc.ëŠ” ë¯¸êµ­ì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ê³  ìˆìœ¼ë©°, ì´ íšŒì‚¬ì˜ ì£¼ìš” ì œí’ˆê³¼ ...</td>\n",
       "      <td>['Tesla, Inc.ëŠ” ë¯¸êµ­ì˜ ë‹¤êµ­ì  ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ì…ë‹ˆë‹¤. ì´ íšŒ...</td>\n",
       "      <td>Tesla, Inc.ëŠ” ë¯¸êµ­ì˜ ë‹¤êµ­ì  ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ë¡œ, ì „ê¸° ìë™ì°¨(...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forbes Global 2000ì—ì„œ í…ŒìŠ¬ë¼ ìˆœìœ„ ë­ì•¼?</td>\n",
       "      <td>['Teslaì˜ ì°¨ëŸ‰ ìƒì‚°ì€ 2008ë…„ Roadsterë¡œ ì‹œì‘í•˜ì—¬ Model S (...</td>\n",
       "      <td>í…ŒìŠ¬ë¼ëŠ” Forbes Global 2000ì—ì„œ 69ìœ„ì— ë­í¬ë˜ì—ˆìŠµë‹ˆë‹¤.</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Tesla, Inc.ëŠ” ë¯¸êµ­ì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ê³  ìˆìœ¼ë©°, ì´ íšŒì‚¬ì˜ ì£¼ìš” ì œí’ˆê³¼ ...   \n",
       "1                    Forbes Global 2000ì—ì„œ í…ŒìŠ¬ë¼ ìˆœìœ„ ë­ì•¼?   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Tesla, Inc.ëŠ” ë¯¸êµ­ì˜ ë‹¤êµ­ì  ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ì…ë‹ˆë‹¤. ì´ íšŒ...   \n",
       "1  ['Teslaì˜ ì°¨ëŸ‰ ìƒì‚°ì€ 2008ë…„ Roadsterë¡œ ì‹œì‘í•˜ì—¬ Model S (...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Tesla, Inc.ëŠ” ë¯¸êµ­ì˜ ë‹¤êµ­ì  ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ë¡œ, ì „ê¸° ìë™ì°¨(...   \n",
       "1            í…ŒìŠ¬ë¼ëŠ” Forbes Global 2000ì—ì„œ 69ìœ„ì— ë­í¬ë˜ì—ˆìŠµë‹ˆë‹¤.   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test ë°ì´í„°ì…‹ì— ëŒ€í•œ QA ìƒì„± ê²°ê³¼ë¥¼ ë¦¬ë·°í•œ í›„ ë‹¤ì‹œ ë¡œë“œ\n",
    "import pandas as pd\n",
    "df_qa_test = pd.read_excel(\"data/testset.xlsx\")\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ì…‹: {df_qa_test.shape[0]}ê°œ ë¬¸ì„œ\")\n",
    "df_qa_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clFRh-lWhS5t"
   },
   "source": [
    "---\n",
    "\n",
    "## **LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í‰ê°€**\n",
    "\n",
    "- **AI í‰ê°€**ëŠ” ë°ì´í„°ì…‹, í‰ê°€ì, í‰ê°€ ë°©ë²•ë¡  ì„¸ ê°€ì§€ í•µì‹¬ ìš”ì†Œë¡œ êµ¬ì„±ë˜ë©°, ì´ˆê¸°ì—ëŠ” **10-20ê°œì˜ ê³ í’ˆì§ˆ ì˜ˆì œ**ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì \n",
    "\n",
    "- í‰ê°€ ë°©ì‹ì€ **ì¸ê°„ í‰ê°€**ì™€ **ìë™í™” í‰ê°€** ë‘ íŠ¸ë™ìœ¼ë¡œ ì§„í–‰ë˜ë©°, ì£¼ê´€ì  íŒë‹¨ì´ í•„ìš”í•œ ì´ˆê¸°ì—ëŠ” ì¸ê°„ í‰ê°€ë¥¼, í™•ì¥ì´ í•„ìš”í•œ ê²½ìš° íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ìë™í™” í‰ê°€ë¥¼ í™œìš©\n",
    "\n",
    "- í‰ê°€ëŠ” **ì˜¤í”„ë¼ì¸**ê³¼ **ì˜¨ë¼ì¸** í™˜ê²½ì—ì„œ ìˆ˜í–‰ë˜ë©°, ë²¤ì¹˜ë§ˆí‚¹, í…ŒìŠ¤íŠ¸, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë“± ìƒí™©ì— ë§ëŠ” ë°©ë²•ë¡ ì„ ì ìš©\n",
    "\n",
    "- ì§€ì†ì ì¸ **CI/CD í†µí•©**ê³¼ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•ì„ í†µí•´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ì˜ íš¨ìœ¨ì„±ê³¼ ì‹ ë¢°ì„±ì„ í™•ë³´í•´ì•¼ í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vd1Qlr-whS5u"
   },
   "source": [
    "### 1) **ë²¡í„°ìŠ¤í† ì–´** ë¡œë“œ\n",
    "\n",
    "- **Chroma DB** ì„¤ì •ì—ì„œ ëª¨ë¸, ì»¬ë ‰ì…˜ëª…, ì €ì¥ ê²½ë¡œ ì§€ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DSlAF-3rhS5v"
   },
   "outputs": [],
   "source": [
    "# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"db_korean_cosine_metadata\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-4jGyhgihS5v"
   },
   "outputs": [],
   "source": [
    "# ë²¡í„°ì €ì¥ì†Œ ê²€ìƒ‰ê¸° ìƒì„±\n",
    "chroma_k = chroma_db.as_retriever(\n",
    "    search_kwargs={'k': 4},\n",
    ")\n",
    "\n",
    "# ë²¡í„°ì €ì¥ì†Œ ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰\n",
    "query = \"Elon MuskëŠ” Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ê³¼ ê²½ì˜ ë³€í™”ì— ì–´ë–»ê²Œ ê´€ì—¬í–ˆìœ¼ë©°, ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ë…¼ë€ì— ì§ë©´í–ˆë‚˜ìš”?\"\n",
    "\n",
    "retrieved_docs = chroma_k.invoke(query)\n",
    "\n",
    "# ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*200)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPEh9XYyhS5v"
   },
   "source": [
    "### 2) **BM25 ê²€ìƒ‰ê¸°** ì¤€ë¹„\n",
    "\n",
    "- **BM25 ê²€ìƒ‰ê¸°** êµ¬í˜„ìœ¼ë¡œ ë¬¸ì„œ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ ê°€ëŠ¥\n",
    "\n",
    "- **í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬**ë¥¼ ìœ„í•œ **Kiwi í† í¬ë‚˜ì´ì €** ì„¤ì •\n",
    "\n",
    "- ì°¸ê³ : https://github.com/bab2min/kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fhgIXiW7hS5v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¡œë“œëœ ë¬¸ì„œ: 39ê°œ\n",
      "('{\"id\":null,\"metadata\":{\"source\":\"data/í…ŒìŠ¬ë¼_KR.md\",\"company\":\"í…ŒìŠ¬ë¼\",\"language\":\"ko\"},\"page_content\":\"<Document>\\\\nTesla, '\n",
      " 'Inc.ëŠ” ë¯¸êµ­ì˜ ë‹¤êµ­ì  ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ì…ë‹ˆë‹¤. ì´ íšŒì‚¬ëŠ” ì „ê¸° ìë™ì°¨(BEV), ê³ ì •í˜• ë°°í„°ë¦¬ ì—ë„ˆì§€ ì €ì¥ ì¥ì¹˜, íƒœì–‘ '\n",
      " 'ì „ì§€íŒ, íƒœì–‘ê´‘ ì§€ë¶•ë„ ë° ê´€ë ¨ ì œí’ˆ/ì„œë¹„ìŠ¤ë¥¼ ì„¤ê³„, ì œì¡° ë° íŒë§¤í•©ë‹ˆë‹¤. 2003ë…„ 7ì›” Martin Eberhardì™€ Marc '\n",
      " 'Tarpenningì´ Tesla Motorsë¡œ ì„¤ë¦½í–ˆìœ¼ë©°, Nikola Teslaë¥¼ ê¸°ë¦¬ê¸° ìœ„í•´ ëª…ëª…ë˜ì—ˆìŠµë‹ˆë‹¤. Elon MuskëŠ” '\n",
      " '2004ë…„ Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ 2008ë…„ì— íšŒì¥ ê²¸ CEOê°€ '\n",
      " \"ë˜ì—ˆìŠµë‹ˆë‹¤.\\\\n</Document>\\\\n<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ \"\n",
      " 'ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\",\"type\":\"Document\"}')\n"
     ]
    }
   ],
   "source": [
    "# korean_docs íŒŒì¼ì„ ë¡œë“œ (jsonlines íŒŒì¼)\n",
    "def load_jsonlines(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        docs = [json.loads(line) for line in f]\n",
    "    return docs\n",
    "\n",
    "korean_docs = load_jsonlines('data/korean_docs_final.jsonl')\n",
    "print(f\"ë¡œë“œëœ ë¬¸ì„œ: {len(korean_docs)}ê°œ\")\n",
    "pprint(korean_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3u6uAD_ChS5w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë³€í™˜ëœ ë¬¸ì„œ: 39ê°œ\n",
      "Document(metadata={'source': 'data/í…ŒìŠ¬ë¼_KR.md', 'company': 'í…ŒìŠ¬ë¼', 'language': 'ko'}, page_content=\"<Document>\\nTesla, Inc.ëŠ” ë¯¸êµ­ì˜ ë‹¤êµ­ì  ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ì…ë‹ˆë‹¤. ì´ íšŒì‚¬ëŠ” ì „ê¸° ìë™ì°¨(BEV), ê³ ì •í˜• ë°°í„°ë¦¬ ì—ë„ˆì§€ ì €ì¥ ì¥ì¹˜, íƒœì–‘ ì „ì§€íŒ, íƒœì–‘ê´‘ ì§€ë¶•ë„ ë° ê´€ë ¨ ì œí’ˆ/ì„œë¹„ìŠ¤ë¥¼ ì„¤ê³„, ì œì¡° ë° íŒë§¤í•©ë‹ˆë‹¤. 2003ë…„ 7ì›” Martin Eberhardì™€ Marc Tarpenningì´ Tesla Motorsë¡œ ì„¤ë¦½í–ˆìœ¼ë©°, Nikola Teslaë¥¼ ê¸°ë¦¬ê¸° ìœ„í•´ ëª…ëª…ë˜ì—ˆìŠµë‹ˆë‹¤. Elon MuskëŠ” 2004ë…„ Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ 2008ë…„ì— íšŒì¥ ê²¸ CEOê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\\n</Document>\\n<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document  # Document í´ë˜ìŠ¤ ì„í¬íŠ¸\n",
    "\n",
    "# ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜\n",
    "if isinstance(korean_docs[0], str):  # ì²« ë²ˆì§¸ í•­ëª©ì´ ë¬¸ìì—´ì¸ì§€ í™•ì¸\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=json.loads(data)['page_content'],  # ë¬¸ìì—´ì„ íŒŒì´ì¬ ê°ì²´ë¡œ ë³€í™˜\n",
    "            metadata=json.loads(data)['metadata']\n",
    "        )\n",
    "        for i, data in enumerate(korean_docs)\n",
    "    ]\n",
    "else:\n",
    "    documents = korean_docs\n",
    "\n",
    "print(f\"ë³€í™˜ëœ ë¬¸ì„œ: {len(documents)}ê°œ\")\n",
    "pprint(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jrjqnTgGhS5w"
   },
   "outputs": [],
   "source": [
    "# BM25 ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì¤€ë¹„\n",
    "from krag.tokenizers import KiwiTokenizer\n",
    "from krag.retrievers import KiWiBM25RetrieverWithScore\n",
    "\n",
    "kiwi_tokenizer = KiwiTokenizer(\n",
    "    model_type='knlm',    # Kiwi ì–¸ì–´ ëª¨ë¸ íƒ€ì…\n",
    "    typos='basic'         # ê¸°ë³¸ ì˜¤íƒ€êµì •\n",
    "    )\n",
    "\n",
    "bm25_db = KiWiBM25RetrieverWithScore(\n",
    "        documents=documents,\n",
    "        kiwi_tokenizer=kiwi_tokenizer,\n",
    "        k=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SnDYuU6thS5x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 ì ìˆ˜: 24.53\n",
      "\n",
      "<Document>\n",
      "TeslaëŠ” ë‚´ë¶€ ê³ ë°œì ë³´ë³µ, ê·¼ë¡œì ê¶Œë¦¬ ì¹¨í•´, ì•ˆì „ ê²°í•¨, í™ë³´ ë¶€ì¡±, Muskì˜ ë…¼ë€ì˜ ì—¬ì§€ê°€ ìˆëŠ” ë°œì–¸ê³¼ ê´€ë ¨ëœ ì†Œì†¡, ì •ë¶€ ì¡°ì‚¬ ë° ë¹„íŒì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "## ì—­ì‚¬\n",
      "\n",
      "### ì°½ë¦½ (2003â€“2004)\n",
      "\n",
      "Tesla Motors, Inc.ëŠ” 2003ë…„ 7ì›” 1ì¼ì— Martin Eberhardì™€ Marc Tarpenningì— ì˜í•´ ì„¤ë¦½ë˜ì—ˆìœ¼ë©°, ê°ê° CEOì™€ CFOë¥¼ ì—­ì„í–ˆìŠµë‹ˆë‹¤. Ian WrightëŠ” ì–¼ë§ˆ ì§€ë‚˜ì§€ ì•Šì•„ í•©ë¥˜í–ˆìŠµë‹ˆë‹¤. 2004ë…„ 2ì›”, Elon MuskëŠ” 750ë§Œ ë‹¬ëŸ¬ì˜ ì‹œë¦¬ì¦ˆ A ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ íšŒì¥ ê²¸ ìµœëŒ€ ì£¼ì£¼ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. J. B. Straubelì€ 2004ë…„ 5ì›” CTOë¡œ í•©ë¥˜í–ˆìŠµë‹ˆë‹¤. ë‹¤ì„¯ ëª… ëª¨ë‘ ê³µë™ ì„¤ë¦½ìë¡œ ì¸ì •ë°›ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### Roadster (2005â€“2009)\n",
      "</Document>\n",
      "<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\n",
      "[ì¶œì²˜: data/í…ŒìŠ¬ë¼_KR.md]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "BM25 ì ìˆ˜: 22.21\n",
      "\n",
      "<Document>\n",
      "Tesla, Inc.ëŠ” ë¯¸êµ­ì˜ ë‹¤êµ­ì  ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ì…ë‹ˆë‹¤. ì´ íšŒì‚¬ëŠ” ì „ê¸° ìë™ì°¨(BEV), ê³ ì •í˜• ë°°í„°ë¦¬ ì—ë„ˆì§€ ì €ì¥ ì¥ì¹˜, íƒœì–‘ ì „ì§€íŒ, íƒœì–‘ê´‘ ì§€ë¶•ë„ ë° ê´€ë ¨ ì œí’ˆ/ì„œë¹„ìŠ¤ë¥¼ ì„¤ê³„, ì œì¡° ë° íŒë§¤í•©ë‹ˆë‹¤. 2003ë…„ 7ì›” Martin Eberhardì™€ Marc Tarpenningì´ Tesla Motorsë¡œ ì„¤ë¦½í–ˆìœ¼ë©°, Nikola Teslaë¥¼ ê¸°ë¦¬ê¸° ìœ„í•´ ëª…ëª…ë˜ì—ˆìŠµë‹ˆë‹¤. Elon MuskëŠ” 2004ë…„ Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ 2008ë…„ì— íšŒì¥ ê²¸ CEOê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "</Document>\n",
      "<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\n",
      "[ì¶œì²˜: data/í…ŒìŠ¬ë¼_KR.md]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "BM25 ì ìˆ˜: 20.31\n",
      "\n",
      "<Document>\n",
      "### Roadster (2005â€“2009)\n",
      "\n",
      "Elon MuskëŠ” ì£¼ë¥˜ ì°¨ëŸ‰ìœ¼ë¡œ í™•ì¥í•˜ê¸° ì „ì— í”„ë¦¬ë¯¸ì—„ ìŠ¤í¬ì¸ ì¹´ë¡œ ì‹œì‘í•˜ëŠ” ì „ëµì— ì´ˆì ì„ ë§ì¶° ì ê·¹ì ì¸ ì—­í• ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. í›„ì† ìê¸ˆ ì¡°ë‹¬ì—ëŠ” Valor Equity Partners (2006)ì™€ Sergey Brin, Larry Page, Jeff Skollê³¼ ê°™ì€ ê¸°ì—…ê°€ì˜ íˆ¬ìê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2007ë…„ 8ì›”, EberhardëŠ” CEOì—ì„œ ë¬¼ëŸ¬ë‚˜ë¼ëŠ” ìš”ì²­ì„ ë°›ì•˜ê³ , Tarpenningì€ 2008ë…„ 1ì›”ì— ì´ì–´ì¡ŒìŠµë‹ˆë‹¤. Michael MarksëŠ” Ze'ev Droriê°€ ì¸ìˆ˜í•˜ê¸° ì „ì— ì„ì‹œ CEOë¥¼ ì—­ì„í–ˆìœ¼ë©°, MuskëŠ” 2008ë…„ 10ì›”ì— ì¸ìˆ˜í–ˆìŠµë‹ˆë‹¤. EberhardëŠ” 2009ë…„ 6ì›” Muskë¥¼ ìƒëŒ€ë¡œ ì†Œì†¡ì„ ì œê¸°í–ˆì§€ë§Œ ë‚˜ì¤‘ì— ê¸°ê°ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "</Document>\n",
      "<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\n",
      "[ì¶œì²˜: data/í…ŒìŠ¬ë¼_KR.md]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "BM25 ì ìˆ˜: 17.35\n",
      "\n",
      "<Document>\n",
      "## íŒŒíŠ¸ë„ˆ\n",
      "\n",
      "TeslaëŠ” Panasonicê³¼ íŒŒíŠ¸ë„ˆì‹­ì„ ë§ºê³  ìˆìœ¼ë©° ë¦¬íŠ¬ ê³µê¸‰ì— ëŒ€í•œ ì¥ê¸° ê³„ì•½ì„ ë§ºê³  ìˆìŠµë‹ˆë‹¤. ì´ì „ íŒŒíŠ¸ë„ˆë¡œëŠ” Daimlerì™€ Toyotaê°€ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "## ì†Œì†¡ ë° ë…¼ë€\n",
      "\n",
      "TeslaëŠ” ì„±í¬ë¡±, ë…¸ë™ ë¶„ìŸ, ì‚¬ê¸° í˜ì˜, ëŒ€ë¦¬ì  ë¶„ìŸ, ì§€ì  ì¬ì‚°ê¶Œ, í™˜ê²½ ìœ„ë°˜, ì¬ì‚° í”¼í•´, ì¸ì¢… ì°¨ë³„, COVID-19 íŒ¬ë°ë¯¹ ëŒ€ì‘ ë° ìˆ˜ë¦¬ ê¶Œë¦¬ì™€ ê´€ë ¨ëœ ì†Œì†¡ ë° ë…¼ë€ì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "## ë¹„íŒ\n",
      "\n",
      "TeslaëŠ” ë°ì´í„° ê°œì¸ ì •ë³´ ë³´í˜¸, ê³µë§¤ë„ì, ì§€ì—°, ì°¨ëŸ‰ ì œí’ˆ ë¬¸ì œ, í™”ì¬, Autopilot ì¶©ëŒ, ì†Œí”„íŠ¸ì›¨ì–´ í•´í‚¹, ê°€ìƒ ì œë™ ë° ì£¼í–‰ ê±°ë¦¬ ì„±ëŠ¥ê³¼ ê´€ë ¨ëœ ë¹„íŒì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.\n",
      "</Document>\n",
      "<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\n",
      "[ì¶œì²˜: data/í…ŒìŠ¬ë¼_KR.md]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# BM25 ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰\n",
    "query = \"Elon MuskëŠ” Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ê³¼ ê²½ì˜ ë³€í™”ì— ì–´ë–»ê²Œ ê´€ì—¬í–ˆìœ¼ë©°, ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ë…¼ë€ì— ì§ë©´í–ˆë‚˜ìš”?\"\n",
    "retrieved_docs = bm25_db.invoke(query, 2)\n",
    "\n",
    "# ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"BM25 ì ìˆ˜: {doc.metadata[\"bm25_score\"]:.2f}\")\n",
    "    print(f\"\\n{doc.page_content}\\n[ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9421e8NvhS5x"
   },
   "source": [
    "### 3) **Emsemble Hybrid Search** ì¤€ë¹„\n",
    "\n",
    "- **BM25**, **ë²¡í„° ê²€ìƒ‰** ê²°ê³¼ë¥¼ **rank-fusion** ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í†µí•© (**EnsembleRetriever**)\n",
    "\n",
    "- ê° ê²€ìƒ‰ê¸°ì˜ **ìˆœìœ„ ì ìˆ˜**ë¥¼ ê³ ë ¤í•œ ìµœì¢… ìˆœìœ„ ê²°ì •\n",
    "\n",
    "- **ì¤‘ë³µ ë¬¸ì„œ** ì œê±°ì™€ **ì¬ìˆœìœ„í™”** ìë™ ìˆ˜í–‰\n",
    "\n",
    "- ë‘ ê²€ìƒ‰ ë°©ì‹ì˜ **ì¥ì ì„ ê²°í•©**í•´ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zbhXiSJqhS5y"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "# ê²€ìƒ‰ê¸° ì´ˆê¸°í™”\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_db, chroma_k],\n",
    "    weights=[0.5, 0.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Jhqm6hWnhS5y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<Document>\n",
      "TeslaëŠ” ë‚´ë¶€ ê³ ë°œì ë³´ë³µ, ê·¼ë¡œì ê¶Œë¦¬ ì¹¨í•´, ì•ˆì „ ê²°í•¨, í™ë³´ ë¶€ì¡±, Muskì˜ ë…¼ë€ì˜ ì—¬ì§€ê°€ ìˆëŠ” ë°œì–¸ê³¼ ê´€ë ¨ëœ ì†Œì†¡, ì •ë¶€ ì¡°ì‚¬ ë° ë¹„íŒì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "## ì—­ì‚¬\n",
      "\n",
      "### ì°½ë¦½ (2003â€“2004)\n",
      "\n",
      "Tesla Motors, Inc.ëŠ” 2003ë…„ 7ì›” 1ì¼ì— Martin Eberhardì™€ Marc Tarpenningì— ì˜í•´ ì„¤ë¦½ë˜ì—ˆìœ¼ë©°, ê°ê° CEOì™€ CFOë¥¼ ì—­ì„í–ˆìŠµë‹ˆë‹¤. Ian WrightëŠ” ì–¼ë§ˆ ì§€ë‚˜ì§€ ì•Šì•„ í•©ë¥˜í–ˆìŠµë‹ˆë‹¤. 2004ë…„ 2ì›”, Elon MuskëŠ” 750ë§Œ ë‹¬ëŸ¬ì˜ ì‹œë¦¬ì¦ˆ A ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ íšŒì¥ ê²¸ ìµœëŒ€ ì£¼ì£¼ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. J. B. Straubelì€ 2004ë…„ 5ì›” CTOë¡œ í•©ë¥˜í–ˆìŠµë‹ˆë‹¤. ë‹¤ì„¯ ëª… ëª¨ë‘ ê³µë™ ì„¤ë¦½ìë¡œ ì¸ì •ë°›ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### Roadster (2005â€“2009)\n",
      "</Document>\n",
      "<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\n",
      "[ì¶œì²˜: data/í…ŒìŠ¬ë¼_KR.md]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "<Document>\n",
      "Tesla, Inc.ëŠ” ë¯¸êµ­ì˜ ë‹¤êµ­ì  ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ì…ë‹ˆë‹¤. ì´ íšŒì‚¬ëŠ” ì „ê¸° ìë™ì°¨(BEV), ê³ ì •í˜• ë°°í„°ë¦¬ ì—ë„ˆì§€ ì €ì¥ ì¥ì¹˜, íƒœì–‘ ì „ì§€íŒ, íƒœì–‘ê´‘ ì§€ë¶•ë„ ë° ê´€ë ¨ ì œí’ˆ/ì„œë¹„ìŠ¤ë¥¼ ì„¤ê³„, ì œì¡° ë° íŒë§¤í•©ë‹ˆë‹¤. 2003ë…„ 7ì›” Martin Eberhardì™€ Marc Tarpenningì´ Tesla Motorsë¡œ ì„¤ë¦½í–ˆìœ¼ë©°, Nikola Teslaë¥¼ ê¸°ë¦¬ê¸° ìœ„í•´ ëª…ëª…ë˜ì—ˆìŠµë‹ˆë‹¤. Elon MuskëŠ” 2004ë…„ Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ 2008ë…„ì— íšŒì¥ ê²¸ CEOê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "</Document>\n",
      "<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\n",
      "[ì¶œì²˜: data/í…ŒìŠ¬ë¼_KR.md]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "<Document>\n",
      "### Roadster (2005â€“2009)\n",
      "\n",
      "Elon MuskëŠ” ì£¼ë¥˜ ì°¨ëŸ‰ìœ¼ë¡œ í™•ì¥í•˜ê¸° ì „ì— í”„ë¦¬ë¯¸ì—„ ìŠ¤í¬ì¸ ì¹´ë¡œ ì‹œì‘í•˜ëŠ” ì „ëµì— ì´ˆì ì„ ë§ì¶° ì ê·¹ì ì¸ ì—­í• ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. í›„ì† ìê¸ˆ ì¡°ë‹¬ì—ëŠ” Valor Equity Partners (2006)ì™€ Sergey Brin, Larry Page, Jeff Skollê³¼ ê°™ì€ ê¸°ì—…ê°€ì˜ íˆ¬ìê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2007ë…„ 8ì›”, EberhardëŠ” CEOì—ì„œ ë¬¼ëŸ¬ë‚˜ë¼ëŠ” ìš”ì²­ì„ ë°›ì•˜ê³ , Tarpenningì€ 2008ë…„ 1ì›”ì— ì´ì–´ì¡ŒìŠµë‹ˆë‹¤. Michael MarksëŠ” Ze'ev Droriê°€ ì¸ìˆ˜í•˜ê¸° ì „ì— ì„ì‹œ CEOë¥¼ ì—­ì„í–ˆìœ¼ë©°, MuskëŠ” 2008ë…„ 10ì›”ì— ì¸ìˆ˜í–ˆìŠµë‹ˆë‹¤. EberhardëŠ” 2009ë…„ 6ì›” Muskë¥¼ ìƒëŒ€ë¡œ ì†Œì†¡ì„ ì œê¸°í–ˆì§€ë§Œ ë‚˜ì¤‘ì— ê¸°ê°ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "</Document>\n",
      "<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\n",
      "[ì¶œì²˜: data/í…ŒìŠ¬ë¼_KR.md]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "<Document>\n",
      "## íŒŒíŠ¸ë„ˆ\n",
      "\n",
      "TeslaëŠ” Panasonicê³¼ íŒŒíŠ¸ë„ˆì‹­ì„ ë§ºê³  ìˆìœ¼ë©° ë¦¬íŠ¬ ê³µê¸‰ì— ëŒ€í•œ ì¥ê¸° ê³„ì•½ì„ ë§ºê³  ìˆìŠµë‹ˆë‹¤. ì´ì „ íŒŒíŠ¸ë„ˆë¡œëŠ” Daimlerì™€ Toyotaê°€ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "## ì†Œì†¡ ë° ë…¼ë€\n",
      "\n",
      "TeslaëŠ” ì„±í¬ë¡±, ë…¸ë™ ë¶„ìŸ, ì‚¬ê¸° í˜ì˜, ëŒ€ë¦¬ì  ë¶„ìŸ, ì§€ì  ì¬ì‚°ê¶Œ, í™˜ê²½ ìœ„ë°˜, ì¬ì‚° í”¼í•´, ì¸ì¢… ì°¨ë³„, COVID-19 íŒ¬ë°ë¯¹ ëŒ€ì‘ ë° ìˆ˜ë¦¬ ê¶Œë¦¬ì™€ ê´€ë ¨ëœ ì†Œì†¡ ë° ë…¼ë€ì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "## ë¹„íŒ\n",
      "\n",
      "TeslaëŠ” ë°ì´í„° ê°œì¸ ì •ë³´ ë³´í˜¸, ê³µë§¤ë„ì, ì§€ì—°, ì°¨ëŸ‰ ì œí’ˆ ë¬¸ì œ, í™”ì¬, Autopilot ì¶©ëŒ, ì†Œí”„íŠ¸ì›¨ì–´ í•´í‚¹, ê°€ìƒ ì œë™ ë° ì£¼í–‰ ê±°ë¦¬ ì„±ëŠ¥ê³¼ ê´€ë ¨ëœ ë¹„íŒì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.\n",
      "</Document>\n",
      "<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\n",
      "[ì¶œì²˜: data/í…ŒìŠ¬ë¼_KR.md]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"Elon MuskëŠ” Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ê³¼ ê²½ì˜ ë³€í™”ì— ì–´ë–»ê²Œ ê´€ì—¬í–ˆìœ¼ë©°, ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ë…¼ë€ì— ì§ë©´í–ˆë‚˜ìš”?\"\n",
    "retrieved_docs = hybrid_retriever.invoke(query)\n",
    "\n",
    "# ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"\\n{doc.page_content}\\n[ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"-\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNqntR0ThS5z"
   },
   "source": [
    "### 4) **RAG ì²´ì¸**\n",
    "\n",
    "- **ë‹µë³€**ê³¼ **ê²€ìƒ‰ ë¬¸ì„œ**ë¥¼ í•¨ê»˜ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SHnf09-WhS5z"
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.runnables import RunnableConfig, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List, Dict\n",
    "\n",
    "def rag_bot(\n",
    "    question: str,\n",
    "    retriever: BaseRetriever,\n",
    "    llm: BaseChatModel,\n",
    "    config: RunnableConfig | None = None,\n",
    ") -> Dict[str, str | List[Document]]:\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ê²€ìƒ‰ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    system_prompt = f\"\"\"ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "- ì œê³µëœ ë¬¸ì„œë§Œ ì°¸ê³ í•˜ì—¬ ë‹µë³€\n",
    "- ë¶ˆí™•ì‹¤í•  ê²½ìš° 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' ë¼ê³  ì‘ë‹µ\n",
    "- 3ë¬¸ì¥ ì´ë‚´ë¡œ ë‹µë³€\n",
    "\n",
    "[ë¬¸ì„œ]\n",
    "{context}\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\\n[ì§ˆë¬¸]{question}\\n\\n[ë‹µë³€]\\n\"},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    docqa_chain = {\n",
    "        \"context\": lambda x: context,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"docs\": lambda x: docs,\n",
    "    } | RunnableParallel({\n",
    "        \"answer\": prompt | llm | StrOutputParser(),\n",
    "        \"documents\": lambda x: x[\"docs\"],\n",
    "    })\n",
    "\n",
    "    return docqa_chain.invoke(question, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PUxbPQ_2hS5z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Elon MuskëŠ” 2004ë…„ Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ íšŒì¥ ê²¸ ìµœëŒ€ ì£¼ì£¼ê°€ ë˜ì—ˆê³ , 2008ë…„ 10ì›” CEOë¡œ ì·¨ì„í–ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” Roadster ê°œë°œ ì „ëµì— ì ê·¹ ì°¸ì—¬í–ˆìœ¼ë©°, 2007ë…„ê³¼ 2008ë…„ CEO êµì²´ ê³¼ì •ì—ì„œ ê²½ì˜ ë³€í™”ì— ê´€ì—¬í–ˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ê³µë™ ì°½ë¦½ì Eberhardê°€ Muskë¥¼ ìƒëŒ€ë¡œ ì†Œì†¡ì„ ì œê¸°í•˜ëŠ” ë“± ë…¼ë€ì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.',\n",
       " 'documents': [KragDocument(metadata={'source': 'data/í…ŒìŠ¬ë¼_KR.md', 'company': 'í…ŒìŠ¬ë¼', 'language': 'ko', 'bm25_score': 24.52713266060816}, page_content=\"<Document>\\nTeslaëŠ” ë‚´ë¶€ ê³ ë°œì ë³´ë³µ, ê·¼ë¡œì ê¶Œë¦¬ ì¹¨í•´, ì•ˆì „ ê²°í•¨, í™ë³´ ë¶€ì¡±, Muskì˜ ë…¼ë€ì˜ ì—¬ì§€ê°€ ìˆëŠ” ë°œì–¸ê³¼ ê´€ë ¨ëœ ì†Œì†¡, ì •ë¶€ ì¡°ì‚¬ ë° ë¹„íŒì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.\\n\\n## ì—­ì‚¬\\n\\n### ì°½ë¦½ (2003â€“2004)\\n\\nTesla Motors, Inc.ëŠ” 2003ë…„ 7ì›” 1ì¼ì— Martin Eberhardì™€ Marc Tarpenningì— ì˜í•´ ì„¤ë¦½ë˜ì—ˆìœ¼ë©°, ê°ê° CEOì™€ CFOë¥¼ ì—­ì„í–ˆìŠµë‹ˆë‹¤. Ian WrightëŠ” ì–¼ë§ˆ ì§€ë‚˜ì§€ ì•Šì•„ í•©ë¥˜í–ˆìŠµë‹ˆë‹¤. 2004ë…„ 2ì›”, Elon MuskëŠ” 750ë§Œ ë‹¬ëŸ¬ì˜ ì‹œë¦¬ì¦ˆ A ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ íšŒì¥ ê²¸ ìµœëŒ€ ì£¼ì£¼ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. J. B. Straubelì€ 2004ë…„ 5ì›” CTOë¡œ í•©ë¥˜í–ˆìŠµë‹ˆë‹¤. ë‹¤ì„¯ ëª… ëª¨ë‘ ê³µë™ ì„¤ë¦½ìë¡œ ì¸ì •ë°›ê³  ìˆìŠµë‹ˆë‹¤.\\n\\n### Roadster (2005â€“2009)\\n</Document>\\n<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\"),\n",
       "  KragDocument(metadata={'source': 'data/í…ŒìŠ¬ë¼_KR.md', 'company': 'í…ŒìŠ¬ë¼', 'language': 'ko', 'bm25_score': 22.21379516396908}, page_content=\"<Document>\\nTesla, Inc.ëŠ” ë¯¸êµ­ì˜ ë‹¤êµ­ì  ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ì…ë‹ˆë‹¤. ì´ íšŒì‚¬ëŠ” ì „ê¸° ìë™ì°¨(BEV), ê³ ì •í˜• ë°°í„°ë¦¬ ì—ë„ˆì§€ ì €ì¥ ì¥ì¹˜, íƒœì–‘ ì „ì§€íŒ, íƒœì–‘ê´‘ ì§€ë¶•ë„ ë° ê´€ë ¨ ì œí’ˆ/ì„œë¹„ìŠ¤ë¥¼ ì„¤ê³„, ì œì¡° ë° íŒë§¤í•©ë‹ˆë‹¤. 2003ë…„ 7ì›” Martin Eberhardì™€ Marc Tarpenningì´ Tesla Motorsë¡œ ì„¤ë¦½í–ˆìœ¼ë©°, Nikola Teslaë¥¼ ê¸°ë¦¬ê¸° ìœ„í•´ ëª…ëª…ë˜ì—ˆìŠµë‹ˆë‹¤. Elon MuskëŠ” 2004ë…„ Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ 2008ë…„ì— íšŒì¥ ê²¸ CEOê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\\n</Document>\\n<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\"),\n",
       "  KragDocument(metadata={'source': 'data/í…ŒìŠ¬ë¼_KR.md', 'company': 'í…ŒìŠ¬ë¼', 'language': 'ko', 'bm25_score': 20.310899408352544}, page_content=\"<Document>\\n### Roadster (2005â€“2009)\\n\\nElon MuskëŠ” ì£¼ë¥˜ ì°¨ëŸ‰ìœ¼ë¡œ í™•ì¥í•˜ê¸° ì „ì— í”„ë¦¬ë¯¸ì—„ ìŠ¤í¬ì¸ ì¹´ë¡œ ì‹œì‘í•˜ëŠ” ì „ëµì— ì´ˆì ì„ ë§ì¶° ì ê·¹ì ì¸ ì—­í• ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. í›„ì† ìê¸ˆ ì¡°ë‹¬ì—ëŠ” Valor Equity Partners (2006)ì™€ Sergey Brin, Larry Page, Jeff Skollê³¼ ê°™ì€ ê¸°ì—…ê°€ì˜ íˆ¬ìê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\\n2007ë…„ 8ì›”, EberhardëŠ” CEOì—ì„œ ë¬¼ëŸ¬ë‚˜ë¼ëŠ” ìš”ì²­ì„ ë°›ì•˜ê³ , Tarpenningì€ 2008ë…„ 1ì›”ì— ì´ì–´ì¡ŒìŠµë‹ˆë‹¤. Michael MarksëŠ” Ze'ev Droriê°€ ì¸ìˆ˜í•˜ê¸° ì „ì— ì„ì‹œ CEOë¥¼ ì—­ì„í–ˆìœ¼ë©°, MuskëŠ” 2008ë…„ 10ì›”ì— ì¸ìˆ˜í–ˆìŠµë‹ˆë‹¤. EberhardëŠ” 2009ë…„ 6ì›” Muskë¥¼ ìƒëŒ€ë¡œ ì†Œì†¡ì„ ì œê¸°í–ˆì§€ë§Œ ë‚˜ì¤‘ì— ê¸°ê°ë˜ì—ˆìŠµë‹ˆë‹¤.\\n</Document>\\n<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\"),\n",
       "  KragDocument(metadata={'source': 'data/í…ŒìŠ¬ë¼_KR.md', 'company': 'í…ŒìŠ¬ë¼', 'language': 'ko', 'bm25_score': 17.35015844174055}, page_content=\"<Document>\\n## íŒŒíŠ¸ë„ˆ\\n\\nTeslaëŠ” Panasonicê³¼ íŒŒíŠ¸ë„ˆì‹­ì„ ë§ºê³  ìˆìœ¼ë©° ë¦¬íŠ¬ ê³µê¸‰ì— ëŒ€í•œ ì¥ê¸° ê³„ì•½ì„ ë§ºê³  ìˆìŠµë‹ˆë‹¤. ì´ì „ íŒŒíŠ¸ë„ˆë¡œëŠ” Daimlerì™€ Toyotaê°€ ìˆìŠµë‹ˆë‹¤.\\n\\n## ì†Œì†¡ ë° ë…¼ë€\\n\\nTeslaëŠ” ì„±í¬ë¡±, ë…¸ë™ ë¶„ìŸ, ì‚¬ê¸° í˜ì˜, ëŒ€ë¦¬ì  ë¶„ìŸ, ì§€ì  ì¬ì‚°ê¶Œ, í™˜ê²½ ìœ„ë°˜, ì¬ì‚° í”¼í•´, ì¸ì¢… ì°¨ë³„, COVID-19 íŒ¬ë°ë¯¹ ëŒ€ì‘ ë° ìˆ˜ë¦¬ ê¶Œë¦¬ì™€ ê´€ë ¨ëœ ì†Œì†¡ ë° ë…¼ë€ì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.\\n\\n## ë¹„íŒ\\n\\nTeslaëŠ” ë°ì´í„° ê°œì¸ ì •ë³´ ë³´í˜¸, ê³µë§¤ë„ì, ì§€ì—°, ì°¨ëŸ‰ ì œí’ˆ ë¬¸ì œ, í™”ì¬, Autopilot ì¶©ëŒ, ì†Œí”„íŠ¸ì›¨ì–´ í•´í‚¹, ê°€ìƒ ì œë™ ë° ì£¼í–‰ ê±°ë¦¬ ì„±ëŠ¥ê³¼ ê´€ë ¨ëœ ë¹„íŒì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.\\n</Document>\\n<Source>ì´ ë¬¸ì„œëŠ” ë¯¸êµ­ ì „ê¸°ì°¨ íšŒì‚¬ì¸ 'í…ŒìŠ¬ë¼'ì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.</Source>\")]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# RAG ì²´ì¸ ì‹¤í–‰\n",
    "rag_bot(\n",
    "    question=\"Elon MuskëŠ” Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ê³¼ ê²½ì˜ ë³€í™”ì— ì–´ë–»ê²Œ ê´€ì—¬í–ˆìœ¼ë©°, ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ë…¼ë€ì— ì§ë©´í–ˆë‚˜ìš”?\",\n",
    "    retriever=hybrid_retriever,\n",
    "    llm=llm,\n",
    "    config={\"callbacks\": [langfuse_handler]},   # ì½œë°± í•¸ë“¤ëŸ¬ ì¶”ê°€\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLB_D7C7hS50"
   },
   "source": [
    "---\n",
    "### **[ì‹¤ìŠµ]**\n",
    "\n",
    "- gemin-1.5-flash ëª¨ë¸ê³¼ ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì²´ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "- ì‹¤í–‰ ê²°ê³¼ë¥¼ langfuse UIì—ì„œ í™•ì¸í•˜ê³ , gpt-4.1-mini ëª¨ë¸ì˜ ë‹µë³€ê³¼ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMODqRK8hS50"
   },
   "outputs": [],
   "source": [
    "# gemini-2.0-flash-001 ëª¨ë¸ê³¼ ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•œ RAG ì²´ì¸ ì‹¤ìŠµ\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Gemini-2.0-flash-001 ëª¨ë¸ ìƒì„±\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\", \n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"Gemini-2.0-flash-001 ëª¨ë¸ ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "question = \"Teslaì˜ ì£¼ìš” ì œí’ˆê³¼ ì„œë¹„ìŠ¤ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "print(f\"\\nì§ˆë¬¸: {question}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 1) Gemini-2.0-flash-001 ëª¨ë¸ë¡œ RAG ì‹¤í–‰\n",
    "print(\"\\n=== Gemini-2.0-flash-001 + ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ê¸° ê²°ê³¼ ===\")\n",
    "try:\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    gemini_response = rag_bot(\n",
    "        question=question,\n",
    "        retriever=chroma_k,  # ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ê¸° ì‚¬ìš©\n",
    "        llm=gemini_llm,\n",
    "        config={\n",
    "            \"callbacks\": [langfuse_handler],\n",
    "            \"tags\": [\"rag_bot\", \"evaluation\", \"gemini\", \"vector_retriever\"],\n",
    "            \"metadata\": {\n",
    "                \"model\": \"gemini-2.0-flash-001\",\n",
    "                \"temperature\": 0,\n",
    "                \"provider\": \"google\",\n",
    "                \"retriever\": \"vector_store\"\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    gemini_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ ì‘ë‹µ ì‹œê°„: {gemini_time:.2f}ì´ˆ\")\n",
    "    print(f\"ğŸ’¬ ë‹µë³€: {gemini_response['answer']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Gemini ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "    gemini_response = {\"answer\": f\"Gemini ëª¨ë¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\"}\n",
    "    gemini_time = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# 2) ë¹„êµë¥¼ ìœ„í•´ GPT-4o-mini ëª¨ë¸ë¡œë„ ë™ì¼í•œ ì§ˆë¬¸ ì‹¤í–‰\n",
    "print(\"=== GPT-4o-mini + ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ê¸° ê²°ê³¼ (ë¹„êµìš©) ===\")\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    gpt_response = rag_bot(\n",
    "        question=question,\n",
    "        retriever=chroma_k,  # ë™ì¼í•œ ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ê¸° ì‚¬ìš©\n",
    "        llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "        config={\n",
    "            \"callbacks\": [langfuse_handler],\n",
    "            \"tags\": [\"rag_bot\", \"evaluation\", \"gpt\", \"vector_retriever\"],\n",
    "            \"metadata\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"temperature\": 0,\n",
    "                \"provider\": \"openai\",\n",
    "                \"retriever\": \"vector_store\"\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    gpt_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ ì‘ë‹µ ì‹œê°„: {gpt_time:.2f}ì´ˆ\")\n",
    "    print(f\"ğŸ’¬ ë‹µë³€: {gpt_response['answer']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ GPT ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "    gpt_response = {\"answer\": f\"GPT ëª¨ë¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\"}\n",
    "    gpt_time = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# 3) ê²°ê³¼ ë¹„êµ ë¶„ì„\n",
    "print(\"=== ğŸ“Š ëª¨ë¸ ë¹„êµ ë¶„ì„ ===\")\n",
    "print(f\"â“ ì§ˆë¬¸: {question}\")\n",
    "print(f\"\\nğŸ¤– Gemini-2.0-flash-001:\")\n",
    "print(f\"   â±ï¸ ì‘ë‹µì‹œê°„: {gemini_time:.2f}ì´ˆ\")\n",
    "print(f\"   ğŸ’¬ ë‹µë³€: {gemini_response['answer']}\")\n",
    "\n",
    "print(f\"\\nğŸ”¥ GPT-4o-mini:\")\n",
    "print(f\"   â±ï¸ ì‘ë‹µì‹œê°„: {gpt_time:.2f}ì´ˆ\") \n",
    "print(f\"   ğŸ’¬ ë‹µë³€: {gpt_response['answer']}\")\n",
    "\n",
    "# ì„±ëŠ¥ ë¹„êµ ìš”ì•½\n",
    "if gemini_time > 0 and gpt_time > 0:\n",
    "    speed_ratio = gemini_time / gpt_time\n",
    "    faster_model = \"GPT-4o-mini\" if speed_ratio > 1 else \"Gemini-2.0-flash-001\"\n",
    "    print(f\"\\nâš¡ ì†ë„ ë¹„êµ: {faster_model}ê°€ ë” ë¹ ë¦„ (ë¹„ìœ¨: {speed_ratio:.1f})\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Langfuse UIì—ì„œ ë‘ ëª¨ë¸ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ì„¸ìš”!\")\n",
    "print(f\"ğŸ”— Langfuse: {os.getenv('LANGFUSE_HOST')}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ì‹¤ìŠµ ìš”ì•½:\")\n",
    "print(f\"- Gemini-2.0-flash-001: Googleì˜ ìµœì‹  ë¹ ë¥¸ ì‘ë‹µ ëª¨ë¸\")\n",
    "print(f\"- GPT-4o-mini: OpenAIì˜ íš¨ìœ¨ì ì¸ ëª¨ë¸\") \n",
    "print(f\"- ê²€ìƒ‰ê¸°: Chroma Vector Store (ë™ì¼í•œ ì¡°ê±´)\")\n",
    "print(f\"- ë¹„êµ í¬ì¸íŠ¸: ì‘ë‹µ í’ˆì§ˆ, ì†ë„, ì–¸ì–´ ì´í•´ë„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-SwiIBXhS50"
   },
   "source": [
    "---\n",
    "\n",
    "## **Comparison (ë¹„êµ í‰ê°€)**\n",
    "\n",
    "- LangChain **ë¹„êµ í‰ê°€ê¸°**ë¡œ ë™ì¼ ì…ë ¥ì— ëŒ€í•œ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì¶œë ¥ì„ ê°ê´€ì  ë¹„êµ\n",
    "\n",
    "- **ì„±ëŠ¥ ì°¨ì´** ë¶„ì„ì„ í†µí•´ ìµœì ì˜ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ ì„ íƒ\n",
    "\n",
    "- **A/B í…ŒìŠ¤íŠ¸**ì™€ **ì„ í˜¸ë„ ì ìˆ˜** ìƒì„±ì— í™œìš©ë˜ë©° LangSmithì™€ í†µí•©\n",
    "\n",
    "- í‰ê°€ ê²°ê³¼(í˜•ì‹):\n",
    "    - value: ì„ í˜¸ë˜ëŠ” ì‘ë‹µ ('A' ë˜ëŠ” 'B'), ì„ í˜¸ë˜ëŠ” ì‘ë‹µì´ ì—†ëŠ” ê²½ìš° ('C')\n",
    "    - score: 0 ë˜ëŠ” 1 (1ì€ ì²« ë²ˆì§¸ ì˜ˆì¸¡ì´ ì„ í˜¸ë¨ì„ ì˜ë¯¸)\n",
    "    - reasoning: í‰ê°€ ê·¼ê±°ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…\n",
    "\n",
    "- ì°¸ì¡°: https://python.langchain.com/api_reference/langchain/evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "642Qwv0vhS50"
   },
   "source": [
    "### **1) Reference-free**\n",
    "\n",
    "- **í‰ê°€ íŠ¹ì§•**: ì°¸ì¡° ë‹µë³€ ì—†ì´ ë‘ RAG ë‹µë³€ ì§ì ‘ ë¹„êµ\n",
    "- **í‰ê°€ ìš”ì†Œ**: ì‚¬ì‹¤ì„±, ê´€ë ¨ì„±, ì¼ê´€ì„± ë“± ìƒëŒ€ ë¹„êµ\n",
    "- **ì¥ì **: ì ˆëŒ€ ê¸°ì¤€ ì—†ì´ë„ RAG ì‹œìŠ¤í…œ ê°„ ì„±ëŠ¥ ì°¨ì´ íŒë‹¨ ê°€ëŠ¥ (ê°ê´€ì  ë¹„êµ ê°€ëŠ¥)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqI7dfI7hS50"
   },
   "source": [
    "`(1) A/B í…ŒìŠ¤íŠ¸ í‰ê°€ - ê¸°ë³¸ ê°œìš”`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "50t3ICgGhS51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: B (0)\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "í‰ê°€ ê·¼ê±°: ë‘ AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ë‹µë³€ì„ ë¹„êµ í‰ê°€í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "Assistant AëŠ” \"íŒŒì´ì¬ì€ ì½ê¸° ì‰½ê³  ê°„ë‹¨í•œ ë¬¸ë²•ì„ ê°€ì§„ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\"ë¼ê³  ë‹µë³€í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” íŒŒì´ì¬ì˜ ê°€ì¥ í° íŠ¹ì§• ì¤‘ í•˜ë‚˜ë¥¼ ì •í™•í•˜ê²Œ ì„¤ëª…í•œ ê¸°ë³¸ì ì¸ ë‹µë³€ì…ë‹ˆë‹¤.\n",
      "\n",
      "Assistant BëŠ” \"íŒŒì´ì¬ì€ ë™ì  íƒ€ì´í•‘ì„ ì§€ì›í•˜ëŠ” ê³ ìˆ˜ì¤€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ë°ì´í„° ê³¼í•™ê³¼ ì›¹ ê°œë°œì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\"ë¼ê³  ë‹µë³€í–ˆìŠµë‹ˆë‹¤. ì´ ë‹µë³€ì€ íŒŒì´ì¬ì˜ ê¸°ìˆ ì  íŠ¹ì§•(ê³ ìˆ˜ì¤€, ë™ì  íƒ€ì´í•‘)ê³¼ ì£¼ìš” í™œìš© ë¶„ì•¼(ë°ì´í„° ê³¼í•™, ì›¹ ê°œë°œ)ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì—¬ ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ë‘ ë‹µë³€ ëª¨ë‘ ì •í™•í•˜ì§€ë§Œ, Assistant Bì˜ ë‹µë³€ì´ íŒŒì´ì¬ì˜ ê¸°ìˆ ì  ìœ„ì¹˜ì™€ ì‹¤ì œ ì“°ì„ìƒˆê¹Œì§€ í¬í•¨í•˜ì—¬ ë” ê¹Šì´ ìˆê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ Assistant Bê°€ ë” ë‚˜ì€ ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[[B]]\n",
      "========================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ë¹„êµ í‰ê°€ê¸° ìƒì„±\n",
    "evaluator = load_evaluator(\n",
    "    \"pairwise_string\",\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\"),\n",
    "    callbacks=[langfuse_handler],\n",
    ")\n",
    "\n",
    "# ë‘ ëª¨ë¸ì˜ ì¶œë ¥ ë¹„êµ\n",
    "result = evaluator.evaluate_string_pairs(\n",
    "    prediction=\"íŒŒì´ì¬ì€ ì½ê¸° ì‰½ê³  ê°„ë‹¨í•œ ë¬¸ë²•ì„ ê°€ì§„ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
    "    prediction_b=\"íŒŒì´ì¬ì€ ë™ì  íƒ€ì´í•‘ì„ ì§€ì›í•˜ëŠ” ê³ ìˆ˜ì¤€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ë°ì´í„° ê³¼í•™ê³¼ ì›¹ ê°œë°œì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\",\n",
    "    input=\"íŒŒì´ì¬ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "print(f\"ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: {result['value']} ({result['score']})\")\n",
    "print(\"-\"*200)\n",
    "print(f\"í‰ê°€ ê·¼ê±°: {result['reasoning']}\")\n",
    "print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHhcb4Z5hS51"
   },
   "source": [
    "`(2) A/B í…ŒìŠ¤íŠ¸ í‰ê°€ - ëª¨ë¸ ë¹„êµ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-YN-jtjEhS51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.1-mini: Elon MuskëŠ” 2004ë…„ Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ íšŒì¥ ê²¸ ìµœëŒ€ ì£¼ì£¼ê°€ ë˜ì—ˆê³ , 2008ë…„ 10ì›” CEOë¡œ ì·¨ì„í–ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” Roadster ê°œë°œ ì „ëµì— ì ê·¹ ì°¸ì—¬í–ˆìœ¼ë©°, ê²½ì˜ì§„ êµì²´ ê³¼ì •ì—ì„œ Eberhardê°€ CEOì—ì„œ ë¬¼ëŸ¬ë‚˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ Eberhardê°€ Muskë¥¼ ìƒëŒ€ë¡œ ì†Œì†¡ì„ ì œê¸°í•˜ëŠ” ë“± ë…¼ë€ì— ì§ë©´í–ˆìœ¼ë‚˜, ì†Œì†¡ì€ ë‚˜ì¤‘ì— ê¸°ê°ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "Gemini-1.5-flash: Elon MuskëŠ” 2004ë…„ 750ë§Œ ë‹¬ëŸ¬ì˜ ì‹œë¦¬ì¦ˆ A ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ Teslaì˜ íšŒì¥ ê²¸ ìµœëŒ€ ì£¼ì£¼ê°€ ë˜ì—ˆê³ , 2008ë…„ 10ì›”ì— CEOì§ì„ ë§¡ì•˜ìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ 2007ë…„ 8ì›” ê³µë™ ì„¤ë¦½ìì¸ Martin Eberhardê°€ CEOì—ì„œ ë¬¼ëŸ¬ë‚˜ë¼ëŠ” ìš”ì²­ì„ ë°›ì•˜ê³ , EberhardëŠ” 2009ë…„ 6ì›” Muskë¥¼ ìƒëŒ€ë¡œ ì†Œì†¡ì„ ì œê¸°í–ˆìœ¼ë‚˜ ë‚˜ì¤‘ì— ê¸°ê°ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: B (0)\n",
      "í‰ê°€ ê·¼ê±°: ë‘ AI ì–´ì‹œìŠ¤í„´íŠ¸ ëª¨ë‘ Elon Muskì˜ Tesla ì´ˆê¸° ê´€ì—¬ì™€ ê´€ë ¨ëœ í•µì‹¬ì ì¸ ì‚¬ì‹¤ì„ ì •í™•í•˜ê²Œ ì „ë‹¬í–ˆìŠµë‹ˆë‹¤. ë‘ ë‹µë³€ ëª¨ë‘ Muskê°€ 2004ë…„ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ê³ , 2008ë…„ì— CEOê°€ ë˜ì—ˆìœ¼ë©°, ê³µë™ ì°½ì—…ì Martin Eberhardì™€ì˜ ì†Œì†¡ ë…¼ë€ì´ ìˆì—ˆê³  ê²°êµ­ ê¸°ê°ë˜ì—ˆë‹¤ëŠ” ì ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "í•˜ì§€ë§Œ Assistant BëŠ” ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ë‹µë³€ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ ê·œëª¨ë¥¼ \"750ë§Œ ë‹¬ëŸ¬ì˜ ì‹œë¦¬ì¦ˆ A\"ë¼ê³  ëª…ì‹œí•˜ê³ , Eberhardê°€ CEOì—ì„œ ë¬¼ëŸ¬ë‚œ ì‹œì ì„ \"2007ë…„ 8ì›”\", ì†Œì†¡ ì œê¸° ì‹œì ì„ \"2009ë…„ 6ì›”\"ë¡œ êµ¬ì²´ì ì¸ ë‚ ì§œë¥¼ ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì„¸ë¶€ ì •ë³´ëŠ” ì‚¬ìš©ìê°€ ì‚¬ê±´ì˜ ì „ê°œ ê³¼ì •ì„ ë” ëª…í™•í•˜ê²Œ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\n",
      "\n",
      "ë°˜ë©´ Assistant AëŠ” \"Roadster ê°œë°œ ì „ëµì— ì ê·¹ ì°¸ì—¬í–ˆë‹¤\"ëŠ” ë‚´ìš©ì„ ì¶”ê°€í–ˆì§€ë§Œ, ì „ë°˜ì ìœ¼ë¡œ Assistant Bê°€ ì œê³µí•˜ëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë‚ ì§œ ì •ë³´ê°€ ì§ˆë¬¸ì˜ í•µì‹¬ì— ë” ë¶€í•©í•˜ë©° ë” ë†’ì€ ìˆ˜ì¤€ì˜ ìƒì„¸í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
      "\n",
      "[[B]]\n",
      "========================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# LLM ëª¨ë¸ ìƒì„±\n",
    "gpt4omini_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "gemini15flash_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "# RAG ì²´ì¸ ì‹¤í–‰\n",
    "question = \"Elon MuskëŠ” Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ê³¼ ê²½ì˜ ë³€í™”ì— ì–´ë–»ê²Œ ê´€ì—¬í–ˆìœ¼ë©°, ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ë…¼ë€ì— ì§ë©´í–ˆë‚˜ìš”?\"\n",
    "gpt_response = rag_bot(\n",
    "    question=question,\n",
    "    retriever=hybrid_retriever,\n",
    "    llm=gpt4omini_llm,\n",
    "    config={\n",
    "        \"callbacks\": [langfuse_handler],\n",
    "        \"tags\": [\"rag_bot\", \"evaluation\"],\n",
    "        \"metadata\": {\n",
    "            \"model\": \"gpt-4.1-mini\",\n",
    "            \"temperature\": 0,\n",
    "            },\n",
    "        },\n",
    ")\n",
    "\n",
    "\n",
    "gemini_response = rag_bot(\n",
    "    question=question,\n",
    "    retriever=hybrid_retriever,\n",
    "    llm=gemini15flash_llm,\n",
    "    config={\n",
    "        \"callbacks\": [langfuse_handler],\n",
    "        \"tags\": [\"rag_bot\", \"evaluation\"],\n",
    "        \"metadata\": {\n",
    "            \"model\": \"gemini-2.0-flash-001\",\n",
    "            \"temperature\": 0,\n",
    "            },\n",
    "        },\n",
    ")\n",
    "\n",
    "# ë¹„êµ í‰ê°€ê¸° ìƒì„± : OpenAI gpt-4.1-mini vs Google Gemini-1.5-flash\n",
    "evaluator = load_evaluator(\n",
    "    \"pairwise_string\",\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0),  # Google Gemini-1.5-pro ë¥¼ í‰ê°€ìë¡œ ì‚¬ìš©\n",
    "    callbacks=[langfuse_handler],\n",
    "    tags=[\"rag_bot\", \"evaluation\", \"pairwise_string\"],\n",
    "    metadata={\n",
    "        \"model_a\": \"gpt-4.1-mini\",\n",
    "        \"model_b\": \"gemini-2.0-flash\",\n",
    "        \"evaluator\": \"gemini-2.0-flash\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# ë‘ ëª¨ë¸ì˜ ì¶œë ¥ ë¹„êµ\n",
    "result = evaluator.evaluate_string_pairs(\n",
    "    prediction=gpt_response[\"answer\"],\n",
    "    prediction_b=gemini_response[\"answer\"],\n",
    "    input=question\n",
    ")\n",
    "\n",
    "print(f\"gpt-4.1-mini: {gpt_response['answer']}\")\n",
    "print(f\"Gemini-1.5-flash: {gemini_response['answer']}\")\n",
    "print(\"-\"*200)\n",
    "print(f\"ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: {result['value']} ({result['score']})\")\n",
    "print(f\"í‰ê°€ ê·¼ê±°: {result['reasoning']}\")\n",
    "print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KQeKNwKhS51"
   },
   "source": [
    "---\n",
    "### **[ì‹¤ìŠµ]**\n",
    "\n",
    "- ollamaì™€ groqì—ì„œ ê°ê° 1ê°œì˜ ëª¨ë¸ì„ ì„ íƒí•˜ê³ , RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "- ë‘ ëª¨ë¸ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. (langfuse UI í™•ì¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "69pNcNNThS52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama ì„¤ì •: http://littletask.kro.kr:1410 - qwen3:30b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaydash\\AppData\\Local\\Temp\\ipykernel_1632\\661396507.py:9: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  ollama_llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq ì„¤ì •: llama3-8b-8192\n",
      "\n",
      "ì§ˆë¬¸: Teslaì˜ ì´ˆê¸° ì°½ë¦½ìëŠ” ëˆ„êµ¬ì´ë©°, ì´ë“¤ì˜ ì—­í• ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?\n",
      "====================================================================================================\n",
      "\n",
      "=== Ollama (qwen3:30b) ê²°ê³¼ ===\n",
      "ë‹µë³€: <think>\n",
      "Okay, let's tackle this query. The user is asking about Tesla's founding CEO and their role. The system says to only use the provided document, and if unsure, say \"I don't know.\"\n",
      "\n",
      "Looking at the document provided, it's empty. The [ë¬¸ì„œ] section is blank. So there's no information there to answer the question. The user might have expected the document to have details, but since it's empty, I can't reference any facts.\n",
      "\n",
      "I need to check if there's any hidden info or if maybe the document was supposed to be included but wasn't. But according to the given data, the document is empty. So, the correct response is to say I don't know. The instructions say if uncertain, respond with \"ëª¨ë¥´ê² ìŠµë‹ˆë‹¤\" which is Korean for \"I don't know.\"\n",
      "\n",
      "Wait, the user's question is in Korean, so the answer should be in Korean. The system prompt specifies to respond in Korean if the question is in Korean. The answer should be \"ëª¨ë¥´ê² ìŠµë‹ˆë‹¤\" since there's no document provided.\n",
      "\n",
      "Double-checking: the document is empty, so no data to answer. So the correct response is \"ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.\"\n",
      "</think>\n",
      "\n",
      "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== Groq (llama3-8b-8192) ê²°ê³¼ ===\n",
      "Groq ì‹¤í–‰ ì˜¤ë¥˜: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== ëª¨ë¸ ë¹„êµ ìš”ì•½ ===\n",
      "ì§ˆë¬¸: Teslaì˜ ì´ˆê¸° ì°½ë¦½ìëŠ” ëˆ„êµ¬ì´ë©°, ì´ë“¤ì˜ ì—­í• ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?\n",
      "\n",
      "Ollama (qwen3:30b): <think>\n",
      "Okay, let's tackle this query. The user is asking about Tesla's founding CEO and their role. The system says to only use the provided document, and if unsure, say \"I don't know.\"\n",
      "\n",
      "Looking at the document provided, it's empty. The [ë¬¸ì„œ] section is blank. So there's no information there to answer the question. The user might have expected the document to have details, but since it's empty, I can't reference any facts.\n",
      "\n",
      "I need to check if there's any hidden info or if maybe the document was supposed to be included but wasn't. But according to the given data, the document is empty. So, the correct response is to say I don't know. The instructions say if uncertain, respond with \"ëª¨ë¥´ê² ìŠµë‹ˆë‹¤\" which is Korean for \"I don't know.\"\n",
      "\n",
      "Wait, the user's question is in Korean, so the answer should be in Korean. The system prompt specifies to respond in Korean if the question is in Korean. The answer should be \"ëª¨ë¥´ê² ìŠµë‹ˆë‹¤\" since there's no document provided.\n",
      "\n",
      "Double-checking: the document is empty, so no data to answer. So the correct response is \"ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.\"\n",
      "</think>\n",
      "\n",
      "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤\n",
      "\n",
      "Groq (llama3-8b-8192): Groq ëª¨ë¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "\n",
      "ğŸ’¡ Langfuse UIì—ì„œ ë‘ ëª¨ë¸ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ì„¸ìš”!\n",
      "\n",
      "ğŸ“‹ í˜„ì¬ ì„¤ì • ì •ë³´:\n",
      "- Ollama: http://littletask.kro.kr:1410 (qwen3:30b)\n",
      "- Groq: llama3-8b-8192\n",
      "- Langfuse: http://shbank.kro.kr:3000\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# .envì—ì„œ Ollama ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "ollama_base_url = os.getenv(\"OLLAMA_BASE_URL\")  # http://littletask.kro.kr:1410  \n",
    "ollama_api_key = os.getenv(\"OLLAMA_API_KEY\")    # Task123!\n",
    "ollama_model = os.getenv(\"OLLAMA_MODEL\")        # qwen3:1.7b\n",
    "\n",
    "# Ollama ëª¨ë¸ ìƒì„± (ì›ê²© ì„œë²„ + API í‚¤ ì‚¬ìš©)\n",
    "ollama_llm = Ollama(\n",
    "    model=ollama_model,\n",
    "    base_url=ollama_base_url,\n",
    "    headers={\"Authorization\": f\"Bearer {ollama_api_key}\"},\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(f\"Ollama ì„¤ì •: {ollama_base_url} - {ollama_model}\")\n",
    "\n",
    "# 2) Groq ëª¨ë¸ ì„¤ì • (llama3-8b-8192 ëª¨ë¸ ì‚¬ìš©)\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Groq ëª¨ë¸ ìƒì„± (API í‚¤ í•„ìš”)\n",
    "groq_llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")  # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°\n",
    ")\n",
    "\n",
    "print(f\"Groq ì„¤ì •: llama3-8b-8192\")\n",
    "\n",
    "# 3) ë‘ ëª¨ë¸ë¡œ RAG ì²´ì¸ ì‹¤í–‰\n",
    "question = \"Teslaì˜ ì´ˆê¸° ì°½ë¦½ìëŠ” ëˆ„êµ¬ì´ë©°, ì´ë“¤ì˜ ì—­í• ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?\"\n",
    "\n",
    "print(f\"\\nì§ˆë¬¸: {question}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Ollama ëª¨ë¸ë¡œ RAG ì‹¤í–‰\n",
    "print(f\"\\n=== Ollama ({ollama_model}) ê²°ê³¼ ===\")\n",
    "try:\n",
    "    ollama_response = rag_bot(\n",
    "        question=question,\n",
    "        retriever=chroma_k,  # ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ê¸° ì‚¬ìš©\n",
    "        llm=ollama_llm,\n",
    "        config={\n",
    "            \"callbacks\": [langfuse_handler],\n",
    "            \"tags\": [\"rag_bot\", \"evaluation\", \"ollama\"],\n",
    "            \"metadata\": {\n",
    "                \"model\": ollama_model,\n",
    "                \"temperature\": 0,\n",
    "                \"provider\": \"ollama\",\n",
    "                \"base_url\": ollama_base_url\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    print(f\"ë‹µë³€: {ollama_response['answer']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ollama ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "    ollama_response = {\"answer\": f\"Ollama ëª¨ë¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\"}\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# Groq ëª¨ë¸ë¡œ RAG ì‹¤í–‰\n",
    "print(\"=== Groq (llama3-8b-8192) ê²°ê³¼ ===\")\n",
    "try:\n",
    "    groq_response = rag_bot(\n",
    "        question=question,\n",
    "        retriever=chroma_k,  # ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ê¸° ì‚¬ìš©\n",
    "        llm=groq_llm,\n",
    "        config={\n",
    "            \"callbacks\": [langfuse_handler],\n",
    "            \"tags\": [\"rag_bot\", \"evaluation\", \"groq\"],\n",
    "            \"metadata\": {\n",
    "                \"model\": \"llama3-8b-8192\",\n",
    "                \"temperature\": 0,\n",
    "                \"provider\": \"groq\"\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    print(f\"ë‹µë³€: {groq_response['answer']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Groq ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "    groq_response = {\"answer\": f\"Groq ëª¨ë¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\"}\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# 4) ë‘ ëª¨ë¸ ê²°ê³¼ ë¹„êµ ì¶œë ¥\n",
    "print(\"=== ëª¨ë¸ ë¹„êµ ìš”ì•½ ===\")\n",
    "print(f\"ì§ˆë¬¸: {question}\")\n",
    "print(f\"\\nOllama ({ollama_model}): {ollama_response['answer']}\")\n",
    "print(f\"\\nGroq (llama3-8b-8192): {groq_response['answer']}\")\n",
    "print(\"\\nğŸ’¡ Langfuse UIì—ì„œ ë‘ ëª¨ë¸ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ì„¸ìš”!\")\n",
    "\n",
    "# ì„¤ì • ì •ë³´ ì¶œë ¥\n",
    "print(\"\\nğŸ“‹ í˜„ì¬ ì„¤ì • ì •ë³´:\")\n",
    "print(f\"- Ollama: {ollama_base_url} ({ollama_model})\")\n",
    "print(f\"- Groq: llama3-8b-8192\")\n",
    "print(f\"- Langfuse: {os.getenv('LANGFUSE_HOST')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nWicV_JhS52"
   },
   "source": [
    "`(3) A/B í…ŒìŠ¤íŠ¸ í‰ê°€ - í”„ë¡¬í”„íŠ¸ ë¹„êµ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "a0YFHVtRhS52"
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import List, Dict\n",
    "\n",
    "def rag_bot_b(\n",
    "    question: str,\n",
    "    retriever: BaseRetriever,\n",
    "    llm: BaseChatModel,\n",
    "    config: RunnableConfig | None = None,\n",
    ") -> Dict[str, str | List[Document]]:\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ê²€ìƒ‰ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    system_prompt = f\"\"\"RAG Assistant to answer questions based on provided documents.\n",
    "\n",
    "    Guidelines:\n",
    "    - Reference only provided context\n",
    "    - Reply \"I don't know\" if uncertain\n",
    "    - Keep responses under 3 sentences\n",
    "\n",
    "    Context:\n",
    "    {context}\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\\n[Question]{question}\\n\\n[Answer]\\n\"},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    docqa_chain = {\n",
    "        \"context\": lambda x: context,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"docs\": lambda x: docs,\n",
    "    } | RunnableParallel({\n",
    "        \"answer\": prompt | llm | StrOutputParser(),\n",
    "        \"documents\": lambda x: x[\"docs\"],\n",
    "    })\n",
    "\n",
    "    return docqa_chain.invoke(question, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RxvY5DH1hS52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.1-mini: Elon MuskëŠ” 2004ë…„ Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ íšŒì¥ ê²¸ ìµœëŒ€ ì£¼ì£¼ê°€ ë˜ì—ˆê³ , 2008ë…„ 10ì›” CEOë¡œ ì·¨ì„í–ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” Roadster ê°œë°œ ì „ëµì— ì ê·¹ ì°¸ì—¬í–ˆìœ¼ë©°, ê²½ì˜ì§„ êµì²´ ê³¼ì •ì—ì„œ Eberhardê°€ CEOì—ì„œ ë¬¼ëŸ¬ë‚˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ Eberhardê°€ Muskë¥¼ ìƒëŒ€ë¡œ ì†Œì†¡ì„ ì œê¸°í•˜ëŠ” ë“± ë…¼ë€ì— ì§ë©´í–ˆìœ¼ë‚˜, ì†Œì†¡ì€ ë‚˜ì¤‘ì— ê¸°ê°ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "gpt-4.1-mini (Prompt B): Elon MuskëŠ” 2004ë…„ Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ íšŒì¥ ê²¸ ìµœëŒ€ ì£¼ì£¼ê°€ ë˜ì—ˆê³ , 2008ë…„ 10ì›” CEOë¡œ ì¸ìˆ˜í–ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” í”„ë¦¬ë¯¸ì—„ ìŠ¤í¬ì¸ ì¹´ ì „ëµì— ì ê·¹ ì°¸ì—¬í–ˆìœ¼ë©°, ê²½ì˜ì§„ êµì²´ ê³¼ì •ì—ì„œ Eberhardê°€ CEOì—ì„œ ë¬¼ëŸ¬ë‚˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ EberhardëŠ” 2009ë…„ Muskë¥¼ ìƒëŒ€ë¡œ ì†Œì†¡ì„ ì œê¸°í–ˆìœ¼ë‚˜ ë‚˜ì¤‘ì— ê¸°ê°ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: A (1)\n",
      "í‰ê°€ ê·¼ê±°: ë‘ AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ë‹µë³€ì€ Elon Muskì˜ Tesla ì´ˆê¸° ê´€ì—¬ì— ëŒ€í•œ í•µì‹¬ì ì¸ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‘ ë‹µë³€ ëª¨ë‘ Muskê°€ 2004ë…„ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ê³ , 2008ë…„ì— CEOê°€ ë˜ì—ˆìœ¼ë©°, ê³µë™ ì°½ì—…ìì¸ Martin Eberhardë¥¼ ì¶•ì¶œí•˜ëŠ” ê³¼ì •ì—ì„œ ë²•ì  ë¶„ìŸì— íœ˜ë§ë ¸ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì˜¬ë°”ë¥´ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "í•˜ì§€ë§Œ ì„¸ë¶€ì ì¸ í‘œí˜„ì—ì„œ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. Assistant AëŠ” Muskê°€ CEOê°€ ëœ ê²ƒì„ 'ì·¨ì„í–ˆë‹¤'ê³  í‘œí˜„í•œ ë°˜ë©´, Assistant BëŠ” 'ì¸ìˆ˜í–ˆë‹¤'ê³  í‘œí˜„í–ˆìŠµë‹ˆë‹¤. 'ì·¨ì„í–ˆë‹¤'ëŠ” ì§ì±…ì„ ë§¡ì•˜ë‹¤ëŠ” ì˜ë¯¸ë¡œ ì´ ë¬¸ë§¥ì— ë” ì •í™•í•œ í‘œí˜„ì…ë‹ˆë‹¤. 'ì¸ìˆ˜í–ˆë‹¤'ëŠ” ë³´í†µ íšŒì‚¬ ìì²´ë¥¼ ì‚¬ë“¤ì¼ ë•Œ ì‚¬ìš©í•˜ëŠ” í‘œí˜„ì´ë¯€ë¡œ ì ì ˆí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ì–¸ì–´ì  ì •í™•ì„±ì„ ê³ ë ¤í•  ë•Œ, Assistant Aì˜ ë‹µë³€ì´ ë¯¸ì„¸í•˜ê²Œ ë” ìš°ìˆ˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "[[A]]\n",
      "========================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# RAG ì²´ì¸ ì‹¤í–‰\n",
    "question = \"Elon MuskëŠ” Teslaì˜ ì´ˆê¸° ìê¸ˆ ì¡°ë‹¬ê³¼ ê²½ì˜ ë³€í™”ì— ì–´ë–»ê²Œ ê´€ì—¬í–ˆìœ¼ë©°, ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ë…¼ë€ì— ì§ë©´í–ˆë‚˜ìš”?\"\n",
    "gpt_prompt_b_response = rag_bot_b(\n",
    "    question=question,\n",
    "    retriever=hybrid_retriever,\n",
    "    llm=gpt4omini_llm,\n",
    "    config={\n",
    "        \"callbacks\": [langfuse_handler],\n",
    "        \"tags\": [\"rag_bot\", \"evaluation\", \"prompt_b\"],\n",
    "        \"metadata\": {\n",
    "            \"model\": \"gpt-4.1-mini\",\n",
    "            \"temperature\": 0,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "# ë‘ ëª¨ë¸ì˜ ì¶œë ¥ ë¹„êµ (í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¹„êµ)\n",
    "result = evaluator.evaluate_string_pairs(\n",
    "    prediction=gpt_response[\"answer\"],\n",
    "    prediction_b=gpt_prompt_b_response[\"answer\"],\n",
    "    input=question\n",
    ")\n",
    "\n",
    "print(f\"gpt-4.1-mini: {gpt_response['answer']}\")\n",
    "print(f\"gpt-4.1-mini (Prompt B): {gpt_prompt_b_response['answer']}\")\n",
    "print(\"-\"*200)\n",
    "print(f\"ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: {result['value']} ({result['score']})\")\n",
    "print(f\"í‰ê°€ ê·¼ê±°: {result['reasoning']}\")\n",
    "print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Abad9FlkhS53"
   },
   "source": [
    "---\n",
    "### **[ì‹¤ìŠµ]**\n",
    "\n",
    "- ë‘ ê°€ì§€ ë²„ì „ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ê³ , ê°ê° ë³„ë„ì˜ RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤. (ëª¨ë¸ì€ ê³µí†µ ì ìš©)\n",
    "- ë‘ ê°€ì§€ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. (langfuse UI í™•ì¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ybgqIlhjhS53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: TeslaëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ë…¼ë€ì— ì§ë©´í–ˆë‚˜ìš”?\n",
      "====================================================================================================\n",
      "\n",
      "=== í”„ë¡¬í”„íŠ¸ ë²„ì „ 1: í•œêµ­ì–´ ì¹œí™”ì  ìŠ¤íƒ€ì¼ ===\n",
      "â±ï¸ ì‘ë‹µ ì‹œê°„: 2.19ì´ˆ\n",
      "ğŸ’¬ ë‹µë³€: TeslaëŠ” ì„±í¬ë¡±, ë…¸ë™ ë¶„ìŸ, ì‚¬ê¸° í˜ì˜, ì§€ì  ì¬ì‚°ê¶Œ, í™˜ê²½ ìœ„ë°˜, ì¸ì¢… ì°¨ë³„ ë“± ë‹¤ì–‘í•œ ë…¼ë€ì— ì§ë©´í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë°ì´í„° ê°œì¸ ì •ë³´ ë³´í˜¸, ì°¨ëŸ‰ ì œí’ˆ ë¬¸ì œ, Autopilot ì¶©ëŒ, ì†Œí”„íŠ¸ì›¨ì–´ í•´í‚¹ê³¼ ê°™ì€ ë¹„íŒë„ ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë“¤ì€ íšŒì‚¬ì˜ ì´ë¯¸ì§€ì™€ ìš´ì˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== í”„ë¡¬í”„íŠ¸ ë²„ì „ 2: ê°„ê²°í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ìŠ¤íƒ€ì¼ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ ì‘ë‹µ ì‹œê°„: 2.60ì´ˆ\n",
      "ğŸ’¬ ë‹µë³€: TeslaëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë…¼ë€ì— ì§ë©´í–ˆìŠµë‹ˆë‹¤:\n",
      "- ì„±í¬ë¡±\n",
      "- ë…¸ë™ ë¶„ìŸ\n",
      "- ì‚¬ê¸° í˜ì˜\n",
      "- ëŒ€ë¦¬ì  ë¶„ìŸ\n",
      "- ì§€ì  ì¬ì‚°ê¶Œ\n",
      "- í™˜ê²½ ìœ„ë°˜\n",
      "- ì¬ì‚° í”¼í•´\n",
      "- ì¸ì¢… ì°¨ë³„\n",
      "- COVID-19 íŒ¬ë°ë¯¹ ëŒ€ì‘\n",
      "- ìˆ˜ë¦¬ ê¶Œë¦¬\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== ğŸ“Š í”„ë¡¬í”„íŠ¸ ë¹„êµ ë¶„ì„ ===\n",
      "â“ ì§ˆë¬¸: TeslaëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ë…¼ë€ì— ì§ë©´í–ˆë‚˜ìš”?\n",
      "\n",
      "ğŸ“ í•œêµ­ì–´ ì¹œí™”ì  ìŠ¤íƒ€ì¼:\n",
      "   â±ï¸ ì‘ë‹µì‹œê°„: 2.19ì´ˆ\n",
      "   ğŸ’¬ ë‹µë³€: TeslaëŠ” ì„±í¬ë¡±, ë…¸ë™ ë¶„ìŸ, ì‚¬ê¸° í˜ì˜, ì§€ì  ì¬ì‚°ê¶Œ, í™˜ê²½ ìœ„ë°˜, ì¸ì¢… ì°¨ë³„ ë“± ë‹¤ì–‘í•œ ë…¼ë€ì— ì§ë©´í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë°ì´í„° ê°œì¸ ì •ë³´ ë³´í˜¸, ì°¨ëŸ‰ ì œí’ˆ ë¬¸ì œ, Autopilot ì¶©ëŒ, ì†Œí”„íŠ¸ì›¨ì–´ í•´í‚¹ê³¼ ê°™ì€ ë¹„íŒë„ ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë“¤ì€ íšŒì‚¬ì˜ ì´ë¯¸ì§€ì™€ ìš´ì˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°„ê²° ìŠ¤íƒ€ì¼:\n",
      "   â±ï¸ ì‘ë‹µì‹œê°„: 2.60ì´ˆ\n",
      "   ğŸ’¬ ë‹µë³€: TeslaëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë…¼ë€ì— ì§ë©´í–ˆìŠµë‹ˆë‹¤:\n",
      "- ì„±í¬ë¡±\n",
      "- ë…¸ë™ ë¶„ìŸ\n",
      "- ì‚¬ê¸° í˜ì˜\n",
      "- ëŒ€ë¦¬ì  ë¶„ìŸ\n",
      "- ì§€ì  ì¬ì‚°ê¶Œ\n",
      "- í™˜ê²½ ìœ„ë°˜\n",
      "- ì¬ì‚° í”¼í•´\n",
      "- ì¸ì¢… ì°¨ë³„\n",
      "- COVID-19 íŒ¬ë°ë¯¹ ëŒ€ì‘\n",
      "- ìˆ˜ë¦¬ ê¶Œë¦¬\n",
      "\n",
      "ğŸ† í‰ê°€ ê²°ê³¼:\n",
      "   ì„ í˜¸ë˜ëŠ” ìŠ¤íƒ€ì¼: í•œêµ­ì–´ ì¹œí™”ì \n",
      "   í‰ê°€ ì ìˆ˜: 1\n",
      "   ğŸ“ í‰ê°€ ê·¼ê±°: ë‘ AI ì–´ì‹œìŠ¤í„´íŠ¸ ëª¨ë‘ Teslaê°€ ì§ë©´í•œ ë‹¤ì–‘í•œ ë…¼ë€ì„ ì •í™•í•˜ê²Œ ë‚˜ì—´í–ˆìŠµë‹ˆë‹¤. ë‘ ë‹µë³€ ëª¨ë‘ ìœ ìš©í•˜ê³  ì •í™•í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸ AëŠ” ë…¼ë€ì„ ë²”ì£¼í™”í•˜ì—¬ ì œì‹œí•©ë‹ˆë‹¤. ì„±í¬ë¡±, ë…¸ë™ ë¶„ìŸ, ì‚¬ê¸°, ì¸ì¢… ì°¨ë³„ê³¼ ê°™ì€ ê¸°ì—… ìš´ì˜ ê´€ë ¨ ë¬¸ì œì™€ ë°ì´í„° ê°œì¸ ì •ë³´ ë³´í˜¸, ì°¨ëŸ‰ ì œí’ˆ ë¬¸ì œ, ì˜¤í† íŒŒì¼ëŸ¿ ì¶©ëŒ, ì†Œí”„íŠ¸ì›¨ì–´ í•´í‚¹ê³¼ ê°™ì€ ì œí’ˆ ë° ê¸°ìˆ  ê´€ë ¨ ë¹„íŒì„ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤. íŠ¹íˆ Teslaì˜ í•µì‹¬ ê¸°ìˆ ì¸ 'ì˜¤í† íŒŒì¼ëŸ¿ ì¶©ëŒ'ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì—¬ ë¬¸ì œì˜ í•µì‹¬ì„ ì˜ ì§šì–´ëƒˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸ BëŠ” ë…¼ë€ì„ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì œì‹œí•˜ì—¬ ê°€ë…ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤. ì–´ì‹œìŠ¤í„´íŠ¸ Aê°€ ì–¸ê¸‰í•˜ì§€ ì•Šì€ 'ëŒ€ë¦¬ì  ë¶„ìŸ', 'COVID-19 íŒ¬ë°ë¯¹ ëŒ€ì‘', 'ìˆ˜ë¦¬ ê¶Œë¦¬'ì™€ ê°™ì€ êµ¬ì²´ì ì¸ ì‚¬ì•ˆë“¤ì„ í¬í•¨í•˜ê³  ìˆì–´ ë” í­ë„“ì€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ë‘ ë‹µë³€ ëª¨ë‘ í›Œë¥­í•˜ì§€ë§Œ, ì–´ì‹œìŠ¤í„´íŠ¸ AëŠ” ë…¼ë€ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  Teslaì˜ ê°€ì¥ í•µì‹¬ì ì¸ ë…¼ë€ ì¤‘ í•˜ë‚˜ì¸ ì˜¤í† íŒŒì¼ëŸ¿ ë¬¸ì œë¥¼ ëª…í™•íˆ ì–¸ê¸‰í–ˆë‹¤ëŠ” ì ì—ì„œ ì¡°ê¸ˆ ë” ê¹Šì´ ìˆëŠ” ë‹µë³€ì„ ì œê³µí–ˆë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤.\n",
      "\n",
      "[[A]]\n",
      "\n",
      "ğŸ’¡ Langfuse UIì—ì„œ ë‘ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ì˜ ì°¨ì´ì ì„ í™•ì¸í•´ë³´ì„¸ìš”!\n",
      "ğŸ”— Langfuse: http://shbank.kro.kr:3000\n",
      "\n",
      "ğŸ“‹ í”„ë¡¬í”„íŠ¸ ë¹„êµ ìš”ì•½:\n",
      "- í•œêµ­ì–´ ì¹œí™”ì : ì •ì¤‘í•œ ì¡´ëŒ“ë§, ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„\n",
      "- ë¹„ì¦ˆë‹ˆìŠ¤ ìŠ¤íƒ€ì¼: ê°„ê²°í•¨, ì§ì„¤ì  ì •ë³´ ì „ë‹¬, ì˜ì–´ í‚¤ì›Œë“œ ì‚¬ìš©\n",
      "- ê³µí†µì : ë™ì¼í•œ ëª¨ë¸(GPT-4o-mini), ë™ì¼í•œ ê²€ìƒ‰ê¸°(Hybrid)\n",
      "- ì°¨ì´ì : ì–¸ì–´ ìŠ¤íƒ€ì¼, ì‘ë‹µ ê¸¸ì´, í‘œí˜„ ë°©ì‹\n"
     ]
    }
   ],
   "source": [
    "# ë‘ ê°€ì§€ í”„ë¡¬í”„íŠ¸ ë²„ì „ ë¹„êµ ì‹¤ìŠµ\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# ê³µí†µ ëª¨ë¸ ì„¤ì • (GPT-4o-mini ì‚¬ìš©)\n",
    "common_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ë²„ì „ 1: í•œêµ­ì–´ ì¹œí™”ì  ìŠ¤íƒ€ì¼\n",
    "def rag_bot_korean_style(\n",
    "    question: str,\n",
    "    retriever,\n",
    "    llm,\n",
    "    config=None,\n",
    "):\n",
    "    \"\"\"í•œêµ­ì–´ ì¹œí™”ì  í”„ë¡¬í”„íŠ¸ ë²„ì „\"\"\"\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    system_prompt = f\"\"\"ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "**ì‘ë‹µ ê·œì¹™:**\n",
    "- ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”\n",
    "- í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ \"í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”  \n",
    "- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ 3ë¬¸ì¥ ì´ë‚´ë¡œ ì„¤ëª…í•˜ì„¸ìš”\n",
    "- ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì—¬ ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”\n",
    "\n",
    "**ì°¸ê³  ë¬¸ì„œ:**\n",
    "{context}\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"ì§ˆë¬¸: {question}\\n\\në‹µë³€ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\"},\n",
    "    ])\n",
    "\n",
    "    chain = {\n",
    "        \"context\": lambda x: context,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"docs\": lambda x: docs,\n",
    "    } | RunnableParallel({\n",
    "        \"answer\": prompt | llm | StrOutputParser(),\n",
    "        \"documents\": lambda x: x[\"docs\"],\n",
    "    })\n",
    "\n",
    "    return chain.invoke(question, config=config)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ë²„ì „ 2: ê°„ê²°í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ìŠ¤íƒ€ì¼  \n",
    "def rag_bot_business_style(\n",
    "    question: str,\n",
    "    retriever,\n",
    "    llm,\n",
    "    config=None,\n",
    "):\n",
    "    \"\"\"ê°„ê²°í•œ ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡¬í”„íŠ¸ ë²„ì „\"\"\"\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    system_prompt = f\"\"\"Business Document AI Assistant\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer based ONLY on provided documents\n",
    "- State \"Information unavailable\" if uncertain\n",
    "- Maximum 2 sentences, factual and direct\n",
    "- Use bullet points for multiple items\n",
    "\n",
    "CONTEXT:\n",
    "{context}\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Query: {question}\\n\\nResponse:\"},\n",
    "    ])\n",
    "\n",
    "    chain = {\n",
    "        \"context\": lambda x: context,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"docs\": lambda x: docs,\n",
    "    } | RunnableParallel({\n",
    "        \"answer\": prompt | llm | StrOutputParser(),\n",
    "        \"documents\": lambda x: x[\"docs\"],\n",
    "    })\n",
    "\n",
    "    return chain.invoke(question, config=config)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "test_question = \"TeslaëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ë…¼ë€ì— ì§ë©´í–ˆë‚˜ìš”?\"\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_question}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ë²„ì „ 1 ì‹¤í–‰ (í•œêµ­ì–´ ì¹œí™”ì )\n",
    "print(\"\\n=== í”„ë¡¬í”„íŠ¸ ë²„ì „ 1: í•œêµ­ì–´ ì¹œí™”ì  ìŠ¤íƒ€ì¼ ===\")\n",
    "try:\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    korean_response = rag_bot_korean_style(\n",
    "        question=test_question,\n",
    "        retriever=hybrid_retriever,\n",
    "        llm=common_llm,\n",
    "        config={\n",
    "            \"callbacks\": [langfuse_handler],\n",
    "            \"tags\": [\"rag_bot\", \"prompt_comparison\", \"korean_style\"],\n",
    "            \"metadata\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"prompt_version\": \"korean_friendly\",\n",
    "                \"style\": \"polite_korean\"\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    korean_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ ì‘ë‹µ ì‹œê°„: {korean_time:.2f}ì´ˆ\")\n",
    "    print(f\"ğŸ’¬ ë‹µë³€: {korean_response['answer']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í•œêµ­ì–´ ìŠ¤íƒ€ì¼ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "    korean_response = {\"answer\": f\"ì˜¤ë¥˜ ë°œìƒ: {e}\"}\n",
    "    korean_time = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ë²„ì „ 2 ì‹¤í–‰ (ë¹„ì¦ˆë‹ˆìŠ¤ ìŠ¤íƒ€ì¼)\n",
    "print(\"=== í”„ë¡¬í”„íŠ¸ ë²„ì „ 2: ê°„ê²°í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ìŠ¤íƒ€ì¼ ===\")\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    business_response = rag_bot_business_style(\n",
    "        question=test_question,\n",
    "        retriever=hybrid_retriever,\n",
    "        llm=common_llm,\n",
    "        config={\n",
    "            \"callbacks\": [langfuse_handler],\n",
    "            \"tags\": [\"rag_bot\", \"prompt_comparison\", \"business_style\"],\n",
    "            \"metadata\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"prompt_version\": \"business_concise\",\n",
    "                \"style\": \"professional_brief\"\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    business_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ ì‘ë‹µ ì‹œê°„: {business_time:.2f}ì´ˆ\")\n",
    "    print(f\"ğŸ’¬ ë‹µë³€: {business_response['answer']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë¹„ì¦ˆë‹ˆìŠ¤ ìŠ¤íƒ€ì¼ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "    business_response = {\"answer\": f\"ì˜¤ë¥˜ ë°œìƒ: {e}\"}\n",
    "    business_time = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¹„êµ í‰ê°€\n",
    "print(\"=== ğŸ“Š í”„ë¡¬í”„íŠ¸ ë¹„êµ ë¶„ì„ ===\")\n",
    "\n",
    "# í‰ê°€ê¸° ìƒì„± (gemini-2.5-proë¡œ ìˆ˜ì •)\n",
    "from langchain.evaluation import load_evaluator\n",
    "evaluator = load_evaluator(\n",
    "    \"pairwise_string\",\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\"),\n",
    "    callbacks=[langfuse_handler],\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ë‘ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ë¹„êµ\n",
    "    comparison_result = evaluator.evaluate_string_pairs(\n",
    "        prediction=korean_response[\"answer\"],\n",
    "        prediction_b=business_response[\"answer\"],\n",
    "        input=test_question\n",
    "    )\n",
    "    \n",
    "    print(f\"â“ ì§ˆë¬¸: {test_question}\")\n",
    "    print(f\"\\nğŸ“ í•œêµ­ì–´ ì¹œí™”ì  ìŠ¤íƒ€ì¼:\")\n",
    "    print(f\"   â±ï¸ ì‘ë‹µì‹œê°„: {korean_time:.2f}ì´ˆ\")\n",
    "    print(f\"   ğŸ’¬ ë‹µë³€: {korean_response['answer']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°„ê²° ìŠ¤íƒ€ì¼:\")\n",
    "    print(f\"   â±ï¸ ì‘ë‹µì‹œê°„: {business_time:.2f}ì´ˆ\")\n",
    "    print(f\"   ğŸ’¬ ë‹µë³€: {business_response['answer']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ† í‰ê°€ ê²°ê³¼:\")\n",
    "    print(f\"   ì„ í˜¸ë˜ëŠ” ìŠ¤íƒ€ì¼: {'í•œêµ­ì–´ ì¹œí™”ì ' if comparison_result['value'] == 'A' else 'ë¹„ì¦ˆë‹ˆìŠ¤ ê°„ê²°'}\")\n",
    "    print(f\"   í‰ê°€ ì ìˆ˜: {comparison_result['score']}\")\n",
    "    print(f\"   ğŸ“ í‰ê°€ ê·¼ê±°: {comparison_result['reasoning']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í‰ê°€ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Langfuse UIì—ì„œ ë‘ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ì˜ ì°¨ì´ì ì„ í™•ì¸í•´ë³´ì„¸ìš”!\")\n",
    "print(f\"ğŸ”— Langfuse: {os.getenv('LANGFUSE_HOST')}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ í”„ë¡¬í”„íŠ¸ ë¹„êµ ìš”ì•½:\")\n",
    "print(f\"- í•œêµ­ì–´ ì¹œí™”ì : ì •ì¤‘í•œ ì¡´ëŒ“ë§, ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„\")\n",
    "print(f\"- ë¹„ì¦ˆë‹ˆìŠ¤ ìŠ¤íƒ€ì¼: ê°„ê²°í•¨, ì§ì„¤ì  ì •ë³´ ì „ë‹¬, ì˜ì–´ í‚¤ì›Œë“œ ì‚¬ìš©\")\n",
    "print(f\"- ê³µí†µì : ë™ì¼í•œ ëª¨ë¸(GPT-4o-mini), ë™ì¼í•œ ê²€ìƒ‰ê¸°(Hybrid)\")\n",
    "print(f\"- ì°¨ì´ì : ì–¸ì–´ ìŠ¤íƒ€ì¼, ì‘ë‹µ ê¸¸ì´, í‘œí˜„ ë°©ì‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXqq5bynhS53"
   },
   "source": [
    "### **2) Reference-based**\n",
    "\n",
    "- **ê¸°ì¤€ í™œìš©**: ì°¸ì¡° ë‹µì•ˆê³¼ RAG ì‘ë‹µì„ ë¹„êµ í‰ê°€\n",
    "- **í‰ê°€ ë°©ì‹**: ìë™í™”ëœ A/B í…ŒìŠ¤íŠ¸ë¡œ ê°ê´€ì  ì„±ëŠ¥ ì¸¡ì •\n",
    "- **ì£¼ìš” ì§€í‘œ**: ì •í™•ë„, ì™„ì„±ë„, ê´€ë ¨ì„± ë“± ì •ëŸ‰ì  í‰ê°€\n",
    "- ì°¸ì¡° ë‹µì•ˆ ê¸°ë°˜ **ì²´ê³„ì ì¸ í’ˆì§ˆ í‰ê°€** ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA6G2-r9hS53"
   },
   "source": [
    "`(1) A/B í…ŒìŠ¤íŠ¸ í‰ê°€ - ëª¨ë¸ ë¹„êµ`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YFggp06hS54"
   },
   "source": [
    "### **[ì‹¤ìŠµ]**\n",
    "\n",
    "- í…ŒìŠ¤íŠ¸ì…‹(df_qa_test)ì—ì„œ í•˜ë‚˜ì˜ ìƒ˜í”Œì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "- ì´ ìƒ˜í”Œì— ëŒ€í•œ RAG ë‹µë³€ì„ ë‘ ê°€ì§€ ëª¨ë¸ë¡œë¶€í„° êµ¬í•©ë‹ˆë‹¤.\n",
    "- ë‘ ê°€ì§€ ì‹¤í–‰ê²°ê³¼ì— ëŒ€í•œ A/B í…ŒìŠ¤íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (langfuse UI í™•ì¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "OdWSL7QwhS54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ğŸ“Š í…ŒìŠ¤íŠ¸ì…‹ ìƒ˜í”Œ ì„ íƒ ===\n",
      "ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ í¬ê¸°: 49ê°œ\n",
      "\n",
      "ì„ íƒëœ ìƒ˜í”Œ (ì¸ë±ìŠ¤ 5):\n",
      "â“ ì§ˆë¬¸: Teslaì˜ Model 3ì— ëŒ€í•œ ì •ë³´ë¥¼ ì–´ë””ì„œ ì°¾ì„ ìˆ˜ ìˆë‚˜ìš”?\n",
      "âœ… ì°¸ì¡° ë‹µë³€: ì œê³µëœ ë¬¸ë§¥ì—ëŠ” Teslaì˜ Model 3ì— ëŒ€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "====================================================================================================\n",
      "\n",
      "=== GPT-4o-mini ëª¨ë¸ ë‹µë³€ ===\n",
      "â±ï¸ ì‘ë‹µ ì‹œê°„: 3.48ì´ˆ\n",
      "ğŸ’¬ ë‹µë³€: Teslaì˜ Model 3ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Model 3ëŠ” 2016ë…„ 4ì›”ì— ê³µê°œë˜ì—ˆìœ¼ë©°, \"ìƒì‚° ì§€ì˜¥\"ìœ¼ë¡œ ë¬˜ì‚¬ëœ ìƒì‚° ë¬¸ì œë¡œ ì¸í•´ ì§€ì—°ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. 2018ë…„ ë§ê¹Œì§€ Model 3ëŠ” ì„¸ê³„ì—ì„œ ê°€ì¥ ë§ì´ íŒ”ë¦° ì „ê¸° ìë™ì°¨ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== Gemini-2.0-flash-001 ëª¨ë¸ ë‹µë³€ ===\n",
      "â±ï¸ ì‘ë‹µ ì‹œê°„: 2.77ì´ˆ\n",
      "ğŸ’¬ ë‹µë³€: Teslaì˜ Model 3 ì„¸ë‹¨ì€ 2016ë…„ 4ì›”ì— ê³µê°œë˜ì—ˆìœ¼ë©°, 2018ë…„ ë§ê¹Œì§€ ì„¸ê³„ì—ì„œ ê°€ì¥ ë§ì´ íŒ”ë¦° ì „ê¸° ìë™ì°¨ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== ğŸ† Reference-based A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: Teslaì˜ Model 3ì— ëŒ€í•œ ì •ë³´ë¥¼ ì–´ë””ì„œ ì°¾ì„ ìˆ˜ ìˆë‚˜ìš”?\n",
      "ğŸ“– ì°¸ì¡° ë‹µë³€: ì œê³µëœ ë¬¸ë§¥ì—ëŠ” Teslaì˜ Model 3ì— ëŒ€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ¤– GPT-4o-mini ë‹µë³€:\n",
      "   â±ï¸ ì‹œê°„: 3.48ì´ˆ\n",
      "   ğŸ’¬ ë‚´ìš©: Teslaì˜ Model 3ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Model 3ëŠ” 2016ë…„ 4ì›”ì— ê³µê°œë˜ì—ˆìœ¼ë©°, \"ìƒì‚° ì§€ì˜¥\"ìœ¼ë¡œ ë¬˜ì‚¬ëœ ìƒì‚° ë¬¸ì œë¡œ ì¸í•´ ì§€ì—°ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. 2018ë…„ ë§ê¹Œì§€ Model 3ëŠ” ì„¸ê³„ì—ì„œ ê°€ì¥ ë§ì´ íŒ”ë¦° ì „ê¸° ìë™ì°¨ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "âœ¨ Gemini-2.0-flash-001 ë‹µë³€:\n",
      "   â±ï¸ ì‹œê°„: 2.77ì´ˆ\n",
      "   ğŸ’¬ ë‚´ìš©: Teslaì˜ Model 3 ì„¸ë‹¨ì€ 2016ë…„ 4ì›”ì— ê³µê°œë˜ì—ˆìœ¼ë©°, 2018ë…„ ë§ê¹Œì§€ ì„¸ê³„ì—ì„œ ê°€ì¥ ë§ì´ íŒ”ë¦° ì „ê¸° ìë™ì°¨ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ† í‰ê°€ ê²°ê³¼ (ì°¸ì¡° ë‹µì•ˆ ê¸°ì¤€):\n",
      "   ì„ í˜¸ ëª¨ë¸: Gemini-2.0-flash-001\n",
      "   í‰ê°€ ì ìˆ˜: 0.5\n",
      "   ğŸ“ í‰ê°€ ê·¼ê±°:\n",
      "   ë‘ AI ì–´ì‹œìŠ¤í„´íŠ¸ ëª¨ë‘ ì œê³µëœ ë¬¸ì„œì— Tesla Model 3ì— ëŒ€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ì˜ëª» ì£¼ì¥í•©ë‹ˆë‹¤. ì°¸ì¡° ë‹µë³€ì— ë”°ë¥´ë©´, í•´ë‹¹ ì •ë³´ëŠ” ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‘ ì‘ë‹µ ëª¨ë‘ ì§ˆë¬¸ì˜ í•µì‹¬ì ì¸ ì¸¡ë©´ì—ì„œ ë¶€ì •í™•í•©ë‹ˆë‹¤. ë‘ ì–´ì‹œìŠ¤í„´íŠ¸ ëª¨ë‘ ë™ì¼í•œ ì‹¤ìˆ˜ë¥¼ ì €ì§ˆë €ê¸° ë•Œë¬¸ì— ì–´ëŠ í•œìª½ì´ ë” ë‚«ë‹¤ê³  í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "[[C]]\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== ğŸ¯ Reference-free A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ===\n",
      "ğŸ† í‰ê°€ ê²°ê³¼ (ì§ì ‘ ë¹„êµ):\n",
      "   ì„ í˜¸ ëª¨ë¸: GPT-4o-mini\n",
      "   í‰ê°€ ì ìˆ˜: 1\n",
      "   ğŸ“ í‰ê°€ ê·¼ê±°:\n",
      "   ë‘ AI ì–´ì‹œìŠ¤í„´íŠ¸ ëª¨ë‘ Tesla Model 3ì— ëŒ€í•œ ì •ë³´ê°€ ì œê³µëœ ë¬¸ì„œì— ìˆë‹¤ëŠ” ì ì„ ì •í™•íˆ ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ë‘ ì–´ì‹œìŠ¤í„´íŠ¸ ëª¨ë‘ ë¬¸ì„œì—ì„œ ì°¾ì€ ê´€ë ¨ ì •ë³´ë¥¼ ìš”ì•½í•˜ì—¬ ì œê³µí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "Assistant AëŠ” Model 3ì˜ ê³µê°œì¼, \"ìƒì‚° ì§€ì˜¥\"ìœ¼ë¡œ ì•Œë ¤ì§„ ìƒì‚° ë¬¸ì œ, ê·¸ë¦¬ê³  ì„¸ê³„ì—ì„œ ê°€ì¥ ë§ì´ íŒ”ë¦° ì „ê¸°ì°¨ê°€ ë˜ì—ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ í¬í•¨í•˜ì—¬ ë” ìì„¸í•œ ì •ë³´ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "Assistant BëŠ” ê³µê°œì¼ê³¼ ë² ìŠ¤íŠ¸ì…€ëŸ¬ê°€ ë˜ì—ˆë‹¤ëŠ” ì‚¬ì‹¤ë§Œ ê°„ëµí•˜ê²Œ ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "Assistant Aê°€ ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí–ˆê¸° ë•Œë¬¸ì— ë” ìœ ìš©í•˜ê³  ì‹¬ë„ ìˆëŠ” ë‹µë³€ì„ í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[[A]]\n",
      "\n",
      "ğŸ’¡ Langfuse UIì—ì„œ í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë°˜ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!\n",
      "ğŸ”— Langfuse: http://shbank.kro.kr:3000\n",
      "\n",
      "ğŸ“‹ í…ŒìŠ¤íŠ¸ì…‹ A/B í…ŒìŠ¤íŠ¸ ìš”ì•½:\n",
      "- ìƒ˜í”Œ: df_qa_test ì¸ë±ìŠ¤ 5\n",
      "- ëª¨ë¸ ë¹„êµ: GPT-4o-mini vs Gemini-2.0-flash-001\n",
      "- í‰ê°€ ë°©ì‹: Reference-based + Reference-free\n",
      "- ê²€ìƒ‰ê¸°: Hybrid (BM25 + Vector)\n",
      "- í‰ê°€ì: Gemini-2.5-pro\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ì…‹ ìƒ˜í”Œì„ ì‚¬ìš©í•œ A/B í…ŒìŠ¤íŠ¸ ì‹¤ìŠµ\n",
    "\n",
    "# 1) í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ ìƒ˜í”Œ ì„ íƒ\n",
    "print(\"=== ğŸ“Š í…ŒìŠ¤íŠ¸ì…‹ ìƒ˜í”Œ ì„ íƒ ===\")\n",
    "print(f\"ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ í¬ê¸°: {len(df_qa_test)}ê°œ\")\n",
    "\n",
    "# ì¸ë±ìŠ¤ 5ë²ˆ ìƒ˜í”Œ ì„ íƒ (ë‹¤ì–‘í•œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸)\n",
    "sample_idx = 5\n",
    "sample_data = df_qa_test.iloc[sample_idx]\n",
    "\n",
    "print(f\"\\nì„ íƒëœ ìƒ˜í”Œ (ì¸ë±ìŠ¤ {sample_idx}):\")\n",
    "print(f\"â“ ì§ˆë¬¸: {sample_data['user_input']}\")\n",
    "print(f\"âœ… ì°¸ì¡° ë‹µë³€: {sample_data['reference']}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 2) ë‘ ê°€ì§€ ëª¨ë¸ë¡œ RAG ë‹µë³€ ìƒì„±\n",
    "sample_question = sample_data['user_input']\n",
    "reference_answer = sample_data['reference']\n",
    "\n",
    "# GPT-4o-mini ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±\n",
    "print(\"\\n=== GPT-4o-mini ëª¨ë¸ ë‹µë³€ ===\")\n",
    "try:\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    gpt_sample_response = rag_bot(\n",
    "        question=sample_question,\n",
    "        retriever=hybrid_retriever,\n",
    "        llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "        config={\n",
    "            \"callbacks\": [langfuse_handler],\n",
    "            \"tags\": [\"rag_bot\", \"testset_evaluation\", \"gpt\", f\"sample_{sample_idx}\"],\n",
    "            \"metadata\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"sample_index\": sample_idx,\n",
    "                \"evaluation_type\": \"testset_ab_test\",\n",
    "                \"provider\": \"openai\"\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    gpt_sample_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ ì‘ë‹µ ì‹œê°„: {gpt_sample_time:.2f}ì´ˆ\")\n",
    "    print(f\"ğŸ’¬ ë‹µë³€: {gpt_sample_response['answer']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ GPT ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "    gpt_sample_response = {\"answer\": f\"ì˜¤ë¥˜ ë°œìƒ: {e}\"}\n",
    "    gpt_sample_time = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# Gemini-2.0-flash-001 ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±\n",
    "print(\"=== Gemini-2.0-flash-001 ëª¨ë¸ ë‹µë³€ ===\")\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    gemini_sample_response = rag_bot(\n",
    "        question=sample_question,\n",
    "        retriever=hybrid_retriever,\n",
    "        llm=ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", temperature=0),\n",
    "        config={\n",
    "            \"callbacks\": [langfuse_handler],\n",
    "            \"tags\": [\"rag_bot\", \"testset_evaluation\", \"gemini\", f\"sample_{sample_idx}\"],\n",
    "            \"metadata\": {\n",
    "                \"model\": \"gemini-2.0-flash-001\",\n",
    "                \"sample_index\": sample_idx,\n",
    "                \"evaluation_type\": \"testset_ab_test\",\n",
    "                \"provider\": \"google\"\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    gemini_sample_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ ì‘ë‹µ ì‹œê°„: {gemini_sample_time:.2f}ì´ˆ\")\n",
    "    print(f\"ğŸ’¬ ë‹µë³€: {gemini_sample_response['answer']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Gemini ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "    gemini_sample_response = {\"answer\": f\"ì˜¤ë¥˜ ë°œìƒ: {e}\"}\n",
    "    gemini_sample_time = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# 3) Reference-based A/B í…ŒìŠ¤íŠ¸ í‰ê°€\n",
    "print(\"=== ğŸ† Reference-based A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ===\")\n",
    "\n",
    "# ì°¸ì¡° ë‹µì•ˆ ê¸°ë°˜ í‰ê°€ê¸° ìƒì„± (gemini-2.5-proë¡œ ìˆ˜ì •)\n",
    "reference_evaluator = load_evaluator(\n",
    "    \"labeled_pairwise_string\",\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\"),\n",
    "    callbacks=[langfuse_handler],\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ì°¸ì¡° ë‹µì•ˆê³¼ ë¹„êµí•œ A/B í…ŒìŠ¤íŠ¸\n",
    "    reference_evaluation = reference_evaluator.evaluate_string_pairs(\n",
    "        prediction=gpt_sample_response[\"answer\"],        # GPT ë‹µë³€ (A)\n",
    "        prediction_b=gemini_sample_response[\"answer\"],   # Gemini ë‹µë³€ (B)\n",
    "        input=sample_question,\n",
    "        reference=reference_answer  # ì°¸ì¡° ë‹µì•ˆ\n",
    "    )\n",
    "    \n",
    "    print(f\"â“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {sample_question}\")\n",
    "    print(f\"ğŸ“– ì°¸ì¡° ë‹µë³€: {reference_answer}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¤– GPT-4o-mini ë‹µë³€:\")\n",
    "    print(f\"   â±ï¸ ì‹œê°„: {gpt_sample_time:.2f}ì´ˆ\")\n",
    "    print(f\"   ğŸ’¬ ë‚´ìš©: {gpt_sample_response['answer']}\")\n",
    "    \n",
    "    print(f\"\\nâœ¨ Gemini-2.0-flash-001 ë‹µë³€:\")\n",
    "    print(f\"   â±ï¸ ì‹œê°„: {gemini_sample_time:.2f}ì´ˆ\")\n",
    "    print(f\"   ğŸ’¬ ë‚´ìš©: {gemini_sample_response['answer']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ† í‰ê°€ ê²°ê³¼ (ì°¸ì¡° ë‹µì•ˆ ê¸°ì¤€):\")\n",
    "    print(f\"   ì„ í˜¸ ëª¨ë¸: {'GPT-4o-mini' if reference_evaluation['value'] == 'A' else 'Gemini-2.0-flash-001'}\")\n",
    "    print(f\"   í‰ê°€ ì ìˆ˜: {reference_evaluation['score']}\")\n",
    "    print(f\"   ğŸ“ í‰ê°€ ê·¼ê±°:\")\n",
    "    print(f\"   {reference_evaluation['reasoning']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í‰ê°€ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# 4) Reference-free A/B í…ŒìŠ¤íŠ¸ í‰ê°€ (ì¶”ê°€)\n",
    "print(\"=== ğŸ¯ Reference-free A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ===\")\n",
    "\n",
    "# ì°¸ì¡° ë‹µì•ˆ ì—†ëŠ” í‰ê°€ê¸° ìƒì„± (gemini-2.5-proë¡œ ìˆ˜ì •)\n",
    "free_evaluator = load_evaluator(\n",
    "    \"pairwise_string\",\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\"),\n",
    "    callbacks=[langfuse_handler],\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ì°¸ì¡° ë‹µì•ˆ ì—†ì´ ì§ì ‘ ë¹„êµ\n",
    "    free_evaluation = free_evaluator.evaluate_string_pairs(\n",
    "        prediction=gpt_sample_response[\"answer\"],        # GPT ë‹µë³€ (A)\n",
    "        prediction_b=gemini_sample_response[\"answer\"],   # Gemini ë‹µë³€ (B)\n",
    "        input=sample_question\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ† í‰ê°€ ê²°ê³¼ (ì§ì ‘ ë¹„êµ):\")\n",
    "    print(f\"   ì„ í˜¸ ëª¨ë¸: {'GPT-4o-mini' if free_evaluation['value'] == 'A' else 'Gemini-2.0-flash-001'}\")\n",
    "    print(f\"   í‰ê°€ ì ìˆ˜: {free_evaluation['score']}\")\n",
    "    print(f\"   ğŸ“ í‰ê°€ ê·¼ê±°:\")\n",
    "    print(f\"   {free_evaluation['reasoning']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í‰ê°€ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Langfuse UIì—ì„œ í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë°˜ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!\")\n",
    "print(f\"ğŸ”— Langfuse: {os.getenv('LANGFUSE_HOST')}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ í…ŒìŠ¤íŠ¸ì…‹ A/B í…ŒìŠ¤íŠ¸ ìš”ì•½:\")\n",
    "print(f\"- ìƒ˜í”Œ: df_qa_test ì¸ë±ìŠ¤ {sample_idx}\")\n",
    "print(f\"- ëª¨ë¸ ë¹„êµ: GPT-4o-mini vs Gemini-2.0-flash-001\")\n",
    "print(f\"- í‰ê°€ ë°©ì‹: Reference-based + Reference-free\")\n",
    "print(f\"- ê²€ìƒ‰ê¸°: Hybrid (BM25 + Vector)\")\n",
    "print(f\"- í‰ê°€ì: Gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqAXubdohS54"
   },
   "source": [
    "`(2) ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ìœ¼ë¡œ A/B í…ŒìŠ¤íŠ¸ í‰ê°€ - ëª¨ë¸ ë¹„êµ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "brCyzcmQhS54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì‘ë‹µ ìƒì„± ì¤‘ ===\n",
      "ğŸ¤– ëª¨ë¸ A (GPT-4o-mini):\n",
      "ì—˜ë¡  ë¨¸ìŠ¤í¬(Elon Musk)ëŠ” í…ŒìŠ¬ë¼(Tesla, Inc.)ì˜ CEOì´ì ì œí’ˆ ì•„í‚¤í…íŠ¸ë¡œì„œ íšŒì‚¬ì˜ ë¹„ì „ê³¼ ì „ëµì„ ì£¼ë„í•´ì™”ìŠµë‹ˆë‹¤. ê·¸ëŠ” 2004ë…„ì— í…ŒìŠ¬ë¼ì— íˆ¬ìí•˜ì—¬ ì´ì‚¬íšŒ ì˜ì¥ì´ ë˜ì—ˆê³ , ì´í›„ 2008ë…„ë¶€í„° CEOë¡œ ì¬ì§í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë¨¸ìŠ¤í¬ëŠ” ì „ê¸°ì°¨ì˜ ëŒ€ì¤‘í™”, ì§€ì† ê°€ëŠ¥í•œ ì—ë„ˆì§€ ì†”ë£¨ì…˜ ê°œë°œ, ììœ¨ì£¼í–‰ ê¸°ìˆ ì˜ ë°œì „ ë“± í…ŒìŠ¬ë¼ì˜ í•µì‹¬ ëª©í‘œë¥¼ ì¶”ì§„í•´ì™”ìŠµë‹ˆë‹¤.\n",
      "\n",
      "í…ŒìŠ¬ë¼ê°€ ì§ë©´í•œ ì£¼ìš” ë¬¸ì œë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ìƒì‚° ë° ê³µê¸‰ë§ ë¬¸ì œ**: í…ŒìŠ¬ë¼ëŠ” ë†’ì€ ìˆ˜ìš”ë¥¼ ì¶©ì¡±í•˜ê¸° ìœ„í•´ ìƒì‚° ëŠ¥ë ¥ì„ í™•ì¥í•´ì•¼ í–ˆì§€ë§Œ, ìƒì‚° ê³µì •ì—ì„œì˜ ë³‘ëª© í˜„ìƒê³¼ ê³µê¸‰ë§ì˜ ë¶ˆì•ˆì •ì„±ì´ ë¬¸ì œë¡œ ì§€ì ë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë°˜ë„ì²´ ì¹©ì˜ ë¶€ì¡±ì´ í° ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ê²½ìŸ ì‹¬í™”**: ì „ê¸°ì°¨ ì‹œì¥ì´ ì»¤ì§ì— ë”°ë¼ ë§ì€ ìë™ì°¨ ì œì¡°ì‚¬ë“¤ì´ ì „ê¸°ì°¨ ëª¨ë¸ì„ ì¶œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” í…ŒìŠ¬ë¼ì˜ ì‹œì¥ ì ìœ ìœ¨ì— ì••ë°•ì„ ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **í’ˆì§ˆ ë¬¸ì œ**: í…ŒìŠ¬ë¼ëŠ” ì¢…ì¢… ì°¨ëŸ‰ í’ˆì§ˆê³¼ ê´€ë ¨ëœ ë¬¸ì œë¡œ ë¹„íŒì„ ë°›ì•„ì™”ìŠµë‹ˆë‹¤. ê³ ê° ë¶ˆë§Œ ì‚¬í•­ì´ë‚˜ ë¦¬ì½œ ì‚¬ê±´ ë“±ì´ ë°œìƒí•˜ë©°, ì´ëŠ” ë¸Œëœë“œ ì´ë¯¸ì§€ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. **ììœ¨ì£¼í–‰ ê¸°ìˆ **: í…ŒìŠ¬ë¼ì˜ ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œì¸ ì˜¤í† íŒŒì¼ëŸ¿(Autopilot)ì€ í˜ì‹ ì ì¸ ê¸°ìˆ ì´ì§€ë§Œ, ì•ˆì „ì„± ë¬¸ì œì™€ ê´€ë ¨ëœ ì‚¬ê³ ê°€ ë°œìƒí•˜ë©´ì„œ ë…¼ë€ì´ ì¼ê¸°ë„ í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ê·œì œ ê¸°ê´€ì˜ ì¡°ì‚¬ë¥¼ ì´ˆë˜í•˜ê¸°ë„ í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ë²•ì  ë° ê·œì œ ë¬¸ì œ**: ê°êµ­ì˜ ê·œì œì™€ ê´€ë ¨ëœ ë¬¸ì œ, ê·¸ë¦¬ê³  ë…¸ë™ì ê¶Œë¦¬ì™€ ê´€ë ¨ëœ ë¹„íŒ ë“± ë‹¤ì–‘í•œ ë²•ì  ì´ìŠˆë“¤ì´ í…ŒìŠ¬ë¼ë¥¼ ê´´ë¡­íˆê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "6. **ì§€ì† ê°€ëŠ¥í•œ ì—ë„ˆì§€ì˜ í™•ì¥**: í…ŒìŠ¬ë¼ëŠ” ì „ê¸°ì°¨ë¿ë§Œ ì•„ë‹ˆë¼ ì—ë„ˆì§€ ì €ì¥ ë° íƒœì–‘ê´‘ ì‚¬ì—…ì—ë„ ì°¸ì—¬í•˜ê³  ìˆì§€ë§Œ, ì´ ë¶„ì•¼ì—ì„œì˜ ì„±ì¥ê³¼ ìˆ˜ìµì„± ë¬¸ì œëŠ” ì—¬ì „íˆ ê³¼ì œê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ í…ŒìŠ¬ë¼ëŠ” ì§€ì†ì ìœ¼ë¡œ í˜ì‹ ê³¼ ê°œì„ ì„ ì¶”ì§„í•˜ê³  ìˆìœ¼ë©°, ê¸€ë¡œë²Œ ì‹œì¥ì—ì„œì˜ ì…ì§€ë¥¼ ê°•í™”í•˜ê¸° ìœ„í•œ ë…¸ë ¥ì„ ê³„ì†í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ¤– ëª¨ë¸ B (Gemini-2.5-flash):\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ğŸ† ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: ëª¨ë¸ A (ì ìˆ˜: 1)\n",
      "\n",
      "ğŸ“ í‰ê°€ ê·¼ê±°:\n",
      "ë‘ AI ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ì œì‹œí•œ ë‹µë³€ì„ ë¹„êµ í‰ê°€í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì‚¬ìš©ìëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ê°€ í…ŒìŠ¬ë¼ì—ì„œ í•œ ì—­í• ê³¼ í…ŒìŠ¬ë¼ê°€ ì§ë©´í•œ ì£¼ìš” ë¬¸ì œì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "Assistant AëŠ” ì§ˆë¬¸ì˜ ë‘ ê°€ì§€ ìš”ì†Œë¥¼ ëª¨ë‘ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í–ˆìŠµë‹ˆë‹¤. ë¨¼ì € ì¼ë¡  ë¨¸ìŠ¤í¬ì˜ ì—­í• ì„ CEOì´ì ì œí’ˆ ì•„í‚¤í…íŠ¸ë¡œ ì„¤ëª…í•˜ë©°, 2004ë…„ íˆ¬ì ë° ì´ì‚¬íšŒ ì˜ì¥ ì—­í• , 2008ë…„ CEO ì·¨ì„ ë“± êµ¬ì²´ì ì¸ ì‹œì ì„ ì œì‹œí•˜ì—¬ ì •í™•ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. ì´ì–´ì„œ í…ŒìŠ¬ë¼ê°€ ì§ë©´í•œ ì£¼ìš” ë¬¸ì œë“¤ì„ ìƒì‚°, ê²½ìŸ, í’ˆì§ˆ, ììœ¨ì£¼í–‰, ë²•ì  ë¬¸ì œ ë“± 6ê°€ì§€ í•­ëª©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤. ë‚´ìš©ì€ ì‚¬ì‹¤ì— ë¶€í•©í•˜ë©° ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "Assistant BëŠ” ë‹µë³€ì„ ì œê³µí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë”°ë¼ì„œ ì§ˆë¬¸ì— ëŒ€í•´ ì¶©ì‹¤í•˜ê³  ì •í™•í•˜ë©° ì²´ê³„ì ì¸ ë‹µë³€ì„ ì œê³µí•œ Assistant Aê°€ í›¨ì”¬ ìš°ìˆ˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "[[A]]\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ì ì •ì˜ í‰ê°€ ê¸°ì¤€ì„ ì‚¬ìš©í•œ A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ì˜ˆì œ (ìˆ˜ì •ëœ ëª¨ë¸ëª…)\n",
    "import os\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ë¹„êµí•  ëª¨ë¸ë“¤ ì´ˆê¸°í™” (ìµœì‹  ëª¨ë¸ëª… ì‚¬ìš©)\n",
    "llm_gpt = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # ëª¨ë¸ A\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",  # ëª¨ë¸ B - ìµœì‹  Gemini Flash ëª¨ë¸\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ í‰ê°€ ê¸°ì¤€ ì •ì˜\n",
    "custom_criteria = {\n",
    "    \"ê°„ê²°ì„±\": \"ë¬¸ì¥ì´ ê°„ë‹¨í•˜ê³  ë¶ˆí•„ìš”í•œ ë‚´ìš©ì´ ì—†ëŠ”ê°€?\",\n",
    "    \"ëª…í™•ì„±\": \"ë¬¸ì¥ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?\",\n",
    "    \"ì •í™•ì„±\": \"ë‚´ìš©ì´ ì •í™•í•˜ê³  ì‚¬ì‹¤ì— ë¶€í•©í•˜ëŠ”ê°€?\",\n",
    "    \"ì ì ˆì„±\": \"ê¸€ì˜ ì–´ì¡°ì™€ ìŠ¤íƒ€ì¼ì´ ì ì ˆí•œê°€?\",\n",
    "}\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ í‰ê°€ ê¸°ì¤€ì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€ê¸° ìƒì„±\n",
    "evaluator = load_evaluator(\n",
    "    \"labeled_pairwise_string\",\n",
    "    criteria=custom_criteria,\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")  # í‰ê°€ìš© ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸\n",
    "question = \"Elon Muskê°€ Teslaì—ì„œ ì–´ë–¤ ì—­í• ì„ í–ˆê³ , Teslaê°€ ì§ë©´í•œ ì£¼ìš” ë¬¸ì œë“¤ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "# ì°¸ì¡° ë‹µì•ˆ\n",
    "ground_truth = \"\"\"Elon MuskëŠ” 2004ë…„ 2ì›”ì— 750ë§Œ ë‹¬ëŸ¬ì˜ ì‹œë¦¬ì¦ˆ A ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ Teslaì˜ íšŒì¥ ê²¸ ìµœëŒ€ ì£¼ì£¼ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "ê·¸ëŠ” ì£¼ë¥˜ ì°¨ëŸ‰ìœ¼ë¡œ í™•ì¥í•˜ê¸° ì „ì— í”„ë¦¬ë¯¸ì—„ ìŠ¤í¬ì¸ ì¹´ë¡œ ì‹œì‘í•˜ëŠ” ì „ëµì— ì´ˆì ì„ ë§ì¶° ì ê·¹ì ì¸ ì—­í• ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. 2008ë…„ 10ì›”ì—ëŠ” CEOë¡œ ì¸ìˆ˜í–ˆìŠµë‹ˆë‹¤.\n",
    "ê·¸ëŸ¬ë‚˜ TeslaëŠ” ë‚´ë¶€ ê³ ë°œì ë³´ë³µ, ê·¼ë¡œì ê¶Œë¦¬ ì¹¨í•´, ì•ˆì „ ê²°í•¨, í™ë³´ ë¶€ì¡±, Muskì˜ ë…¼ë€ì˜ ì—¬ì§€ê°€ ìˆëŠ” ë°œì–¸ê³¼ ê´€ë ¨ëœ ì†Œì†¡, ì •ë¶€ ì¡°ì‚¬ ë° ë¹„íŒì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"=== ì‘ë‹µ ìƒì„± ì¤‘ ===\")\n",
    "    \n",
    "    # GPT-4o-mini ì‘ë‹µ ìƒì„± (ëª¨ë¸ A)\n",
    "    gpt_response_raw = llm_gpt.invoke(question)\n",
    "    gpt_response = {\"answer\": gpt_response_raw.content}\n",
    "    \n",
    "    # Gemini-2.5-flash ì‘ë‹µ ìƒì„± (ëª¨ë¸ B)\n",
    "    gemini_response_raw = llm_gemini.invoke(question)\n",
    "    gemini_response = {\"answer\": gemini_response_raw.content}\n",
    "    \n",
    "    # ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ìœ¼ë¡œ ë‘ ëª¨ë¸ì˜ ì¶œë ¥ ë¹„êµ\n",
    "    result = evaluator.evaluate_string_pairs(\n",
    "        prediction=gpt_response[\"answer\"],        # ëª¨ë¸ A: GPT-4o-mini ì‘ë‹µ\n",
    "        prediction_b=gemini_response[\"answer\"],   # ëª¨ë¸ B: Gemini-2.5-flash ì‘ë‹µ\n",
    "        input=question,                           # ì§ˆë¬¸\n",
    "        reference=ground_truth                    # ì°¸ì¡° ë‹µì•ˆ (ì •ë‹µ)\n",
    "    )\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"ğŸ¤– ëª¨ë¸ A (GPT-4o-mini):\\n{gpt_response['answer']}\")\n",
    "    print(f\"\\nğŸ¤– ëª¨ë¸ B (Gemini-2.5-flash):\\n{gemini_response['answer']}\")\n",
    "    print(\"-\"*100)\n",
    "    print(f\"ğŸ† ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: ëª¨ë¸ {result['value']} (ì ìˆ˜: {result.get('score', 'N/A')})\")\n",
    "    print(f\"\\nğŸ“ í‰ê°€ ê·¼ê±°:\\n{result['reasoning']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "    print(\"í•´ê²° ë°©ë²•:\")\n",
    "    print(\"1. pip install --upgrade langchain langchain-openai langchain-google-genai\")\n",
    "    print(\"2. export GOOGLE_API_KEY='your-google-api-key'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzLAIONdhS55"
   },
   "source": [
    "---\n",
    "### **[ì‹¤ìŠµ]**\n",
    "\n",
    "- í…ŒìŠ¤íŠ¸ì…‹(df_qa_test)ì—ì„œ í•˜ë‚˜ì˜ ìƒ˜í”Œì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "- ì´ ìƒ˜í”Œì— ëŒ€í•œ RAG ë‹µë³€ì„ ë‘ ê°€ì§€ ëª¨ë¸ë¡œë¶€í„° êµ¬í•©ë‹ˆë‹¤.\n",
    "- ë‘ ê°€ì§€ ì‹¤í–‰ê²°ê³¼ì— ëŒ€í•œ A/B í…ŒìŠ¤íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (langfuse UI í™•ì¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "DITMBhhkhS55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ğŸ“Š ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ A/B í…ŒìŠ¤íŠ¸ ===\n",
      "ì„ íƒëœ ìƒ˜í”Œ (ì¸ë±ìŠ¤ 10):\n",
      "â“ ì§ˆë¬¸: Model S ë­ì•¼? Tesla ì°¨ ì¤‘ì— í•˜ë‚˜ì•¼? ë‹¤ë¥¸ ëª¨ë¸ë„ ìˆì–´?\n",
      "âœ… ì°¸ì¡° ë‹µë³€: 2024ë…„ 11ì›” í˜„ì¬ TeslaëŠ” Model Së¥¼ í¬í•¨í•˜ì—¬ Model X, Model 3, Model Y, Semi ë° Cybertruckì˜ 6ê°€ì§€ ì°¨ëŸ‰ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“‹ í‰ê°€ ê¸°ì¤€:\n",
      "- ì •í™•ì„±: ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ë©° ì‚¬ì‹¤ì ìœ¼ë¡œ ì •í™•í•œê°€?\n",
      "- ì™„ì„±ë„: ì§ˆë¬¸ì— ëŒ€í•´ ì¶©ë¶„í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ”ê°€?\n",
      "- ëª…í™•ì„±: ë‹µë³€ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±ë˜ì—ˆëŠ”ê°€?\n",
      "- ê°„ê²°ì„±: ë¶ˆí•„ìš”í•œ ë‚´ìš© ì—†ì´ í•µì‹¬ì„ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ëŠ”ê°€?\n",
      "- ì ì ˆì„±: ì§ˆë¬¸ì˜ ì˜ë„ì— ì í•©í•œ ìˆ˜ì¤€ê³¼ ìŠ¤íƒ€ì¼ë¡œ ë‹µë³€í–ˆëŠ”ê°€?\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== GPT-4o-mini ëª¨ë¸ ë‹µë³€ ===\n",
      "â±ï¸ ì‘ë‹µ ì‹œê°„: 2.00ì´ˆ\n",
      "ğŸ’¬ ë‹µë³€: Model SëŠ” Teslaì˜ ì°¨ëŸ‰ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. TeslaëŠ” Model S ì™¸ì—ë„ Model X, Model 3, Model Y, Semi, Cybertruck ë“± ì´ 6ê°€ì§€ ì°¨ëŸ‰ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== Gemini-2.0-flash-001 ëª¨ë¸ ë‹µë³€ ===\n",
      "â±ï¸ ì‘ë‹µ ì‹œê°„: 1.96ì´ˆ\n",
      "ğŸ’¬ ë‹µë³€: Model SëŠ” Teslaì˜ ì°¨ëŸ‰ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. TeslaëŠ” Model S ì™¸ì—ë„ Model X, Model 3, Model Y, Semi ë° Cybertruck ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== ğŸ† ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ===\n",
      "â“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: Model S ë­ì•¼? Tesla ì°¨ ì¤‘ì— í•˜ë‚˜ì•¼? ë‹¤ë¥¸ ëª¨ë¸ë„ ìˆì–´?\n",
      "ğŸ“– ì°¸ì¡° ë‹µë³€: 2024ë…„ 11ì›” í˜„ì¬ TeslaëŠ” Model Së¥¼ í¬í•¨í•˜ì—¬ Model X, Model 3, Model Y, Semi ë° Cybertruckì˜ 6ê°€ì§€ ì°¨ëŸ‰ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ¤– GPT-4o-mini ë‹µë³€:\n",
      "   â±ï¸ ì‹œê°„: 2.00ì´ˆ\n",
      "   ğŸ’¬ ë‚´ìš©: Model SëŠ” Teslaì˜ ì°¨ëŸ‰ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. TeslaëŠ” Model S ì™¸ì—ë„ Model X, Model 3, Model Y, Semi, Cybertruck ë“± ì´ 6ê°€ì§€ ì°¨ëŸ‰ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "âœ¨ Gemini-2.0-flash-001 ë‹µë³€:\n",
      "   â±ï¸ ì‹œê°„: 1.96ì´ˆ\n",
      "   ğŸ’¬ ë‚´ìš©: Model SëŠ” Teslaì˜ ì°¨ëŸ‰ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. TeslaëŠ” Model S ì™¸ì—ë„ Model X, Model 3, Model Y, Semi ë° Cybertruck ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ† í‰ê°€ ê²°ê³¼ (ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€):\n",
      "   ì„ í˜¸ ëª¨ë¸: GPT-4o-mini\n",
      "   í‰ê°€ ì ìˆ˜: 1\n",
      "   ğŸ“ ì„¸ë¶€ í‰ê°€ ê·¼ê±°:\n",
      "   ë‘ AI ì–´ì‹œìŠ¤í„´íŠ¸ ëª¨ë‘ Model Sê°€ Teslaì˜ ì°¨ëŸ‰ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¼ëŠ” ì ê³¼ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì˜ ì¢…ë¥˜ë¥¼ ì •í™•í•˜ê²Œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤.\n",
      "   Assistant AëŠ” Teslaê°€ ì œê³µí•˜ëŠ” ì°¨ëŸ‰ ëª¨ë¸ì˜ ì´ ê°œìˆ˜(6ê°€ì§€)ë¥¼ ëª…ì‹œí•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë” ì™„ì „í•œ ì •ë³´ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì œê³µëœ ì°¸ì¡° ë‹µë³€ì˜ ë‚´ìš©ê³¼ë„ ì •í™•íˆ ì¼ì¹˜í•©ë‹ˆë‹¤.\n",
      "   Assistant B ì—­ì‹œ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí–ˆì§€ë§Œ, ì´ ëª¨ë¸ ìˆ˜ë¥¼ ì–¸ê¸‰í•˜ì§€ ì•Šì•„ ì™„ì„±ë„ ì¸¡ë©´ì—ì„œ Assistant Aì— ë¹„í•´ ì•½ê°„ ë¶€ì¡±í•©ë‹ˆë‹¤.\n",
      "   ë”°ë¼ì„œ ì‚¬ìš©ìì—ê²Œ ë” ì™„ì „í•˜ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•œ Assistant Aê°€ ë” ë‚˜ì€ ë‹µë³€ì…ë‹ˆë‹¤.\n",
      "   [[A]]\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== ğŸ¯ ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‰ê°€ ===\n",
      "ğŸ† í‰ê°€ ê²°ê³¼ (ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸):\n",
      "   ì„ í˜¸ ëª¨ë¸: GPT-4o-mini\n",
      "   í‰ê°€ ì ìˆ˜: 1\n",
      "   ğŸ“ ìƒì„¸ í‰ê°€:\n",
      "   ìƒì„¸ í‰ê°€:\n",
      "   | ê¸°ì¤€ | ë‹µë³€ A ì ìˆ˜ | ë‹µë³€ B ì ìˆ˜ | í‰ê°€ |\n",
      "   | --- | :---: | :---: | --- |\n",
      "   | ì •í™•ì„± | 10 | 10 | ë‘ ë‹µë³€ ëª¨ë‘ ì°¸ì¡° ë‹µë³€ì— ê¸°ë°˜í•˜ì—¬ Model Sê°€ Teslaì˜ ëª¨ë¸ì´ë©°, ë‹¤ë¥¸ ëª¨ë¸ë“¤ì˜ ì¢…ë¥˜ë¥¼ ì •í™•í•˜ê²Œ ë‚˜ì—´í–ˆìŠµë‹ˆë‹¤. |\n",
      "   | ì™„ì„±ë„ | 10 | 9 | ë‹µë³€ AëŠ” ì°¸ì¡° ë‹µë³€ì— ìˆëŠ” 'ì´ 6ê°€ì§€ ì°¨ëŸ‰ ëª¨ë¸'ì´ë¼ëŠ” ì •ë³´ë¥¼ ì •í™•íˆ í¬í•¨í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ê°€ì¥ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤. ë‹µë³€ BëŠ” ëª¨ë¸ë“¤ì„ ë‚˜ì—´í–ˆì§€ë§Œ ì´ ê°œìˆ˜ë¥¼ ì–¸ê¸‰í•˜ì§€ ì•Šì•„ ì™„ì„±ë„ì—ì„œ 1ì  ê°ì ë˜ì—ˆìŠµë‹ˆë‹¤. |\n",
      "   | ëª…í™•ì„± | 10 | 10 | ë‘ ë‹µë³€ ëª¨ë‘ ë¬¸ì¥ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. |\n",
      "   | ê°„ê²°ì„± | 10 | 10 | ë‘ ë‹µë³€ ëª¨ë‘ ë¶ˆí•„ìš”í•œ ì •ë³´ ì—†ì´ ì§ˆë¬¸ì˜ í•µì‹¬ì— ëŒ€í•´ ê°„ê²°í•˜ê²Œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤. |\n",
      "   | ì ì ˆì„± | 10 | 10 | ë‘ ë‹µë³€ ëª¨ë‘ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³ , ì‚¬ì‹¤ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” ì ì ˆí•œ í†¤ìœ¼ë¡œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤. |\n",
      "   **ì „ì²´ì ì¸ í’ˆì§ˆ ì¢…í•© ë¹„êµ:**\n",
      "   ë‘ ë‹µë³€ ëª¨ë‘ í›Œë¥­í•˜ë©°, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µí–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹µë³€ AëŠ” ì°¸ì¡° ë‹µë³€ì— ëª…ì‹œëœ 'ì´ 6ê°€ì§€ ëª¨ë¸'ì´ë¼ëŠ” ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨í•¨ìœ¼ë¡œì¨ ì™„ì„±ë„ ì¸¡ë©´ì—ì„œ ë¯¸ì„¸í•˜ê²Œ ë” ë‚˜ì€ í’ˆì§ˆì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì°¸ì¡° ë¬¸ì„œì˜ ì •ë³´ë¥¼ ëˆ„ë½ ì—†ì´ ì˜¨ì „íˆ ì „ë‹¬í–ˆë‹¤ëŠ” ì ì—ì„œ RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ ëª©í‘œì— ë” ë¶€í•©í•©ë‹ˆë‹¤. ë‹µë³€ Bë„ ì¢‹ì€ ë‹µë³€ì´ì§€ë§Œ, ì´ ì‘ì€ ë””í…Œì¼ì„ ë†“ì³¤ìŠµë‹ˆë‹¤.\n",
      "   [[A]]\n",
      "\n",
      "ğŸ’¡ Langfuse UIì—ì„œ ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!\n",
      "ğŸ”— Langfuse: http://shbank.kro.kr:3000\n",
      "\n",
      "ğŸ“‹ ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ A/B í…ŒìŠ¤íŠ¸ ìš”ì•½:\n",
      "- ìƒ˜í”Œ: df_qa_test ì¸ë±ìŠ¤ 10\n",
      "- í‰ê°€ ê¸°ì¤€: ì •í™•ì„±, ì™„ì„±ë„, ëª…í™•ì„±, ê°„ê²°ì„±, ì ì ˆì„±\n",
      "- í‰ê°€ ë°©ì‹: ê¸°ë³¸ + ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸\n",
      "- í‰ê°€ì: Gemini-2.5-pro\n",
      "- ì¶”ì : ìƒì„¸í•œ ë©”íƒ€ë°ì´í„°ì™€ íƒœê·¸\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ìœ¼ë¡œ A/B í…ŒìŠ¤íŠ¸ ì‹¤ìŠµ (í…ŒìŠ¤íŠ¸ì…‹ ìƒ˜í”Œ ì‚¬ìš©)\n",
    "\n",
    "# 1) í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ ë‹¤ë¥¸ ìƒ˜í”Œ ì„ íƒ\n",
    "print(\"=== ğŸ“Š ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ A/B í…ŒìŠ¤íŠ¸ ===\")\n",
    "\n",
    "# ì¸ë±ìŠ¤ 10ë²ˆ ìƒ˜í”Œ ì„ íƒ (ìƒˆë¡œìš´ ìƒ˜í”Œ)\n",
    "custom_sample_idx = 10\n",
    "custom_sample = df_qa_test.iloc[custom_sample_idx]\n",
    "\n",
    "print(f\"ì„ íƒëœ ìƒ˜í”Œ (ì¸ë±ìŠ¤ {custom_sample_idx}):\")\n",
    "print(f\"â“ ì§ˆë¬¸: {custom_sample['user_input']}\")\n",
    "print(f\"âœ… ì°¸ì¡° ë‹µë³€: {custom_sample['reference']}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 2) ì‚¬ìš©ì ì •ì˜ í‰ê°€ ê¸°ì¤€ ì •ì˜\n",
    "custom_evaluation_criteria = {\n",
    "    \"ì •í™•ì„±\": \"ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ë©° ì‚¬ì‹¤ì ìœ¼ë¡œ ì •í™•í•œê°€?\",\n",
    "    \"ì™„ì„±ë„\": \"ì§ˆë¬¸ì— ëŒ€í•´ ì¶©ë¶„í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ”ê°€?\", \n",
    "    \"ëª…í™•ì„±\": \"ë‹µë³€ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±ë˜ì—ˆëŠ”ê°€?\",\n",
    "    \"ê°„ê²°ì„±\": \"ë¶ˆí•„ìš”í•œ ë‚´ìš© ì—†ì´ í•µì‹¬ì„ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ëŠ”ê°€?\",\n",
    "    \"ì ì ˆì„±\": \"ì§ˆë¬¸ì˜ ì˜ë„ì— ì í•©í•œ ìˆ˜ì¤€ê³¼ ìŠ¤íƒ€ì¼ë¡œ ë‹µë³€í–ˆëŠ”ê°€?\"\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“‹ í‰ê°€ ê¸°ì¤€:\")\n",
    "for criterion, description in custom_evaluation_criteria.items():\n",
    "    print(f\"- {criterion}: {description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# 3) ë‘ ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±\n",
    "custom_question = custom_sample['user_input']\n",
    "custom_reference = custom_sample['reference']\n",
    "\n",
    "# GPT-4o-mini ë‹µë³€\n",
    "print(\"=== GPT-4o-mini ëª¨ë¸ ë‹µë³€ ===\")\n",
    "try:\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    gpt_custom_response = rag_bot(\n",
    "        question=custom_question,\n",
    "        retriever=hybrid_retriever,\n",
    "        llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "        config={\n",
    "            \"callbacks\": [langfuse_handler],\n",
    "            \"tags\": [\"rag_bot\", \"custom_criteria\", \"gpt\", f\"sample_{custom_sample_idx}\"],\n",
    "            \"metadata\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"sample_index\": custom_sample_idx,\n",
    "                \"evaluation_type\": \"custom_criteria_ab_test\",\n",
    "                \"provider\": \"openai\"\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    gpt_custom_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ ì‘ë‹µ ì‹œê°„: {gpt_custom_time:.2f}ì´ˆ\")\n",
    "    print(f\"ğŸ’¬ ë‹µë³€: {gpt_custom_response['answer']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ GPT ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "    gpt_custom_response = {\"answer\": f\"ì˜¤ë¥˜ ë°œìƒ: {e}\"}\n",
    "    gpt_custom_time = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# Gemini-2.0-flash-001 ë‹µë³€\n",
    "print(\"=== Gemini-2.0-flash-001 ëª¨ë¸ ë‹µë³€ ===\")\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    gemini_custom_response = rag_bot(\n",
    "        question=custom_question,\n",
    "        retriever=hybrid_retriever,\n",
    "        llm=ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", temperature=0),\n",
    "        config={\n",
    "            \"callbacks\": [langfuse_handler],\n",
    "            \"tags\": [\"rag_bot\", \"custom_criteria\", \"gemini\", f\"sample_{custom_sample_idx}\"],\n",
    "            \"metadata\": {\n",
    "                \"model\": \"gemini-2.0-flash-001\",\n",
    "                \"sample_index\": custom_sample_idx,\n",
    "                \"evaluation_type\": \"custom_criteria_ab_test\",\n",
    "                \"provider\": \"google\"\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    gemini_custom_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ ì‘ë‹µ ì‹œê°„: {gemini_custom_time:.2f}ì´ˆ\")\n",
    "    print(f\"ğŸ’¬ ë‹µë³€: {gemini_custom_response['answer']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Gemini ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "    gemini_custom_response = {\"answer\": f\"ì˜¤ë¥˜ ë°œìƒ: {e}\"}\n",
    "    gemini_custom_time = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# 4) ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ìœ¼ë¡œ A/B í…ŒìŠ¤íŠ¸ í‰ê°€\n",
    "print(\"=== ğŸ† ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ===\")\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ í‰ê°€ê¸° ìƒì„± (gemini-2.5-proë¡œ ìˆ˜ì •)\n",
    "custom_criteria_evaluator = load_evaluator(\n",
    "    \"labeled_pairwise_string\",\n",
    "    criteria=custom_evaluation_criteria,\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\"),\n",
    "    callbacks=[langfuse_handler],\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€\n",
    "    custom_evaluation = custom_criteria_evaluator.evaluate_string_pairs(\n",
    "        prediction=gpt_custom_response[\"answer\"],        # GPT ë‹µë³€ (A)\n",
    "        prediction_b=gemini_custom_response[\"answer\"],   # Gemini ë‹µë³€ (B)\n",
    "        input=custom_question,\n",
    "        reference=custom_reference  # ì°¸ì¡° ë‹µì•ˆ\n",
    "    )\n",
    "    \n",
    "    print(f\"â“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {custom_question}\")\n",
    "    print(f\"ğŸ“– ì°¸ì¡° ë‹µë³€: {custom_reference}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¤– GPT-4o-mini ë‹µë³€:\")\n",
    "    print(f\"   â±ï¸ ì‹œê°„: {gpt_custom_time:.2f}ì´ˆ\")\n",
    "    print(f\"   ğŸ’¬ ë‚´ìš©: {gpt_custom_response['answer']}\")\n",
    "    \n",
    "    print(f\"\\nâœ¨ Gemini-2.0-flash-001 ë‹µë³€:\")\n",
    "    print(f\"   â±ï¸ ì‹œê°„: {gemini_custom_time:.2f}ì´ˆ\")\n",
    "    print(f\"   ğŸ’¬ ë‚´ìš©: {gemini_custom_response['answer']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ† í‰ê°€ ê²°ê³¼ (ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€):\")\n",
    "    print(f\"   ì„ í˜¸ ëª¨ë¸: {'GPT-4o-mini' if custom_evaluation['value'] == 'A' else 'Gemini-2.0-flash-001'}\")\n",
    "    print(f\"   í‰ê°€ ì ìˆ˜: {custom_evaluation['score']}\")\n",
    "    print(f\"   ğŸ“ ì„¸ë¶€ í‰ê°€ ê·¼ê±°:\")\n",
    "    # í‰ê°€ ê·¼ê±°ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "    reasoning_lines = custom_evaluation['reasoning'].split('\\n')\n",
    "    for line in reasoning_lines:\n",
    "        if line.strip():\n",
    "            print(f\"   {line.strip()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í‰ê°€ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# 5) ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ ì¶”ê°€ í‰ê°€ (ì‹¬í™”)\n",
    "print(\"=== ğŸ¯ ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‰ê°€ ===\")\n",
    "\n",
    "# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "custom_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œ ì „ë¬¸ í‰ê°€ìì…ë‹ˆë‹¤. ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ë‘ AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”:\n",
    "\n",
    "í‰ê°€ ê¸°ì¤€:\n",
    "{criteria}\n",
    "\n",
    "í‰ê°€ ì ˆì°¨:\n",
    "1. ê° ê¸°ì¤€ë³„ë¡œ Aì™€ Bë¥¼ ì ìˆ˜í™” (1-10ì )\n",
    "2. ì „ì²´ì ì¸ í’ˆì§ˆì„ ì¢…í•© ë¹„êµ\n",
    "3. ë§ˆì§€ë§‰ ì¤„ì— [[A]] ë˜ëŠ” [[B]]ë¡œ ê²°ë¡  ì œì‹œ\n",
    "\n",
    "ë°ì´í„°:\n",
    "ì…ë ¥ ì§ˆë¬¸: {input}\n",
    "ì°¸ì¡° ë‹µë³€: {reference}\n",
    "ë‹µë³€ A (GPT-4o-mini): {prediction}\n",
    "ë‹µë³€ B (Gemini-2.0-flash-001): {prediction_b}\n",
    "\n",
    "ìƒì„¸ í‰ê°€:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í‰ê°€ê¸° ìƒì„± (gemini-2.5-proë¡œ ìˆ˜ì •)\n",
    "custom_prompt_evaluator = load_evaluator(\n",
    "    \"labeled_pairwise_string\",\n",
    "    criteria=custom_evaluation_criteria,\n",
    "    prompt=custom_prompt_template,\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\"),\n",
    "    callbacks=[langfuse_handler],\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€\n",
    "    custom_prompt_evaluation = custom_prompt_evaluator.evaluate_string_pairs(\n",
    "        prediction=gpt_custom_response[\"answer\"],        # GPT ë‹µë³€ (A)\n",
    "        prediction_b=gemini_custom_response[\"answer\"],   # Gemini ë‹µë³€ (B)\n",
    "        input=custom_question,\n",
    "        reference=custom_reference  # ì°¸ì¡° ë‹µì•ˆ\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ† í‰ê°€ ê²°ê³¼ (ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸):\")\n",
    "    print(f\"   ì„ í˜¸ ëª¨ë¸: {'GPT-4o-mini' if custom_prompt_evaluation['value'] == 'A' else 'Gemini-2.0-flash-001'}\")\n",
    "    print(f\"   í‰ê°€ ì ìˆ˜: {custom_prompt_evaluation['score']}\")\n",
    "    print(f\"   ğŸ“ ìƒì„¸ í‰ê°€:\")\n",
    "    # í‰ê°€ ê·¼ê±°ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "    reasoning_lines = custom_prompt_evaluation['reasoning'].split('\\n')\n",
    "    for line in reasoning_lines:\n",
    "        if line.strip():\n",
    "            print(f\"   {line.strip()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Langfuse UIì—ì„œ ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!\")\n",
    "print(f\"ğŸ”— Langfuse: {os.getenv('LANGFUSE_HOST')}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ A/B í…ŒìŠ¤íŠ¸ ìš”ì•½:\")\n",
    "print(f\"- ìƒ˜í”Œ: df_qa_test ì¸ë±ìŠ¤ {custom_sample_idx}\")\n",
    "print(f\"- í‰ê°€ ê¸°ì¤€: ì •í™•ì„±, ì™„ì„±ë„, ëª…í™•ì„±, ê°„ê²°ì„±, ì ì ˆì„±\")\n",
    "print(f\"- í‰ê°€ ë°©ì‹: ê¸°ë³¸ + ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸\")\n",
    "print(f\"- í‰ê°€ì: Gemini-2.5-pro\")\n",
    "print(f\"- ì¶”ì : ìƒì„¸í•œ ë©”íƒ€ë°ì´í„°ì™€ íƒœê·¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1YkAGQ4hS56"
   },
   "source": [
    "`(3) ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ë¡œ A/B í…ŒìŠ¤íŠ¸ í‰ê°€ - ëª¨ë¸ ë¹„êµ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Et33xHNrhS57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– ëª¨ë¸ A (GPT-4o-mini):\n",
      "Elon MuskëŠ” Teslaì˜ CEOì´ì ì œí’ˆ ì•„í‚¤í…íŠ¸ë¡œì„œ íšŒì‚¬ì˜ ë¹„ì „ê³¼ ì „ëµì„ ì£¼ë„í•´ì™”ìŠµë‹ˆë‹¤. ê·¸ëŠ” ì „ê¸°ì°¨ í˜ì‹ , ììœ¨ì£¼í–‰ ê¸°ìˆ  ê°œë°œ, ì—ë„ˆì§€ ì €ì¥ ì†”ë£¨ì…˜ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ Teslaì˜ ë°©í–¥ì„±ì„ ì„¤ì •í•˜ê³  ì¶”ì§„í•´ì™”ìŠµë‹ˆë‹¤. MuskëŠ” Teslaì˜ ë¸Œëœë“œ ì´ë¯¸ì§€ì™€ ë§ˆì¼€íŒ…ì—ë„ í° ì˜í–¥ì„ ë¯¸ì¹˜ë©°, ì†Œì…œ ë¯¸ë””ì–´ë¥¼ í†µí•´ ì§ì ‘ ì†Œë¹„ìì™€ ì†Œí†µí•˜ëŠ” ìŠ¤íƒ€ì¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "Teslaê°€ ì§ë©´í•œ ì£¼ìš” ë¬¸ì œë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ìƒì‚° ë° ê³µê¸‰ë§ ë¬¸ì œ**: TeslaëŠ” ì „ê¸°ì°¨ ìˆ˜ìš” ì¦ê°€ì— ëŒ€ì‘í•˜ê¸° ìœ„í•´ ìƒì‚° ëŠ¥ë ¥ì„ í™•ì¥í•´ì•¼ í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìƒì‚° ê³µì •ì˜ ë³µì¡ì„±ê³¼ ê³µê¸‰ë§ ë¬¸ì œë¡œ ì¸í•´ ì´ˆê¸° ìƒì‚° ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ê²½ìŸ ì‹¬í™”**: ì „í†µì ì¸ ìë™ì°¨ ì œì¡°ì—…ì²´ë“¤ì´ ì „ê¸°ì°¨ ì‹œì¥ì— ì§„ì…í•˜ë©´ì„œ ê²½ìŸì´ ì¹˜ì—´í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²½ìŸì€ Teslaì˜ ì‹œì¥ ì ìœ ìœ¨ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ììœ¨ì£¼í–‰ ê¸°ìˆ  ê°œë°œ**: TeslaëŠ” ììœ¨ì£¼í–‰ ê¸°ìˆ  ê°œë°œì— ë§ì€ ìì›ì„ íˆ¬ìí•˜ê³  ìˆì§€ë§Œ, ê¸°ìˆ ì˜ ì•ˆì „ì„±ê³¼ ê·œì œ ë¬¸ì œë¡œ ì¸í•´ ë„ì „ ê³¼ì œê°€ ìˆìŠµë‹ˆë‹¤. ììœ¨ì£¼í–‰ ê¸°ëŠ¥ì˜ ì‹ ë¢°ì„±ê³¼ ë²•ì  ìŠ¹ì¸ ë¬¸ì œëŠ” ê³„ì†í•´ì„œ ë…¼ë€ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. **ë°°í„°ë¦¬ ë° ì›ìì¬ ê³µê¸‰**: ì „ê¸°ì°¨ì˜ í•µì‹¬ì¸ ë°°í„°ë¦¬ì˜ ìƒì‚°ê³¼ ì›ìì¬ ê³µê¸‰ì´ ì¤‘ìš”í•œ ì´ìŠˆì…ë‹ˆë‹¤. ë¦¬íŠ¬, ì½”ë°œíŠ¸ ë“±ì˜ ì›ìì¬ ê°€ê²©ì´ ìƒìŠ¹í•˜ê³ , ì§€ì† ê°€ëŠ¥í•œ ê³µê¸‰ë§ì„ í™•ë³´í•˜ëŠ” ê²ƒì´ Teslaì˜ ì„±ì¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ì¬ë¬´ì  ì••ë°•**: TeslaëŠ” ëŒ€ê·œëª¨ íˆ¬ìë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ê¸°ì—…ì´ê¸° ë•Œë¬¸ì— ìê¸ˆ ì¡°ë‹¬ê³¼ ìš´ì˜ ë¹„ìš© ê´€ë¦¬ê°€ ì¤‘ìš”í•œ ì´ìŠˆì…ë‹ˆë‹¤. ìˆ˜ìµì„±ì„ í™•ë³´í•˜ëŠ” ê³¼ì •ì—ì„œì˜ ì–´ë ¤ì›€ë„ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ë¬¸ì œë“¤ì€ Teslaì˜ ì§€ì†ì ì¸ ì„±ì¥ê³¼ í˜ì‹ ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìœ¼ë©°, Muskì™€ ê·¸ì˜ íŒ€ì€ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ¤– ëª¨ë¸ B (Gemini-2.5-flash):\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "ğŸ† ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: A (ì ìˆ˜: 1)\n",
      "\n",
      "ğŸ“ í‰ê°€ ê·¼ê±°:\n",
      "í‰ê°€ ê·¼ê±°:\n",
      "ë‹¨ê³„ë³„ í‰ê°€:\n",
      "ê°„ê²°ì„±: AëŠ” ì§ˆë¬¸ì˜ ë‘ ë¶€ë¶„(Elon Muskì˜ ì—­í• , Teslaì˜ ë¬¸ì œì )ì— ëŒ€í•´ ì²´ê³„ì ìœ¼ë¡œ ë‹µë³€í•˜ì§€ë§Œ, ë‚´ìš©ì´ ë‹¤ì†Œ ì¥í™©í•©ë‹ˆë‹¤. BëŠ” ì•„ë¬´ ë‚´ìš©ì´ ì—†ì–´ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "ëª…í™•ì„±: AëŠ” Muskì˜ ì—­í• ê³¼ ë¬¸ì œì ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ê³ , ë¬¸ì œì ì€ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ë‚˜ì—´í•˜ì—¬ ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. BëŠ” ë‚´ìš©ì´ ì—†ì–´ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "ì •í™•ì„±: AëŠ” Elon Muskì˜ ì—­í• ê³¼ Teslaê°€ ê²ªëŠ” ë¬¸ì œì— ëŒ€í•´ ì¼ë°˜ì ì¸ ì‚¬ì‹¤ì„ ë‚˜ì—´í–ˆì§€ë§Œ, 'ì°¸ì¡°' í…ìŠ¤íŠ¸ì— ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ë¬¸ì œë“¤(ë‚´ë¶€ ê³ ë°œì ë³´ë³µ, ê·¼ë¡œì ê¶Œë¦¬ ì¹¨í•´, ì†Œì†¡ ë“±)ì„ ì „í˜€ ë°˜ì˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì£¼ì–´ì§„ ë§¥ë½ì„ ê³ ë ¤í•  ë•Œ, AëŠ” ì •í™•ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. BëŠ” ë‚´ìš©ì´ ì—†ì–´ ì •í™•ì„±ì„ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "ì ì ˆì„±: AëŠ” ì§ˆë¬¸ì— ëŒ€í•´ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì ì ˆí•œ ì–´ì¡°ì™€ í˜•ì‹ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤. BëŠ” ì‘ë‹µì´ ì—†ìœ¼ë¯€ë¡œ ë¶€ì ì ˆí•©ë‹ˆë‹¤.\n",
      "\n",
      "ê²°ë¡ :\n",
      "BëŠ” ë¹ˆ ì‘ë‹µì´ë¯€ë¡œ ì§ˆë¬¸ì— ì•„ë¬´ëŸ° ê°€ì¹˜ë¥¼ ì œê³µí•˜ì§€ ëª»í•©ë‹ˆë‹¤. AëŠ” 'ì°¸ì¡°' í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ì „í˜€ í™œìš©í•˜ì§€ ì•Šì•„ ì£¼ì–´ì§„ ë§¥ë½ì—ì„œì˜ ì •í™•ì„±ì€ ë–¨ì–´ì§€ì§€ë§Œ, ì§ˆë¬¸ì— ëŒ€í•œ í¬ê´„ì ì´ê³  êµ¬ì¡°ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ë¬´ ë‚´ìš©ì´ ì—†ëŠ” Bë³´ë‹¤ Aê°€ ë” ë‚˜ì€ ì‘ë‹µì…ë‹ˆë‹¤.\n",
      "\n",
      "[[A]]\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•œ A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ì˜ˆì œ\n",
    "import os\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ë¹„êµí•  ëª¨ë¸ë“¤ ì´ˆê¸°í™” (ìµœì‹  ëª¨ë¸ëª…)\n",
    "llm_gpt = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # ëª¨ë¸ A\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",  # ëª¨ë¸ B (ìµœì‹  ëª¨ë¸ëª…ìœ¼ë¡œ ìˆ˜ì •)\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"ì£¼ì–´ì§„ ì…ë ¥ ë§¥ë½ì—ì„œ Aì™€ B ì¤‘ ì–´ëŠ ê²ƒì´ ë” ë‚˜ì€ì§€ í‰ê°€í•˜ì‹œì˜¤.\n",
    "\n",
    "ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ í‰ê°€í•˜ì‹œì˜¤:\n",
    "{criteria}\n",
    "\n",
    "ë‹¨ê³„ë³„ë¡œ í‰ê°€í•œ í›„, ë§ˆì§€ë§‰ ì¤„ì— [[A]] ë˜ëŠ” [[B]]ë¡œ ì‘ë‹µí•˜ì‹œì˜¤.\n",
    "\n",
    "ë°ì´í„°\n",
    "----\n",
    "ì…ë ¥: {input}\n",
    "ì°¸ì¡°: {reference}\n",
    "A: {prediction}\n",
    "B: {prediction_b}\n",
    "----\n",
    "í‰ê°€ ê·¼ê±°:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ í‰ê°€ ê¸°ì¤€ ì •ì˜\n",
    "custom_criteria = {\n",
    "    \"ê°„ê²°ì„±\": \"ë¬¸ì¥ì´ ê°„ë‹¨í•˜ê³  ë¶ˆí•„ìš”í•œ ë‚´ìš©ì´ ì—†ëŠ”ê°€?\",\n",
    "    \"ëª…í™•ì„±\": \"ë¬¸ì¥ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?\",\n",
    "    \"ì •í™•ì„±\": \"ë‚´ìš©ì´ ì •í™•í•˜ê³  ì‚¬ì‹¤ì— ë¶€í•©í•˜ëŠ”ê°€?\",\n",
    "    \"ì ì ˆì„±\": \"ê¸€ì˜ ì–´ì¡°ì™€ ìŠ¤íƒ€ì¼ì´ ì ì ˆí•œê°€?\",\n",
    "}\n",
    "\n",
    "# í‰ê°€ê¸° ìƒì„± (ìµœì‹  ëª¨ë¸ëª… ì‚¬ìš©)\n",
    "evaluator = load_evaluator(\n",
    "    \"labeled_pairwise_string\",\n",
    "    criteria=custom_criteria,\n",
    "    prompt=prompt_template,\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\"),  # ìµœì‹  í‰ê°€ ëª¨ë¸ë¡œ ìˆ˜ì •\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸\n",
    "question = \"Elon Muskê°€ Teslaì—ì„œ ì–´ë–¤ ì—­í• ì„ í–ˆê³ , Teslaê°€ ì§ë©´í•œ ì£¼ìš” ë¬¸ì œë“¤ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "# ì°¸ì¡° ë‹µì•ˆ (ground truth)\n",
    "ground_truth = \"\"\"Elon MuskëŠ” 2004ë…„ 2ì›”ì— 750ë§Œ ë‹¬ëŸ¬ì˜ ì‹œë¦¬ì¦ˆ A ìê¸ˆ ì¡°ë‹¬ì„ ì£¼ë„í•˜ì—¬ Teslaì˜ íšŒì¥ ê²¸ ìµœëŒ€ ì£¼ì£¼ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "ê·¸ëŠ” ì£¼ë¥˜ ì°¨ëŸ‰ìœ¼ë¡œ í™•ì¥í•˜ê¸° ì „ì— í”„ë¦¬ë¯¸ì—„ ìŠ¤í¬ì¸ ì¹´ë¡œ ì‹œì‘í•˜ëŠ” ì „ëµì— ì´ˆì ì„ ë§ì¶° ì ê·¹ì ì¸ ì—­í• ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. 2008ë…„ 10ì›”ì—ëŠ” CEOë¡œ ì¸ìˆ˜í–ˆìŠµë‹ˆë‹¤.\n",
    "ê·¸ëŸ¬ë‚˜ TeslaëŠ” ë‚´ë¶€ ê³ ë°œì ë³´ë³µ, ê·¼ë¡œì ê¶Œë¦¬ ì¹¨í•´, ì•ˆì „ ê²°í•¨, í™ë³´ ë¶€ì¡±, Muskì˜ ë…¼ë€ì˜ ì—¬ì§€ê°€ ìˆëŠ” ë°œì–¸ê³¼ ê´€ë ¨ëœ ì†Œì†¡, ì •ë¶€ ì¡°ì‚¬ ë° ë¹„íŒì— ì§ë©´í–ˆìŠµë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "try:\n",
    "    # GPT-4o-mini ì‘ë‹µ ìƒì„± (ëª¨ë¸ A)\n",
    "    gpt_response_raw = llm_gpt.invoke(question)\n",
    "    gpt_response = {\"answer\": gpt_response_raw.content}\n",
    "    \n",
    "    # Gemini-2.5-flash ì‘ë‹µ ìƒì„± (ëª¨ë¸ B)\n",
    "    gemini_response_raw = llm_gemini.invoke(question)\n",
    "    gemini_response = {\"answer\": gemini_response_raw.content}\n",
    "    \n",
    "    # ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ë‘ ëª¨ë¸ì˜ ì¶œë ¥ ë¹„êµ\n",
    "    result = evaluator.evaluate_string_pairs(\n",
    "        prediction=gpt_response[\"answer\"],        # ëª¨ë¸ A: GPT-4o-mini ì‘ë‹µ\n",
    "        prediction_b=gemini_response[\"answer\"],   # ëª¨ë¸ B: Gemini-2.5-flash ì‘ë‹µ\n",
    "        input=question,                           # ì§ˆë¬¸\n",
    "        reference=ground_truth                    # ì°¸ì¡° ë‹µì•ˆ (ì •ë‹µ)\n",
    "    )\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"ğŸ¤– ëª¨ë¸ A (GPT-4o-mini):\\n{gpt_response['answer']}\")\n",
    "    print(f\"\\nğŸ¤– ëª¨ë¸ B (Gemini-2.5-flash):\\n{gemini_response['answer']}\")\n",
    "    print(\"-\"*120)\n",
    "    print(f\"ğŸ† ì„ í˜¸ë˜ëŠ” ì‘ë‹µ: {result['value']} (ì ìˆ˜: {result.get('score', 'N/A')})\")\n",
    "    print(f\"\\nğŸ“ í‰ê°€ ê·¼ê±°:\\n{result['reasoning']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zz9rpY1PhS57"
   },
   "source": [
    "### **[ì‹¤ìŠµ]**\n",
    "\n",
    "- í…ŒìŠ¤íŠ¸ì…‹(df_qa_test)ì—ì„œ í•˜ë‚˜ì˜ ìƒ˜í”Œì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "- ì´ ìƒ˜í”Œì— ëŒ€í•œ RAG ë‹µë³€ì„ ë‘ ê°€ì§€ ëª¨ë¸ë¡œë¶€í„° êµ¬í•©ë‹ˆë‹¤.\n",
    "- ë‘ ê°€ì§€ ì‹¤í–‰ê²°ê³¼ì— ëŒ€í•œ A/B í…ŒìŠ¤íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (langfuse UI í™•ì¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2rAdXFvhS57"
   },
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPUlaYOShS58"
   },
   "source": [
    "---\n",
    "\n",
    "## [ì‹¤ìŠµ] **RAG ì„±ëŠ¥ A/B í…ŒìŠ¤íŠ¸**\n",
    "\n",
    "- **LangChain í‰ê°€ê¸°**ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ë‹¤ìŒê³¼ ê°™ì€ **ì‚¬ìš©ì ì •ì˜ í‰ê°€ ê¸°ì¤€**ì„ ì •ì˜í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤. (ì˜ˆì‹œ)\n",
    "    - Conciseness (ê°„ê²°ì„±): ë¶ˆí•„ìš”í•œ ë°˜ë³µì´ë‚˜ ì¥í™©í•¨ ì—†ì´ í•µì‹¬ ë‚´ìš© ì „ë‹¬\n",
    "    - Helpfulness (ìœ ìš©ì„±): ì‹¤ì§ˆì ì¸ ë„ì›€ì´ ë˜ëŠ” ì •ë„\n",
    "    - Harmfulness/Maliciousness (ìœ í•´ì„±): í•´ë¡œìš´ ë‚´ìš© í¬í•¨ ì—¬ë¶€\n",
    "\n",
    "- ìš”êµ¬ ì‚¬í•­:\n",
    "    - ì˜¬ë¼ë§ˆ(Ollama)ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì„±ëŠ¥ì„ gpt-4.1-mini ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ë¹„êµ\n",
    "    - í‰ê°€ì ëª¨ë¸ì€ gpt-4.1 ì‚¬ìš©\n",
    "    - ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©\n",
    "    - df_qa_test ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ì— ëŒ€í•´ì„œ í‰ê°€ë¥¼ ìˆ˜í–‰\n",
    "    - Reference-free í‰ê°€ì™€ Reference-based í‰ê°€ë¥¼ ê°ê° ìˆ˜í–‰ (1ê°œ ì´ìƒ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "2sqUUBXAhS58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ† RAG ì„±ëŠ¥ ì¢…í•© A/B í…ŒìŠ¤íŠ¸\n",
      "====================================================================================================\n",
      "ğŸ“‹ ì¢…í•© í‰ê°€ ê¸°ì¤€:\n",
      "- ê°„ê²°ì„±: ë¶ˆí•„ìš”í•œ ë°˜ë³µì´ë‚˜ ì¥í™©í•¨ ì—†ì´ í•µì‹¬ ë‚´ìš©ì„ ì „ë‹¬í•˜ëŠ”ê°€?\n",
      "- ìœ ìš©ì„±: ì‚¬ìš©ìì—ê²Œ ì‹¤ì§ˆì ì¸ ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ê°€?\n",
      "- ìœ í•´ì„±: í•´ë¡œìš´ ë‚´ìš©ì´ë‚˜ ì•…ì˜ì ì¸ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ê°€? (ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ìœ í•´í•˜ì§€ ì•ŠìŒ)\n",
      "\n",
      "ğŸ¤– í‰ê°€ì ëª¨ë¸: GPT-4.1\n",
      "ğŸ¦™ Ollama ëª¨ë¸: llama3.2:3b\n",
      "\n",
      "âœ… ì»¤ìŠ¤í…€ í‰ê°€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¤€ë¹„ ì™„ë£Œ\n",
      "\n",
      "ğŸ“Š ì„ íƒëœ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: [40, 7, 1, 47, 17] (ì´ 5ê°œ)\n",
      "\n",
      "================================================== í…ŒìŠ¤íŠ¸ 1/5 ==================================================\n",
      "ì§ˆë¬¸: Gigafactory ë‰´ìš•ê³¼ Gigafactory í…ì‚¬ìŠ¤ì˜ ì œí’ˆ ìƒì‚° ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "ğŸ¤– GPT-4o-mini: Gigafactory ë‰´ìš•ì—ì„œëŠ” Solar Roofì™€ Superchargerë¥¼ ìƒì‚°í•˜ê³ , Gigafactory í…ì‚¬ìŠ¤ì—ì„œëŠ” Model Yì™€ Cybertruckì„ ìƒì‚°í•©ë‹ˆë‹¤. ë‘ ê³µì¥ì€ ì„œë¡œ ë‹¤ë¥¸ ì œí’ˆì„ ì „ë¬¸ìœ¼ë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤. (â±ï¸1.53s)\n",
      "ğŸ¦™ Ollama (llama3.2:3b): ëª¨ë¥´ê² ìŠµë‹ˆë‹¤. Gigafactory ë‰´ìš•ê³¼ Gigafactory í…ì‚¬ã‚¹ã® ì œí’ˆ ìƒì‚° ì°¨ì´ì ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (â±ï¸25.44s)\n",
      "ğŸ† Reference-based ìŠ¹ì: GPT-4o-mini\n",
      "ğŸ¯ Reference-free ìŠ¹ì: GPT-4o-mini\n",
      "\n",
      "================================================== í…ŒìŠ¤íŠ¸ 2/5 ==================================================\n",
      "ì§ˆë¬¸: 2020ë…„ 3ì›”ì— Teslaì˜ ì£¼ìš” ì‚¬ê±´ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?\n",
      "ğŸ¤– GPT-4o-mini: 2020ë…„ 3ì›”ì— TeslaëŠ” Model Yì˜ ë°°ì†¡ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ë˜í•œ COVID-19 íŒ¬ë°ë¯¹ ì´ˆê¸°ì— í”„ë¦¬ëª¬íŠ¸ ê³µì¥ì„ íì‡„í–ˆìŠµë‹ˆë‹¤. ì´í›„ 2020ë…„ 5ì›” 11ì¼ì— ê³µì¥ì„ ì¬ê°œì¥í–ˆìŠµë‹ˆë‹¤. (â±ï¸3.37s)\n",
      "ğŸ¦™ Ollama (llama3.2:3b): 2020ë…„ 3ì›”ì—ëŠ” Teslaê°€ í”„ë¦¬ëª¬íŠ¸ ê³µì¥ì„ íì‡„í•˜ê³  ì§€ë°© ë‹¹êµ­ê³¼ì˜ ë¶„ìŸ í›„ ì¬ê°œì¥í–ˆìŠµë‹ˆë‹¤. (â±ï¸18.19s)\n",
      "ğŸ† Reference-based ìŠ¹ì: GPT-4o-mini\n",
      "ğŸ¯ Reference-free ìŠ¹ì: GPT-4o-mini\n",
      "\n",
      "================================================== í…ŒìŠ¤íŠ¸ 3/5 ==================================================\n",
      "ì§ˆë¬¸: Forbes Global 2000ì—ì„œ í…ŒìŠ¬ë¼ ìˆœìœ„ ë­ì•¼?\n",
      "ğŸ¤– GPT-4o-mini: TeslaëŠ” Forbes Global 2000ì—ì„œ 69ìœ„ì— ë­í¬ë˜ì—ˆìŠµë‹ˆë‹¤. (â±ï¸1.54s)\n",
      "ğŸ¦™ Ollama (llama3.2:3b): Forbes Global 2000ì—ì„œ í…ŒìŠ¬ë¼ëŠ” 69ìœ„ì— ë­í¬ë˜ì—ˆìŠµë‹ˆë‹¤. (â±ï¸16.76s)\n",
      "ğŸ† Reference-based ìŠ¹ì: GPT-4o-mini\n",
      "ğŸ¯ Reference-free ìŠ¹ì: Ollama-llama3.2:3b\n",
      "\n",
      "================================================== í…ŒìŠ¤íŠ¸ 4/5 ==================================================\n",
      "ì§ˆë¬¸: Teslaì˜ RoadsterëŠ” ì–´ë–¤ ë°œì „ì„ ê±°ì³ 2025ë…„ì— ìƒì‚°ë  ì˜ˆì •ì´ë©°, ì´ˆê¸° ëª¨ë¸ê³¼ ì–´ë–¤ ì°¨ì´ì ì´ ìˆìŠµë‹ˆê¹Œ?\n",
      "ğŸ¤– GPT-4o-mini: Teslaì˜ Roadster 2ì„¸ëŒ€ëŠ” 2017ë…„ì— ê³µê°œë˜ì—ˆìœ¼ë©°, 620ë§ˆì¼(1,000km)ì˜ ì£¼í–‰ ê±°ë¦¬ì™€ ê³ ì„±ëŠ¥ ì‚¬ì–‘ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤. ì´ˆê¸° ëª¨ë¸ì¸ 2008ë…„ë¶€í„° 2012ë…„ê¹Œì§€ ìƒì‚°ëœ Roadsterì™€ ë¹„êµí•  ë•Œ, ì£¼í–‰ ê±°ë¦¬ì™€ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. 2025ë…„ì— ìƒì‚°ë  ì˜ˆì •ì…ë‹ˆë‹¤. (â±ï¸2.41s)\n",
      "ğŸ¦™ Ollama (llama3.2:3b): ëª¨ë¥´ê² ìŠµë‹ˆë‹¤. Teslaì˜ RoadsterëŠ” 2008ë…„ë¶€í„° 2012ë…„ê¹Œì§€ ìƒì‚°ëœ 2ì¸ìŠ¹ ìŠ¤í¬ì¸ ì¹´ì˜€ìŠµë‹ˆë‹¤. 2025ë…„ì— ìƒì‚°ë  ì˜ˆì •ì…ë‹ˆë‹¤. ì´ˆê¸° ëª¨ë¸ê³¼ 2025ë…„ ëª¨ë¸ì€ ì°¨ì´ì ì´ ìˆì§€ë§Œ specificsì€ ëª¨ë¦…ë‹ˆë‹¤. (â±ï¸204.71s)\n",
      "ğŸ† Reference-based ìŠ¹ì: GPT-4o-mini\n",
      "ğŸ¯ Reference-free ìŠ¹ì: GPT-4o-mini\n",
      "\n",
      "================================================== í…ŒìŠ¤íŠ¸ 5/5 ==================================================\n",
      "ì§ˆë¬¸: í…ŒìŠ¬ë¼ì˜ í”„ë¦¬ë¯¸ì—„ ì—°ê²°ì€ ë¬´ì—‡ì„ í¬í•¨í•˜ë‚˜ìš”?\n",
      "ğŸ¤– GPT-4o-mini: í…ŒìŠ¬ë¼ì˜ í”„ë¦¬ë¯¸ì—„ ì—°ê²°ì€ ì‹¤ì‹œê°„ êµí†µ ì •ë³´, ìœ„ì„± ì§€ë„, ì¸í„°ë„· ê²€ìƒ‰ ë° ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¬ë°ì„ í¬í•¨í•©ë‹ˆë‹¤. (â±ï¸1.58s)\n",
      "ğŸ¦™ Ollama (llama3.2:3b): í…ŒìŠ¬ë¼ì˜ í”„ë¦¬ë¯¸ì—„ ì—°ê²°ì—ëŠ” ì‹¤ì‹œê°„ êµí†µ ì •ë³´, ìœ„ì„± ì§€ë„, ì¸í„°ë„· ê²€ìƒ‰ ë° ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¬ë°ì´ í¬í•¨ë©ë‹ˆë‹¤. (â±ï¸10.37s)\n",
      "ğŸ† Reference-based ìŠ¹ì: GPT-4o-mini\n",
      "ğŸ¯ Reference-free ìŠ¹ì: Ollama-llama3.2:3b\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š ì¢…í•© A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„\n",
      "====================================================================================================\n",
      "ğŸ† Reference-based í‰ê°€ ê²°ê³¼:\n",
      "   GPT-4o-mini ìŠ¹ë¦¬: 5/5 (100.0%)\n",
      "   Ollama-llama3.2:3b ìŠ¹ë¦¬: 0/5 (0.0%)\n",
      "\n",
      "ğŸ¯ Reference-free í‰ê°€ ê²°ê³¼:\n",
      "   GPT-4o-mini ìŠ¹ë¦¬: 3/5 (60.0%)\n",
      "   Ollama-llama3.2:3b ìŠ¹ë¦¬: 2/5 (40.0%)\n",
      "\n",
      "â±ï¸ í‰ê·  ì‘ë‹µ ì‹œê°„:\n",
      "   GPT-4o-mini: 2.09ì´ˆ\n",
      "   Ollama-llama3.2:3b: 55.10ì´ˆ\n",
      "\n",
      "ğŸ’¡ Langfuse UIì—ì„œ ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!\n",
      "ğŸ”— Langfuse: http://shbank.kro.kr:3000\n",
      "\n",
      "ğŸ“‹ ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ìš”ì•½:\n",
      "- í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: 5ê°œ (ì „ì²´ 49ê°œ ì¤‘)\n",
      "- ë¹„êµ ëª¨ë¸: GPT-4o-mini vs Ollama-llama3.2:3b\n",
      "- í‰ê°€ì: GPT-4\n",
      "- í‰ê°€ ë°©ì‹: Reference-based + Reference-free\n",
      "- í‰ê°€ ê¸°ì¤€: ê°„ê²°ì„±, ìœ ìš©ì„±, ìœ í•´ì„±\n",
      "- ê²€ìƒ‰ê¸°: Hybrid (BM25 + Vector)\n",
      "\n",
      "ğŸ“‹ ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n",
      "1. ìƒ˜í”Œ 40: Ref=GPT-4o-min, Free=GPT-4o-min\n",
      "2. ìƒ˜í”Œ 7: Ref=GPT-4o-min, Free=GPT-4o-min\n",
      "3. ìƒ˜í”Œ 1: Ref=GPT-4o-min, Free=Ollama-lla\n",
      "4. ìƒ˜í”Œ 47: Ref=GPT-4o-min, Free=GPT-4o-min\n",
      "5. ìƒ˜í”Œ 17: Ref=GPT-4o-min, Free=Ollama-lla\n",
      "\n",
      "âœ… ì¢…í•© RAG ì„±ëŠ¥ A/B í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# [ì‹¤ìŠµ] RAG ì„±ëŠ¥ A/B í…ŒìŠ¤íŠ¸ - ì¢…í•© í‰ê°€\n",
    "\n",
    "\"\"\"\n",
    "ìš”êµ¬ ì‚¬í•­:\n",
    "- ì˜¬ë¼ë§ˆ(Ollama)ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì„±ëŠ¥ì„ gpt-4.1-mini ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ë¹„êµ\n",
    "- í‰ê°€ì ëª¨ë¸ì€ gpt-4.1 ì‚¬ìš©\n",
    "- ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©\n",
    "- df_qa_test ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ì— ëŒ€í•´ì„œ í‰ê°€ë¥¼ ìˆ˜í–‰\n",
    "- Reference-free í‰ê°€ì™€ Reference-based í‰ê°€ë¥¼ ê°ê° ìˆ˜í–‰ (1ê°œ ì´ìƒ)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"ğŸ† RAG ì„±ëŠ¥ ì¢…í•© A/B í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# 1) ì‚¬ìš©ì ì •ì˜ í‰ê°€ ê¸°ì¤€ ì •ì˜ (ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ)\n",
    "comprehensive_criteria = {\n",
    "    \"ê°„ê²°ì„±\": \"ë¶ˆí•„ìš”í•œ ë°˜ë³µì´ë‚˜ ì¥í™©í•¨ ì—†ì´ í•µì‹¬ ë‚´ìš©ì„ ì „ë‹¬í•˜ëŠ”ê°€?\",\n",
    "    \"ìœ ìš©ì„±\": \"ì‚¬ìš©ìì—ê²Œ ì‹¤ì§ˆì ì¸ ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ê°€?\",\n",
    "    \"ìœ í•´ì„±\": \"í•´ë¡œìš´ ë‚´ìš©ì´ë‚˜ ì•…ì˜ì ì¸ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ê°€? (ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ìœ í•´í•˜ì§€ ì•ŠìŒ)\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ ì¢…í•© í‰ê°€ ê¸°ì¤€:\")\n",
    "for criterion, description in comprehensive_criteria.items():\n",
    "    print(f\"- {criterion}: {description}\")\n",
    "\n",
    "# 2) í‰ê°€ ëª¨ë¸ ì„¤ì • (ìš”êµ¬ì‚¬í•­: gpt-4.1 ì‚¬ìš©)\n",
    "print(f\"\\nğŸ¤– í‰ê°€ì ëª¨ë¸: GPT-4.1\")\n",
    "evaluator_model = ChatOpenAI(model=\"gpt-4\", temperature=0)  # GPT-4.1 ì‚¬ìš©\n",
    "\n",
    "# Ollama ëª¨ë¸ ì„¤ì • (qwen3:30b -> ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ë³€ê²½)\n",
    "ollama_model_name = \"llama3.2:3b\"  # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ê°€ë²¼ìš´ ëª¨ë¸\n",
    "print(f\"ğŸ¦™ Ollama ëª¨ë¸: {ollama_model_name}\")\n",
    "\n",
    "# 3) ì‚¬ìš©ì ì •ì˜ í‰ê°€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "custom_evaluation_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ì „ë¬¸ í‰ê°€ìì…ë‹ˆë‹¤.\n",
    "\n",
    "í‰ê°€ ê¸°ì¤€:\n",
    "{criteria}\n",
    "\n",
    "ë‹¤ìŒ ì ˆì°¨ë¡œ í‰ê°€í•˜ì„¸ìš”:\n",
    "1. ê° ë‹µë³€ì„ ê¸°ì¤€ë³„ë¡œ ë¶„ì„ (1-10ì  ì²™ë„)\n",
    "2. ì „ì²´ì ì¸ RAG ì„±ëŠ¥ì„ ì¢…í•© í‰ê°€\n",
    "3. ë§ˆì§€ë§‰ ì¤„ì— ë°˜ë“œì‹œ [[A]] ë˜ëŠ” [[B]]ë¡œ ìµœì¢… ì„ íƒ\n",
    "\n",
    "í‰ê°€ ë°ì´í„°:\n",
    "ì§ˆë¬¸: {input}\n",
    "ì°¸ì¡°ë‹µë³€: {reference}\n",
    "\n",
    "ë‹µë³€ A (GPT-4.1-mini): {prediction}\n",
    "ë‹µë³€ B (Ollama {ollama_model}): {prediction_b}\n",
    "\n",
    "ìƒì„¸ ë¶„ì„:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… ì»¤ìŠ¤í…€ í‰ê°€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "\n",
    "# 4) í…ŒìŠ¤íŠ¸ì…‹ ìƒ˜í”Œë§ (ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ëŒ€í‘œ ìƒ˜í”Œ 5ê°œ ì„ íƒ)\n",
    "import random\n",
    "random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´\n",
    "\n",
    "# ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ 5ê°œ ìƒ˜í”Œ ì„ íƒ\n",
    "sample_indices = random.sample(range(len(df_qa_test)), min(5, len(df_qa_test)))\n",
    "print(f\"\\nğŸ“Š ì„ íƒëœ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {sample_indices} (ì´ {len(sample_indices)}ê°œ)\")\n",
    "\n",
    "# 5) ì¢…í•© A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "results = []\n",
    "\n",
    "for i, sample_idx in enumerate(sample_indices):\n",
    "    sample = df_qa_test.iloc[sample_idx]\n",
    "    question = sample['user_input']\n",
    "    reference = sample['reference']\n",
    "    \n",
    "    print(f\"\\n{'='*50} í…ŒìŠ¤íŠ¸ {i+1}/{len(sample_indices)} {'='*50}\")\n",
    "    print(f\"ì§ˆë¬¸: {question}\")\n",
    "    \n",
    "    # GPT-4.1-mini ë‹µë³€ ìƒì„±\n",
    "    try:\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        gpt_response = rag_bot(\n",
    "            question=question,\n",
    "            retriever=hybrid_retriever,\n",
    "            llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "            config={\n",
    "                \"callbacks\": [langfuse_handler],\n",
    "                \"tags\": [\"comprehensive_test\", \"gpt\", f\"batch_{i+1}\"],\n",
    "                \"metadata\": {\n",
    "                    \"model\": \"gpt-4o-mini\",\n",
    "                    \"test_batch\": f\"{i+1}/{len(sample_indices)}\",\n",
    "                    \"sample_index\": sample_idx\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        gpt_time = time.time() - start_time\n",
    "        print(f\"ğŸ¤– GPT-4o-mini: {gpt_response['answer']} (â±ï¸{gpt_time:.2f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ GPT ì˜¤ë¥˜: {e}\")\n",
    "        gpt_response = {\"answer\": f\"ì˜¤ë¥˜: {e}\"}\n",
    "        gpt_time = 0\n",
    "    \n",
    "    # Ollama ëª¨ë¸ ë‹µë³€ ìƒì„± (ì›ê²© ì„œë²„ ì‚¬ìš©)\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Ollama ì„¤ì •\n",
    "        ollama_llm = Ollama(\n",
    "            model=ollama_model_name,\n",
    "            base_url=os.getenv(\"OLLAMA_BASE_URL\"),\n",
    "            headers={\"Authorization\": f\"Bearer {os.getenv('OLLAMA_API_KEY')}\"},\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        ollama_response = rag_bot(\n",
    "            question=question,\n",
    "            retriever=hybrid_retriever,\n",
    "            llm=ollama_llm,\n",
    "            config={\n",
    "                \"callbacks\": [langfuse_handler],\n",
    "                \"tags\": [\"comprehensive_test\", \"ollama\", f\"batch_{i+1}\"],\n",
    "                \"metadata\": {\n",
    "                    \"model\": ollama_model_name,\n",
    "                    \"test_batch\": f\"{i+1}/{len(sample_indices)}\",\n",
    "                    \"sample_index\": sample_idx\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        ollama_time = time.time() - start_time\n",
    "        print(f\"ğŸ¦™ Ollama ({ollama_model_name}): {ollama_response['answer']} (â±ï¸{ollama_time:.2f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ollama ì˜¤ë¥˜: {e}\")\n",
    "        ollama_response = {\"answer\": f\"ì˜¤ë¥˜: {e}\"}\n",
    "        ollama_time = 0\n",
    "    \n",
    "    # Reference-based í‰ê°€\n",
    "    try:\n",
    "        reference_evaluator = load_evaluator(\n",
    "            \"labeled_pairwise_string\",\n",
    "            criteria=comprehensive_criteria,\n",
    "            prompt=custom_evaluation_prompt.partial(ollama_model=ollama_model_name),\n",
    "            llm=evaluator_model,\n",
    "            callbacks=[langfuse_handler],\n",
    "        )\n",
    "        \n",
    "        ref_eval = reference_evaluator.evaluate_string_pairs(\n",
    "            prediction=gpt_response[\"answer\"],\n",
    "            prediction_b=ollama_response[\"answer\"],\n",
    "            input=question,\n",
    "            reference=reference\n",
    "        )\n",
    "        \n",
    "        ref_winner = \"GPT-4o-mini\" if ref_eval['value'] == 'A' else f\"Ollama-{ollama_model_name}\"\n",
    "        print(f\"ğŸ† Reference-based ìŠ¹ì: {ref_winner}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Reference-based í‰ê°€ ì˜¤ë¥˜: {e}\")\n",
    "        ref_eval = {\"value\": \"C\", \"score\": 0.5, \"reasoning\": f\"í‰ê°€ ì˜¤ë¥˜: {e}\"}\n",
    "        ref_winner = \"í‰ê°€ì‹¤íŒ¨\"\n",
    "    \n",
    "    # Reference-free í‰ê°€\n",
    "    try:\n",
    "        free_evaluator = load_evaluator(\n",
    "            \"pairwise_string\",\n",
    "            llm=evaluator_model,\n",
    "            callbacks=[langfuse_handler],\n",
    "        )\n",
    "        \n",
    "        free_eval = free_evaluator.evaluate_string_pairs(\n",
    "            prediction=gpt_response[\"answer\"],\n",
    "            prediction_b=ollama_response[\"answer\"],\n",
    "            input=question\n",
    "        )\n",
    "        \n",
    "        free_winner = \"GPT-4o-mini\" if free_eval['value'] == 'A' else f\"Ollama-{ollama_model_name}\"\n",
    "        print(f\"ğŸ¯ Reference-free ìŠ¹ì: {free_winner}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Reference-free í‰ê°€ ì˜¤ë¥˜: {e}\")\n",
    "        free_eval = {\"value\": \"C\", \"score\": 0.5, \"reasoning\": f\"í‰ê°€ ì˜¤ë¥˜: {e}\"}\n",
    "        free_winner = \"í‰ê°€ì‹¤íŒ¨\"\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    results.append({\n",
    "        \"sample_idx\": sample_idx,\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"gpt_answer\": gpt_response[\"answer\"],\n",
    "        \"ollama_answer\": ollama_response[\"answer\"],\n",
    "        \"gpt_time\": gpt_time,\n",
    "        \"ollama_time\": ollama_time,\n",
    "        \"ref_winner\": ref_winner,\n",
    "        \"free_winner\": free_winner,\n",
    "        \"ref_eval\": ref_eval,\n",
    "        \"free_eval\": free_eval\n",
    "    })\n",
    "\n",
    "# 6) ì¢…í•© ê²°ê³¼ ë¶„ì„\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"ğŸ“Š ì¢…í•© A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# ìŠ¹ë¥  ê³„ì‚°\n",
    "gpt_ref_wins = sum(1 for r in results if r[\"ref_winner\"] == \"GPT-4o-mini\")\n",
    "ollama_ref_wins = sum(1 for r in results if f\"Ollama-{ollama_model_name}\" in r[\"ref_winner\"])\n",
    "\n",
    "gpt_free_wins = sum(1 for r in results if r[\"free_winner\"] == \"GPT-4o-mini\")\n",
    "ollama_free_wins = sum(1 for r in results if f\"Ollama-{ollama_model_name}\" in r[\"free_winner\"])\n",
    "\n",
    "total_tests = len(results)\n",
    "\n",
    "print(f\"ğŸ† Reference-based í‰ê°€ ê²°ê³¼:\")\n",
    "print(f\"   GPT-4o-mini ìŠ¹ë¦¬: {gpt_ref_wins}/{total_tests} ({gpt_ref_wins/total_tests*100:.1f}%)\")\n",
    "print(f\"   Ollama-{ollama_model_name} ìŠ¹ë¦¬: {ollama_ref_wins}/{total_tests} ({ollama_ref_wins/total_tests*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Reference-free í‰ê°€ ê²°ê³¼:\")\n",
    "print(f\"   GPT-4o-mini ìŠ¹ë¦¬: {gpt_free_wins}/{total_tests} ({gpt_free_wins/total_tests*100:.1f}%)\")\n",
    "print(f\"   Ollama-{ollama_model_name} ìŠ¹ë¦¬: {ollama_free_wins}/{total_tests} ({ollama_free_wins/total_tests*100:.1f}%)\")\n",
    "\n",
    "# í‰ê·  ì‘ë‹µ ì‹œê°„\n",
    "avg_gpt_time = sum(r[\"gpt_time\"] for r in results) / total_tests\n",
    "avg_ollama_time = sum(r[\"ollama_time\"] for r in results if r[\"ollama_time\"] > 0)\n",
    "if avg_ollama_time:\n",
    "    avg_ollama_time = avg_ollama_time / sum(1 for r in results if r[\"ollama_time\"] > 0)\n",
    "\n",
    "print(f\"\\nâ±ï¸ í‰ê·  ì‘ë‹µ ì‹œê°„:\")\n",
    "print(f\"   GPT-4o-mini: {avg_gpt_time:.2f}ì´ˆ\")\n",
    "print(f\"   Ollama-{ollama_model_name}: {avg_ollama_time:.2f}ì´ˆ\" if avg_ollama_time else \"   Ollama: ì¸¡ì • ë¶ˆê°€\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Langfuse UIì—ì„œ ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!\")\n",
    "print(f\"ğŸ”— Langfuse: {os.getenv('LANGFUSE_HOST')}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ìš”ì•½:\")\n",
    "print(f\"- í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {total_tests}ê°œ (ì „ì²´ {len(df_qa_test)}ê°œ ì¤‘)\")\n",
    "print(f\"- ë¹„êµ ëª¨ë¸: GPT-4o-mini vs Ollama-{ollama_model_name}\")\n",
    "print(f\"- í‰ê°€ì: GPT-4\")\n",
    "print(f\"- í‰ê°€ ë°©ì‹: Reference-based + Reference-free\")\n",
    "print(f\"- í‰ê°€ ê¸°ì¤€: ê°„ê²°ì„±, ìœ ìš©ì„±, ìœ í•´ì„±\")\n",
    "print(f\"- ê²€ìƒ‰ê¸°: Hybrid (BM25 + Vector)\")\n",
    "\n",
    "# ê°œë³„ ê²°ê³¼ ìƒì„¸ ì¶œë ¥ (ì˜µì…˜)\n",
    "print(f\"\\nğŸ“‹ ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"{i+1}. ìƒ˜í”Œ {result['sample_idx']}: Ref={result['ref_winner'][:10]}, Free={result['free_winner'][:10]}\")\n",
    "\n",
    "print(f\"\\nâœ… ì¢…í•© RAG ì„±ëŠ¥ A/B í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65t8f69vc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PromptTemplate imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Add the missing import to fix the NameError\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "print(\"âœ… PromptTemplate imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "p1dv13vo8q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fixed evaluator with gemini-2.5-flash model\n"
     ]
    }
   ],
   "source": [
    "# Fix the first instance in the A/B test evaluation cell\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Fixed evaluator - using gemini-2.5-flash instead of gemini-1.5-flash\n",
    "evaluator = load_evaluator(\n",
    "    \"labeled_pairwise_string\",\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    ")\n",
    "\n",
    "print(\"âœ… Fixed evaluator with gemini-2.5-flash model\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "modu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
