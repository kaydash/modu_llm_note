{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be97462",
   "metadata": {},
   "source": [
    "#  RAG 체인 구성\n",
    "- Naïve RAG 구현\n",
    "\n",
    "### **학습 목표:**  RAG 기반의 질의응답 시스템을 구현할 수 있다\n",
    "\n",
    "### **실습 자료**: \n",
    "\n",
    "- data/transformer.pdf\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641ec21",
   "metadata": {},
   "source": [
    "# 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be5eb8",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5989430",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa02fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ad46a",
   "metadata": {},
   "source": [
    "`(3) 문서 로드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e634849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 로더 초기화\n",
    "pdf_loader = PyPDFLoader('./data/transformer.pdf')\n",
    "\n",
    "# 동기 로딩\n",
    "pdf_docs = pdf_loader.load()\n",
    "print(f'PDF 문서 개수: {len(pdf_docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1c434",
   "metadata": {},
   "source": [
    "`(4) 텍스트 분할`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Hugging Face의 임베딩 모델 생성\n",
    "embeddings_huggingface = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# 토크나이저 직접 접근\n",
    "tokenizer = embeddings_huggingface._client.tokenizer\n",
    "\n",
    "# 토크나이저를 사용한 예시\n",
    "text = \"테스트 텍스트입니다.\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)\n",
    "\n",
    "# 토크나이저 설정 확인\n",
    "print(tokenizer.model_max_length)  # 최대 토큰 길이\n",
    "print(tokenizer.vocab_size)        # 어휘 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03df2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 수를 계산하는 함수\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer(text)['input_ids'])\n",
    "\n",
    "# 토큰 수 계산\n",
    "text = \"테스트 텍스트입니다.\"\n",
    "print(count_tokens(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad431bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 텍스트 분할기 생성\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,                      \n",
    "    chunk_overlap=100,           \n",
    "    length_function=count_tokens,         # 토큰 수를 기준으로 분할\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],   # 구분자 - 재귀적으로 순차적으로 적용 \n",
    ")\n",
    "\n",
    "# 텍스트 분할\n",
    "chunks = text_splitter.split_documents(pdf_docs)\n",
    "print(f\"생성된 텍스트 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "print(f\"각 청크의 토큰 수: {list(count_tokens(chunk.page_content) for chunk in chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769db6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청크의 텍스트 확인\n",
    "print(chunks[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1114cf",
   "metadata": {},
   "source": [
    "# 벡터 저장소 기반 RAG 검색기 (Retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e55d88",
   "metadata": {},
   "source": [
    "`(1) 벡터 저장소 초기화`\n",
    "- chroma 사용\n",
    "- cosine distance 기준으로 인덱싱 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783bdbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Chroma 벡터 저장소 생성하기\n",
    "chroma_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_huggingface,    # huggingface 임베딩 사용\n",
    "    collection_name=\"db_transformer\",    # 컬렉션 이름\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_metadata = {'hnsw:space': 'cosine'}, # l2, ip, cosine 중에서 선택 \n",
    ")\n",
    "\n",
    "# 현재 저장된 컬렉션 데이터 확인\n",
    "chroma_db.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd1b80",
   "metadata": {},
   "source": [
    "`(2) Top K`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978859b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_k_retriever = chroma_db.as_retriever(\n",
    "    search_kwargs={\"k\": 2},\n",
    ")\n",
    "\n",
    "query = \"대표적인 시퀀스 모델은 어떤 것들이 있나요?\"\n",
    "retrieved_docs = chroma_k_retriever.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"-{i}-\\n{doc.page_content[:100]}...{doc.page_content[-100:]} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ee428",
   "metadata": {},
   "source": [
    "`(3) 임계값 지정`\n",
    "- Similarity score threshold (기준 스코어 이상인 문서를 대상으로 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8987f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utils.math import cosine_similarity\n",
    "\n",
    "chroma_threshold_retriever = chroma_db.as_retriever(\n",
    "    search_type='similarity_score_threshold',       # cosine 유사도\n",
    "    search_kwargs={'score_threshold': 0.5, 'k':2},  # 0.5 이상인 문서를 추출\n",
    ")\n",
    "\n",
    "query = \"대표적인 시퀀스 모델은 어떤 것들이 있나요?\"\n",
    "retrieved_docs = chroma_threshold_retriever.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    score = cosine_similarity(\n",
    "        [embeddings_huggingface.embed_query(query)], \n",
    "        [embeddings_huggingface.embed_query(doc.page_content)]\n",
    "        )[0][0]\n",
    "    print(f\"-{i}-\\n{doc.page_content[:100]}...{doc.page_content[-100:]} [유사도: {score}]\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d4655",
   "metadata": {},
   "source": [
    "`(4) MMR(Maximal Marginal Relevance) 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ccc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR - 다양성 고려 (lambda_mult 작을수록 더 다양하게 추출)\n",
    "chroma_mmr = chroma_db.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={\n",
    "        'k': 3,                 # 검색할 문서의 수\n",
    "        'fetch_k': 8,           # mmr 알고리즘에 전달할 문서의 수 (fetch_k > k)\n",
    "        'lambda_mult': 0.5,     # 다양성을 고려하는 정도 (1은 최소 다양성, 0은 최대 다양성을 의미. 기본값은 0.5)\n",
    "        },\n",
    ")\n",
    "\n",
    "\n",
    "query = \"대표적인 시퀀스 모델은 어떤 것들이 있나요?\"\n",
    "retrieved_docs = chroma_mmr.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    score = cosine_similarity(\n",
    "        [embeddings_huggingface.embed_query(query)], \n",
    "        [embeddings_huggingface.embed_query(doc.page_content)]\n",
    "        )[0][0]\n",
    "    print(f\"-{i}-\\n{doc.page_content[:100]}...{doc.page_content[-100:]} [유사도: {score}]\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc506b09",
   "metadata": {},
   "source": [
    "`(5) metadata 필터링 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메타데이터 확인\n",
    "chunks[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994408c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 객체의 metadata를 이용한 필터링\n",
    "chrom_metadata = chroma_db.as_retriever(\n",
    "    search_kwargs={\n",
    "        'filter': {'source': './data/transformer.pdf'},\n",
    "        'k': 5, \n",
    "        }\n",
    ")\n",
    "\n",
    "query = \"대표적인 시퀀스 모델은 어떤 것들이 있나요?\"\n",
    "retrieved_docs = chrom_metadata.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"-{i}-\\n{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275ac2c",
   "metadata": {},
   "source": [
    "`(6) page_content 본문 필터링 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d282211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_content를 이용한 필터링\n",
    "chroma_content = chroma_db.as_retriever(\n",
    "    search_kwargs={\n",
    "        'k': 2,\n",
    "        'where_document': {'$contains': 'recurrent'},\n",
    "        }\n",
    ")\n",
    "\n",
    "query = \"대표적인 시퀀스 모델은 어떤 것들이 있나요?\"\n",
    "retrieved_docs = chroma_content.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"-{i}-\\n{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb7b5f",
   "metadata": {},
   "source": [
    "# [실습 프로젝트] Naive RAG 구현 \n",
    "\n",
    "- 각 단계별 지시사항에 따라 코드를 완성하세요. \n",
    "- 제시된 지시사항과 LangChain 문서를 참조하여 시스템을 구성합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af65b2f",
   "metadata": {},
   "source": [
    "`(1) 벡터 저장소 설정`\n",
    "- HuggingFace에서 지원하는 BAAI/bge-m3 임베딩 모델을 사용하여 문서를 벡터화\n",
    "- FAISS DB를 벡터 스토어로 사용 (IndexFlatL2 사용: 유클리드 거리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings  \n",
    "\n",
    "# Hugging Face의 임베딩 모델 생성\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# 임베딩 차원 확인\n",
    "embedding = embeddings_model.embed_query(\"test\")\n",
    "print(f\"임베딩 차원: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama 임베딩 모델을 사용한 FAISS 벡터 저장소 생성\n",
    "import faiss \n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# FAISS 인덱스 초기화 (유클리드 거리 사용)\n",
    "dim = 1024  # 임베딩 차원\n",
    "faiss_index = faiss.IndexFlatL2(dim)  \n",
    "\n",
    "# FAISS 벡터 저장소 생성\n",
    "faiss_db = FAISS(\n",
    "    embedding_function=embeddings_model,\n",
    "    index=faiss_index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# 저장된 문서의 갯수 확인\n",
    "print(faiss_db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# 문서 id 생성\n",
    "doc_ids = [str(uuid.uuid4()) for _ in range(len(chunks))]\n",
    "\n",
    "# 문서를 벡터 저장소에 저장\n",
    "added_doc_ids = faiss_db.add_documents(chunks, ids=doc_ids)\n",
    "\n",
    "# 벡터 저장소에 저장된 문서를 확인\n",
    "print(f\"{len(added_doc_ids)}개의 문서가 성공적으로 벡터 저장소에 추가되었습니다.\")\n",
    "print(added_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be466ac",
   "metadata": {},
   "source": [
    "`(2) 검색기 정의`\n",
    "- mmr 검색으로 상위 3개 문서 검색하는 Retriever 사용\n",
    "- 다양성을 높이는 설정을 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03949ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmr 검색기 생성\n",
    "faiss_mmr_retriever = faiss_db.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={\n",
    "        'k': 3,                 # 검색할 문서의 수\n",
    "        'fetch_k': 8,           # mmr 알고리즘에 전달할 문서의 수 (fetch_k > k)\n",
    "        'lambda_mult': 0.3,     # 다양성을 높이는 설정 (0에 가까울수록 더 다양함)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 테스트 \n",
    "query = \"대표적인 시퀀스 모델은 어떤 것들이 있나요?\"\n",
    "retrieved_docs = faiss_mmr_retriever.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"검색 결과:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"-{i}-\\n{doc.page_content[:100]}...{doc.page_content[-100:]}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdac8f9",
   "metadata": {},
   "source": [
    "`(3) RAG 프롬프트 구성`\n",
    "\n",
    "- 작성 기준: \n",
    "    - LangChain의 ChatPromptTemplate 클래스 사용\n",
    "    - 변수 처리는 {context}, {question} 형식 사용\n",
    "    - 답변은 한글로 출력되도록 프롬프트 작성\n",
    "    \n",
    "- 아래 템플릿 코드를 기반으로 다음 내용을 참고하여 작성합니다. \n",
    "\n",
    "    1. 프롬프트 구성요소:\n",
    "        - 작업 지침\n",
    "        - 컨텍스트 영역\n",
    "        - 질문 영역\n",
    "        - 답변 형식 가이드\n",
    "\n",
    "    2. 작업 지침:\n",
    "        - 컨텍스트 기반 답변 원칙\n",
    "        - 외부 지식 사용 제한\n",
    "        - 불확실성 처리 방법\n",
    "        - 답변 불가능한 경우의 처리 방법\n",
    "\n",
    "    3. 답변 형식:\n",
    "        - 핵심 답변 섹션\n",
    "        - 근거 제시 섹션\n",
    "        - 추가 설명 섹션 (필요시)\n",
    "\n",
    "    4. 제약사항 반영:\n",
    "        - 답변은 사실에 기반해야 함\n",
    "        - 추측이나 가정을 최소화해야 함\n",
    "        - 명확한 근거 제시가 필요함\n",
    "        - 구조화된 형태로 작성되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fb87d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 템플릿 (예시)\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question] \n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730bcf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 템플릿 (여기에 작성하세요)\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"다음 컨텍스트를 기반으로 질문에 답하세요. 컨텍스트에서 찾을 수 없는 내용은 추측하지 말고, 모르겠다고 답하세요.\n",
    "\n",
    "[작업 지침]\n",
    "- 주어진 컨텍스트의 정보만을 사용하여 답변하세요\n",
    "- 외부 지식을 사용하지 마세요\n",
    "- 불확실한 정보는 추측하지 말고 명확히 표시하세요\n",
    "- 답변할 수 없다면 솔직히 말하세요\n",
    "\n",
    "[컨텍스트]\n",
    "{context}\n",
    "\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "[답변]\n",
    "위 컨텍스트를 바탕으로 답변드리겠습니다:\n",
    "\n",
    "**핵심 답변:**\n",
    "\n",
    "**근거:**\n",
    "\n",
    "**추가 설명 (해당되는 경우):**\n",
    "\n",
    "답변은 한국어로 작성하며, 사실에 기반하여 명확하게 제시하겠습니다.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 템플릿 출력\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da7e3f8",
   "metadata": {},
   "source": [
    "`(4) RAG 체인 구성`\n",
    "- LangChain의 LCEL 문법을 사용\n",
    "- 검색 결과를 프롬프트의 'context'로 전달하고,\n",
    "- 사용자가 입력한 질문을 그래도 프롬프트의 'question'에 전달\n",
    "- LLM 설정:\n",
    "    - ChatOpenAI 사용 ('gpt-4.1-mini' 모델)\n",
    "    - temperature: 답변의 일관성을 가져가는 설정값을 사용 \n",
    "    - 기타 필요한 설정 \n",
    "- 출력 파서: 문자열 부분만 출력되도록 구성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d736e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 설정\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\", \n",
    "    temperature=0  # 일관성을 위한 설정\n",
    ")\n",
    "\n",
    "# 문서 포맷팅\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"{doc.page_content}\" for doc in docs])\n",
    "\n",
    "# RAG 체인 생성\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": faiss_mmr_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 체인 실행\n",
    "query = \"대표적인 시퀀스 모델은 어떤 것들이 있나요?\"\n",
    "output = rag_chain.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(\"답변:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe159c82",
   "metadata": {},
   "source": [
    "`(5) Gradio 스트리밍 구현`\n",
    "- ChatInterface 사용\n",
    "- `chain.stream()`으로 응답을 청크 단위로 스트리밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import Iterator\n",
    "\n",
    "# 스트리밍 응답 생성 함수\n",
    "def get_streaming_response(message: str, history) -> Iterator[str]:\n",
    "    \n",
    "    # RAG Chain 실행 및 스트리밍 응답 생성\n",
    "    response = \"\"\n",
    "    for chunk in rag_chain.stream(message):\n",
    "        if isinstance(chunk, str):\n",
    "            response += chunk\n",
    "            yield response\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=get_streaming_response,\n",
    "    title=\"RAG 기반 질의응답 시스템\",\n",
    "    description=\"Transformer PDF 문서를 기반으로 질문에 답변합니다.\",\n",
    "    examples=[\n",
    "        \"대표적인 시퀀스 모델은 어떤 것들이 있나요?\",\n",
    "        \"Transformer의 주요 특징은 무엇인가요?\", \n",
    "        \"어텐션 메커니즘이란 무엇인가요?\",\n",
    "        \"인코더-디코더 구조에 대해 설명해주세요.\"\n",
    "    ],\n",
    "    cache_examples=False\n",
    ")\n",
    "\n",
    "# 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo 실행 종료\n",
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
